{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting started \u00b6 What is vectorbt? \u00b6 vectorbt is a Python package for quantitative analysis that takes a novel approach to backtesting: it operates entirely on pandas and NumPy objects, and is accelerated by Numba to analyze any data at speed and scale. This allows for testing of many thousands of strategies in seconds . In contrast to other backtesters, vectorbt represents complex data as (structured) NumPy arrays. This enables superfast computation using vectorized operations with NumPy and non-vectorized but dynamically compiled operations with Numba. It also integrates Plotly and Jupyter Widgets to display complex charts and dashboards akin to Tableau right in the Jupyter notebook. Due to high performance, vectorbt can process large amounts of data even without GPU and parallelization and enables the user to interact with data-hungry widgets without significant delays. With vectorbt, you can Backtest strategies in a couple of lines of Python code Enjoy the best of both worlds: the ecosystem of Python and the speed of C Retain full control over execution and your data (as opposed to web-based services such as TradingView) Optimize your trading strategy against many parameters, assets, and periods in one go Uncover hidden patterns in financial markets Analyze time series and engineer new features for ML models Supercharge pandas and your favorite tools to run much faster Visualize strategy performance using interactive charts and dashboards (both in Jupyter and browser) Fetch and process data periodically, send Telegram notifications, and more Why vectorbt? \u00b6 While there are many great backtesting packages for Python, vectorbt combines an extremely fast backtester and a data science tool: it excels at processing performance and offers interactive tools to explore complex phenomena in trading. With it, you can traverse a huge number of strategy configurations, time periods, and instruments in little time, to explore where your strategy performs best and to uncover hidden patterns in data. Accessing and analyzing this information for yourself could give you an information advantage in your own trading. How it works \u00b6 vectorbt was implemented to address common performance shortcomings of backtesting libraries. It builds upon the idea that each instance of a trading strategy can be represented in a vectorized form, so multiple strategy instances can be packed into a single multi-dimensional array, processed in a highly efficient manner, and compared easily. It overhauls the traditional OOP approach that represents strategies as classes and other data structures, which are easier to write and extend compared to vectors, but harder to analyze and also require additional effort to do it quickly. Thanks to the time-series nature of trading data, most of the aspects related to backtesting can be translated into vectors. Instead of processing one element at a time, vectorization allows us to avoid naive looping and perform the same operation on all elements at the same time. The path-dependency problem related to vectorization is solved by using Numba - it allows both writing iterative code and compiling slow Python loops to be run at the native machine code speed. Example \u00b6 Let's say we have a complex strategy that has lots of (hyper-)parameters that have to be tuned. While brute-forcing all combinations seems to be a rather unrealistic attempt, we can still interpolate, and vectorbt makes exactly this possible. It doesn't care whether we have one strategy instance or millions. As soon as their vectors can be concatenated into a matrix and we have enough memory, we can analyze them in one go. Let's start with fetching the daily price of Bitcoin: >>> import numpy as np >>> import pandas as pd >>> from datetime import datetime >>> import vectorbt as vbt >>> # Prepare data >>> start = '2019-01-01 UTC' # crypto is in UTC >>> end = '2020-01-01 UTC' >>> btc_price = vbt . YFData . download ( 'BTC-USD' , start = start , end = end ) . get ( 'Close' ) >>> btc_price Date 2019-01-01 00:00:00+00:00 3843.520020 2019-01-02 00:00:00+00:00 3943.409424 2019-01-03 00:00:00+00:00 3836.741211 ... ... 2019-12-30 00:00:00+00:00 7292.995117 2019-12-31 00:00:00+00:00 7193.599121 2020-01-01 00:00:00+00:00 7200.174316 Freq: D, Name: Close, Length: 366, dtype: float64 We are going to test a simple Dual Moving Average Crossover (DMAC) strategy. For this, we are going to use MA class for calculating moving averages and generating signals. Our first test is rather simple: buy when the 10-day moving average crosses above the 20-day moving average, and sell when opposite. >>> fast_ma = vbt . MA . run ( btc_price , 10 , short_name = 'fast' ) >>> slow_ma = vbt . MA . run ( btc_price , 20 , short_name = 'slow' ) >>> entries = fast_ma . ma_crossed_above ( slow_ma ) >>> entries Date 2019-01-01 00:00:00+00:00 False 2019-01-02 00:00:00+00:00 False 2019-01-03 00:00:00+00:00 False ... ... 2019-12-30 00:00:00+00:00 False 2019-12-31 00:00:00+00:00 False 2020-01-01 00:00:00+00:00 False Freq: D, Length: 366, dtype: bool >>> exits = fast_ma . ma_crossed_below ( slow_ma ) >>> exits Date 2019-01-01 00:00:00+00:00 False 2019-01-02 00:00:00+00:00 False 2019-01-03 00:00:00+00:00 False ... ... 2019-12-30 00:00:00+00:00 False 2019-12-31 00:00:00+00:00 False 2020-01-01 00:00:00+00:00 False Freq: D, Length: 366, dtype: bool >>> pf = vbt . Portfolio . from_signals ( btc_price , entries , exits ) >>> pf . total_return () 0.636680693047752 One strategy instance of DMAC produced one column in signals and one performance value. Adding one more strategy instance is as simple as adding one more column. Here we are passing an array of window sizes instead of a single value. For each window size in this array, it computes a moving average over the entire price series and stores it in a distinct column. >>> # Multiple strategy instances: (10, 30) and (20, 30) >>> fast_ma = vbt . MA . run ( btc_price , [ 10 , 20 ], short_name = 'fast' ) >>> slow_ma = vbt . MA . run ( btc_price , [ 30 , 30 ], short_name = 'slow' ) >>> entries = fast_ma . ma_crossed_above ( slow_ma ) >>> entries fast_window 10 20 slow_window 30 30 Date 2019-01-01 00:00:00+00:00 False False 2019-01-02 00:00:00+00:00 False False 2019-01-03 00:00:00+00:00 False False ... ... ... 2019-12-30 00:00:00+00:00 False False 2019-12-31 00:00:00+00:00 False False 2020-01-01 00:00:00+00:00 False False [366 rows x 2 columns] >>> exits = fast_ma . ma_crossed_below ( slow_ma ) >>> exits fast_window 10 20 slow_window 30 30 Date 2019-01-01 00:00:00+00:00 False False 2019-01-02 00:00:00+00:00 False False 2019-01-03 00:00:00+00:00 False False ... ... ... 2019-12-30 00:00:00+00:00 False False 2019-12-31 00:00:00+00:00 False False 2020-01-01 00:00:00+00:00 False False [366 rows x 2 columns] >>> pf = vbt . Portfolio . from_signals ( btc_price , entries , exits ) >>> pf . total_return () fast_window slow_window 10 30 0.848840 20 30 0.543411 Name: total_return, dtype: float64 For the sake of convenience, vectorbt has created the column levels fast_window and slow_window for us to easily distinguish which window size corresponds to which column. Notice how signal generation part remains the same for each example - most functions in vectorbt work on time series of any shape. This allows creation of analysis pipelines that are universal to input data. The representation of different features as columns offers endless possibilities for backtesting. We could, for example, go a step further and conduct the same tests for Ethereum. To compare both instruments, combine price series for Bitcoin and Ethereum into one DataFrame and run the same backtesting pipeline. >>> # Multiple strategy instances and instruments >>> eth_price = vbt . YFData . download ( 'ETH-USD' , start = start , end = end ) . get ( 'Close' ) >>> comb_price = btc_price . vbt . concat ( eth_price , ... keys = pd . Index ([ 'BTC' , 'ETH' ], name = 'symbol' )) >>> comb_price . vbt . drop_levels ( - 1 , inplace = True ) >>> comb_price symbol BTC ETH Date 2019-01-01 00:00:00+00:00 3843.520020 140.819412 2019-01-02 00:00:00+00:00 3943.409424 155.047684 2019-01-03 00:00:00+00:00 3836.741211 149.135010 ... ... ... 2019-12-30 00:00:00+00:00 7292.995117 132.633484 2019-12-31 00:00:00+00:00 7193.599121 129.610855 2020-01-01 00:00:00+00:00 7200.174316 130.802002 [366 rows x 2 columns] >>> fast_ma = vbt . MA . run ( comb_price , [ 10 , 20 ], short_name = 'fast' ) >>> slow_ma = vbt . MA . run ( comb_price , [ 30 , 30 ], short_name = 'slow' ) >>> entries = fast_ma . ma_crossed_above ( slow_ma ) >>> entries fast_window 10 20 slow_window 30 30 symbol BTC ETH BTC ETH Date 2019-01-01 00:00:00+00:00 False False False False 2019-01-02 00:00:00+00:00 False False False False 2019-01-03 00:00:00+00:00 False False False False ... ... ... ... ... 2019-12-30 00:00:00+00:00 False False False False 2019-12-31 00:00:00+00:00 False False False False 2020-01-01 00:00:00+00:00 False False False False [366 rows x 4 columns] >>> exits = fast_ma . ma_crossed_below ( slow_ma ) >>> exits fast_window 10 20 slow_window 30 30 symbol BTC ETH BTC ETH Date 2019-01-01 00:00:00+00:00 False False False False 2019-01-02 00:00:00+00:00 False False False False 2019-01-03 00:00:00+00:00 False False False False ... ... ... ... ... 2019-12-30 00:00:00+00:00 False False False False 2019-12-31 00:00:00+00:00 False False False False 2020-01-01 00:00:00+00:00 False False False False [366 rows x 4 columns] >>> pf = vbt . Portfolio . from_signals ( comb_price , entries , exits ) >>> pf . total_return () fast_window slow_window symbol 10 30 BTC 0.848840 ETH 0.244204 20 30 BTC 0.543411 ETH -0.319102 Name: total_return, dtype: float64 >>> mean_return = pf . total_return () . groupby ( 'symbol' ) . mean () >>> mean_return . vbt . barplot ( xaxis_title = 'Symbol' , yaxis_title = 'Mean total return' ) Not only strategies and instruments can act as separate features, but also time. If we want to find out when our strategy performs best, it's reasonable to backtest over multiple time periods. vectorbt allows us to split one time period into many, given they have the same length and frequency, and represent them as distinct columns. For example, let's split the whole time period into two equal time periods and backest them at once. >>> # Multiple strategy instances, instruments, and time periods >>> mult_comb_price , _ = comb_price . vbt . range_split ( n = 2 ) >>> mult_comb_price split_idx 0 1 symbol BTC ETH BTC ETH 0 3843.520020 140.819412 11961.269531 303.099976 1 3943.409424 155.047684 11215.437500 284.523224 2 3836.741211 149.135010 10978.459961 287.997528 ... ... ... ... ... 180 10817.155273 290.695984 7292.995117 132.633484 181 10583.134766 293.641113 7193.599121 129.610855 182 10801.677734 291.596436 7200.174316 130.802002 [183 rows x 4 columns] >>> fast_ma = vbt . MA . run ( mult_comb_price , [ 10 , 20 ], short_name = 'fast' ) >>> slow_ma = vbt . MA . run ( mult_comb_price , [ 30 , 30 ], short_name = 'slow' ) >>> entries = fast_ma . ma_crossed_above ( slow_ma ) >>> exits = fast_ma . ma_crossed_below ( slow_ma ) >>> pf = vbt . Portfolio . from_signals ( mult_comb_price , entries , exits , freq = '1D' ) >>> pf . total_return () fast_window slow_window split_idx symbol 10 30 0 BTC 1.632259 ETH 0.946786 1 BTC -0.288720 ETH -0.308387 20 30 0 BTC 1.721449 ETH 0.343274 1 BTC -0.418280 ETH -0.257947 Name: total_return, dtype: float64 Notice how index is no more datetime-like, since it captures multiple time periods. That's why it's required here to pass the frequency freq to the Portfolio class in order to be able to compute performance metrics such as the Sharpe ratio. The index hierarchy of the final performance series can be then used to group the performance by any feature, such as window pair, symbol, and time period. >>> mean_return = pf . total_return () . groupby ([ 'split_idx' , 'symbol' ]) . mean () >>> mean_return . unstack ( level =- 1 ) . vbt . barplot ( ... xaxis_title = 'Split index' , ... yaxis_title = 'Mean total return' , ... legend_title_text = 'Symbol' ) There is much more to backtesting than simply stacking columns: vectorbt offers functions for most parts of a backtesting pipeline - from building indicators and generating signals, to modeling portfolio performance and visualizing results. Disclaimer \u00b6 This software is for educational purposes only. Do not risk money which you are afraid to lose. USE THE SOFTWARE AT YOUR OWN RISK. THE AUTHORS AND ALL AFFILIATES ASSUME NO RESPONSIBILITY FOR YOUR TRADING RESULTS.","title":"Getting started"},{"location":"#getting-started","text":"","title":"Getting started"},{"location":"#what-is-vectorbt","text":"vectorbt is a Python package for quantitative analysis that takes a novel approach to backtesting: it operates entirely on pandas and NumPy objects, and is accelerated by Numba to analyze any data at speed and scale. This allows for testing of many thousands of strategies in seconds . In contrast to other backtesters, vectorbt represents complex data as (structured) NumPy arrays. This enables superfast computation using vectorized operations with NumPy and non-vectorized but dynamically compiled operations with Numba. It also integrates Plotly and Jupyter Widgets to display complex charts and dashboards akin to Tableau right in the Jupyter notebook. Due to high performance, vectorbt can process large amounts of data even without GPU and parallelization and enables the user to interact with data-hungry widgets without significant delays. With vectorbt, you can Backtest strategies in a couple of lines of Python code Enjoy the best of both worlds: the ecosystem of Python and the speed of C Retain full control over execution and your data (as opposed to web-based services such as TradingView) Optimize your trading strategy against many parameters, assets, and periods in one go Uncover hidden patterns in financial markets Analyze time series and engineer new features for ML models Supercharge pandas and your favorite tools to run much faster Visualize strategy performance using interactive charts and dashboards (both in Jupyter and browser) Fetch and process data periodically, send Telegram notifications, and more","title":"What is vectorbt?"},{"location":"#why-vectorbt","text":"While there are many great backtesting packages for Python, vectorbt combines an extremely fast backtester and a data science tool: it excels at processing performance and offers interactive tools to explore complex phenomena in trading. With it, you can traverse a huge number of strategy configurations, time periods, and instruments in little time, to explore where your strategy performs best and to uncover hidden patterns in data. Accessing and analyzing this information for yourself could give you an information advantage in your own trading.","title":"Why vectorbt?"},{"location":"#how-it-works","text":"vectorbt was implemented to address common performance shortcomings of backtesting libraries. It builds upon the idea that each instance of a trading strategy can be represented in a vectorized form, so multiple strategy instances can be packed into a single multi-dimensional array, processed in a highly efficient manner, and compared easily. It overhauls the traditional OOP approach that represents strategies as classes and other data structures, which are easier to write and extend compared to vectors, but harder to analyze and also require additional effort to do it quickly. Thanks to the time-series nature of trading data, most of the aspects related to backtesting can be translated into vectors. Instead of processing one element at a time, vectorization allows us to avoid naive looping and perform the same operation on all elements at the same time. The path-dependency problem related to vectorization is solved by using Numba - it allows both writing iterative code and compiling slow Python loops to be run at the native machine code speed.","title":"How it works"},{"location":"#example","text":"Let's say we have a complex strategy that has lots of (hyper-)parameters that have to be tuned. While brute-forcing all combinations seems to be a rather unrealistic attempt, we can still interpolate, and vectorbt makes exactly this possible. It doesn't care whether we have one strategy instance or millions. As soon as their vectors can be concatenated into a matrix and we have enough memory, we can analyze them in one go. Let's start with fetching the daily price of Bitcoin: >>> import numpy as np >>> import pandas as pd >>> from datetime import datetime >>> import vectorbt as vbt >>> # Prepare data >>> start = '2019-01-01 UTC' # crypto is in UTC >>> end = '2020-01-01 UTC' >>> btc_price = vbt . YFData . download ( 'BTC-USD' , start = start , end = end ) . get ( 'Close' ) >>> btc_price Date 2019-01-01 00:00:00+00:00 3843.520020 2019-01-02 00:00:00+00:00 3943.409424 2019-01-03 00:00:00+00:00 3836.741211 ... ... 2019-12-30 00:00:00+00:00 7292.995117 2019-12-31 00:00:00+00:00 7193.599121 2020-01-01 00:00:00+00:00 7200.174316 Freq: D, Name: Close, Length: 366, dtype: float64 We are going to test a simple Dual Moving Average Crossover (DMAC) strategy. For this, we are going to use MA class for calculating moving averages and generating signals. Our first test is rather simple: buy when the 10-day moving average crosses above the 20-day moving average, and sell when opposite. >>> fast_ma = vbt . MA . run ( btc_price , 10 , short_name = 'fast' ) >>> slow_ma = vbt . MA . run ( btc_price , 20 , short_name = 'slow' ) >>> entries = fast_ma . ma_crossed_above ( slow_ma ) >>> entries Date 2019-01-01 00:00:00+00:00 False 2019-01-02 00:00:00+00:00 False 2019-01-03 00:00:00+00:00 False ... ... 2019-12-30 00:00:00+00:00 False 2019-12-31 00:00:00+00:00 False 2020-01-01 00:00:00+00:00 False Freq: D, Length: 366, dtype: bool >>> exits = fast_ma . ma_crossed_below ( slow_ma ) >>> exits Date 2019-01-01 00:00:00+00:00 False 2019-01-02 00:00:00+00:00 False 2019-01-03 00:00:00+00:00 False ... ... 2019-12-30 00:00:00+00:00 False 2019-12-31 00:00:00+00:00 False 2020-01-01 00:00:00+00:00 False Freq: D, Length: 366, dtype: bool >>> pf = vbt . Portfolio . from_signals ( btc_price , entries , exits ) >>> pf . total_return () 0.636680693047752 One strategy instance of DMAC produced one column in signals and one performance value. Adding one more strategy instance is as simple as adding one more column. Here we are passing an array of window sizes instead of a single value. For each window size in this array, it computes a moving average over the entire price series and stores it in a distinct column. >>> # Multiple strategy instances: (10, 30) and (20, 30) >>> fast_ma = vbt . MA . run ( btc_price , [ 10 , 20 ], short_name = 'fast' ) >>> slow_ma = vbt . MA . run ( btc_price , [ 30 , 30 ], short_name = 'slow' ) >>> entries = fast_ma . ma_crossed_above ( slow_ma ) >>> entries fast_window 10 20 slow_window 30 30 Date 2019-01-01 00:00:00+00:00 False False 2019-01-02 00:00:00+00:00 False False 2019-01-03 00:00:00+00:00 False False ... ... ... 2019-12-30 00:00:00+00:00 False False 2019-12-31 00:00:00+00:00 False False 2020-01-01 00:00:00+00:00 False False [366 rows x 2 columns] >>> exits = fast_ma . ma_crossed_below ( slow_ma ) >>> exits fast_window 10 20 slow_window 30 30 Date 2019-01-01 00:00:00+00:00 False False 2019-01-02 00:00:00+00:00 False False 2019-01-03 00:00:00+00:00 False False ... ... ... 2019-12-30 00:00:00+00:00 False False 2019-12-31 00:00:00+00:00 False False 2020-01-01 00:00:00+00:00 False False [366 rows x 2 columns] >>> pf = vbt . Portfolio . from_signals ( btc_price , entries , exits ) >>> pf . total_return () fast_window slow_window 10 30 0.848840 20 30 0.543411 Name: total_return, dtype: float64 For the sake of convenience, vectorbt has created the column levels fast_window and slow_window for us to easily distinguish which window size corresponds to which column. Notice how signal generation part remains the same for each example - most functions in vectorbt work on time series of any shape. This allows creation of analysis pipelines that are universal to input data. The representation of different features as columns offers endless possibilities for backtesting. We could, for example, go a step further and conduct the same tests for Ethereum. To compare both instruments, combine price series for Bitcoin and Ethereum into one DataFrame and run the same backtesting pipeline. >>> # Multiple strategy instances and instruments >>> eth_price = vbt . YFData . download ( 'ETH-USD' , start = start , end = end ) . get ( 'Close' ) >>> comb_price = btc_price . vbt . concat ( eth_price , ... keys = pd . Index ([ 'BTC' , 'ETH' ], name = 'symbol' )) >>> comb_price . vbt . drop_levels ( - 1 , inplace = True ) >>> comb_price symbol BTC ETH Date 2019-01-01 00:00:00+00:00 3843.520020 140.819412 2019-01-02 00:00:00+00:00 3943.409424 155.047684 2019-01-03 00:00:00+00:00 3836.741211 149.135010 ... ... ... 2019-12-30 00:00:00+00:00 7292.995117 132.633484 2019-12-31 00:00:00+00:00 7193.599121 129.610855 2020-01-01 00:00:00+00:00 7200.174316 130.802002 [366 rows x 2 columns] >>> fast_ma = vbt . MA . run ( comb_price , [ 10 , 20 ], short_name = 'fast' ) >>> slow_ma = vbt . MA . run ( comb_price , [ 30 , 30 ], short_name = 'slow' ) >>> entries = fast_ma . ma_crossed_above ( slow_ma ) >>> entries fast_window 10 20 slow_window 30 30 symbol BTC ETH BTC ETH Date 2019-01-01 00:00:00+00:00 False False False False 2019-01-02 00:00:00+00:00 False False False False 2019-01-03 00:00:00+00:00 False False False False ... ... ... ... ... 2019-12-30 00:00:00+00:00 False False False False 2019-12-31 00:00:00+00:00 False False False False 2020-01-01 00:00:00+00:00 False False False False [366 rows x 4 columns] >>> exits = fast_ma . ma_crossed_below ( slow_ma ) >>> exits fast_window 10 20 slow_window 30 30 symbol BTC ETH BTC ETH Date 2019-01-01 00:00:00+00:00 False False False False 2019-01-02 00:00:00+00:00 False False False False 2019-01-03 00:00:00+00:00 False False False False ... ... ... ... ... 2019-12-30 00:00:00+00:00 False False False False 2019-12-31 00:00:00+00:00 False False False False 2020-01-01 00:00:00+00:00 False False False False [366 rows x 4 columns] >>> pf = vbt . Portfolio . from_signals ( comb_price , entries , exits ) >>> pf . total_return () fast_window slow_window symbol 10 30 BTC 0.848840 ETH 0.244204 20 30 BTC 0.543411 ETH -0.319102 Name: total_return, dtype: float64 >>> mean_return = pf . total_return () . groupby ( 'symbol' ) . mean () >>> mean_return . vbt . barplot ( xaxis_title = 'Symbol' , yaxis_title = 'Mean total return' ) Not only strategies and instruments can act as separate features, but also time. If we want to find out when our strategy performs best, it's reasonable to backtest over multiple time periods. vectorbt allows us to split one time period into many, given they have the same length and frequency, and represent them as distinct columns. For example, let's split the whole time period into two equal time periods and backest them at once. >>> # Multiple strategy instances, instruments, and time periods >>> mult_comb_price , _ = comb_price . vbt . range_split ( n = 2 ) >>> mult_comb_price split_idx 0 1 symbol BTC ETH BTC ETH 0 3843.520020 140.819412 11961.269531 303.099976 1 3943.409424 155.047684 11215.437500 284.523224 2 3836.741211 149.135010 10978.459961 287.997528 ... ... ... ... ... 180 10817.155273 290.695984 7292.995117 132.633484 181 10583.134766 293.641113 7193.599121 129.610855 182 10801.677734 291.596436 7200.174316 130.802002 [183 rows x 4 columns] >>> fast_ma = vbt . MA . run ( mult_comb_price , [ 10 , 20 ], short_name = 'fast' ) >>> slow_ma = vbt . MA . run ( mult_comb_price , [ 30 , 30 ], short_name = 'slow' ) >>> entries = fast_ma . ma_crossed_above ( slow_ma ) >>> exits = fast_ma . ma_crossed_below ( slow_ma ) >>> pf = vbt . Portfolio . from_signals ( mult_comb_price , entries , exits , freq = '1D' ) >>> pf . total_return () fast_window slow_window split_idx symbol 10 30 0 BTC 1.632259 ETH 0.946786 1 BTC -0.288720 ETH -0.308387 20 30 0 BTC 1.721449 ETH 0.343274 1 BTC -0.418280 ETH -0.257947 Name: total_return, dtype: float64 Notice how index is no more datetime-like, since it captures multiple time periods. That's why it's required here to pass the frequency freq to the Portfolio class in order to be able to compute performance metrics such as the Sharpe ratio. The index hierarchy of the final performance series can be then used to group the performance by any feature, such as window pair, symbol, and time period. >>> mean_return = pf . total_return () . groupby ([ 'split_idx' , 'symbol' ]) . mean () >>> mean_return . unstack ( level =- 1 ) . vbt . barplot ( ... xaxis_title = 'Split index' , ... yaxis_title = 'Mean total return' , ... legend_title_text = 'Symbol' ) There is much more to backtesting than simply stacking columns: vectorbt offers functions for most parts of a backtesting pipeline - from building indicators and generating signals, to modeling portfolio performance and visualizing results.","title":"Example"},{"location":"#disclaimer","text":"This software is for educational purposes only. Do not risk money which you are afraid to lose. USE THE SOFTWARE AT YOUR OWN RISK. THE AUTHORS AND ALL AFFILIATES ASSUME NO RESPONSIBILITY FOR YOUR TRADING RESULTS.","title":"Disclaimer"},{"location":"vectorbt-pro/","text":"vectorbt PRO \u00b6 Hi, I'm @polakowo , the author of the library. As you might have noticed, vectorbt is an incredibly complex library, which addresses slow backtesting by an innovative approach. Since there is nothing comparable in the Open Source, most of the tools were designed and developed from scratch. The codebase has grown to some mind-blowing 40k lines of code (excluding the documentation), which already matches some high-tier corporate projects. While vectorbt steadily gains in popularity, maintaining this beast on my own is getting more difficult and expensive than ever. But here's the deal. To make the development economically feasible, I'm launching a proprietary sister project vectorbt PRO that will become available on a subscription basis. The project should be seen as a successor to vectorbt - it's actively developed, will receive proper documentation, and there is a Discord server for discussions and announcements. If you want to be part of the new growing community and get access to cutting-edge software, complete a sponsorship on GitHub Sponsors . As always, happy coding!","title":"vectorbt PRO"},{"location":"vectorbt-pro/#vectorbt-pro","text":"Hi, I'm @polakowo , the author of the library. As you might have noticed, vectorbt is an incredibly complex library, which addresses slow backtesting by an innovative approach. Since there is nothing comparable in the Open Source, most of the tools were designed and developed from scratch. The codebase has grown to some mind-blowing 40k lines of code (excluding the documentation), which already matches some high-tier corporate projects. While vectorbt steadily gains in popularity, maintaining this beast on my own is getting more difficult and expensive than ever. But here's the deal. To make the development economically feasible, I'm launching a proprietary sister project vectorbt PRO that will become available on a subscription basis. The project should be seen as a successor to vectorbt - it's actively developed, will receive proper documentation, and there is a Discord server for discussions and announcements. If you want to be part of the new growing community and get access to cutting-edge software, complete a sponsorship on GitHub Sponsors . As always, happy coding!","title":"vectorbt PRO"},{"location":"api/","text":"vectorbt package \u00b6 Sub-packages \u00b6 vectorbt.base vectorbt.data vectorbt.generic vectorbt.indicators vectorbt.labels vectorbt.messaging vectorbt.portfolio vectorbt.records vectorbt.returns vectorbt.signals vectorbt.utils Sub-modules \u00b6 vectorbt._settings vectorbt.ohlcv_accessors vectorbt.px_accessors vectorbt.root_accessors","title":"API"},{"location":"api/#vectorbt","text":"","title":"vectorbt"},{"location":"api/#sub-packages","text":"vectorbt.base vectorbt.data vectorbt.generic vectorbt.indicators vectorbt.labels vectorbt.messaging vectorbt.portfolio vectorbt.records vectorbt.returns vectorbt.signals vectorbt.utils","title":"Sub-packages"},{"location":"api/#sub-modules","text":"vectorbt._settings vectorbt.ohlcv_accessors vectorbt.px_accessors vectorbt.root_accessors","title":"Sub-modules"},{"location":"api/_settings/","text":"_settings module \u00b6 Global settings. settings config is also accessible via vectorbt.settings . Here are the main properties of the settings config: It's a nested config, that is, a config that consists of multiple sub-configs. one per sub-package (e.g., 'data'), module (e.g., 'array_wrapper'), or even class (e.g., 'configured'). Each sub-config may consist of other sub-configs. It has frozen keys - you cannot add other sub-configs or remove the existing ones, but you can modify them. Each sub-config can either inherit the properties of the parent one by using dict or overwrite them by using its own Config . The main reason for defining an own config is to allow adding new keys (e.g., 'plotting.layout'). For example, you can change default width and height of each plot: >>> import vectorbt as vbt >>> vbt . settings [ 'plotting' ][ 'layout' ][ 'width' ] = 800 >>> vbt . settings [ 'plotting' ][ 'layout' ][ 'height' ] = 400 The main sub-configs such as for plotting can be also accessed/modified using the dot notation: >>> vbt.settings.plotting['layout']['width'] = 800 Some sub-configs allow the dot notation too but this depends whether they inherit the rules of the root config. >>> vbt.settings.data - ok >>> vbt.settings.data.binance - ok >>> vbt.settings.data.binance.api_key - error >>> vbt.settings.data.binance['api_key'] - ok Since this is only visible when looking at the source code, the advice is to always use the bracket notation. Note Any change takes effect immediately. But whether its reflected immediately depends upon the place that accesses the settings. For example, changing 'array_wrapper.freq` has an immediate effect because the value is resolved every time ArrayWrapper.freq is called. On the other hand, changing 'portfolio.fillna_close' has only effect on Portfolio instances created in the future, not the existing ones, because the value is resolved upon the construction. But mostly you can still force-update the default value by replacing the instance using Configured.replace() . All places in vectorbt import settings from settings , not from vectorbt . Overwriting vectorbt.settings only overwrites the reference created for the user. Consider updating the settings config instead of replacing it. Saving \u00b6 Like any other class subclassing Config , we can save settings to the disk, load it back, and update in-place: >>> vbt . settings . save ( 'my_settings' ) >>> vbt . settings [ 'caching' ][ 'enabled' ] = False >>> vbt . settings [ 'caching' ][ 'enabled' ] False >>> vbt . settings . load_update ( 'my_settings' ) # load() would return a new object! >>> vbt . settings [ 'caching' ][ 'enabled' ] True Bonus: You can do the same with any sub-config inside settings ! settings SettingsConfig \u00b6 Global settings config. numba Settings applied to Numba. Co nf ig( { \"check_func_type\" : true , \"check_func_suffix\" : false } ) config Settings applied to Config . Co nf ig( {} ) configured Settings applied to Configured . Co nf ig( { \"config\" : { \"readonly\" : true } } ) caching Settings applied across vectorbt.utils.decorators . See should_cache() . Co nf ig( { \"enabled\" : true , \"whitelist\" : [ { \"instance\" : null , \"func\" : null , \"cls\" : null , \"base_cls\" : \"<class 'vectorbt.base.array_wrapper.ArrayWrapper'>\" , \"flags\" : null , \"rank\" : null }, { \"instance\" : null , \"func\" : null , \"cls\" : null , \"base_cls\" : \"<class 'vectorbt.base.column_grouper.ColumnGrouper'>\" , \"flags\" : null , \"rank\" : null }, { \"instance\" : null , \"func\" : null , \"cls\" : null , \"base_cls\" : \"<class 'vectorbt.records.col_mapper.ColumnMapper'>\" , \"flags\" : null , \"rank\" : null } ], \"blacklist\" : [] } ) broadcasting Settings applied across vectorbt.base.reshape_fns . Co nf ig( { \"align_index\" : false , \"align_columns\" : true , \"index_from\" : \"strict\" , \"columns_from\" : \"stack\" , \"ignore_sr_names\" : true , \"drop_duplicates\" : true , \"keep\" : \"last\" , \"drop_redundant\" : true , \"ignore_default\" : true } ) array_wrapper Settings applied to ArrayWrapper . Co nf ig( { \"column_only_select\" : false , \"group_select\" : true , \"freq\" : null , \"silence_warnings\" : false } ) datetime Settings applied across vectorbt.utils.datetime_ . Co nf ig( { \"naive_tz\" : \"UTC+01:00\" , \"to_py_timezone\" : true } ) data Settings applied across vectorbt.data . Co nf ig( { \"tz_localize\" : \"UTC\" , \"tz_convert\" : \"UTC\" , \"missing_index\" : \"nan\" , \"missing_columns\" : \"raise\" , \"alpaca\" : { \"key_id\" : null , \"secret_key\" : null }, \"binance\" : { \"api_key\" : null , \"api_secret\" : null }, \"ccxt\" : { \"enableRateLimit\" : true }, \"stats\" : {}, \"plots\" : {} } ) binance: See binance.client.Client . ccxt: See Configuring API Keys . Keys can be defined per exchange. If a key is defined at the root, it applies to all exchanges. plotting Settings applied to plotting Plotly figures. Co nf ig( { \"use_widgets\" : true , \"show_kwargs\" : {}, \"color_schema\" : { \"increasing\" : \"#1b9e76\" , \"decreasing\" : \"#d95f02\" , \"blue\" : \"#1f77b4\" , \"orange\" : \"#ff7f0e\" , \"green\" : \"#2ca02c\" , \"red\" : \"#dc3912\" , \"purple\" : \"#9467bd\" , \"brown\" : \"#8c564b\" , \"pink\" : \"#e377c2\" , \"gray\" : \"#7f7f7f\" , \"yellow\" : \"#bcbd22\" , \"cyan\" : \"#17becf\" }, \"contrast_color_schema\" : { \"blue\" : \"#4285F4\" , \"orange\" : \"#FFAA00\" , \"green\" : \"#37B13F\" , \"red\" : \"#EA4335\" , \"gray\" : \"#E2E2E2\" }, \"themes\" : { \"light\" : { \"color_schema\" : { \"blue\" : \"#1f77b4\" , \"orange\" : \"#ff7f0e\" , \"green\" : \"#2ca02c\" , \"red\" : \"#dc3912\" , \"purple\" : \"#9467bd\" , \"brown\" : \"#8c564b\" , \"pink\" : \"#e377c2\" , \"gray\" : \"#7f7f7f\" , \"yellow\" : \"#bcbd22\" , \"cyan\" : \"#17becf\" }, \"template\" : \"{ ... templates/light.json ... }\" }, \"dark\" : { \"color_schema\" : { \"blue\" : \"#1f77b4\" , \"orange\" : \"#ff7f0e\" , \"green\" : \"#2ca02c\" , \"red\" : \"#dc3912\" , \"purple\" : \"#9467bd\" , \"brown\" : \"#8c564b\" , \"pink\" : \"#e377c2\" , \"gray\" : \"#7f7f7f\" , \"yellow\" : \"#bcbd22\" , \"cyan\" : \"#17becf\" }, \"template\" : \"{ ... templates/dark.json ... }\" }, \"seaborn\" : { \"color_schema\" : { \"blue\" : \"rgb(76,114,176)\" , \"orange\" : \"rgb(221,132,82)\" , \"green\" : \"rgb(129,114,179)\" , \"red\" : \"rgb(85,168,104)\" , \"purple\" : \"rgb(218,139,195)\" , \"brown\" : \"rgb(204,185,116)\" , \"pink\" : \"rgb(140,140,140)\" , \"gray\" : \"rgb(100,181,205)\" , \"yellow\" : \"rgb(147,120,96)\" , \"cyan\" : \"rgb(196,78,82)\" }, \"template\" : \"{ ... templates/seaborn.json ... }\" } }, \"layout\" : { \"width\" : 700 , \"height\" : 350 , \"margin\" : { \"t\" : 30 , \"b\" : 30 , \"l\" : 30 , \"r\" : 30 }, \"legend\" : { \"orientation\" : \"h\" , \"yanchor\" : \"bottom\" , \"y\" : 1.02 , \"xanchor\" : \"right\" , \"x\" : 1 , \"traceorder\" : \"normal\" }, \"template\" : \"vbt_light\" } } ) stats_builder Settings applied to StatsBuilderMixin . Co nf ig( { \"metrics\" : \"all\" , \"tags\" : \"all\" , \"silence_warnings\" : false , \"template_mapping\" : {}, \"filters\" : { \"is_not_grouped\" : { \"filter_func\" : \"<function <lambda> at 0x7facbae4b048>\" , \"warning_message\" : \"Sub(template=\\\"Metric '$metric_name' does not support grouped data\\\", mapping={})\" }, \"has_freq\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d048>\" , \"warning_message\" : \"Sub(template=\\\"Metric '$metric_name' requires frequency to be set\\\", mapping={})\" } }, \"settings\" : { \"to_timedelta\" : null , \"use_caching\" : true }, \"metric_settings\" : {} } ) plots_builder Settings applied to PlotsBuilderMixin . Co nf ig( { \"subplots\" : \"all\" , \"tags\" : \"all\" , \"silence_warnings\" : false , \"template_mapping\" : {}, \"filters\" : { \"is_not_grouped\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d0d0>\" , \"warning_message\" : \"Sub(template=\\\"Subplot '$subplot_name' does not support grouped data\\\", mapping={})\" }, \"has_freq\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d158>\" , \"warning_message\" : \"Sub(template=\\\"Subplot '$subplot_name' requires frequency to be set\\\", mapping={})\" } }, \"settings\" : { \"use_caching\" : true , \"hline_shape_kwargs\" : { \"type\" : \"line\" , \"line\" : { \"color\" : \"gray\" , \"dash\" : \"dash\" } } }, \"subplot_settings\" : {}, \"show_titles\" : true , \"hide_id_labels\" : true , \"group_id_labels\" : true , \"make_subplots_kwargs\" : {}, \"layout_kwargs\" : {} } ) generic Settings applied across vectorbt.generic . Co nf ig( { \"stats\" : { \"filters\" : { \"has_mapping\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d1e0>\" } }, \"settings\" : { \"incl_all_keys\" : false } }, \"plots\" : {} } ) ranges Settings applied across vectorbt.generic.ranges . Co nf ig( { \"stats\" : {}, \"plots\" : {} } ) drawdowns Settings applied across vectorbt.generic.drawdowns . Co nf ig( { \"stats\" : { \"settings\" : { \"incl_active\" : false } }, \"plots\" : {} } ) ohlcv Settings applied across vectorbt.ohlcv_accessors . Co nf ig( { \"plot_type\" : \"OHLC\" , \"column_names\" : { \"open\" : \"Open\" , \"high\" : \"High\" , \"low\" : \"Low\" , \"close\" : \"Close\" , \"volume\" : \"Volume\" }, \"stats\" : {}, \"plots\" : {} } ) signals Settings applied across vectorbt.signals . Co nf ig( { \"stats\" : { \"filters\" : { \"silent_has_other\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d268>\" } }, \"settings\" : { \"other\" : null , \"other_name\" : \"Other\" , \"from_other\" : false } }, \"plots\" : {} } ) returns Settings applied across vectorbt.returns . Co nf ig( { \"year_freq\" : \"365 days\" , \"defaults\" : { \"start_value\" : 0.0 , \"window\" : 10 , \"minp\" : null , \"ddof\" : 1 , \"risk_free\" : 0.0 , \"levy_alpha\" : 2.0 , \"required_return\" : 0.0 , \"cutoff\" : 0.05 }, \"stats\" : { \"filters\" : { \"has_year_freq\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d2f0>\" , \"warning_message\" : \"Sub(template=\\\"Metric '$metric_name' requires year frequency to be set\\\", mapping={})\" }, \"has_benchmark_rets\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d378>\" , \"warning_message\" : \"Sub(template=\\\"Metric '$metric_name' requires benchmark_rets to be set\\\", mapping={})\" } }, \"settings\" : { \"check_is_not_grouped\" : true } }, \"plots\" : {} } ) qs_adapter Settings applied across vectorbt.returns.qs_adapter . Co nf ig( { \"defaults\" : {} } ) records Settings applied across vectorbt.records.base . Co nf ig( { \"stats\" : {}, \"plots\" : {} } ) mapped_array Settings applied across vectorbt.records.mapped_array . Co nf ig( { \"stats\" : { \"filters\" : { \"has_mapping\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d400>\" } }, \"settings\" : { \"incl_all_keys\" : false } }, \"plots\" : {} } ) orders Settings applied across vectorbt.portfolio.orders . Co nf ig( { \"stats\" : {}, \"plots\" : {} } ) trades Settings applied across vectorbt.portfolio.trades . Co nf ig( { \"stats\" : { \"settings\" : { \"incl_open\" : false }, \"template_mapping\" : { \"incl_open_tags\" : \"RepEval(expression=\\\"['open', 'closed'] if incl_open else ['closed']\\\", mapping={})\" } }, \"plots\" : {} } ) logs Settings applied across vectorbt.portfolio.logs . Co nf ig( { \"stats\" : {} } ) portfolio Settings applied to Portfolio . Co nf ig( { \"call_seq\" : \"default\" , \"init_cash\" : 100.0 , \"size\" : I nf i n i t y , \"size_type\" : \"amount\" , \"fees\" : 0.0 , \"fixed_fees\" : 0.0 , \"slippage\" : 0.0 , \"reject_prob\" : 0.0 , \"min_size\" : 1e-08 , \"max_size\" : I nf i n i t y , \"size_granularity\" : NaN , \"lock_cash\" : false , \"allow_partial\" : true , \"raise_reject\" : false , \"val_price\" : I nf i n i t y , \"accumulate\" : false , \"sl_stop\" : NaN , \"sl_trail\" : false , \"tp_stop\" : NaN , \"stop_entry_price\" : \"close\" , \"stop_exit_price\" : \"stoplimit\" , \"stop_conflict_mode\" : \"exit\" , \"upon_stop_exit\" : \"close\" , \"upon_stop_update\" : \"override\" , \"use_stops\" : null , \"log\" : false , \"upon_long_conflict\" : \"ignore\" , \"upon_short_conflict\" : \"ignore\" , \"upon_dir_conflict\" : \"ignore\" , \"upon_opposite_entry\" : \"reversereduce\" , \"signal_direction\" : \"longonly\" , \"order_direction\" : \"both\" , \"cash_sharing\" : false , \"call_pre_segment\" : false , \"call_post_segment\" : false , \"ffill_val_price\" : true , \"update_value\" : false , \"fill_pos_record\" : true , \"row_wise\" : false , \"flexible\" : false , \"use_numba\" : true , \"seed\" : null , \"freq\" : null , \"attach_call_seq\" : false , \"fillna_close\" : true , \"trades_type\" : \"exittrades\" , \"stats\" : { \"filters\" : { \"has_year_freq\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d488>\" , \"warning_message\" : \"Sub(template=\\\"Metric '$metric_name' requires year frequency to be set\\\", mapping={})\" } }, \"settings\" : { \"use_asset_returns\" : false , \"incl_open\" : false }, \"template_mapping\" : { \"incl_open_tags\" : \"RepEval(expression=\\\"['open', 'closed'] if incl_open else ['closed']\\\", mapping={})\" } }, \"plots\" : { \"subplots\" : [ \"orders\" , \"trade_pnl\" , \"cum_returns\" ], \"settings\" : { \"use_asset_returns\" : false } } } ) messaging Settings applied across vectorbt.messaging . Co nf ig( { \"telegram\" : { \"token\" : null , \"use_context\" : true , \"persistence\" : \"telegram_bot.pickle\" , \"defaults\" : {}, \"drop_pending_updates\" : true }, \"giphy\" : { \"api_key\" : null , \"weirdness\" : 5 } } ) telegram: Settings applied to python-telegram-bot . Set persistence to string to use as filename in telegram.ext.PicklePersistence . For defaults , see telegram.ext.Defaults . Other settings will be distributed across telegram.ext.Updater and telegram.ext.updater.Updater.start_polling . giphy: Settings applied to GIPHY Translate Endpoint . SettingsConfig class \u00b6 Extends Config for global settings. Superclasses Config Documented Pickleable PickleableDict builtins.dict Inherited members Config.as_attrs_ Config.clear() Config.convert_dicts_ Config.copy() Config.copy_kwargs_ Config.dumps() Config.frozen_keys_ Config.load_update() Config.loads() Config.make_checkpoint() Config.merge_with() Config.nested_ Config.pop() Config.popitem() Config.readonly_ Config.reset() Config.reset_dct_ Config.reset_dct_copy_kwargs_ Config.to_dict() Config.to_doc() Config.update() Pickleable.load() Pickleable.save() register_template method \u00b6 SettingsConfig . register_template ( theme ) Register template of a theme. register_templates method \u00b6 SettingsConfig . register_templates () Register templates of all themes. reset_theme method \u00b6 SettingsConfig . reset_theme () Reset to default theme. set_theme method \u00b6 SettingsConfig . set_theme ( theme ) Set default theme.","title":"_settings"},{"location":"api/_settings/#vectorbt._settings","text":"Global settings. settings config is also accessible via vectorbt.settings . Here are the main properties of the settings config: It's a nested config, that is, a config that consists of multiple sub-configs. one per sub-package (e.g., 'data'), module (e.g., 'array_wrapper'), or even class (e.g., 'configured'). Each sub-config may consist of other sub-configs. It has frozen keys - you cannot add other sub-configs or remove the existing ones, but you can modify them. Each sub-config can either inherit the properties of the parent one by using dict or overwrite them by using its own Config . The main reason for defining an own config is to allow adding new keys (e.g., 'plotting.layout'). For example, you can change default width and height of each plot: >>> import vectorbt as vbt >>> vbt . settings [ 'plotting' ][ 'layout' ][ 'width' ] = 800 >>> vbt . settings [ 'plotting' ][ 'layout' ][ 'height' ] = 400 The main sub-configs such as for plotting can be also accessed/modified using the dot notation: >>> vbt.settings.plotting['layout']['width'] = 800 Some sub-configs allow the dot notation too but this depends whether they inherit the rules of the root config. >>> vbt.settings.data - ok >>> vbt.settings.data.binance - ok >>> vbt.settings.data.binance.api_key - error >>> vbt.settings.data.binance['api_key'] - ok Since this is only visible when looking at the source code, the advice is to always use the bracket notation. Note Any change takes effect immediately. But whether its reflected immediately depends upon the place that accesses the settings. For example, changing 'array_wrapper.freq` has an immediate effect because the value is resolved every time ArrayWrapper.freq is called. On the other hand, changing 'portfolio.fillna_close' has only effect on Portfolio instances created in the future, not the existing ones, because the value is resolved upon the construction. But mostly you can still force-update the default value by replacing the instance using Configured.replace() . All places in vectorbt import settings from settings , not from vectorbt . Overwriting vectorbt.settings only overwrites the reference created for the user. Consider updating the settings config instead of replacing it.","title":"vectorbt._settings"},{"location":"api/_settings/#saving","text":"Like any other class subclassing Config , we can save settings to the disk, load it back, and update in-place: >>> vbt . settings . save ( 'my_settings' ) >>> vbt . settings [ 'caching' ][ 'enabled' ] = False >>> vbt . settings [ 'caching' ][ 'enabled' ] False >>> vbt . settings . load_update ( 'my_settings' ) # load() would return a new object! >>> vbt . settings [ 'caching' ][ 'enabled' ] True Bonus: You can do the same with any sub-config inside settings !","title":"Saving"},{"location":"api/_settings/#vectorbt._settings.settings","text":"Global settings config. numba Settings applied to Numba. Co nf ig( { \"check_func_type\" : true , \"check_func_suffix\" : false } ) config Settings applied to Config . Co nf ig( {} ) configured Settings applied to Configured . Co nf ig( { \"config\" : { \"readonly\" : true } } ) caching Settings applied across vectorbt.utils.decorators . See should_cache() . Co nf ig( { \"enabled\" : true , \"whitelist\" : [ { \"instance\" : null , \"func\" : null , \"cls\" : null , \"base_cls\" : \"<class 'vectorbt.base.array_wrapper.ArrayWrapper'>\" , \"flags\" : null , \"rank\" : null }, { \"instance\" : null , \"func\" : null , \"cls\" : null , \"base_cls\" : \"<class 'vectorbt.base.column_grouper.ColumnGrouper'>\" , \"flags\" : null , \"rank\" : null }, { \"instance\" : null , \"func\" : null , \"cls\" : null , \"base_cls\" : \"<class 'vectorbt.records.col_mapper.ColumnMapper'>\" , \"flags\" : null , \"rank\" : null } ], \"blacklist\" : [] } ) broadcasting Settings applied across vectorbt.base.reshape_fns . Co nf ig( { \"align_index\" : false , \"align_columns\" : true , \"index_from\" : \"strict\" , \"columns_from\" : \"stack\" , \"ignore_sr_names\" : true , \"drop_duplicates\" : true , \"keep\" : \"last\" , \"drop_redundant\" : true , \"ignore_default\" : true } ) array_wrapper Settings applied to ArrayWrapper . Co nf ig( { \"column_only_select\" : false , \"group_select\" : true , \"freq\" : null , \"silence_warnings\" : false } ) datetime Settings applied across vectorbt.utils.datetime_ . Co nf ig( { \"naive_tz\" : \"UTC+01:00\" , \"to_py_timezone\" : true } ) data Settings applied across vectorbt.data . Co nf ig( { \"tz_localize\" : \"UTC\" , \"tz_convert\" : \"UTC\" , \"missing_index\" : \"nan\" , \"missing_columns\" : \"raise\" , \"alpaca\" : { \"key_id\" : null , \"secret_key\" : null }, \"binance\" : { \"api_key\" : null , \"api_secret\" : null }, \"ccxt\" : { \"enableRateLimit\" : true }, \"stats\" : {}, \"plots\" : {} } ) binance: See binance.client.Client . ccxt: See Configuring API Keys . Keys can be defined per exchange. If a key is defined at the root, it applies to all exchanges. plotting Settings applied to plotting Plotly figures. Co nf ig( { \"use_widgets\" : true , \"show_kwargs\" : {}, \"color_schema\" : { \"increasing\" : \"#1b9e76\" , \"decreasing\" : \"#d95f02\" , \"blue\" : \"#1f77b4\" , \"orange\" : \"#ff7f0e\" , \"green\" : \"#2ca02c\" , \"red\" : \"#dc3912\" , \"purple\" : \"#9467bd\" , \"brown\" : \"#8c564b\" , \"pink\" : \"#e377c2\" , \"gray\" : \"#7f7f7f\" , \"yellow\" : \"#bcbd22\" , \"cyan\" : \"#17becf\" }, \"contrast_color_schema\" : { \"blue\" : \"#4285F4\" , \"orange\" : \"#FFAA00\" , \"green\" : \"#37B13F\" , \"red\" : \"#EA4335\" , \"gray\" : \"#E2E2E2\" }, \"themes\" : { \"light\" : { \"color_schema\" : { \"blue\" : \"#1f77b4\" , \"orange\" : \"#ff7f0e\" , \"green\" : \"#2ca02c\" , \"red\" : \"#dc3912\" , \"purple\" : \"#9467bd\" , \"brown\" : \"#8c564b\" , \"pink\" : \"#e377c2\" , \"gray\" : \"#7f7f7f\" , \"yellow\" : \"#bcbd22\" , \"cyan\" : \"#17becf\" }, \"template\" : \"{ ... templates/light.json ... }\" }, \"dark\" : { \"color_schema\" : { \"blue\" : \"#1f77b4\" , \"orange\" : \"#ff7f0e\" , \"green\" : \"#2ca02c\" , \"red\" : \"#dc3912\" , \"purple\" : \"#9467bd\" , \"brown\" : \"#8c564b\" , \"pink\" : \"#e377c2\" , \"gray\" : \"#7f7f7f\" , \"yellow\" : \"#bcbd22\" , \"cyan\" : \"#17becf\" }, \"template\" : \"{ ... templates/dark.json ... }\" }, \"seaborn\" : { \"color_schema\" : { \"blue\" : \"rgb(76,114,176)\" , \"orange\" : \"rgb(221,132,82)\" , \"green\" : \"rgb(129,114,179)\" , \"red\" : \"rgb(85,168,104)\" , \"purple\" : \"rgb(218,139,195)\" , \"brown\" : \"rgb(204,185,116)\" , \"pink\" : \"rgb(140,140,140)\" , \"gray\" : \"rgb(100,181,205)\" , \"yellow\" : \"rgb(147,120,96)\" , \"cyan\" : \"rgb(196,78,82)\" }, \"template\" : \"{ ... templates/seaborn.json ... }\" } }, \"layout\" : { \"width\" : 700 , \"height\" : 350 , \"margin\" : { \"t\" : 30 , \"b\" : 30 , \"l\" : 30 , \"r\" : 30 }, \"legend\" : { \"orientation\" : \"h\" , \"yanchor\" : \"bottom\" , \"y\" : 1.02 , \"xanchor\" : \"right\" , \"x\" : 1 , \"traceorder\" : \"normal\" }, \"template\" : \"vbt_light\" } } ) stats_builder Settings applied to StatsBuilderMixin . Co nf ig( { \"metrics\" : \"all\" , \"tags\" : \"all\" , \"silence_warnings\" : false , \"template_mapping\" : {}, \"filters\" : { \"is_not_grouped\" : { \"filter_func\" : \"<function <lambda> at 0x7facbae4b048>\" , \"warning_message\" : \"Sub(template=\\\"Metric '$metric_name' does not support grouped data\\\", mapping={})\" }, \"has_freq\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d048>\" , \"warning_message\" : \"Sub(template=\\\"Metric '$metric_name' requires frequency to be set\\\", mapping={})\" } }, \"settings\" : { \"to_timedelta\" : null , \"use_caching\" : true }, \"metric_settings\" : {} } ) plots_builder Settings applied to PlotsBuilderMixin . Co nf ig( { \"subplots\" : \"all\" , \"tags\" : \"all\" , \"silence_warnings\" : false , \"template_mapping\" : {}, \"filters\" : { \"is_not_grouped\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d0d0>\" , \"warning_message\" : \"Sub(template=\\\"Subplot '$subplot_name' does not support grouped data\\\", mapping={})\" }, \"has_freq\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d158>\" , \"warning_message\" : \"Sub(template=\\\"Subplot '$subplot_name' requires frequency to be set\\\", mapping={})\" } }, \"settings\" : { \"use_caching\" : true , \"hline_shape_kwargs\" : { \"type\" : \"line\" , \"line\" : { \"color\" : \"gray\" , \"dash\" : \"dash\" } } }, \"subplot_settings\" : {}, \"show_titles\" : true , \"hide_id_labels\" : true , \"group_id_labels\" : true , \"make_subplots_kwargs\" : {}, \"layout_kwargs\" : {} } ) generic Settings applied across vectorbt.generic . Co nf ig( { \"stats\" : { \"filters\" : { \"has_mapping\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d1e0>\" } }, \"settings\" : { \"incl_all_keys\" : false } }, \"plots\" : {} } ) ranges Settings applied across vectorbt.generic.ranges . Co nf ig( { \"stats\" : {}, \"plots\" : {} } ) drawdowns Settings applied across vectorbt.generic.drawdowns . Co nf ig( { \"stats\" : { \"settings\" : { \"incl_active\" : false } }, \"plots\" : {} } ) ohlcv Settings applied across vectorbt.ohlcv_accessors . Co nf ig( { \"plot_type\" : \"OHLC\" , \"column_names\" : { \"open\" : \"Open\" , \"high\" : \"High\" , \"low\" : \"Low\" , \"close\" : \"Close\" , \"volume\" : \"Volume\" }, \"stats\" : {}, \"plots\" : {} } ) signals Settings applied across vectorbt.signals . Co nf ig( { \"stats\" : { \"filters\" : { \"silent_has_other\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d268>\" } }, \"settings\" : { \"other\" : null , \"other_name\" : \"Other\" , \"from_other\" : false } }, \"plots\" : {} } ) returns Settings applied across vectorbt.returns . Co nf ig( { \"year_freq\" : \"365 days\" , \"defaults\" : { \"start_value\" : 0.0 , \"window\" : 10 , \"minp\" : null , \"ddof\" : 1 , \"risk_free\" : 0.0 , \"levy_alpha\" : 2.0 , \"required_return\" : 0.0 , \"cutoff\" : 0.05 }, \"stats\" : { \"filters\" : { \"has_year_freq\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d2f0>\" , \"warning_message\" : \"Sub(template=\\\"Metric '$metric_name' requires year frequency to be set\\\", mapping={})\" }, \"has_benchmark_rets\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d378>\" , \"warning_message\" : \"Sub(template=\\\"Metric '$metric_name' requires benchmark_rets to be set\\\", mapping={})\" } }, \"settings\" : { \"check_is_not_grouped\" : true } }, \"plots\" : {} } ) qs_adapter Settings applied across vectorbt.returns.qs_adapter . Co nf ig( { \"defaults\" : {} } ) records Settings applied across vectorbt.records.base . Co nf ig( { \"stats\" : {}, \"plots\" : {} } ) mapped_array Settings applied across vectorbt.records.mapped_array . Co nf ig( { \"stats\" : { \"filters\" : { \"has_mapping\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d400>\" } }, \"settings\" : { \"incl_all_keys\" : false } }, \"plots\" : {} } ) orders Settings applied across vectorbt.portfolio.orders . Co nf ig( { \"stats\" : {}, \"plots\" : {} } ) trades Settings applied across vectorbt.portfolio.trades . Co nf ig( { \"stats\" : { \"settings\" : { \"incl_open\" : false }, \"template_mapping\" : { \"incl_open_tags\" : \"RepEval(expression=\\\"['open', 'closed'] if incl_open else ['closed']\\\", mapping={})\" } }, \"plots\" : {} } ) logs Settings applied across vectorbt.portfolio.logs . Co nf ig( { \"stats\" : {} } ) portfolio Settings applied to Portfolio . Co nf ig( { \"call_seq\" : \"default\" , \"init_cash\" : 100.0 , \"size\" : I nf i n i t y , \"size_type\" : \"amount\" , \"fees\" : 0.0 , \"fixed_fees\" : 0.0 , \"slippage\" : 0.0 , \"reject_prob\" : 0.0 , \"min_size\" : 1e-08 , \"max_size\" : I nf i n i t y , \"size_granularity\" : NaN , \"lock_cash\" : false , \"allow_partial\" : true , \"raise_reject\" : false , \"val_price\" : I nf i n i t y , \"accumulate\" : false , \"sl_stop\" : NaN , \"sl_trail\" : false , \"tp_stop\" : NaN , \"stop_entry_price\" : \"close\" , \"stop_exit_price\" : \"stoplimit\" , \"stop_conflict_mode\" : \"exit\" , \"upon_stop_exit\" : \"close\" , \"upon_stop_update\" : \"override\" , \"use_stops\" : null , \"log\" : false , \"upon_long_conflict\" : \"ignore\" , \"upon_short_conflict\" : \"ignore\" , \"upon_dir_conflict\" : \"ignore\" , \"upon_opposite_entry\" : \"reversereduce\" , \"signal_direction\" : \"longonly\" , \"order_direction\" : \"both\" , \"cash_sharing\" : false , \"call_pre_segment\" : false , \"call_post_segment\" : false , \"ffill_val_price\" : true , \"update_value\" : false , \"fill_pos_record\" : true , \"row_wise\" : false , \"flexible\" : false , \"use_numba\" : true , \"seed\" : null , \"freq\" : null , \"attach_call_seq\" : false , \"fillna_close\" : true , \"trades_type\" : \"exittrades\" , \"stats\" : { \"filters\" : { \"has_year_freq\" : { \"filter_func\" : \"<function <lambda> at 0x7facbaf9d488>\" , \"warning_message\" : \"Sub(template=\\\"Metric '$metric_name' requires year frequency to be set\\\", mapping={})\" } }, \"settings\" : { \"use_asset_returns\" : false , \"incl_open\" : false }, \"template_mapping\" : { \"incl_open_tags\" : \"RepEval(expression=\\\"['open', 'closed'] if incl_open else ['closed']\\\", mapping={})\" } }, \"plots\" : { \"subplots\" : [ \"orders\" , \"trade_pnl\" , \"cum_returns\" ], \"settings\" : { \"use_asset_returns\" : false } } } ) messaging Settings applied across vectorbt.messaging . Co nf ig( { \"telegram\" : { \"token\" : null , \"use_context\" : true , \"persistence\" : \"telegram_bot.pickle\" , \"defaults\" : {}, \"drop_pending_updates\" : true }, \"giphy\" : { \"api_key\" : null , \"weirdness\" : 5 } } ) telegram: Settings applied to python-telegram-bot . Set persistence to string to use as filename in telegram.ext.PicklePersistence . For defaults , see telegram.ext.Defaults . Other settings will be distributed across telegram.ext.Updater and telegram.ext.updater.Updater.start_polling . giphy: Settings applied to GIPHY Translate Endpoint .","title":"settings"},{"location":"api/_settings/#vectorbt._settings.SettingsConfig","text":"Extends Config for global settings. Superclasses Config Documented Pickleable PickleableDict builtins.dict Inherited members Config.as_attrs_ Config.clear() Config.convert_dicts_ Config.copy() Config.copy_kwargs_ Config.dumps() Config.frozen_keys_ Config.load_update() Config.loads() Config.make_checkpoint() Config.merge_with() Config.nested_ Config.pop() Config.popitem() Config.readonly_ Config.reset() Config.reset_dct_ Config.reset_dct_copy_kwargs_ Config.to_dict() Config.to_doc() Config.update() Pickleable.load() Pickleable.save()","title":"SettingsConfig"},{"location":"api/_settings/#vectorbt._settings.SettingsConfig.register_template","text":"SettingsConfig . register_template ( theme ) Register template of a theme.","title":"register_template()"},{"location":"api/_settings/#vectorbt._settings.SettingsConfig.register_templates","text":"SettingsConfig . register_templates () Register templates of all themes.","title":"register_templates()"},{"location":"api/_settings/#vectorbt._settings.SettingsConfig.reset_theme","text":"SettingsConfig . reset_theme () Reset to default theme.","title":"reset_theme()"},{"location":"api/_settings/#vectorbt._settings.SettingsConfig.set_theme","text":"SettingsConfig . set_theme ( theme ) Set default theme.","title":"set_theme()"},{"location":"api/ohlcv_accessors/","text":"ohlcv_accessors module \u00b6 Custom pandas accessors for OHLC(V) data. Methods can be accessed as follows: OHLCVDFAccessor -> pd.DataFrame.vbt.ohlc.* OHLCVDFAccessor -> pd.DataFrame.vbt.ohlcv.* The accessors inherit vectorbt.generic.accessors . Note Accessors do not utilize caching. Column names \u00b6 By default, vectorbt searches for columns with names 'open', 'high', 'low', 'close', and 'volume' (case doesn't matter). You can change the naming either using ohlcv.column_names in settings , or by providing column_names directly to the accessor. >>> import pandas as pd >>> import vectorbt as vbt >>> df = pd . DataFrame ({ ... 'my_open1' : [ 2 , 3 , 4 , 3.5 , 2.5 ], ... 'my_high2' : [ 3 , 4 , 4.5 , 4 , 3 ], ... 'my_low3' : [ 1.5 , 2.5 , 3.5 , 2.5 , 1.5 ], ... 'my_close4' : [ 2.5 , 3.5 , 4 , 3 , 2 ], ... 'my_volume5' : [ 10 , 11 , 10 , 9 , 10 ] ... }) >>> # vectorbt can't find columns >>> df . vbt . ohlcv . get_column ( 'open' ) None >>> my_column_names = dict ( ... open = 'my_open1' , ... high = 'my_high2' , ... low = 'my_low3' , ... close = 'my_close4' , ... volume = 'my_volume5' , ... ) >>> ohlcv_acc = df . vbt . ohlcv ( freq = 'd' , column_names = my_column_names ) >>> ohlcv_acc . get_column ( 'open' ) 0 2.0 1 3.0 2 4.0 3 3.5 4 2.5 Name: my_open1, dtype: float64 Stats \u00b6 Hint See StatsBuilderMixin.stats() and OHLCVDFAccessor.metrics . >>> ohlcv_acc . stats () Start 0 End 4 Period 5 days 00:00:00 First Price 2.0 Lowest Price 1.5 Highest Price 4.5 Last Price 2.0 First Volume 10 Lowest Volume 9 Highest Volume 11 Last Volume 10 Name: agg_func_mean, dtype: object Plots \u00b6 Hint See PlotsBuilderMixin.plots() and OHLCVDFAccessor.subplots . OHLCVDFAccessor class has a single subplot based on OHLCVDFAccessor.plot() (without volume): >>> ohlcv_acc . plots ( settings = dict ( plot_type = 'candlestick' )) OHLCVDFAccessor class \u00b6 Accessor on top of OHLCV data. For DataFrames only. Accessible through pd.DataFrame.vbt.ohlcv . Superclasses AttrResolver BaseAccessor BaseDFAccessor Configured Documented GenericAccessor GenericDFAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.bshift() GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.diff() GenericAccessor.drawdown() GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.fshift() GenericAccessor.get_drawdowns() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.lineplot() GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.pct_change() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.resolve_self() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.zscore() GenericDFAccessor.config GenericDFAccessor.df_accessor_cls GenericDFAccessor.drawdowns GenericDFAccessor.flatten_grouped() GenericDFAccessor.heatmap() GenericDFAccessor.iloc GenericDFAccessor.indexing_kwargs GenericDFAccessor.loc GenericDFAccessor.mapping GenericDFAccessor.obj GenericDFAccessor.ranges GenericDFAccessor.self_aliases GenericDFAccessor.squeeze_grouped() GenericDFAccessor.sr_accessor_cls GenericDFAccessor.ts_heatmap() GenericDFAccessor.wrapper GenericDFAccessor.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj() close property \u00b6 Close series. column_names property \u00b6 Column names. get_column method \u00b6 OHLCVDFAccessor . get_column ( col_name ) Get column from OHLCVDFAccessor.column_names . high property \u00b6 High series. low property \u00b6 Low series. metrics class variable \u00b6 Metrics supported by OHLCVDFAccessor . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2afbf8>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2afc80>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2afd08>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"first_price\" : { \"title\" : \"First Price\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2afd90>\" , \"resolve_ohlc\" : true , \"tags\" : [ \"ohlcv\" , \"ohlc\" ] }, \"lowest_price\" : { \"title\" : \"Lowest Price\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2afe18>\" , \"resolve_ohlc\" : true , \"tags\" : [ \"ohlcv\" , \"ohlc\" ] }, \"highest_price\" : { \"title\" : \"Highest Price\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2afea0>\" , \"resolve_ohlc\" : true , \"tags\" : [ \"ohlcv\" , \"ohlc\" ] }, \"last_price\" : { \"title\" : \"Last Price\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2aff28>\" , \"resolve_ohlc\" : true , \"tags\" : [ \"ohlcv\" , \"ohlc\" ] }, \"first_volume\" : { \"title\" : \"First Volume\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2b7048>\" , \"resolve_volume\" : true , \"tags\" : [ \"ohlcv\" , \"volume\" ] }, \"lowest_volume\" : { \"title\" : \"Lowest Volume\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2b70d0>\" , \"resolve_volume\" : true , \"tags\" : [ \"ohlcv\" , \"volume\" ] }, \"highest_volume\" : { \"title\" : \"Highest Volume\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2b7158>\" , \"resolve_volume\" : true , \"tags\" : [ \"ohlcv\" , \"volume\" ] }, \"last_volume\" : { \"title\" : \"Last Volume\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2b71e0>\" , \"resolve_volume\" : true , \"tags\" : [ \"ohlcv\" , \"volume\" ] } } ) Returns OHLCVDFAccessor._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable OHLCVDFAccessor._metrics . ohlc property \u00b6 Open, high, low, and close series. open property \u00b6 Open series. plot method \u00b6 OHLCVDFAccessor . plot ( plot_type = None , show_volume = None , ohlc_kwargs = None , volume_kwargs = None , ohlc_add_trace_kwargs = None , volume_add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot OHLCV data. Args plot_type Either 'OHLC', 'Candlestick' or Plotly trace. Pass None to use the default. show_volume :\u2002 bool If True, shows volume as bar chart. ohlc_kwargs :\u2002 dict Keyword arguments passed to plot_type . volume_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Bar . ohlc_add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace for OHLC. volume_add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace for volume. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import vectorbt as vbt >>> vbt . YFData . download ( \"BTC-USD\" ) . get () . vbt . ohlcv . plot () plots_defaults property \u00b6 Defaults for PlotsBuilderMixin.plots() . Merges GenericAccessor.plots_defaults and ohlcv.plots from settings . stats_defaults property \u00b6 Defaults for StatsBuilderMixin.stats() . Merges GenericAccessor.stats_defaults and ohlcv.stats from settings . subplots class variable \u00b6 Subplots supported by OHLCVDFAccessor . Co nf ig( { \"plot\" : { \"title\" : \"OHLC\" , \"xaxis_kwargs\" : { \"showgrid\" : true , \"rangeslider_visible\" : false }, \"yaxis_kwargs\" : { \"showgrid\" : true }, \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"show_volume\" : false , \"tags\" : \"ohlcv\" } } ) Returns OHLCVDFAccessor._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable OHLCVDFAccessor._subplots . volume property \u00b6 Volume series.","title":"ohlcv_accessors"},{"location":"api/ohlcv_accessors/#vectorbt.ohlcv_accessors","text":"Custom pandas accessors for OHLC(V) data. Methods can be accessed as follows: OHLCVDFAccessor -> pd.DataFrame.vbt.ohlc.* OHLCVDFAccessor -> pd.DataFrame.vbt.ohlcv.* The accessors inherit vectorbt.generic.accessors . Note Accessors do not utilize caching.","title":"vectorbt.ohlcv_accessors"},{"location":"api/ohlcv_accessors/#column-names","text":"By default, vectorbt searches for columns with names 'open', 'high', 'low', 'close', and 'volume' (case doesn't matter). You can change the naming either using ohlcv.column_names in settings , or by providing column_names directly to the accessor. >>> import pandas as pd >>> import vectorbt as vbt >>> df = pd . DataFrame ({ ... 'my_open1' : [ 2 , 3 , 4 , 3.5 , 2.5 ], ... 'my_high2' : [ 3 , 4 , 4.5 , 4 , 3 ], ... 'my_low3' : [ 1.5 , 2.5 , 3.5 , 2.5 , 1.5 ], ... 'my_close4' : [ 2.5 , 3.5 , 4 , 3 , 2 ], ... 'my_volume5' : [ 10 , 11 , 10 , 9 , 10 ] ... }) >>> # vectorbt can't find columns >>> df . vbt . ohlcv . get_column ( 'open' ) None >>> my_column_names = dict ( ... open = 'my_open1' , ... high = 'my_high2' , ... low = 'my_low3' , ... close = 'my_close4' , ... volume = 'my_volume5' , ... ) >>> ohlcv_acc = df . vbt . ohlcv ( freq = 'd' , column_names = my_column_names ) >>> ohlcv_acc . get_column ( 'open' ) 0 2.0 1 3.0 2 4.0 3 3.5 4 2.5 Name: my_open1, dtype: float64","title":"Column names"},{"location":"api/ohlcv_accessors/#stats","text":"Hint See StatsBuilderMixin.stats() and OHLCVDFAccessor.metrics . >>> ohlcv_acc . stats () Start 0 End 4 Period 5 days 00:00:00 First Price 2.0 Lowest Price 1.5 Highest Price 4.5 Last Price 2.0 First Volume 10 Lowest Volume 9 Highest Volume 11 Last Volume 10 Name: agg_func_mean, dtype: object","title":"Stats"},{"location":"api/ohlcv_accessors/#plots","text":"Hint See PlotsBuilderMixin.plots() and OHLCVDFAccessor.subplots . OHLCVDFAccessor class has a single subplot based on OHLCVDFAccessor.plot() (without volume): >>> ohlcv_acc . plots ( settings = dict ( plot_type = 'candlestick' ))","title":"Plots"},{"location":"api/ohlcv_accessors/#vectorbt.ohlcv_accessors.OHLCVDFAccessor","text":"Accessor on top of OHLCV data. For DataFrames only. Accessible through pd.DataFrame.vbt.ohlcv . Superclasses AttrResolver BaseAccessor BaseDFAccessor Configured Documented GenericAccessor GenericDFAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.bshift() GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.diff() GenericAccessor.drawdown() GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.fshift() GenericAccessor.get_drawdowns() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.lineplot() GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.pct_change() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.resolve_self() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.zscore() GenericDFAccessor.config GenericDFAccessor.df_accessor_cls GenericDFAccessor.drawdowns GenericDFAccessor.flatten_grouped() GenericDFAccessor.heatmap() GenericDFAccessor.iloc GenericDFAccessor.indexing_kwargs GenericDFAccessor.loc GenericDFAccessor.mapping GenericDFAccessor.obj GenericDFAccessor.ranges GenericDFAccessor.self_aliases GenericDFAccessor.squeeze_grouped() GenericDFAccessor.sr_accessor_cls GenericDFAccessor.ts_heatmap() GenericDFAccessor.wrapper GenericDFAccessor.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"OHLCVDFAccessor"},{"location":"api/ohlcv_accessors/#vectorbt.ohlcv_accessors.OHLCVDFAccessor.close","text":"Close series.","title":"close"},{"location":"api/ohlcv_accessors/#vectorbt.ohlcv_accessors.OHLCVDFAccessor.column_names","text":"Column names.","title":"column_names"},{"location":"api/ohlcv_accessors/#vectorbt.ohlcv_accessors.OHLCVDFAccessor.get_column","text":"OHLCVDFAccessor . get_column ( col_name ) Get column from OHLCVDFAccessor.column_names .","title":"get_column()"},{"location":"api/ohlcv_accessors/#vectorbt.ohlcv_accessors.OHLCVDFAccessor.high","text":"High series.","title":"high"},{"location":"api/ohlcv_accessors/#vectorbt.ohlcv_accessors.OHLCVDFAccessor.low","text":"Low series.","title":"low"},{"location":"api/ohlcv_accessors/#vectorbt.ohlcv_accessors.OHLCVDFAccessor.metrics","text":"Metrics supported by OHLCVDFAccessor . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2afbf8>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2afc80>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2afd08>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"first_price\" : { \"title\" : \"First Price\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2afd90>\" , \"resolve_ohlc\" : true , \"tags\" : [ \"ohlcv\" , \"ohlc\" ] }, \"lowest_price\" : { \"title\" : \"Lowest Price\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2afe18>\" , \"resolve_ohlc\" : true , \"tags\" : [ \"ohlcv\" , \"ohlc\" ] }, \"highest_price\" : { \"title\" : \"Highest Price\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2afea0>\" , \"resolve_ohlc\" : true , \"tags\" : [ \"ohlcv\" , \"ohlc\" ] }, \"last_price\" : { \"title\" : \"Last Price\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2aff28>\" , \"resolve_ohlc\" : true , \"tags\" : [ \"ohlcv\" , \"ohlc\" ] }, \"first_volume\" : { \"title\" : \"First Volume\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2b7048>\" , \"resolve_volume\" : true , \"tags\" : [ \"ohlcv\" , \"volume\" ] }, \"lowest_volume\" : { \"title\" : \"Lowest Volume\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2b70d0>\" , \"resolve_volume\" : true , \"tags\" : [ \"ohlcv\" , \"volume\" ] }, \"highest_volume\" : { \"title\" : \"Highest Volume\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2b7158>\" , \"resolve_volume\" : true , \"tags\" : [ \"ohlcv\" , \"volume\" ] }, \"last_volume\" : { \"title\" : \"Last Volume\" , \"calc_func\" : \"<function OHLCVDFAccessor.<lambda> at 0x7facbe2b71e0>\" , \"resolve_volume\" : true , \"tags\" : [ \"ohlcv\" , \"volume\" ] } } ) Returns OHLCVDFAccessor._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable OHLCVDFAccessor._metrics .","title":"metrics"},{"location":"api/ohlcv_accessors/#vectorbt.ohlcv_accessors.OHLCVDFAccessor.ohlc","text":"Open, high, low, and close series.","title":"ohlc"},{"location":"api/ohlcv_accessors/#vectorbt.ohlcv_accessors.OHLCVDFAccessor.open","text":"Open series.","title":"open"},{"location":"api/ohlcv_accessors/#vectorbt.ohlcv_accessors.OHLCVDFAccessor.plot","text":"OHLCVDFAccessor . plot ( plot_type = None , show_volume = None , ohlc_kwargs = None , volume_kwargs = None , ohlc_add_trace_kwargs = None , volume_add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot OHLCV data. Args plot_type Either 'OHLC', 'Candlestick' or Plotly trace. Pass None to use the default. show_volume :\u2002 bool If True, shows volume as bar chart. ohlc_kwargs :\u2002 dict Keyword arguments passed to plot_type . volume_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Bar . ohlc_add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace for OHLC. volume_add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace for volume. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import vectorbt as vbt >>> vbt . YFData . download ( \"BTC-USD\" ) . get () . vbt . ohlcv . plot ()","title":"plot()"},{"location":"api/ohlcv_accessors/#vectorbt.ohlcv_accessors.OHLCVDFAccessor.plots_defaults","text":"Defaults for PlotsBuilderMixin.plots() . Merges GenericAccessor.plots_defaults and ohlcv.plots from settings .","title":"plots_defaults"},{"location":"api/ohlcv_accessors/#vectorbt.ohlcv_accessors.OHLCVDFAccessor.stats_defaults","text":"Defaults for StatsBuilderMixin.stats() . Merges GenericAccessor.stats_defaults and ohlcv.stats from settings .","title":"stats_defaults"},{"location":"api/ohlcv_accessors/#vectorbt.ohlcv_accessors.OHLCVDFAccessor.subplots","text":"Subplots supported by OHLCVDFAccessor . Co nf ig( { \"plot\" : { \"title\" : \"OHLC\" , \"xaxis_kwargs\" : { \"showgrid\" : true , \"rangeslider_visible\" : false }, \"yaxis_kwargs\" : { \"showgrid\" : true }, \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"show_volume\" : false , \"tags\" : \"ohlcv\" } } ) Returns OHLCVDFAccessor._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable OHLCVDFAccessor._subplots .","title":"subplots"},{"location":"api/ohlcv_accessors/#vectorbt.ohlcv_accessors.OHLCVDFAccessor.volume","text":"Volume series.","title":"volume"},{"location":"api/px_accessors/","text":"px_accessors module \u00b6 Plotly Express pandas accessors. Note Accessors do not utilize caching. attach_px_methods function \u00b6 attach_px_methods ( cls ) Class decorator to attach Plotly Express methods. PXAccessor class \u00b6 Accessor for running Plotly Express functions. Accessible through pd.Series.vbt.px and pd.DataFrame.vbt.px . Usage >>> import pandas as pd >>> import vectorbt as vbt >>> vbt . settings . set_theme ( 'seaborn' ) >>> pd . Series ([ 1 , 2 , 3 ]) . vbt . px . bar () Superclasses AttrResolver BaseAccessor Configured Documented IndexingBase PandasIndexer Pickleable Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.config BaseAccessor.df_accessor_cls BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.iloc BaseAccessor.indexing_func() BaseAccessor.indexing_kwargs BaseAccessor.loc BaseAccessor.make_symmetric() BaseAccessor.obj BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.self_aliases BaseAccessor.sr_accessor_cls BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() BaseAccessor.wrapper BaseAccessor.writeable_attrs Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses PXDFAccessor PXSRAccessor area method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) bar method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) bar_polar method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) box method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) choropleth method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) choropleth_mapbox method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) density_contour method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) density_heatmap method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) density_mapbox method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) ecdf method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) funnel method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) funnel_area method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) histogram method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) icicle method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) imshow method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) line method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) line_3d method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) line_geo method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) line_mapbox method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) line_polar method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) line_ternary method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) parallel_categories method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) parallel_coordinates method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) pie method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) scatter method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) scatter_3d method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) scatter_geo method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) scatter_mapbox method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) scatter_matrix method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) scatter_polar method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) scatter_ternary method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) strip method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) sunburst method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) timeline method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) treemap method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) violin method \u00b6 attach_px_methods .< locals >. plot_func ( * args , ** kwargs ) PXDFAccessor class \u00b6 Accessor for running Plotly Express functions. For DataFrames only. Accessible through pd.DataFrame.vbt.px . Superclasses AttrResolver BaseAccessor BaseDFAccessor Configured Documented IndexingBase PXAccessor PandasIndexer Pickleable Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PXAccessor.config PXAccessor.df_accessor_cls PXAccessor.iloc PXAccessor.indexing_kwargs PXAccessor.loc PXAccessor.obj PXAccessor.self_aliases PXAccessor.sr_accessor_cls PXAccessor.wrapper PXAccessor.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() PXSRAccessor class \u00b6 Accessor for running Plotly Express functions. For Series only. Accessible through pd.Series.vbt.px . Superclasses AttrResolver BaseAccessor BaseSRAccessor Configured Documented IndexingBase PXAccessor PandasIndexer Pickleable Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PXAccessor.config PXAccessor.df_accessor_cls PXAccessor.iloc PXAccessor.indexing_kwargs PXAccessor.loc PXAccessor.obj PXAccessor.self_aliases PXAccessor.sr_accessor_cls PXAccessor.wrapper PXAccessor.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"px_accessors"},{"location":"api/px_accessors/#vectorbt.px_accessors","text":"Plotly Express pandas accessors. Note Accessors do not utilize caching.","title":"vectorbt.px_accessors"},{"location":"api/px_accessors/#vectorbt.px_accessors.attach_px_methods","text":"attach_px_methods ( cls ) Class decorator to attach Plotly Express methods.","title":"attach_px_methods()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor","text":"Accessor for running Plotly Express functions. Accessible through pd.Series.vbt.px and pd.DataFrame.vbt.px . Usage >>> import pandas as pd >>> import vectorbt as vbt >>> vbt . settings . set_theme ( 'seaborn' ) >>> pd . Series ([ 1 , 2 , 3 ]) . vbt . px . bar () Superclasses AttrResolver BaseAccessor Configured Documented IndexingBase PandasIndexer Pickleable Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.config BaseAccessor.df_accessor_cls BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.iloc BaseAccessor.indexing_func() BaseAccessor.indexing_kwargs BaseAccessor.loc BaseAccessor.make_symmetric() BaseAccessor.obj BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.self_aliases BaseAccessor.sr_accessor_cls BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() BaseAccessor.wrapper BaseAccessor.writeable_attrs Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses PXDFAccessor PXSRAccessor","title":"PXAccessor"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.area","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"area()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.bar","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"bar()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.bar_polar","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"bar_polar()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.box","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"box()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.choropleth","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"choropleth()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.choropleth_mapbox","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"choropleth_mapbox()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.density_contour","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"density_contour()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.density_heatmap","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"density_heatmap()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.density_mapbox","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"density_mapbox()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.ecdf","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"ecdf()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.funnel","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"funnel()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.funnel_area","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"funnel_area()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.histogram","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"histogram()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.icicle","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"icicle()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.imshow","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"imshow()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.line","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"line()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.line_3d","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"line_3d()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.line_geo","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"line_geo()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.line_mapbox","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"line_mapbox()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.line_polar","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"line_polar()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.line_ternary","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"line_ternary()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.parallel_categories","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"parallel_categories()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.parallel_coordinates","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"parallel_coordinates()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.pie","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"pie()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.scatter","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"scatter()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.scatter_3d","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"scatter_3d()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.scatter_geo","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"scatter_geo()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.scatter_mapbox","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"scatter_mapbox()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.scatter_matrix","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"scatter_matrix()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.scatter_polar","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"scatter_polar()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.scatter_ternary","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"scatter_ternary()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.strip","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"strip()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.sunburst","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"sunburst()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.timeline","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"timeline()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.treemap","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"treemap()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXAccessor.violin","text":"attach_px_methods .< locals >. plot_func ( * args , ** kwargs )","title":"violin()"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXDFAccessor","text":"Accessor for running Plotly Express functions. For DataFrames only. Accessible through pd.DataFrame.vbt.px . Superclasses AttrResolver BaseAccessor BaseDFAccessor Configured Documented IndexingBase PXAccessor PandasIndexer Pickleable Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PXAccessor.config PXAccessor.df_accessor_cls PXAccessor.iloc PXAccessor.indexing_kwargs PXAccessor.loc PXAccessor.obj PXAccessor.self_aliases PXAccessor.sr_accessor_cls PXAccessor.wrapper PXAccessor.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"PXDFAccessor"},{"location":"api/px_accessors/#vectorbt.px_accessors.PXSRAccessor","text":"Accessor for running Plotly Express functions. For Series only. Accessible through pd.Series.vbt.px . Superclasses AttrResolver BaseAccessor BaseSRAccessor Configured Documented IndexingBase PXAccessor PandasIndexer Pickleable Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PXAccessor.config PXAccessor.df_accessor_cls PXAccessor.iloc PXAccessor.indexing_kwargs PXAccessor.loc PXAccessor.obj PXAccessor.self_aliases PXAccessor.sr_accessor_cls PXAccessor.wrapper PXAccessor.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"PXSRAccessor"},{"location":"api/root_accessors/","text":"root_accessors module \u00b6 Root pandas accessors. An accessor adds additional \u201cnamespace\u201d to pandas objects. The vectorbt.root_accessors registers a custom vbt accessor on top of each pd.Series and pd.DataFrame object. It is the main entry point for all other accessors: vbt.base.accessors.BaseSR/DFAccessor -> pd.Series/DataFrame.vbt.* vbt.generic.accessors.GenericSR/DFAccessor -> pd.Series/DataFrame.vbt.* vbt.signals.accessors.SignalsSR/DFAccessor -> pd.Series/DataFrame.vbt.signals.* vbt.returns.accessors.ReturnsSR/DFAccessor -> pd.Series/DataFrame.vbt.returns.* vbt.ohlcv.accessors.OHLCVDFAccessor -> pd.DataFrame.vbt.ohlc.* and pd.DataFrame.vbt.ohlcv.* vbt.px_accessors.PXAccessor -> pd.DataFrame.vbt.px.* Additionally, some accessors subclass other accessors building the following inheritance hiearchy: vbt.base.accessors.BaseSR/DFAccessor -> vbt.generic.accessors.GenericSR/DFAccessor -> vbt.cat_accessors.CatSR/DFAccessor -> vbt.signals.accessors.SignalsSR/DFAccessor -> vbt.returns.accessors.ReturnsSR/DFAccessor -> vbt.ohlcv_accessors.OHLCVDFAccessor -> vbt.px_accessors.PXSR/DFAccessor So, for example, the method pd.Series.vbt.to_2d_array is also available as pd.Series.vbt.returns.to_2d_array . Note Accessors in vectorbt are not cached, so querying df.vbt twice will also call Vbt_DFAccessor twice. register_accessor function \u00b6 register_accessor ( name , cls ) Register a custom accessor. cls should subclass pandas.core.accessor.DirNamesMixin . register_dataframe_accessor function \u00b6 register_dataframe_accessor ( name ) Decorator to register a custom pd.DataFrame accessor on top of the pd.DataFrame . register_dataframe_vbt_accessor function \u00b6 register_dataframe_vbt_accessor ( name , parent = vectorbt . root_accessors . Vbt_DFAccessor ) Decorator to register a pd.DataFrame accessor on top of a parent accessor. register_series_accessor function \u00b6 register_series_accessor ( name ) Decorator to register a custom pd.Series accessor on top of the pd.Series . register_series_vbt_accessor function \u00b6 register_series_vbt_accessor ( name , parent = vectorbt . root_accessors . Vbt_SRAccessor ) Decorator to register a pd.Series accessor on top of a parent accessor. Accessor class \u00b6 Custom property-like object. Note In contrast to other pandas accessors, this accessor is not cached! This prevents from using old data if the object has been changed in-place. Vbt_DFAccessor class \u00b6 The main vectorbt accessor for pd.DataFrame . Superclasses AttrResolver BaseAccessor BaseDFAccessor Configured Documented GenericAccessor GenericDFAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping pandas.core.accessor.DirNamesMixin Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.bshift() GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.diff() GenericAccessor.drawdown() GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.fshift() GenericAccessor.get_drawdowns() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.lineplot() GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.pct_change() GenericAccessor.plot() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.resolve_self() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.zscore() GenericDFAccessor.config GenericDFAccessor.df_accessor_cls GenericDFAccessor.drawdowns GenericDFAccessor.flatten_grouped() GenericDFAccessor.heatmap() GenericDFAccessor.iloc GenericDFAccessor.indexing_kwargs GenericDFAccessor.loc GenericDFAccessor.mapping GenericDFAccessor.obj GenericDFAccessor.plots_defaults GenericDFAccessor.ranges GenericDFAccessor.self_aliases GenericDFAccessor.squeeze_grouped() GenericDFAccessor.sr_accessor_cls GenericDFAccessor.stats_defaults GenericDFAccessor.ts_heatmap() GenericDFAccessor.wrapper GenericDFAccessor.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj() ohlc class variable \u00b6 Accessor on top of OHLCV data. For DataFrames only. Accessible through pd.DataFrame.vbt.ohlcv . ohlcv class variable \u00b6 Accessor on top of OHLCV data. For DataFrames only. Accessible through pd.DataFrame.vbt.ohlcv . px class variable \u00b6 Accessor for running Plotly Express functions. For DataFrames only. Accessible through pd.DataFrame.vbt.px . returns class variable \u00b6 Accessor on top of return series. For DataFrames only. Accessible through pd.DataFrame.vbt.returns . signals class variable \u00b6 Accessor on top of signal series. For DataFrames only. Accessible through pd.DataFrame.vbt.signals . Vbt_SRAccessor class \u00b6 The main vectorbt accessor for pd.Series . Superclasses AttrResolver BaseAccessor BaseSRAccessor Configured Documented GenericAccessor GenericSRAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping pandas.core.accessor.DirNamesMixin Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.bshift() GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.diff() GenericAccessor.drawdown() GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.fshift() GenericAccessor.get_drawdowns() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.lineplot() GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.pct_change() GenericAccessor.plot() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.resolve_self() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.zscore() GenericSRAccessor.config GenericSRAccessor.df_accessor_cls GenericSRAccessor.drawdowns GenericSRAccessor.flatten_grouped() GenericSRAccessor.heatmap() GenericSRAccessor.iloc GenericSRAccessor.indexing_kwargs GenericSRAccessor.loc GenericSRAccessor.mapping GenericSRAccessor.obj GenericSRAccessor.overlay_with_heatmap() GenericSRAccessor.plot_against() GenericSRAccessor.plots_defaults GenericSRAccessor.qqplot() GenericSRAccessor.ranges GenericSRAccessor.self_aliases GenericSRAccessor.squeeze_grouped() GenericSRAccessor.sr_accessor_cls GenericSRAccessor.stats_defaults GenericSRAccessor.ts_heatmap() GenericSRAccessor.volume() GenericSRAccessor.wrapper GenericSRAccessor.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj() px class variable \u00b6 Accessor for running Plotly Express functions. For Series only. Accessible through pd.Series.vbt.px . returns class variable \u00b6 Accessor on top of return series. For Series only. Accessible through pd.Series.vbt.returns . signals class variable \u00b6 Accessor on top of signal series. For Series only. Accessible through pd.Series.vbt.signals .","title":"root_accessors"},{"location":"api/root_accessors/#vectorbt.root_accessors","text":"Root pandas accessors. An accessor adds additional \u201cnamespace\u201d to pandas objects. The vectorbt.root_accessors registers a custom vbt accessor on top of each pd.Series and pd.DataFrame object. It is the main entry point for all other accessors: vbt.base.accessors.BaseSR/DFAccessor -> pd.Series/DataFrame.vbt.* vbt.generic.accessors.GenericSR/DFAccessor -> pd.Series/DataFrame.vbt.* vbt.signals.accessors.SignalsSR/DFAccessor -> pd.Series/DataFrame.vbt.signals.* vbt.returns.accessors.ReturnsSR/DFAccessor -> pd.Series/DataFrame.vbt.returns.* vbt.ohlcv.accessors.OHLCVDFAccessor -> pd.DataFrame.vbt.ohlc.* and pd.DataFrame.vbt.ohlcv.* vbt.px_accessors.PXAccessor -> pd.DataFrame.vbt.px.* Additionally, some accessors subclass other accessors building the following inheritance hiearchy: vbt.base.accessors.BaseSR/DFAccessor -> vbt.generic.accessors.GenericSR/DFAccessor -> vbt.cat_accessors.CatSR/DFAccessor -> vbt.signals.accessors.SignalsSR/DFAccessor -> vbt.returns.accessors.ReturnsSR/DFAccessor -> vbt.ohlcv_accessors.OHLCVDFAccessor -> vbt.px_accessors.PXSR/DFAccessor So, for example, the method pd.Series.vbt.to_2d_array is also available as pd.Series.vbt.returns.to_2d_array . Note Accessors in vectorbt are not cached, so querying df.vbt twice will also call Vbt_DFAccessor twice.","title":"vectorbt.root_accessors"},{"location":"api/root_accessors/#vectorbt.root_accessors.register_accessor","text":"register_accessor ( name , cls ) Register a custom accessor. cls should subclass pandas.core.accessor.DirNamesMixin .","title":"register_accessor()"},{"location":"api/root_accessors/#vectorbt.root_accessors.register_dataframe_accessor","text":"register_dataframe_accessor ( name ) Decorator to register a custom pd.DataFrame accessor on top of the pd.DataFrame .","title":"register_dataframe_accessor()"},{"location":"api/root_accessors/#vectorbt.root_accessors.register_dataframe_vbt_accessor","text":"register_dataframe_vbt_accessor ( name , parent = vectorbt . root_accessors . Vbt_DFAccessor ) Decorator to register a pd.DataFrame accessor on top of a parent accessor.","title":"register_dataframe_vbt_accessor()"},{"location":"api/root_accessors/#vectorbt.root_accessors.register_series_accessor","text":"register_series_accessor ( name ) Decorator to register a custom pd.Series accessor on top of the pd.Series .","title":"register_series_accessor()"},{"location":"api/root_accessors/#vectorbt.root_accessors.register_series_vbt_accessor","text":"register_series_vbt_accessor ( name , parent = vectorbt . root_accessors . Vbt_SRAccessor ) Decorator to register a pd.Series accessor on top of a parent accessor.","title":"register_series_vbt_accessor()"},{"location":"api/root_accessors/#vectorbt.root_accessors.Accessor","text":"Custom property-like object. Note In contrast to other pandas accessors, this accessor is not cached! This prevents from using old data if the object has been changed in-place.","title":"Accessor"},{"location":"api/root_accessors/#vectorbt.root_accessors.Vbt_DFAccessor","text":"The main vectorbt accessor for pd.DataFrame . Superclasses AttrResolver BaseAccessor BaseDFAccessor Configured Documented GenericAccessor GenericDFAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping pandas.core.accessor.DirNamesMixin Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.bshift() GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.diff() GenericAccessor.drawdown() GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.fshift() GenericAccessor.get_drawdowns() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.lineplot() GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.pct_change() GenericAccessor.plot() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.resolve_self() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.zscore() GenericDFAccessor.config GenericDFAccessor.df_accessor_cls GenericDFAccessor.drawdowns GenericDFAccessor.flatten_grouped() GenericDFAccessor.heatmap() GenericDFAccessor.iloc GenericDFAccessor.indexing_kwargs GenericDFAccessor.loc GenericDFAccessor.mapping GenericDFAccessor.obj GenericDFAccessor.plots_defaults GenericDFAccessor.ranges GenericDFAccessor.self_aliases GenericDFAccessor.squeeze_grouped() GenericDFAccessor.sr_accessor_cls GenericDFAccessor.stats_defaults GenericDFAccessor.ts_heatmap() GenericDFAccessor.wrapper GenericDFAccessor.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"Vbt_DFAccessor"},{"location":"api/root_accessors/#vectorbt.root_accessors.Vbt_DFAccessor.ohlc","text":"Accessor on top of OHLCV data. For DataFrames only. Accessible through pd.DataFrame.vbt.ohlcv .","title":"ohlc"},{"location":"api/root_accessors/#vectorbt.root_accessors.Vbt_DFAccessor.ohlcv","text":"Accessor on top of OHLCV data. For DataFrames only. Accessible through pd.DataFrame.vbt.ohlcv .","title":"ohlcv"},{"location":"api/root_accessors/#vectorbt.root_accessors.Vbt_DFAccessor.px","text":"Accessor for running Plotly Express functions. For DataFrames only. Accessible through pd.DataFrame.vbt.px .","title":"px"},{"location":"api/root_accessors/#vectorbt.root_accessors.Vbt_DFAccessor.returns","text":"Accessor on top of return series. For DataFrames only. Accessible through pd.DataFrame.vbt.returns .","title":"returns"},{"location":"api/root_accessors/#vectorbt.root_accessors.Vbt_DFAccessor.signals","text":"Accessor on top of signal series. For DataFrames only. Accessible through pd.DataFrame.vbt.signals .","title":"signals"},{"location":"api/root_accessors/#vectorbt.root_accessors.Vbt_SRAccessor","text":"The main vectorbt accessor for pd.Series . Superclasses AttrResolver BaseAccessor BaseSRAccessor Configured Documented GenericAccessor GenericSRAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping pandas.core.accessor.DirNamesMixin Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.bshift() GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.diff() GenericAccessor.drawdown() GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.fshift() GenericAccessor.get_drawdowns() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.lineplot() GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.pct_change() GenericAccessor.plot() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.resolve_self() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.zscore() GenericSRAccessor.config GenericSRAccessor.df_accessor_cls GenericSRAccessor.drawdowns GenericSRAccessor.flatten_grouped() GenericSRAccessor.heatmap() GenericSRAccessor.iloc GenericSRAccessor.indexing_kwargs GenericSRAccessor.loc GenericSRAccessor.mapping GenericSRAccessor.obj GenericSRAccessor.overlay_with_heatmap() GenericSRAccessor.plot_against() GenericSRAccessor.plots_defaults GenericSRAccessor.qqplot() GenericSRAccessor.ranges GenericSRAccessor.self_aliases GenericSRAccessor.squeeze_grouped() GenericSRAccessor.sr_accessor_cls GenericSRAccessor.stats_defaults GenericSRAccessor.ts_heatmap() GenericSRAccessor.volume() GenericSRAccessor.wrapper GenericSRAccessor.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"Vbt_SRAccessor"},{"location":"api/root_accessors/#vectorbt.root_accessors.Vbt_SRAccessor.px","text":"Accessor for running Plotly Express functions. For Series only. Accessible through pd.Series.vbt.px .","title":"px"},{"location":"api/root_accessors/#vectorbt.root_accessors.Vbt_SRAccessor.returns","text":"Accessor on top of return series. For Series only. Accessible through pd.Series.vbt.returns .","title":"returns"},{"location":"api/root_accessors/#vectorbt.root_accessors.Vbt_SRAccessor.signals","text":"Accessor on top of signal series. For Series only. Accessible through pd.Series.vbt.signals .","title":"signals"},{"location":"api/base/","text":"base package \u00b6 Modules with base classes and utilities for pandas objects, such as broadcasting. Sub-modules \u00b6 vectorbt.base.accessors vectorbt.base.array_wrapper vectorbt.base.column_grouper vectorbt.base.combine_fns vectorbt.base.index_fns vectorbt.base.indexing vectorbt.base.reshape_fns","title":"base"},{"location":"api/base/#vectorbt.base","text":"Modules with base classes and utilities for pandas objects, such as broadcasting.","title":"vectorbt.base"},{"location":"api/base/#sub-modules","text":"vectorbt.base.accessors vectorbt.base.array_wrapper vectorbt.base.column_grouper vectorbt.base.combine_fns vectorbt.base.index_fns vectorbt.base.indexing vectorbt.base.reshape_fns","title":"Sub-modules"},{"location":"api/base/accessors/","text":"accessors module \u00b6 Custom pandas accessors. Methods can be accessed as follows: BaseSRAccessor -> pd.Series.vbt.* BaseDFAccessor -> pd.DataFrame.vbt.* For example: >>> import pandas as pd >>> import vectorbt as vbt >>> # vectorbt.base.accessors.BaseAccessor.make_symmetric >>> pd . Series ([ 1 , 2 , 3 ]) . vbt . make_symmetric () 0 1 2 0 1.0 2.0 3.0 1 2.0 NaN NaN 2 3.0 NaN NaN It contains base methods for working with pandas objects. Most of these methods are adaptations of combine/reshape/index functions that can work with pandas objects. For example, broadcast() can take an arbitrary number of pandas objects, thus you can find its variations as accessor methods. >>> sr = pd . Series ([ 1 ]) >>> df = pd . DataFrame ([ 1 , 2 , 3 ]) >>> vbt . base . reshape_fns . broadcast_to ( sr , df ) 0 0 1 1 1 2 1 >>> sr . vbt . broadcast_to ( df ) 0 0 1 1 1 2 1 Additionally, BaseAccessor implements arithmetic (such as + ), comparison (such as > ) and logical operators (such as & ) by doing 1) NumPy-like broadcasting and 2) the compuation with NumPy under the hood, which is mostly much faster than with pandas. >>> df = pd . DataFrame ( np . random . uniform ( size = ( 1000 , 1000 ))) >>> % timeit df * 2 # pandas 296 ms \u00b1 27.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) >>> % timeit df . vbt * 2 # vectorbt 5.48 ms \u00b1 1.12 ms per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) Note You should ensure that your *.vbt operand is on the left if the other operand is an array. Accessors do not utilize caching. Grouping is only supported by the methods that accept the group_by argument. BaseAccessor class \u00b6 Accessor on top of Series and DataFrames. Accessible through pd.Series.vbt and pd.DataFrame.vbt , and all child accessors. Series is just a DataFrame with one column, hence to avoid defining methods exclusively for 1-dim data, we will convert any Series to a DataFrame and perform matrix computation on it. Afterwards, by using BaseAccessor.wrapper , we will convert the 2-dim output back to a Series. **kwargs will be passed to ArrayWrapper . Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() Wrapping.config Wrapping.iloc Wrapping.indexing_kwargs Wrapping.loc Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Wrapping.self_aliases Wrapping.wrapper Wrapping.writeable_attrs Subclasses BaseDFAccessor BaseSRAccessor GenericAccessor PXAccessor align_to method \u00b6 BaseAccessor . align_to ( other , wrap_kwargs = None ) Align to other on their axes. Usage >>> import vectorbt as vbt >>> import pandas as pd >>> df1 = pd . DataFrame ([[ 1 , 2 ], [ 3 , 4 ]], index = [ 'x' , 'y' ], columns = [ 'a' , 'b' ]) >>> df1 a b x 1 2 y 3 4 >>> df2 = pd . DataFrame ([[ 5 , 6 , 7 , 8 ], [ 9 , 10 , 11 , 12 ]], index = [ 'x' , 'y' ], ... columns = pd . MultiIndex . from_arrays ([[ 1 , 1 , 2 , 2 ], [ 'a' , 'b' , 'a' , 'b' ]])) >>> df2 1 2 a b a b x 5 6 7 8 y 9 10 11 12 >>> df1 . vbt . align_to ( df2 ) 1 2 a b a b x 1 2 1 2 y 3 4 3 4 apply method \u00b6 BaseAccessor . apply ( * args , apply_func = None , keep_pd = False , to_2d = False , wrap_kwargs = None , ** kwargs ) Apply a function apply_func . Args *args Variable arguments passed to apply_func . apply_func :\u2002 callable Apply function. Can be Numba-compiled. keep_pd :\u2002 bool Whether to keep inputs as pandas objects, otherwise convert to NumPy arrays. to_2d :\u2002 bool Whether to reshape inputs to 2-dim arrays, otherwise keep as-is. wrap_kwargs :\u2002 dict Keyword arguments passed to ArrayWrapper.wrap() . **kwargs Keyword arguments passed to combine_func . Note The resulted array must have the same shape as the original array. Usage >>> import vectorbt as vbt >>> import pandas as pd >>> sr = pd . Series ([ 1 , 2 ], index = [ 'x' , 'y' ]) >>> sr2 . vbt . apply ( apply_func = lambda x : x ** 2 ) i2 x2 1 y2 4 z2 9 Name: a2, dtype: int64 apply_and_concat method \u00b6 BaseAccessor . apply_and_concat ( ntimes , * args , apply_func = None , keep_pd = False , to_2d = False , numba_loop = False , use_ray = False , keys = None , wrap_kwargs = None , ** kwargs ) Apply apply_func ntimes times and concatenate the results along columns. See apply_and_concat_one() . Args ntimes :\u2002 int Number of times to call apply_func . *args Variable arguments passed to apply_func . apply_func :\u2002 callable Apply function. Can be Numba-compiled. keep_pd :\u2002 bool Whether to keep inputs as pandas objects, otherwise convert to NumPy arrays. to_2d :\u2002 bool Whether to reshape inputs to 2-dim arrays, otherwise keep as-is. numba_loop :\u2002 bool Whether to loop using Numba. Set to True when iterating large number of times over small input, but note that Numba doesn't support variable keyword arguments. use_ray :\u2002 bool Whether to use Ray to execute combine_func in parallel. Only works with numba_loop set to False and concat is set to True. See ray_apply() for related keyword arguments. keys :\u2002 index_like Outermost column level. wrap_kwargs :\u2002 dict Keyword arguments passed to ArrayWrapper.wrap() . **kwargs Keyword arguments passed to combine_func . Note The resulted arrays to be concatenated must have the same shape as broadcast input arrays. Usage >>> import vectorbt as vbt >>> import pandas as pd >>> df = pd . DataFrame ([[ 3 , 4 ], [ 5 , 6 ]], index = [ 'x' , 'y' ], columns = [ 'a' , 'b' ]) >>> df . vbt . apply_and_concat ( 3 , [ 1 , 2 , 3 ], ... apply_func = lambda i , a , b : a * b [ i ], keys = [ 'c' , 'd' , 'e' ]) c d e a b a b a b x 3 4 6 8 9 12 y 5 6 10 12 15 18 Use Ray for small inputs and large processing times: >>> def apply_func ( i , a ): ... time . sleep ( 1 ) ... return a >>> sr = pd . Series ([ 1 , 2 , 3 ]) >>> % timeit sr . vbt . apply_and_concat ( 3 , apply_func = apply_func ) 3.01 s \u00b1 2.15 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) >>> % timeit sr . vbt . apply_and_concat ( 3 , apply_func = apply_func , use_ray = True ) 1.01 s \u00b1 2.31 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) apply_on_index method \u00b6 BaseAccessor . apply_on_index ( apply_func , * args , axis = 1 , inplace = False , ** kwargs ) Apply function apply_func on index of the pandas object. Set axis to 1 for columns and 0 for index. If inplace is True, modifies the pandas object. Otherwise, returns a copy. broadcast class method \u00b6 BaseAccessor . broadcast ( * others , ** kwargs ) See broadcast() . broadcast_to method \u00b6 BaseAccessor . broadcast_to ( other , ** kwargs ) See broadcast_to() . combine method \u00b6 BaseAccessor . combine ( other , * args , allow_multiple = True , combine_func = None , keep_pd = False , to_2d = False , concat = False , numba_loop = False , use_ray = False , broadcast = True , broadcast_kwargs = None , keys = None , wrap_kwargs = None , ** kwargs ) Combine with other using combine_func . Args other :\u2002 array_like Object to combine this array with. *args Variable arguments passed to combine_func . allow_multiple :\u2002 bool Whether a tuple/list will be considered as multiple objects in other . combine_func :\u2002 callable Function to combine two arrays. Can be Numba-compiled. keep_pd :\u2002 bool Whether to keep inputs as pandas objects, otherwise convert to NumPy arrays. to_2d :\u2002 bool Whether to reshape inputs to 2-dim arrays, otherwise keep as-is. concat :\u2002 bool Whether to concatenate the results along the column axis. Otherwise, pairwise combine into a Series/DataFrame of the same shape. If True, see combine_and_concat() . If False, see combine_multiple() . numba_loop :\u2002 bool Whether to loop using Numba. Set to True when iterating large number of times over small input, but note that Numba doesn't support variable keyword arguments. use_ray :\u2002 bool Whether to use Ray to execute combine_func in parallel. Only works with numba_loop set to False and concat is set to True. See ray_apply() for related keyword arguments. broadcast :\u2002 bool Whether to broadcast all inputs. broadcast_kwargs :\u2002 dict Keyword arguments passed to broadcast() . keys :\u2002 index_like Outermost column level. wrap_kwargs :\u2002 dict Keyword arguments passed to ArrayWrapper.wrap() . **kwargs Keyword arguments passed to combine_func . Note If combine_func is Numba-compiled, will broadcast using WRITEABLE and C_CONTIGUOUS flags, which can lead to an expensive computation overhead if passed objects are large and have different shape/memory order. You also must ensure that all objects have the same data type. Also remember to bring each in *args to a Numba-compatible format. Usage >>> import vectorbt as vbt >>> import pandas as pd >>> sr = pd . Series ([ 1 , 2 ], index = [ 'x' , 'y' ]) >>> df = pd . DataFrame ([[ 3 , 4 ], [ 5 , 6 ]], index = [ 'x' , 'y' ], columns = [ 'a' , 'b' ]) >>> sr . vbt . combine ( df , combine_func = lambda x , y : x + y ) a b x 4 5 y 7 8 >>> sr . vbt . combine ([ df , df * 2 ], combine_func = lambda x , y : x + y ) a b x 10 13 y 17 20 >>> sr . vbt . combine ([ df , df * 2 ], combine_func = lambda x , y : x + y , concat = True , keys = [ 'c' , 'd' ]) c d a b a b x 4 5 7 9 y 7 8 12 14 Use Ray for small inputs and large processing times: >>> def combine_func ( a , b ): ... time . sleep ( 1 ) ... return a + b >>> sr = pd . Series ([ 1 , 2 , 3 ]) >>> % timeit sr . vbt . combine ([ 1 , 1 , 1 ], combine_func = combine_func ) 3.01 s \u00b1 2.98 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) >>> % timeit sr . vbt . combine ([ 1 , 1 , 1 ], combine_func = combine_func , concat = True , use_ray = True ) 1.02 s \u00b1 2.32 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) concat class method \u00b6 BaseAccessor . concat ( * others , broadcast_kwargs = None , keys = None ) Concatenate with others along columns. Args *others :\u2002 array_like List of objects to be concatenated with this array. broadcast_kwargs :\u2002 dict Keyword arguments passed to broadcast() . keys :\u2002 index_like Outermost column level. Usage >>> import vectorbt as vbt >>> import pandas as pd >>> sr = pd . Series ([ 1 , 2 ], index = [ 'x' , 'y' ]) >>> df = pd . DataFrame ([[ 3 , 4 ], [ 5 , 6 ]], index = [ 'x' , 'y' ], columns = [ 'a' , 'b' ]) >>> sr . vbt . concat ( df , keys = [ 'c' , 'd' ]) c d a b a b x 1 1 3 4 y 2 2 5 6 df_accessor_cls property \u00b6 Accessor class for pd.DataFrame . drop_duplicate_levels method \u00b6 BaseAccessor . drop_duplicate_levels ( keep = None , axis = 1 , inplace = False ) See drop_duplicate_levels() . See BaseAccessor.apply_on_index() for other keyword arguments. drop_levels method \u00b6 BaseAccessor . drop_levels ( levels , axis = 1 , inplace = False , strict = True ) See drop_levels() . See BaseAccessor.apply_on_index() for other keyword arguments. drop_redundant_levels method \u00b6 BaseAccessor . drop_redundant_levels ( axis = 1 , inplace = False ) See drop_redundant_levels() . See BaseAccessor.apply_on_index() for other keyword arguments. empty class method \u00b6 BaseAccessor . empty ( shape , fill_value = nan , ** kwargs ) Generate an empty Series/DataFrame of shape shape and fill with fill_value . empty_like class method \u00b6 BaseAccessor . empty_like ( other , fill_value = nan , ** kwargs ) Generate an empty Series/DataFrame like other and fill with fill_value . indexing_func method \u00b6 BaseAccessor . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on BaseAccessor . is_frame class method \u00b6 BaseAccessor . is_frame () is_series class method \u00b6 BaseAccessor . is_series () make_symmetric method \u00b6 BaseAccessor . make_symmetric () See make_symmetric() . obj property \u00b6 Pandas object. rename_levels method \u00b6 BaseAccessor . rename_levels ( name_dict , axis = 1 , inplace = False , strict = True ) See rename_levels() . See BaseAccessor.apply_on_index() for other keyword arguments. repeat method \u00b6 BaseAccessor . repeat ( n , keys = None , axis = 1 , wrap_kwargs = None ) See repeat() . Set axis to 1 for columns and 0 for index. Use keys as the outermost level. select_levels method \u00b6 BaseAccessor . select_levels ( level_names , axis = 1 , inplace = False ) See select_levels() . See BaseAccessor.apply_on_index() for other keyword arguments. sr_accessor_cls property \u00b6 Accessor class for pd.Series . stack_index method \u00b6 BaseAccessor . stack_index ( index , on_top = True , axis = 1 , inplace = False , ** kwargs ) See stack_indexes() . Set on_top to False to stack at bottom. See BaseAccessor.apply_on_index() for other keyword arguments. tile method \u00b6 BaseAccessor . tile ( n , keys = None , axis = 1 , wrap_kwargs = None ) See tile() . Set axis to 1 for columns and 0 for index. Use keys as the outermost level. to_1d_array method \u00b6 BaseAccessor . to_1d_array () Convert to 1-dim NumPy array See to_1d() . to_2d_array method \u00b6 BaseAccessor . to_2d_array () Convert to 2-dim NumPy array. See to_2d() . to_dict method \u00b6 BaseAccessor . to_dict ( ** kwargs ) See to_dict() . unstack_to_array method \u00b6 BaseAccessor . unstack_to_array ( ** kwargs ) See unstack_to_array() . unstack_to_df method \u00b6 BaseAccessor . unstack_to_df ( ** kwargs ) See unstack_to_df() . BaseDFAccessor class \u00b6 Accessor on top of DataFrames. Accessible through pd.DataFrame.vbt and all child accessors. Superclasses AttrResolver BaseAccessor Configured Documented IndexingBase PandasIndexer Pickleable Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.config BaseAccessor.df_accessor_cls BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.iloc BaseAccessor.indexing_func() BaseAccessor.indexing_kwargs BaseAccessor.loc BaseAccessor.make_symmetric() BaseAccessor.obj BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.self_aliases BaseAccessor.sr_accessor_cls BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() BaseAccessor.wrapper BaseAccessor.writeable_attrs Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses GenericDFAccessor PXDFAccessor is_frame class method \u00b6 BaseDFAccessor . is_frame () is_series class method \u00b6 BaseDFAccessor . is_series () BaseSRAccessor class \u00b6 Accessor on top of Series. Accessible through pd.Series.vbt and all child accessors. Superclasses AttrResolver BaseAccessor Configured Documented IndexingBase PandasIndexer Pickleable Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.config BaseAccessor.df_accessor_cls BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.iloc BaseAccessor.indexing_func() BaseAccessor.indexing_kwargs BaseAccessor.loc BaseAccessor.make_symmetric() BaseAccessor.obj BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.self_aliases BaseAccessor.sr_accessor_cls BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() BaseAccessor.wrapper BaseAccessor.writeable_attrs Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses GenericSRAccessor PXSRAccessor is_frame class method \u00b6 BaseSRAccessor . is_frame () is_series class method \u00b6 BaseSRAccessor . is_series ()","title":"accessors"},{"location":"api/base/accessors/#vectorbt.base.accessors","text":"Custom pandas accessors. Methods can be accessed as follows: BaseSRAccessor -> pd.Series.vbt.* BaseDFAccessor -> pd.DataFrame.vbt.* For example: >>> import pandas as pd >>> import vectorbt as vbt >>> # vectorbt.base.accessors.BaseAccessor.make_symmetric >>> pd . Series ([ 1 , 2 , 3 ]) . vbt . make_symmetric () 0 1 2 0 1.0 2.0 3.0 1 2.0 NaN NaN 2 3.0 NaN NaN It contains base methods for working with pandas objects. Most of these methods are adaptations of combine/reshape/index functions that can work with pandas objects. For example, broadcast() can take an arbitrary number of pandas objects, thus you can find its variations as accessor methods. >>> sr = pd . Series ([ 1 ]) >>> df = pd . DataFrame ([ 1 , 2 , 3 ]) >>> vbt . base . reshape_fns . broadcast_to ( sr , df ) 0 0 1 1 1 2 1 >>> sr . vbt . broadcast_to ( df ) 0 0 1 1 1 2 1 Additionally, BaseAccessor implements arithmetic (such as + ), comparison (such as > ) and logical operators (such as & ) by doing 1) NumPy-like broadcasting and 2) the compuation with NumPy under the hood, which is mostly much faster than with pandas. >>> df = pd . DataFrame ( np . random . uniform ( size = ( 1000 , 1000 ))) >>> % timeit df * 2 # pandas 296 ms \u00b1 27.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) >>> % timeit df . vbt * 2 # vectorbt 5.48 ms \u00b1 1.12 ms per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) Note You should ensure that your *.vbt operand is on the left if the other operand is an array. Accessors do not utilize caching. Grouping is only supported by the methods that accept the group_by argument.","title":"vectorbt.base.accessors"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor","text":"Accessor on top of Series and DataFrames. Accessible through pd.Series.vbt and pd.DataFrame.vbt , and all child accessors. Series is just a DataFrame with one column, hence to avoid defining methods exclusively for 1-dim data, we will convert any Series to a DataFrame and perform matrix computation on it. Afterwards, by using BaseAccessor.wrapper , we will convert the 2-dim output back to a Series. **kwargs will be passed to ArrayWrapper . Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() Wrapping.config Wrapping.iloc Wrapping.indexing_kwargs Wrapping.loc Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Wrapping.self_aliases Wrapping.wrapper Wrapping.writeable_attrs Subclasses BaseDFAccessor BaseSRAccessor GenericAccessor PXAccessor","title":"BaseAccessor"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.align_to","text":"BaseAccessor . align_to ( other , wrap_kwargs = None ) Align to other on their axes. Usage >>> import vectorbt as vbt >>> import pandas as pd >>> df1 = pd . DataFrame ([[ 1 , 2 ], [ 3 , 4 ]], index = [ 'x' , 'y' ], columns = [ 'a' , 'b' ]) >>> df1 a b x 1 2 y 3 4 >>> df2 = pd . DataFrame ([[ 5 , 6 , 7 , 8 ], [ 9 , 10 , 11 , 12 ]], index = [ 'x' , 'y' ], ... columns = pd . MultiIndex . from_arrays ([[ 1 , 1 , 2 , 2 ], [ 'a' , 'b' , 'a' , 'b' ]])) >>> df2 1 2 a b a b x 5 6 7 8 y 9 10 11 12 >>> df1 . vbt . align_to ( df2 ) 1 2 a b a b x 1 2 1 2 y 3 4 3 4","title":"align_to()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.apply","text":"BaseAccessor . apply ( * args , apply_func = None , keep_pd = False , to_2d = False , wrap_kwargs = None , ** kwargs ) Apply a function apply_func . Args *args Variable arguments passed to apply_func . apply_func :\u2002 callable Apply function. Can be Numba-compiled. keep_pd :\u2002 bool Whether to keep inputs as pandas objects, otherwise convert to NumPy arrays. to_2d :\u2002 bool Whether to reshape inputs to 2-dim arrays, otherwise keep as-is. wrap_kwargs :\u2002 dict Keyword arguments passed to ArrayWrapper.wrap() . **kwargs Keyword arguments passed to combine_func . Note The resulted array must have the same shape as the original array. Usage >>> import vectorbt as vbt >>> import pandas as pd >>> sr = pd . Series ([ 1 , 2 ], index = [ 'x' , 'y' ]) >>> sr2 . vbt . apply ( apply_func = lambda x : x ** 2 ) i2 x2 1 y2 4 z2 9 Name: a2, dtype: int64","title":"apply()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.apply_and_concat","text":"BaseAccessor . apply_and_concat ( ntimes , * args , apply_func = None , keep_pd = False , to_2d = False , numba_loop = False , use_ray = False , keys = None , wrap_kwargs = None , ** kwargs ) Apply apply_func ntimes times and concatenate the results along columns. See apply_and_concat_one() . Args ntimes :\u2002 int Number of times to call apply_func . *args Variable arguments passed to apply_func . apply_func :\u2002 callable Apply function. Can be Numba-compiled. keep_pd :\u2002 bool Whether to keep inputs as pandas objects, otherwise convert to NumPy arrays. to_2d :\u2002 bool Whether to reshape inputs to 2-dim arrays, otherwise keep as-is. numba_loop :\u2002 bool Whether to loop using Numba. Set to True when iterating large number of times over small input, but note that Numba doesn't support variable keyword arguments. use_ray :\u2002 bool Whether to use Ray to execute combine_func in parallel. Only works with numba_loop set to False and concat is set to True. See ray_apply() for related keyword arguments. keys :\u2002 index_like Outermost column level. wrap_kwargs :\u2002 dict Keyword arguments passed to ArrayWrapper.wrap() . **kwargs Keyword arguments passed to combine_func . Note The resulted arrays to be concatenated must have the same shape as broadcast input arrays. Usage >>> import vectorbt as vbt >>> import pandas as pd >>> df = pd . DataFrame ([[ 3 , 4 ], [ 5 , 6 ]], index = [ 'x' , 'y' ], columns = [ 'a' , 'b' ]) >>> df . vbt . apply_and_concat ( 3 , [ 1 , 2 , 3 ], ... apply_func = lambda i , a , b : a * b [ i ], keys = [ 'c' , 'd' , 'e' ]) c d e a b a b a b x 3 4 6 8 9 12 y 5 6 10 12 15 18 Use Ray for small inputs and large processing times: >>> def apply_func ( i , a ): ... time . sleep ( 1 ) ... return a >>> sr = pd . Series ([ 1 , 2 , 3 ]) >>> % timeit sr . vbt . apply_and_concat ( 3 , apply_func = apply_func ) 3.01 s \u00b1 2.15 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) >>> % timeit sr . vbt . apply_and_concat ( 3 , apply_func = apply_func , use_ray = True ) 1.01 s \u00b1 2.31 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)","title":"apply_and_concat()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.apply_on_index","text":"BaseAccessor . apply_on_index ( apply_func , * args , axis = 1 , inplace = False , ** kwargs ) Apply function apply_func on index of the pandas object. Set axis to 1 for columns and 0 for index. If inplace is True, modifies the pandas object. Otherwise, returns a copy.","title":"apply_on_index()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.broadcast","text":"BaseAccessor . broadcast ( * others , ** kwargs ) See broadcast() .","title":"broadcast()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.broadcast_to","text":"BaseAccessor . broadcast_to ( other , ** kwargs ) See broadcast_to() .","title":"broadcast_to()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.combine","text":"BaseAccessor . combine ( other , * args , allow_multiple = True , combine_func = None , keep_pd = False , to_2d = False , concat = False , numba_loop = False , use_ray = False , broadcast = True , broadcast_kwargs = None , keys = None , wrap_kwargs = None , ** kwargs ) Combine with other using combine_func . Args other :\u2002 array_like Object to combine this array with. *args Variable arguments passed to combine_func . allow_multiple :\u2002 bool Whether a tuple/list will be considered as multiple objects in other . combine_func :\u2002 callable Function to combine two arrays. Can be Numba-compiled. keep_pd :\u2002 bool Whether to keep inputs as pandas objects, otherwise convert to NumPy arrays. to_2d :\u2002 bool Whether to reshape inputs to 2-dim arrays, otherwise keep as-is. concat :\u2002 bool Whether to concatenate the results along the column axis. Otherwise, pairwise combine into a Series/DataFrame of the same shape. If True, see combine_and_concat() . If False, see combine_multiple() . numba_loop :\u2002 bool Whether to loop using Numba. Set to True when iterating large number of times over small input, but note that Numba doesn't support variable keyword arguments. use_ray :\u2002 bool Whether to use Ray to execute combine_func in parallel. Only works with numba_loop set to False and concat is set to True. See ray_apply() for related keyword arguments. broadcast :\u2002 bool Whether to broadcast all inputs. broadcast_kwargs :\u2002 dict Keyword arguments passed to broadcast() . keys :\u2002 index_like Outermost column level. wrap_kwargs :\u2002 dict Keyword arguments passed to ArrayWrapper.wrap() . **kwargs Keyword arguments passed to combine_func . Note If combine_func is Numba-compiled, will broadcast using WRITEABLE and C_CONTIGUOUS flags, which can lead to an expensive computation overhead if passed objects are large and have different shape/memory order. You also must ensure that all objects have the same data type. Also remember to bring each in *args to a Numba-compatible format. Usage >>> import vectorbt as vbt >>> import pandas as pd >>> sr = pd . Series ([ 1 , 2 ], index = [ 'x' , 'y' ]) >>> df = pd . DataFrame ([[ 3 , 4 ], [ 5 , 6 ]], index = [ 'x' , 'y' ], columns = [ 'a' , 'b' ]) >>> sr . vbt . combine ( df , combine_func = lambda x , y : x + y ) a b x 4 5 y 7 8 >>> sr . vbt . combine ([ df , df * 2 ], combine_func = lambda x , y : x + y ) a b x 10 13 y 17 20 >>> sr . vbt . combine ([ df , df * 2 ], combine_func = lambda x , y : x + y , concat = True , keys = [ 'c' , 'd' ]) c d a b a b x 4 5 7 9 y 7 8 12 14 Use Ray for small inputs and large processing times: >>> def combine_func ( a , b ): ... time . sleep ( 1 ) ... return a + b >>> sr = pd . Series ([ 1 , 2 , 3 ]) >>> % timeit sr . vbt . combine ([ 1 , 1 , 1 ], combine_func = combine_func ) 3.01 s \u00b1 2.98 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) >>> % timeit sr . vbt . combine ([ 1 , 1 , 1 ], combine_func = combine_func , concat = True , use_ray = True ) 1.02 s \u00b1 2.32 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)","title":"combine()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.concat","text":"BaseAccessor . concat ( * others , broadcast_kwargs = None , keys = None ) Concatenate with others along columns. Args *others :\u2002 array_like List of objects to be concatenated with this array. broadcast_kwargs :\u2002 dict Keyword arguments passed to broadcast() . keys :\u2002 index_like Outermost column level. Usage >>> import vectorbt as vbt >>> import pandas as pd >>> sr = pd . Series ([ 1 , 2 ], index = [ 'x' , 'y' ]) >>> df = pd . DataFrame ([[ 3 , 4 ], [ 5 , 6 ]], index = [ 'x' , 'y' ], columns = [ 'a' , 'b' ]) >>> sr . vbt . concat ( df , keys = [ 'c' , 'd' ]) c d a b a b x 1 1 3 4 y 2 2 5 6","title":"concat()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.df_accessor_cls","text":"Accessor class for pd.DataFrame .","title":"df_accessor_cls"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.drop_duplicate_levels","text":"BaseAccessor . drop_duplicate_levels ( keep = None , axis = 1 , inplace = False ) See drop_duplicate_levels() . See BaseAccessor.apply_on_index() for other keyword arguments.","title":"drop_duplicate_levels()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.drop_levels","text":"BaseAccessor . drop_levels ( levels , axis = 1 , inplace = False , strict = True ) See drop_levels() . See BaseAccessor.apply_on_index() for other keyword arguments.","title":"drop_levels()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.drop_redundant_levels","text":"BaseAccessor . drop_redundant_levels ( axis = 1 , inplace = False ) See drop_redundant_levels() . See BaseAccessor.apply_on_index() for other keyword arguments.","title":"drop_redundant_levels()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.empty","text":"BaseAccessor . empty ( shape , fill_value = nan , ** kwargs ) Generate an empty Series/DataFrame of shape shape and fill with fill_value .","title":"empty()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.empty_like","text":"BaseAccessor . empty_like ( other , fill_value = nan , ** kwargs ) Generate an empty Series/DataFrame like other and fill with fill_value .","title":"empty_like()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.indexing_func","text":"BaseAccessor . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on BaseAccessor .","title":"indexing_func()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.is_frame","text":"BaseAccessor . is_frame ()","title":"is_frame()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.is_series","text":"BaseAccessor . is_series ()","title":"is_series()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.make_symmetric","text":"BaseAccessor . make_symmetric () See make_symmetric() .","title":"make_symmetric()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.obj","text":"Pandas object.","title":"obj"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.rename_levels","text":"BaseAccessor . rename_levels ( name_dict , axis = 1 , inplace = False , strict = True ) See rename_levels() . See BaseAccessor.apply_on_index() for other keyword arguments.","title":"rename_levels()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.repeat","text":"BaseAccessor . repeat ( n , keys = None , axis = 1 , wrap_kwargs = None ) See repeat() . Set axis to 1 for columns and 0 for index. Use keys as the outermost level.","title":"repeat()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.select_levels","text":"BaseAccessor . select_levels ( level_names , axis = 1 , inplace = False ) See select_levels() . See BaseAccessor.apply_on_index() for other keyword arguments.","title":"select_levels()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.sr_accessor_cls","text":"Accessor class for pd.Series .","title":"sr_accessor_cls"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.stack_index","text":"BaseAccessor . stack_index ( index , on_top = True , axis = 1 , inplace = False , ** kwargs ) See stack_indexes() . Set on_top to False to stack at bottom. See BaseAccessor.apply_on_index() for other keyword arguments.","title":"stack_index()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.tile","text":"BaseAccessor . tile ( n , keys = None , axis = 1 , wrap_kwargs = None ) See tile() . Set axis to 1 for columns and 0 for index. Use keys as the outermost level.","title":"tile()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.to_1d_array","text":"BaseAccessor . to_1d_array () Convert to 1-dim NumPy array See to_1d() .","title":"to_1d_array()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.to_2d_array","text":"BaseAccessor . to_2d_array () Convert to 2-dim NumPy array. See to_2d() .","title":"to_2d_array()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.to_dict","text":"BaseAccessor . to_dict ( ** kwargs ) See to_dict() .","title":"to_dict()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.unstack_to_array","text":"BaseAccessor . unstack_to_array ( ** kwargs ) See unstack_to_array() .","title":"unstack_to_array()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseAccessor.unstack_to_df","text":"BaseAccessor . unstack_to_df ( ** kwargs ) See unstack_to_df() .","title":"unstack_to_df()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseDFAccessor","text":"Accessor on top of DataFrames. Accessible through pd.DataFrame.vbt and all child accessors. Superclasses AttrResolver BaseAccessor Configured Documented IndexingBase PandasIndexer Pickleable Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.config BaseAccessor.df_accessor_cls BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.iloc BaseAccessor.indexing_func() BaseAccessor.indexing_kwargs BaseAccessor.loc BaseAccessor.make_symmetric() BaseAccessor.obj BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.self_aliases BaseAccessor.sr_accessor_cls BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() BaseAccessor.wrapper BaseAccessor.writeable_attrs Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses GenericDFAccessor PXDFAccessor","title":"BaseDFAccessor"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseDFAccessor.is_frame","text":"BaseDFAccessor . is_frame ()","title":"is_frame()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseDFAccessor.is_series","text":"BaseDFAccessor . is_series ()","title":"is_series()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseSRAccessor","text":"Accessor on top of Series. Accessible through pd.Series.vbt and all child accessors. Superclasses AttrResolver BaseAccessor Configured Documented IndexingBase PandasIndexer Pickleable Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.config BaseAccessor.df_accessor_cls BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.iloc BaseAccessor.indexing_func() BaseAccessor.indexing_kwargs BaseAccessor.loc BaseAccessor.make_symmetric() BaseAccessor.obj BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.self_aliases BaseAccessor.sr_accessor_cls BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() BaseAccessor.wrapper BaseAccessor.writeable_attrs Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses GenericSRAccessor PXSRAccessor","title":"BaseSRAccessor"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseSRAccessor.is_frame","text":"BaseSRAccessor . is_frame ()","title":"is_frame()"},{"location":"api/base/accessors/#vectorbt.base.accessors.BaseSRAccessor.is_series","text":"BaseSRAccessor . is_series ()","title":"is_series()"},{"location":"api/base/array_wrapper/","text":"array_wrapper module \u00b6 Class for wrapping NumPy arrays into Series/DataFrames. vectorbt's functionality is based upon the ability to perform the most essential pandas operations using NumPy+Numba stack. One has to convert the Series/DataFrame into the NumPy format, perform the computation, and put the array back into the pandas format. The last step is done using ArrayWrapper . It stores metadata of the original pandas object and offers methods wrap and wrap_reduced for wrapping NumPy arrays to match the stored metadata as closest as possible. >>> import numpy as np >>> import pandas as pd >>> import vectorbt as vbt >>> from vectorbt.base.array_wrapper import ArrayWrapper >>> aw = ArrayWrapper ( index = [ 'x' , 'y' , 'z' ], columns = [ 'a' , 'b' , 'c' ], ndim = 2 ) >>> aw . _config { 'columns': Index(['a', 'b', 'c'], dtype='object'), 'group_select': None, 'ndim': 2, 'freq': None, 'column_only_select': None, 'grouped_ndim': None, 'index': ['x', 'y', 'z'], 'allow_modify': True, 'allow_enable': True, 'group_by': None, 'allow_disable': True } >>> np . random . seed ( 42 ) >>> a = np . random . uniform ( size = ( 3 , 3 )) >>> aw . wrap ( a ) a b c x 0.374540 0.950714 0.731994 y 0.598658 0.156019 0.155995 z 0.058084 0.866176 0.601115 >>> aw . wrap_reduced ( np . sum ( a , axis = 0 )) a 1.031282 b 1.972909 c 1.489103 dtype: float64 It can also be indexed as a regular pandas object and integrates ColumnGrouper : >>> aw . loc [ 'x' : 'y' , 'a' ] . _config { 'columns': Index(['a'], dtype='object'), 'group_select': None, 'ndim': 1, 'freq': None, 'column_only_select': None, 'grouped_ndim': None, 'index': Index(['x', 'y'], dtype='object'), 'allow_modify': True, 'allow_enable': True, 'group_by': None, 'allow_disable': True } >>> aw . regroup ( np . array ([ 0 , 0 , 1 ])) . _config { 'columns': Index(['a', 'b', 'c'], dtype='object'), 'group_select': None, 'ndim': 2, 'freq': None, 'column_only_select': None, 'grouped_ndim': None, 'index': ['x', 'y', 'z'], 'allow_modify': True, 'allow_enable': True, 'group_by': array([0, 0, 1]), 'allow_disable': True } Class Wrapping is a convenience class meant to be subclassed by classes that do not want to subclass ArrayWrapper but rather use it as an attribute (which is a better SE design pattern anyway!). ArrayWrapper class \u00b6 Class that stores index, columns and shape metadata for wrapping NumPy arrays. Tightly integrated with ColumnGrouper . If the underlying object is a Series, pass [sr.name] as columns . **kwargs are passed to ColumnGrouper . Note This class is meant to be immutable. To change any attribute, use Configured.replace() . Use methods that begin with get_ to get group-aware results. Superclasses Configured Documented IndexingBase PandasIndexer Pickleable Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs PandasIndexer.iloc PandasIndexer.indexing_kwargs PandasIndexer.loc PandasIndexer.xs() Pickleable.load() Pickleable.save() column_only_select property \u00b6 Whether to perform indexing on columns only. columns property \u00b6 Columns. dummy method \u00b6 ArrayWrapper . dummy ( group_by = None , ** kwargs ) Create a dummy Series/DataFrame. fill method \u00b6 ArrayWrapper . fill ( fill_value , group_by = None , ** kwargs ) Fill a Series/DataFrame. fill_reduced method \u00b6 ArrayWrapper . fill_reduced ( fill_value , group_by = None , ** kwargs ) Fill a reduced Series/DataFrame. freq property \u00b6 Index frequency. from_obj class method \u00b6 ArrayWrapper . from_obj ( obj , * args , ** kwargs ) Derive metadata from an object. from_shape class method \u00b6 ArrayWrapper . from_shape ( shape , * args , ** kwargs ) Derive metadata from shape. get_columns method \u00b6 ArrayWrapper . get_columns ( group_by = None ) Get group-aware ArrayWrapper.columns . get_name method \u00b6 ArrayWrapper . get_name ( group_by = None ) Get group-aware ArrayWrapper.name . get_ndim method \u00b6 ArrayWrapper . get_ndim ( group_by = None ) Get group-aware ArrayWrapper.ndim . get_shape method \u00b6 ArrayWrapper . get_shape ( group_by = None ) Get group-aware ArrayWrapper.shape . get_shape_2d method \u00b6 ArrayWrapper . get_shape_2d ( group_by = None ) Get group-aware ArrayWrapper.shape_2d . group_select property \u00b6 Whether to perform indexing on groups. grouped_ndim property \u00b6 Number of dimensions under column grouping. grouper property \u00b6 Column grouper. index property \u00b6 Index. indexing_func method \u00b6 ArrayWrapper . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on ArrayWrapper indexing_func_meta method \u00b6 ArrayWrapper . indexing_func_meta ( pd_indexing_func , index = None , columns = None , column_only_select = None , group_select = None , group_by = None ) Perform indexing on ArrayWrapper and also return indexing metadata. Takes into account column grouping. Set column_only_select to True to index the array wrapper as a Series of columns. This way, selection of index (axis 0) can be avoided. Set group_select to True to select groups rather than columns. Takes effect only if grouping is enabled. Note If column_only_select is True, make sure to index the array wrapper as a Series of columns rather than a DataFrame. For example, the operation .iloc[:, :2] should become .iloc[:2] . Operations are not allowed if the object is already a Series and thus has only one column/group. name property \u00b6 Name. ndim property \u00b6 Number of dimensions. regroup method \u00b6 ArrayWrapper . regroup ( group_by , ** kwargs ) Regroup this object. Only creates a new instance if grouping has changed, otherwise returns itself. resolve method \u00b6 ArrayWrapper . resolve ( group_by = None , ** kwargs ) Resolve this object. Replaces columns and other metadata with groups. shape property \u00b6 Shape. shape_2d property \u00b6 Shape as if the object was two-dimensional. to_timedelta method \u00b6 ArrayWrapper . to_timedelta ( a , to_pd = False , silence_warnings = None ) Convert array to duration using ArrayWrapper.freq . wrap method \u00b6 ArrayWrapper . wrap ( arr , index = None , columns = None , fillna = None , dtype = None , group_by = None , to_timedelta = False , to_index = False , silence_warnings = None ) Wrap a NumPy array using the stored metadata. Runs the following pipeline: 1) Converts to NumPy array 2) Fills NaN (optional) 3) Wraps using index, columns, and dtype (optional) 4) Converts to index (optional) 5) Converts to timedelta using ArrayWrapper.to_timedelta() (optional) wrap_reduced method \u00b6 ArrayWrapper . wrap_reduced ( arr , name_or_index = None , columns = None , fillna = None , dtype = None , group_by = None , to_timedelta = False , to_index = False , silence_warnings = None ) Wrap result of reduction. name_or_index can be the name of the resulting series if reducing to a scalar per column, or the index of the resulting series/dataframe if reducing to an array per column. columns can be set to override object's default columns. See ArrayWrapper.wrap() for the pipeline. Wrapping class \u00b6 Class that uses ArrayWrapper globally. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() AttrResolver.self_aliases Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs PandasIndexer.iloc PandasIndexer.indexing_kwargs PandasIndexer.loc PandasIndexer.xs() Pickleable.load() Pickleable.save() Subclasses BaseAccessor ColumnMapper Data IndicatorBase MappedArray Portfolio Records indexing_func method \u00b6 Wrapping . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on Wrapping . regroup method \u00b6 Wrapping . regroup ( group_by , ** kwargs ) Regroup this object. Only creates a new instance if grouping has changed, otherwise returns itself. **kwargs will be passed to ArrayWrapper.regroup() . resolve_self method \u00b6 Wrapping . resolve_self ( cond_kwargs = None , custom_arg_names = None , impacts_caching = True , silence_warnings = None ) Resolve self. Creates a copy of this instance if a different freq can be found in cond_kwargs . select_one method \u00b6 Wrapping . select_one ( column = None , group_by = None , ** kwargs ) Select one column/group. column can be a label-based position as well as an integer position (if label fails). select_one_from_obj static method \u00b6 Wrapping . select_one_from_obj ( obj , wrapper , column = None ) Select one column/group from a pandas object. column can be a label-based position as well as an integer position (if label fails). wrapper property \u00b6 Array wrapper.","title":"array_wrapper"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper","text":"Class for wrapping NumPy arrays into Series/DataFrames. vectorbt's functionality is based upon the ability to perform the most essential pandas operations using NumPy+Numba stack. One has to convert the Series/DataFrame into the NumPy format, perform the computation, and put the array back into the pandas format. The last step is done using ArrayWrapper . It stores metadata of the original pandas object and offers methods wrap and wrap_reduced for wrapping NumPy arrays to match the stored metadata as closest as possible. >>> import numpy as np >>> import pandas as pd >>> import vectorbt as vbt >>> from vectorbt.base.array_wrapper import ArrayWrapper >>> aw = ArrayWrapper ( index = [ 'x' , 'y' , 'z' ], columns = [ 'a' , 'b' , 'c' ], ndim = 2 ) >>> aw . _config { 'columns': Index(['a', 'b', 'c'], dtype='object'), 'group_select': None, 'ndim': 2, 'freq': None, 'column_only_select': None, 'grouped_ndim': None, 'index': ['x', 'y', 'z'], 'allow_modify': True, 'allow_enable': True, 'group_by': None, 'allow_disable': True } >>> np . random . seed ( 42 ) >>> a = np . random . uniform ( size = ( 3 , 3 )) >>> aw . wrap ( a ) a b c x 0.374540 0.950714 0.731994 y 0.598658 0.156019 0.155995 z 0.058084 0.866176 0.601115 >>> aw . wrap_reduced ( np . sum ( a , axis = 0 )) a 1.031282 b 1.972909 c 1.489103 dtype: float64 It can also be indexed as a regular pandas object and integrates ColumnGrouper : >>> aw . loc [ 'x' : 'y' , 'a' ] . _config { 'columns': Index(['a'], dtype='object'), 'group_select': None, 'ndim': 1, 'freq': None, 'column_only_select': None, 'grouped_ndim': None, 'index': Index(['x', 'y'], dtype='object'), 'allow_modify': True, 'allow_enable': True, 'group_by': None, 'allow_disable': True } >>> aw . regroup ( np . array ([ 0 , 0 , 1 ])) . _config { 'columns': Index(['a', 'b', 'c'], dtype='object'), 'group_select': None, 'ndim': 2, 'freq': None, 'column_only_select': None, 'grouped_ndim': None, 'index': ['x', 'y', 'z'], 'allow_modify': True, 'allow_enable': True, 'group_by': array([0, 0, 1]), 'allow_disable': True } Class Wrapping is a convenience class meant to be subclassed by classes that do not want to subclass ArrayWrapper but rather use it as an attribute (which is a better SE design pattern anyway!).","title":"vectorbt.base.array_wrapper"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper","text":"Class that stores index, columns and shape metadata for wrapping NumPy arrays. Tightly integrated with ColumnGrouper . If the underlying object is a Series, pass [sr.name] as columns . **kwargs are passed to ColumnGrouper . Note This class is meant to be immutable. To change any attribute, use Configured.replace() . Use methods that begin with get_ to get group-aware results. Superclasses Configured Documented IndexingBase PandasIndexer Pickleable Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs PandasIndexer.iloc PandasIndexer.indexing_kwargs PandasIndexer.loc PandasIndexer.xs() Pickleable.load() Pickleable.save()","title":"ArrayWrapper"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.column_only_select","text":"Whether to perform indexing on columns only.","title":"column_only_select"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.columns","text":"Columns.","title":"columns"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.dummy","text":"ArrayWrapper . dummy ( group_by = None , ** kwargs ) Create a dummy Series/DataFrame.","title":"dummy()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.fill","text":"ArrayWrapper . fill ( fill_value , group_by = None , ** kwargs ) Fill a Series/DataFrame.","title":"fill()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.fill_reduced","text":"ArrayWrapper . fill_reduced ( fill_value , group_by = None , ** kwargs ) Fill a reduced Series/DataFrame.","title":"fill_reduced()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.freq","text":"Index frequency.","title":"freq"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.from_obj","text":"ArrayWrapper . from_obj ( obj , * args , ** kwargs ) Derive metadata from an object.","title":"from_obj()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.from_shape","text":"ArrayWrapper . from_shape ( shape , * args , ** kwargs ) Derive metadata from shape.","title":"from_shape()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.get_columns","text":"ArrayWrapper . get_columns ( group_by = None ) Get group-aware ArrayWrapper.columns .","title":"get_columns()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.get_name","text":"ArrayWrapper . get_name ( group_by = None ) Get group-aware ArrayWrapper.name .","title":"get_name()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.get_ndim","text":"ArrayWrapper . get_ndim ( group_by = None ) Get group-aware ArrayWrapper.ndim .","title":"get_ndim()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.get_shape","text":"ArrayWrapper . get_shape ( group_by = None ) Get group-aware ArrayWrapper.shape .","title":"get_shape()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.get_shape_2d","text":"ArrayWrapper . get_shape_2d ( group_by = None ) Get group-aware ArrayWrapper.shape_2d .","title":"get_shape_2d()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.group_select","text":"Whether to perform indexing on groups.","title":"group_select"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.grouped_ndim","text":"Number of dimensions under column grouping.","title":"grouped_ndim"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.grouper","text":"Column grouper.","title":"grouper"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.index","text":"Index.","title":"index"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.indexing_func","text":"ArrayWrapper . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on ArrayWrapper","title":"indexing_func()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.indexing_func_meta","text":"ArrayWrapper . indexing_func_meta ( pd_indexing_func , index = None , columns = None , column_only_select = None , group_select = None , group_by = None ) Perform indexing on ArrayWrapper and also return indexing metadata. Takes into account column grouping. Set column_only_select to True to index the array wrapper as a Series of columns. This way, selection of index (axis 0) can be avoided. Set group_select to True to select groups rather than columns. Takes effect only if grouping is enabled. Note If column_only_select is True, make sure to index the array wrapper as a Series of columns rather than a DataFrame. For example, the operation .iloc[:, :2] should become .iloc[:2] . Operations are not allowed if the object is already a Series and thus has only one column/group.","title":"indexing_func_meta()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.name","text":"Name.","title":"name"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.ndim","text":"Number of dimensions.","title":"ndim"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.regroup","text":"ArrayWrapper . regroup ( group_by , ** kwargs ) Regroup this object. Only creates a new instance if grouping has changed, otherwise returns itself.","title":"regroup()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.resolve","text":"ArrayWrapper . resolve ( group_by = None , ** kwargs ) Resolve this object. Replaces columns and other metadata with groups.","title":"resolve()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.shape","text":"Shape.","title":"shape"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.shape_2d","text":"Shape as if the object was two-dimensional.","title":"shape_2d"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.to_timedelta","text":"ArrayWrapper . to_timedelta ( a , to_pd = False , silence_warnings = None ) Convert array to duration using ArrayWrapper.freq .","title":"to_timedelta()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.wrap","text":"ArrayWrapper . wrap ( arr , index = None , columns = None , fillna = None , dtype = None , group_by = None , to_timedelta = False , to_index = False , silence_warnings = None ) Wrap a NumPy array using the stored metadata. Runs the following pipeline: 1) Converts to NumPy array 2) Fills NaN (optional) 3) Wraps using index, columns, and dtype (optional) 4) Converts to index (optional) 5) Converts to timedelta using ArrayWrapper.to_timedelta() (optional)","title":"wrap()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.ArrayWrapper.wrap_reduced","text":"ArrayWrapper . wrap_reduced ( arr , name_or_index = None , columns = None , fillna = None , dtype = None , group_by = None , to_timedelta = False , to_index = False , silence_warnings = None ) Wrap result of reduction. name_or_index can be the name of the resulting series if reducing to a scalar per column, or the index of the resulting series/dataframe if reducing to an array per column. columns can be set to override object's default columns. See ArrayWrapper.wrap() for the pipeline.","title":"wrap_reduced()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.Wrapping","text":"Class that uses ArrayWrapper globally. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() AttrResolver.self_aliases Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs PandasIndexer.iloc PandasIndexer.indexing_kwargs PandasIndexer.loc PandasIndexer.xs() Pickleable.load() Pickleable.save() Subclasses BaseAccessor ColumnMapper Data IndicatorBase MappedArray Portfolio Records","title":"Wrapping"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.Wrapping.indexing_func","text":"Wrapping . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on Wrapping .","title":"indexing_func()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.Wrapping.regroup","text":"Wrapping . regroup ( group_by , ** kwargs ) Regroup this object. Only creates a new instance if grouping has changed, otherwise returns itself. **kwargs will be passed to ArrayWrapper.regroup() .","title":"regroup()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.Wrapping.resolve_self","text":"Wrapping . resolve_self ( cond_kwargs = None , custom_arg_names = None , impacts_caching = True , silence_warnings = None ) Resolve self. Creates a copy of this instance if a different freq can be found in cond_kwargs .","title":"resolve_self()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.Wrapping.select_one","text":"Wrapping . select_one ( column = None , group_by = None , ** kwargs ) Select one column/group. column can be a label-based position as well as an integer position (if label fails).","title":"select_one()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.Wrapping.select_one_from_obj","text":"Wrapping . select_one_from_obj ( obj , wrapper , column = None ) Select one column/group from a pandas object. column can be a label-based position as well as an integer position (if label fails).","title":"select_one_from_obj()"},{"location":"api/base/array_wrapper/#vectorbt.base.array_wrapper.Wrapping.wrapper","text":"Array wrapper.","title":"wrapper"},{"location":"api/base/column_grouper/","text":"column_grouper module \u00b6 Class that exposes methods to group columns. Class ColumnGrouper stores metadata related to grouping columns. It can return, for example, the number of groups, the start indices of groups, and other information useful for reducing operations that utilize grouping. It also allows to dynamically enable/disable/modify groups and checks whether a certain operation is permitted. get_group_lens_nb function \u00b6 get_group_lens_nb ( groups ) Return count per group. get_groups_and_index function \u00b6 get_groups_and_index ( index , group_by ) Return array of group indices pointing to the original index, and grouped index. group_by_to_index function \u00b6 group_by_to_index ( index , group_by ) Convert mapper group_by to pd.Index . Note Index and mapper must have the same length. ColumnGrouper class \u00b6 Class that exposes methods to group columns. group_by can be: boolean (False for no grouping, True for one group), integer (level by position), string (level by name), sequence of integers or strings that is shorter than columns (multiple levels), any other sequence that has the same length as columns (group per column). Set allow_enable to False to prohibit grouping if ColumnGrouper.group_by is None. Set allow_disable to False to prohibit disabling of grouping if ColumnGrouper.group_by is not None. Set allow_modify to False to prohibit modifying groups (you can still change their labels). All properties are read-only to enable caching. Note Columns should build groups that are coherent and sorted for using get_group_lens_nb() . Note This class is meant to be immutable. To change any attribute, use Configured.replace() . Superclasses Configured Documented Pickleable Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() allow_disable property \u00b6 Whether to allow disabling grouping. allow_enable property \u00b6 Whether to allow enabling grouping. allow_modify property \u00b6 Whether to allow changing groups. check_group_by method \u00b6 ColumnGrouper . check_group_by ( group_by = None , allow_enable = None , allow_disable = None , allow_modify = None ) Check passed group_by object against restrictions. columns property \u00b6 Original columns. get_columns method \u00b6 ColumnGrouper . get_columns ( ** kwargs ) Return grouped columns. get_group_count method \u00b6 ColumnGrouper . get_group_count ( ** kwargs ) Get number of groups. get_group_end_idxs method \u00b6 ColumnGrouper . get_group_end_idxs ( ** kwargs ) Get end index of each group as an array. get_group_lens method \u00b6 ColumnGrouper . get_group_lens ( group_by = None , ** kwargs ) See get_group_lens_nb. get_group_start_idxs method \u00b6 ColumnGrouper . get_group_start_idxs ( ** kwargs ) Get first index of each group as an array. get_groups method \u00b6 ColumnGrouper . get_groups ( ** kwargs ) Return groups array. get_groups_and_columns method \u00b6 ColumnGrouper . get_groups_and_columns ( group_by = None , ** kwargs ) See get_groups_and_index() . group_by property \u00b6 Mapper for grouping. is_group_count_changed method \u00b6 ColumnGrouper . is_group_count_changed ( group_by = None ) Check whether the number of groups has changed. is_grouped method \u00b6 ColumnGrouper . is_grouped ( group_by = None ) Check whether columns are grouped. is_grouping_changed method \u00b6 ColumnGrouper . is_grouping_changed ( group_by = None ) Check whether column grouping has changed in any way. is_grouping_disabled method \u00b6 ColumnGrouper . is_grouping_disabled ( group_by = None ) Check whether column grouping has been disabled. is_grouping_enabled method \u00b6 ColumnGrouper . is_grouping_enabled ( group_by = None ) Check whether column grouping has been enabled. is_grouping_modified method \u00b6 ColumnGrouper . is_grouping_modified ( group_by = None ) Check whether column grouping has been modified. Doesn't care if grouping labels have been changed. is_sorted method \u00b6 ColumnGrouper . is_sorted ( group_by = None , ** kwargs ) Return whether groups are coherent and sorted. resolve_group_by method \u00b6 ColumnGrouper . resolve_group_by ( group_by = None , ** kwargs ) Resolve group_by from either object variable or keyword argument.","title":"column_grouper"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper","text":"Class that exposes methods to group columns. Class ColumnGrouper stores metadata related to grouping columns. It can return, for example, the number of groups, the start indices of groups, and other information useful for reducing operations that utilize grouping. It also allows to dynamically enable/disable/modify groups and checks whether a certain operation is permitted.","title":"vectorbt.base.column_grouper"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.get_group_lens_nb","text":"get_group_lens_nb ( groups ) Return count per group.","title":"get_group_lens_nb()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.get_groups_and_index","text":"get_groups_and_index ( index , group_by ) Return array of group indices pointing to the original index, and grouped index.","title":"get_groups_and_index()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.group_by_to_index","text":"group_by_to_index ( index , group_by ) Convert mapper group_by to pd.Index . Note Index and mapper must have the same length.","title":"group_by_to_index()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper","text":"Class that exposes methods to group columns. group_by can be: boolean (False for no grouping, True for one group), integer (level by position), string (level by name), sequence of integers or strings that is shorter than columns (multiple levels), any other sequence that has the same length as columns (group per column). Set allow_enable to False to prohibit grouping if ColumnGrouper.group_by is None. Set allow_disable to False to prohibit disabling of grouping if ColumnGrouper.group_by is not None. Set allow_modify to False to prohibit modifying groups (you can still change their labels). All properties are read-only to enable caching. Note Columns should build groups that are coherent and sorted for using get_group_lens_nb() . Note This class is meant to be immutable. To change any attribute, use Configured.replace() . Superclasses Configured Documented Pickleable Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save()","title":"ColumnGrouper"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.allow_disable","text":"Whether to allow disabling grouping.","title":"allow_disable"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.allow_enable","text":"Whether to allow enabling grouping.","title":"allow_enable"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.allow_modify","text":"Whether to allow changing groups.","title":"allow_modify"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.check_group_by","text":"ColumnGrouper . check_group_by ( group_by = None , allow_enable = None , allow_disable = None , allow_modify = None ) Check passed group_by object against restrictions.","title":"check_group_by()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.columns","text":"Original columns.","title":"columns"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.get_columns","text":"ColumnGrouper . get_columns ( ** kwargs ) Return grouped columns.","title":"get_columns()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.get_group_count","text":"ColumnGrouper . get_group_count ( ** kwargs ) Get number of groups.","title":"get_group_count()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.get_group_end_idxs","text":"ColumnGrouper . get_group_end_idxs ( ** kwargs ) Get end index of each group as an array.","title":"get_group_end_idxs()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.get_group_lens","text":"ColumnGrouper . get_group_lens ( group_by = None , ** kwargs ) See get_group_lens_nb.","title":"get_group_lens()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.get_group_start_idxs","text":"ColumnGrouper . get_group_start_idxs ( ** kwargs ) Get first index of each group as an array.","title":"get_group_start_idxs()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.get_groups","text":"ColumnGrouper . get_groups ( ** kwargs ) Return groups array.","title":"get_groups()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.get_groups_and_columns","text":"ColumnGrouper . get_groups_and_columns ( group_by = None , ** kwargs ) See get_groups_and_index() .","title":"get_groups_and_columns()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.group_by","text":"Mapper for grouping.","title":"group_by"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.is_group_count_changed","text":"ColumnGrouper . is_group_count_changed ( group_by = None ) Check whether the number of groups has changed.","title":"is_group_count_changed()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.is_grouped","text":"ColumnGrouper . is_grouped ( group_by = None ) Check whether columns are grouped.","title":"is_grouped()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.is_grouping_changed","text":"ColumnGrouper . is_grouping_changed ( group_by = None ) Check whether column grouping has changed in any way.","title":"is_grouping_changed()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.is_grouping_disabled","text":"ColumnGrouper . is_grouping_disabled ( group_by = None ) Check whether column grouping has been disabled.","title":"is_grouping_disabled()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.is_grouping_enabled","text":"ColumnGrouper . is_grouping_enabled ( group_by = None ) Check whether column grouping has been enabled.","title":"is_grouping_enabled()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.is_grouping_modified","text":"ColumnGrouper . is_grouping_modified ( group_by = None ) Check whether column grouping has been modified. Doesn't care if grouping labels have been changed.","title":"is_grouping_modified()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.is_sorted","text":"ColumnGrouper . is_sorted ( group_by = None , ** kwargs ) Return whether groups are coherent and sorted.","title":"is_sorted()"},{"location":"api/base/column_grouper/#vectorbt.base.column_grouper.ColumnGrouper.resolve_group_by","text":"ColumnGrouper . resolve_group_by ( group_by = None , ** kwargs ) Resolve group_by from either object variable or keyword argument.","title":"resolve_group_by()"},{"location":"api/base/combine_fns/","text":"combine_fns module \u00b6 Functions for combining arrays. Combine functions combine two or more NumPy arrays using a custom function. The emphasis here is done upon stacking the results into one NumPy array - since vectorbt is all about brute-forcing large spaces of hyperparameters, concatenating the results of each hyperparameter combination into a single DataFrame is important. All functions are available in both Python and Numba-compiled form. apply_and_concat_multiple function \u00b6 apply_and_concat_multiple ( n , apply_func , * args , show_progress = False , tqdm_kwargs = None , ** kwargs ) Identical to apply_and_concat_one() , except that the result of apply_func must be multiple 1-dim or 2-dim arrays. Each of these arrays at i will be concatenated with the array at the same position at i+1 . apply_and_concat_multiple_nb function \u00b6 apply_and_concat_multiple_nb ( n , apply_func_nb , * args ) A Numba-compiled version of apply_and_concat_multiple() . Note Output of apply_func_nb must be strictly homogeneous apply_func_nb must be Numba-compiled *args must be Numba-compatible No support for **kwargs apply_and_concat_multiple_ray function \u00b6 apply_and_concat_multiple_ray ( * args , ** kwargs ) Distributed version of apply_and_concat_multiple() . apply_and_concat_none function \u00b6 apply_and_concat_none ( n , apply_func , * args , show_progress = False , tqdm_kwargs = None , ** kwargs ) For each value i from 0 to n , apply apply_func with arguments *args and **kwargs , and output nothing. Meant for in-place outputs. apply_func must accept arguments i , *args and **kwargs . apply_and_concat_none_nb function \u00b6 apply_and_concat_none_nb ( n , apply_func_nb , * args ) A Numba-compiled version of apply_and_concat_none() . Note apply_func_nb must be Numba-compiled *args must be Numba-compatible No support for **kwargs apply_and_concat_one function \u00b6 apply_and_concat_one ( n , apply_func , * args , show_progress = False , tqdm_kwargs = None , ** kwargs ) For each value i from 0 to n , apply apply_func with arguments *args and **kwargs , and concat the results along axis 1. The result of apply_func must be a single 1-dim or 2-dim array. apply_func must accept arguments i , *args and **kwargs . apply_and_concat_one_nb function \u00b6 apply_and_concat_one_nb ( n , apply_func_nb , * args ) A Numba-compiled version of apply_and_concat_one() . Note apply_func_nb must be Numba-compiled *args must be Numba-compatible No support for **kwargs apply_and_concat_one_ray function \u00b6 apply_and_concat_one_ray ( * args , ** kwargs ) Distributed version of apply_and_concat_one() . combine_and_concat function \u00b6 combine_and_concat ( obj , others , combine_func , * args , ** kwargs ) Use apply_and_concat_one() to combine obj with each element from others using combine_func . combine_and_concat_nb function \u00b6 combine_and_concat_nb ( obj , others , combine_func_nb , * args ) A Numba-compiled version of combine_and_concat() . combine_and_concat_ray function \u00b6 combine_and_concat_ray ( obj , others , combine_func , * args , ** kwargs ) Distributed version of combine_and_concat() . combine_multiple function \u00b6 combine_multiple ( objs , combine_func , * args , ** kwargs ) Combine objs pairwise into a single object. combine_multiple_nb function \u00b6 combine_multiple_nb ( objs , combine_func_nb , * args ) A Numba-compiled version of combine_multiple() . Note combine_func_nb must be Numba-compiled objs and *args must be Numba-compatible objs must be strictly homogeneous No support for **kwargs ray_apply function \u00b6 ray_apply ( n , apply_func , * args , ray_force_init = False , ray_func_kwargs = None , ray_init_kwargs = None , ray_shutdown = False , ** kwargs ) Run apply_func in distributed manner. Set ray_reinit to True to terminate the Ray runtime and initialize a new one. ray_func_kwargs will be passed to ray.remote and ray_init_kwargs to ray.init . Set ray_shutdown to True to terminate the Ray runtime upon the job end. select_and_combine function \u00b6 select_and_combine ( i , obj , others , combine_func , * args , ** kwargs ) Combine obj and an element from others at i using combine_func . select_and_combine_nb function \u00b6 select_and_combine_nb ( i , obj , others , combine_func_nb , * args ) A Numba-compiled version of select_and_combine() . Note combine_func_nb must be Numba-compiled obj , others and *args must be Numba-compatible others must be strictly homogeneous No support for **kwargs to_2d_multiple_nb function \u00b6 to_2d_multiple_nb ( a ) Expand the dimensions of each array in a along axis 1. Note a must be strictly homogeneous to_2d_one_nb function \u00b6 to_2d_one_nb ( a ) Expand the dimensions of array a along axis 1. Note a must be strictly homogeneous","title":"combine_fns"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns","text":"Functions for combining arrays. Combine functions combine two or more NumPy arrays using a custom function. The emphasis here is done upon stacking the results into one NumPy array - since vectorbt is all about brute-forcing large spaces of hyperparameters, concatenating the results of each hyperparameter combination into a single DataFrame is important. All functions are available in both Python and Numba-compiled form.","title":"vectorbt.base.combine_fns"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.apply_and_concat_multiple","text":"apply_and_concat_multiple ( n , apply_func , * args , show_progress = False , tqdm_kwargs = None , ** kwargs ) Identical to apply_and_concat_one() , except that the result of apply_func must be multiple 1-dim or 2-dim arrays. Each of these arrays at i will be concatenated with the array at the same position at i+1 .","title":"apply_and_concat_multiple()"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.apply_and_concat_multiple_nb","text":"apply_and_concat_multiple_nb ( n , apply_func_nb , * args ) A Numba-compiled version of apply_and_concat_multiple() . Note Output of apply_func_nb must be strictly homogeneous apply_func_nb must be Numba-compiled *args must be Numba-compatible No support for **kwargs","title":"apply_and_concat_multiple_nb()"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.apply_and_concat_multiple_ray","text":"apply_and_concat_multiple_ray ( * args , ** kwargs ) Distributed version of apply_and_concat_multiple() .","title":"apply_and_concat_multiple_ray()"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.apply_and_concat_none","text":"apply_and_concat_none ( n , apply_func , * args , show_progress = False , tqdm_kwargs = None , ** kwargs ) For each value i from 0 to n , apply apply_func with arguments *args and **kwargs , and output nothing. Meant for in-place outputs. apply_func must accept arguments i , *args and **kwargs .","title":"apply_and_concat_none()"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.apply_and_concat_none_nb","text":"apply_and_concat_none_nb ( n , apply_func_nb , * args ) A Numba-compiled version of apply_and_concat_none() . Note apply_func_nb must be Numba-compiled *args must be Numba-compatible No support for **kwargs","title":"apply_and_concat_none_nb()"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.apply_and_concat_one","text":"apply_and_concat_one ( n , apply_func , * args , show_progress = False , tqdm_kwargs = None , ** kwargs ) For each value i from 0 to n , apply apply_func with arguments *args and **kwargs , and concat the results along axis 1. The result of apply_func must be a single 1-dim or 2-dim array. apply_func must accept arguments i , *args and **kwargs .","title":"apply_and_concat_one()"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.apply_and_concat_one_nb","text":"apply_and_concat_one_nb ( n , apply_func_nb , * args ) A Numba-compiled version of apply_and_concat_one() . Note apply_func_nb must be Numba-compiled *args must be Numba-compatible No support for **kwargs","title":"apply_and_concat_one_nb()"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.apply_and_concat_one_ray","text":"apply_and_concat_one_ray ( * args , ** kwargs ) Distributed version of apply_and_concat_one() .","title":"apply_and_concat_one_ray()"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.combine_and_concat","text":"combine_and_concat ( obj , others , combine_func , * args , ** kwargs ) Use apply_and_concat_one() to combine obj with each element from others using combine_func .","title":"combine_and_concat()"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.combine_and_concat_nb","text":"combine_and_concat_nb ( obj , others , combine_func_nb , * args ) A Numba-compiled version of combine_and_concat() .","title":"combine_and_concat_nb()"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.combine_and_concat_ray","text":"combine_and_concat_ray ( obj , others , combine_func , * args , ** kwargs ) Distributed version of combine_and_concat() .","title":"combine_and_concat_ray()"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.combine_multiple","text":"combine_multiple ( objs , combine_func , * args , ** kwargs ) Combine objs pairwise into a single object.","title":"combine_multiple()"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.combine_multiple_nb","text":"combine_multiple_nb ( objs , combine_func_nb , * args ) A Numba-compiled version of combine_multiple() . Note combine_func_nb must be Numba-compiled objs and *args must be Numba-compatible objs must be strictly homogeneous No support for **kwargs","title":"combine_multiple_nb()"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.ray_apply","text":"ray_apply ( n , apply_func , * args , ray_force_init = False , ray_func_kwargs = None , ray_init_kwargs = None , ray_shutdown = False , ** kwargs ) Run apply_func in distributed manner. Set ray_reinit to True to terminate the Ray runtime and initialize a new one. ray_func_kwargs will be passed to ray.remote and ray_init_kwargs to ray.init . Set ray_shutdown to True to terminate the Ray runtime upon the job end.","title":"ray_apply()"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.select_and_combine","text":"select_and_combine ( i , obj , others , combine_func , * args , ** kwargs ) Combine obj and an element from others at i using combine_func .","title":"select_and_combine()"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.select_and_combine_nb","text":"select_and_combine_nb ( i , obj , others , combine_func_nb , * args ) A Numba-compiled version of select_and_combine() . Note combine_func_nb must be Numba-compiled obj , others and *args must be Numba-compatible others must be strictly homogeneous No support for **kwargs","title":"select_and_combine_nb()"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.to_2d_multiple_nb","text":"to_2d_multiple_nb ( a ) Expand the dimensions of each array in a along axis 1. Note a must be strictly homogeneous","title":"to_2d_multiple_nb()"},{"location":"api/base/combine_fns/#vectorbt.base.combine_fns.to_2d_one_nb","text":"to_2d_one_nb ( a ) Expand the dimensions of array a along axis 1. Note a must be strictly homogeneous","title":"to_2d_one_nb()"},{"location":"api/base/index_fns/","text":"index_fns module \u00b6 Functions for working with index/columns. Index functions perform operations on index objects, such as stacking, combining, and cleansing MultiIndex levels. \"Index\" in pandas context is referred to both index and columns. align_index_to function \u00b6 align_index_to ( index1 , index2 ) Align index1 to have the same shape as index2 if they have any levels in common. Returns index slice for the aligning. align_indexes function \u00b6 align_indexes ( indexes ) Align multiple indexes to each other. combine_indexes function \u00b6 combine_indexes ( indexes , ignore_default = None , ** kwargs ) Combine each index in indexes using Cartesian product. Keyword arguments will be passed to stack_indexes() . drop_duplicate_levels function \u00b6 drop_duplicate_levels ( index , keep = None ) Drop levels in index with the same name and values. Set keep to 'last' to keep last levels, otherwise 'first'. Set keep to None to use the default. drop_levels function \u00b6 drop_levels ( index , levels , strict = True ) Drop levels in index by their name/position. drop_redundant_levels function \u00b6 drop_redundant_levels ( index ) Drop levels in index that either have a single unnamed value or a range from 0 to n. find_first_occurrence function \u00b6 find_first_occurrence ( index_value , index ) Return index of the first occurrence in index . get_index function \u00b6 get_index ( arg , axis ) Get index of arg by axis . index_from_values function \u00b6 index_from_values ( values , name = None ) Create a new pd.Index with name by parsing an iterable values . Each in values will correspond to an element in the new index. pick_levels function \u00b6 pick_levels ( index , required_levels = None , optional_levels = None ) Pick optional and required levels and return their indices. Raises an exception if index has less or more levels than expected. rename_levels function \u00b6 rename_levels ( index , name_dict , strict = True ) Rename levels in index by name_dict . repeat_index function \u00b6 repeat_index ( index , n , ignore_default = None ) Repeat each element in index n times. Set ignore_default to None to use the default. select_levels function \u00b6 select_levels ( index , level_names ) Build a new index by selecting one or multiple level_names from index . stack_indexes function \u00b6 stack_indexes ( indexes , drop_duplicates = None , keep = None , drop_redundant = None ) Stack each index in indexes on top of each other, from top to bottom. Set drop_duplicates , keep , or drop_redundant to None to use the default. tile_index function \u00b6 tile_index ( index , n , ignore_default = None ) Tile the whole index n times. Set ignore_default to None to use the default. to_any_index function \u00b6 to_any_index ( index_like ) Convert any index-like object to an index. Index objects are kept as-is.","title":"index_fns"},{"location":"api/base/index_fns/#vectorbt.base.index_fns","text":"Functions for working with index/columns. Index functions perform operations on index objects, such as stacking, combining, and cleansing MultiIndex levels. \"Index\" in pandas context is referred to both index and columns.","title":"vectorbt.base.index_fns"},{"location":"api/base/index_fns/#vectorbt.base.index_fns.align_index_to","text":"align_index_to ( index1 , index2 ) Align index1 to have the same shape as index2 if they have any levels in common. Returns index slice for the aligning.","title":"align_index_to()"},{"location":"api/base/index_fns/#vectorbt.base.index_fns.align_indexes","text":"align_indexes ( indexes ) Align multiple indexes to each other.","title":"align_indexes()"},{"location":"api/base/index_fns/#vectorbt.base.index_fns.combine_indexes","text":"combine_indexes ( indexes , ignore_default = None , ** kwargs ) Combine each index in indexes using Cartesian product. Keyword arguments will be passed to stack_indexes() .","title":"combine_indexes()"},{"location":"api/base/index_fns/#vectorbt.base.index_fns.drop_duplicate_levels","text":"drop_duplicate_levels ( index , keep = None ) Drop levels in index with the same name and values. Set keep to 'last' to keep last levels, otherwise 'first'. Set keep to None to use the default.","title":"drop_duplicate_levels()"},{"location":"api/base/index_fns/#vectorbt.base.index_fns.drop_levels","text":"drop_levels ( index , levels , strict = True ) Drop levels in index by their name/position.","title":"drop_levels()"},{"location":"api/base/index_fns/#vectorbt.base.index_fns.drop_redundant_levels","text":"drop_redundant_levels ( index ) Drop levels in index that either have a single unnamed value or a range from 0 to n.","title":"drop_redundant_levels()"},{"location":"api/base/index_fns/#vectorbt.base.index_fns.find_first_occurrence","text":"find_first_occurrence ( index_value , index ) Return index of the first occurrence in index .","title":"find_first_occurrence()"},{"location":"api/base/index_fns/#vectorbt.base.index_fns.get_index","text":"get_index ( arg , axis ) Get index of arg by axis .","title":"get_index()"},{"location":"api/base/index_fns/#vectorbt.base.index_fns.index_from_values","text":"index_from_values ( values , name = None ) Create a new pd.Index with name by parsing an iterable values . Each in values will correspond to an element in the new index.","title":"index_from_values()"},{"location":"api/base/index_fns/#vectorbt.base.index_fns.pick_levels","text":"pick_levels ( index , required_levels = None , optional_levels = None ) Pick optional and required levels and return their indices. Raises an exception if index has less or more levels than expected.","title":"pick_levels()"},{"location":"api/base/index_fns/#vectorbt.base.index_fns.rename_levels","text":"rename_levels ( index , name_dict , strict = True ) Rename levels in index by name_dict .","title":"rename_levels()"},{"location":"api/base/index_fns/#vectorbt.base.index_fns.repeat_index","text":"repeat_index ( index , n , ignore_default = None ) Repeat each element in index n times. Set ignore_default to None to use the default.","title":"repeat_index()"},{"location":"api/base/index_fns/#vectorbt.base.index_fns.select_levels","text":"select_levels ( index , level_names ) Build a new index by selecting one or multiple level_names from index .","title":"select_levels()"},{"location":"api/base/index_fns/#vectorbt.base.index_fns.stack_indexes","text":"stack_indexes ( indexes , drop_duplicates = None , keep = None , drop_redundant = None ) Stack each index in indexes on top of each other, from top to bottom. Set drop_duplicates , keep , or drop_redundant to None to use the default.","title":"stack_indexes()"},{"location":"api/base/index_fns/#vectorbt.base.index_fns.tile_index","text":"tile_index ( index , n , ignore_default = None ) Tile the whole index n times. Set ignore_default to None to use the default.","title":"tile_index()"},{"location":"api/base/index_fns/#vectorbt.base.index_fns.to_any_index","text":"to_any_index ( index_like ) Convert any index-like object to an index. Index objects are kept as-is.","title":"to_any_index()"},{"location":"api/base/indexing/","text":"indexing module \u00b6 Classes for indexing. The main purpose of indexing classes is to provide pandas-like indexing to user-defined classes holding objects that have rows and/or columns. This is done by forwarding indexing commands to each structured object and constructing the new user-defined class using them. This way, one can manipulate complex classes with dozens of pandas objects using a single command. build_param_indexer function \u00b6 build_param_indexer ( param_names , class_name = 'ParamIndexer' , module_name = None ) A factory to create a class with parameter indexing. Parameter indexer enables accessing a group of rows and columns by a parameter array (similar to loc ). This way, one can query index/columns by another Series called a parameter mapper, which is just a pd.Series that maps columns (its index) to params (its values). Parameter indexing is important, since querying by column/index labels alone is not always the best option. For example, pandas doesn't let you query by list at a specific index/column level. Args param_names :\u2002 list of str Names of the parameters. class_name :\u2002 str Name of the generated class. module_name :\u2002 str Name of the module to which the class should be bound. Usage >>> import pandas as pd >>> from vectorbt.base.indexing import build_param_indexer , indexing_on_mapper >>> MyParamIndexer = build_param_indexer ([ 'my_param' ]) >>> class C ( MyParamIndexer ): ... def __init__ ( self , df , param_mapper ): ... self . df = df ... self . _my_param_mapper = param_mapper ... super () . __init__ ([ param_mapper ]) ... ... def indexing_func ( self , pd_indexing_func ): ... return self . __class__ ( ... pd_indexing_func ( self . df ), ... indexing_on_mapper ( self . _my_param_mapper , self . df , pd_indexing_func ) ... ) >>> df = pd . DataFrame ({ 'a' : [ 1 , 2 ], 'b' : [ 3 , 4 ]}) >>> param_mapper = pd . Series ([ 'First' , 'Second' ], index = [ 'a' , 'b' ]) >>> c = C ( df , param_mapper ) >>> c . my_param_loc [ 'First' ] . df 0 1 1 2 Name: a, dtype: int64 >>> c . my_param_loc [ 'Second' ] . df 0 3 1 4 Name: b, dtype: int64 >>> c . my_param_loc [[ 'First' , 'First' , 'Second' , 'Second' ]] . df a b 0 1 1 3 3 1 2 2 4 4 indexing_on_mapper function \u00b6 indexing_on_mapper ( mapper , ref_obj , pd_indexing_func ) Broadcast mapper Series to ref_obj and perform pandas indexing using pd_indexing_func . IndexingBase class \u00b6 Class that supports indexing through IndexingBase.indexing_func() . Subclasses PandasIndexer vectorbt.indicators.basic.ParamIndexer vectorbt.indicators.basic.ParamIndexer vectorbt.indicators.basic.ParamIndexer vectorbt.indicators.basic.ParamIndexer vectorbt.indicators.basic.ParamIndexer vectorbt.indicators.basic.ParamIndexer vectorbt.indicators.basic.ParamIndexer vectorbt.indicators.basic.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer indexing_func method \u00b6 IndexingBase . indexing_func ( pd_indexing_func , ** kwargs ) Apply pd_indexing_func on all pandas objects in question and return a new instance of the class. Should be overridden. IndexingError class \u00b6 Exception raised when an indexing error has occurred. Superclasses builtins.BaseException builtins.Exception Loc class \u00b6 Forwards pd.Series.loc / pd.DataFrame.loc operation to each Series/DataFrame and returns a new class instance. Superclasses LocBase Inherited members LocBase.indexing_func LocBase.indexing_kwargs LocBase class \u00b6 Class that implements location-based indexing. Subclasses Loc ParamLoc iLoc indexing_func property \u00b6 Indexing function. indexing_kwargs property \u00b6 Keyword arguments passed to LocBase.indexing_func . PandasIndexer class \u00b6 Implements indexing using iloc , loc , xs and __getitem__ . Usage >>> import pandas as pd >>> from vectorbt.base.indexing import PandasIndexer >>> class C ( PandasIndexer ): ... def __init__ ( self , df1 , df2 ): ... self . df1 = df1 ... self . df2 = df2 ... super () . __init__ () ... ... def indexing_func ( self , pd_indexing_func ): ... return self . __class__ ( ... pd_indexing_func ( self . df1 ), ... pd_indexing_func ( self . df2 ) ... ) >>> df1 = pd . DataFrame ({ 'a' : [ 1 , 2 ], 'b' : [ 3 , 4 ]}) >>> df2 = pd . DataFrame ({ 'a' : [ 5 , 6 ], 'b' : [ 7 , 8 ]}) >>> c = C ( df1 , df2 ) >>> c . iloc [:, 0 ] <__main__.C object at 0x1a1cacbbe0> >>> c . iloc [:, 0 ] . df1 0 1 1 2 Name: a, dtype: int64 >>> c . iloc [:, 0 ] . df2 0 5 1 6 Name: a, dtype: int64 Superclasses IndexingBase Inherited members IndexingBase.indexing_func() Subclasses ArrayWrapper Wrapping iloc property \u00b6 Forwards pd.Series.iloc / pd.DataFrame.iloc operation to each Series/DataFrame and returns a new class instance. indexing_kwargs property \u00b6 Indexing keyword arguments. loc property \u00b6 Forwards pd.Series.loc / pd.DataFrame.loc operation to each Series/DataFrame and returns a new class instance. xs method \u00b6 PandasIndexer . xs ( * args , ** kwargs ) Forwards pd.Series.xs / pd.DataFrame.xs operation to each Series/DataFrame and returns a new class instance. ParamLoc class \u00b6 Access a group of columns by parameter using pd.Series.loc . Uses mapper to establish link between columns and parameter values. Superclasses LocBase Inherited members LocBase.indexing_func LocBase.indexing_kwargs get_indices method \u00b6 ParamLoc . get_indices ( key ) Get array of indices affected by this key. level_name property \u00b6 Level name. mapper property \u00b6 Mapper. iLoc class \u00b6 Forwards pd.Series.iloc / pd.DataFrame.iloc operation to each Series/DataFrame and returns a new class instance. Superclasses LocBase Inherited members LocBase.indexing_func LocBase.indexing_kwargs","title":"indexing"},{"location":"api/base/indexing/#vectorbt.base.indexing","text":"Classes for indexing. The main purpose of indexing classes is to provide pandas-like indexing to user-defined classes holding objects that have rows and/or columns. This is done by forwarding indexing commands to each structured object and constructing the new user-defined class using them. This way, one can manipulate complex classes with dozens of pandas objects using a single command.","title":"vectorbt.base.indexing"},{"location":"api/base/indexing/#vectorbt.base.indexing.build_param_indexer","text":"build_param_indexer ( param_names , class_name = 'ParamIndexer' , module_name = None ) A factory to create a class with parameter indexing. Parameter indexer enables accessing a group of rows and columns by a parameter array (similar to loc ). This way, one can query index/columns by another Series called a parameter mapper, which is just a pd.Series that maps columns (its index) to params (its values). Parameter indexing is important, since querying by column/index labels alone is not always the best option. For example, pandas doesn't let you query by list at a specific index/column level. Args param_names :\u2002 list of str Names of the parameters. class_name :\u2002 str Name of the generated class. module_name :\u2002 str Name of the module to which the class should be bound. Usage >>> import pandas as pd >>> from vectorbt.base.indexing import build_param_indexer , indexing_on_mapper >>> MyParamIndexer = build_param_indexer ([ 'my_param' ]) >>> class C ( MyParamIndexer ): ... def __init__ ( self , df , param_mapper ): ... self . df = df ... self . _my_param_mapper = param_mapper ... super () . __init__ ([ param_mapper ]) ... ... def indexing_func ( self , pd_indexing_func ): ... return self . __class__ ( ... pd_indexing_func ( self . df ), ... indexing_on_mapper ( self . _my_param_mapper , self . df , pd_indexing_func ) ... ) >>> df = pd . DataFrame ({ 'a' : [ 1 , 2 ], 'b' : [ 3 , 4 ]}) >>> param_mapper = pd . Series ([ 'First' , 'Second' ], index = [ 'a' , 'b' ]) >>> c = C ( df , param_mapper ) >>> c . my_param_loc [ 'First' ] . df 0 1 1 2 Name: a, dtype: int64 >>> c . my_param_loc [ 'Second' ] . df 0 3 1 4 Name: b, dtype: int64 >>> c . my_param_loc [[ 'First' , 'First' , 'Second' , 'Second' ]] . df a b 0 1 1 3 3 1 2 2 4 4","title":"build_param_indexer()"},{"location":"api/base/indexing/#vectorbt.base.indexing.indexing_on_mapper","text":"indexing_on_mapper ( mapper , ref_obj , pd_indexing_func ) Broadcast mapper Series to ref_obj and perform pandas indexing using pd_indexing_func .","title":"indexing_on_mapper()"},{"location":"api/base/indexing/#vectorbt.base.indexing.IndexingBase","text":"Class that supports indexing through IndexingBase.indexing_func() . Subclasses PandasIndexer vectorbt.indicators.basic.ParamIndexer vectorbt.indicators.basic.ParamIndexer vectorbt.indicators.basic.ParamIndexer vectorbt.indicators.basic.ParamIndexer vectorbt.indicators.basic.ParamIndexer vectorbt.indicators.basic.ParamIndexer vectorbt.indicators.basic.ParamIndexer vectorbt.indicators.basic.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.labels.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer vectorbt.signals.generators.ParamIndexer","title":"IndexingBase"},{"location":"api/base/indexing/#vectorbt.base.indexing.IndexingBase.indexing_func","text":"IndexingBase . indexing_func ( pd_indexing_func , ** kwargs ) Apply pd_indexing_func on all pandas objects in question and return a new instance of the class. Should be overridden.","title":"indexing_func()"},{"location":"api/base/indexing/#vectorbt.base.indexing.IndexingError","text":"Exception raised when an indexing error has occurred. Superclasses builtins.BaseException builtins.Exception","title":"IndexingError"},{"location":"api/base/indexing/#vectorbt.base.indexing.Loc","text":"Forwards pd.Series.loc / pd.DataFrame.loc operation to each Series/DataFrame and returns a new class instance. Superclasses LocBase Inherited members LocBase.indexing_func LocBase.indexing_kwargs","title":"Loc"},{"location":"api/base/indexing/#vectorbt.base.indexing.LocBase","text":"Class that implements location-based indexing. Subclasses Loc ParamLoc iLoc","title":"LocBase"},{"location":"api/base/indexing/#vectorbt.base.indexing.LocBase.indexing_func","text":"Indexing function.","title":"indexing_func"},{"location":"api/base/indexing/#vectorbt.base.indexing.LocBase.indexing_kwargs","text":"Keyword arguments passed to LocBase.indexing_func .","title":"indexing_kwargs"},{"location":"api/base/indexing/#vectorbt.base.indexing.PandasIndexer","text":"Implements indexing using iloc , loc , xs and __getitem__ . Usage >>> import pandas as pd >>> from vectorbt.base.indexing import PandasIndexer >>> class C ( PandasIndexer ): ... def __init__ ( self , df1 , df2 ): ... self . df1 = df1 ... self . df2 = df2 ... super () . __init__ () ... ... def indexing_func ( self , pd_indexing_func ): ... return self . __class__ ( ... pd_indexing_func ( self . df1 ), ... pd_indexing_func ( self . df2 ) ... ) >>> df1 = pd . DataFrame ({ 'a' : [ 1 , 2 ], 'b' : [ 3 , 4 ]}) >>> df2 = pd . DataFrame ({ 'a' : [ 5 , 6 ], 'b' : [ 7 , 8 ]}) >>> c = C ( df1 , df2 ) >>> c . iloc [:, 0 ] <__main__.C object at 0x1a1cacbbe0> >>> c . iloc [:, 0 ] . df1 0 1 1 2 Name: a, dtype: int64 >>> c . iloc [:, 0 ] . df2 0 5 1 6 Name: a, dtype: int64 Superclasses IndexingBase Inherited members IndexingBase.indexing_func() Subclasses ArrayWrapper Wrapping","title":"PandasIndexer"},{"location":"api/base/indexing/#vectorbt.base.indexing.PandasIndexer.iloc","text":"Forwards pd.Series.iloc / pd.DataFrame.iloc operation to each Series/DataFrame and returns a new class instance.","title":"iloc"},{"location":"api/base/indexing/#vectorbt.base.indexing.PandasIndexer.indexing_kwargs","text":"Indexing keyword arguments.","title":"indexing_kwargs"},{"location":"api/base/indexing/#vectorbt.base.indexing.PandasIndexer.loc","text":"Forwards pd.Series.loc / pd.DataFrame.loc operation to each Series/DataFrame and returns a new class instance.","title":"loc"},{"location":"api/base/indexing/#vectorbt.base.indexing.PandasIndexer.xs","text":"PandasIndexer . xs ( * args , ** kwargs ) Forwards pd.Series.xs / pd.DataFrame.xs operation to each Series/DataFrame and returns a new class instance.","title":"xs()"},{"location":"api/base/indexing/#vectorbt.base.indexing.ParamLoc","text":"Access a group of columns by parameter using pd.Series.loc . Uses mapper to establish link between columns and parameter values. Superclasses LocBase Inherited members LocBase.indexing_func LocBase.indexing_kwargs","title":"ParamLoc"},{"location":"api/base/indexing/#vectorbt.base.indexing.ParamLoc.get_indices","text":"ParamLoc . get_indices ( key ) Get array of indices affected by this key.","title":"get_indices()"},{"location":"api/base/indexing/#vectorbt.base.indexing.ParamLoc.level_name","text":"Level name.","title":"level_name"},{"location":"api/base/indexing/#vectorbt.base.indexing.ParamLoc.mapper","text":"Mapper.","title":"mapper"},{"location":"api/base/indexing/#vectorbt.base.indexing.iLoc","text":"Forwards pd.Series.iloc / pd.DataFrame.iloc operation to each Series/DataFrame and returns a new class instance. Superclasses LocBase Inherited members LocBase.indexing_func LocBase.indexing_kwargs","title":"iLoc"},{"location":"api/base/reshape_fns/","text":"reshape_fns module \u00b6 Functions for reshaping arrays. Reshape functions transform a pandas object/NumPy array in some way, such as tiling, broadcasting, and unstacking. IndexFromLike _GenericAlias \u00b6 Any object that can be coerced into a index_from argument. broadcast function \u00b6 broadcast ( * args , to_shape = None , to_pd = None , to_frame = None , align_index = None , align_columns = None , index_from = None , columns_from = None , require_kwargs = None , keep_raw = False , return_meta = False , ** kwargs ) Bring any array-like object in args to the same shape by using NumPy broadcasting. See Broadcasting . Can broadcast pandas objects by broadcasting their index/columns with broadcast_index() . Args *args :\u2002 array_like Array-like objects. to_shape :\u2002 tuple of int Target shape. If set, will broadcast every element in args to to_shape . to_pd :\u2002 bool or list of bool Whether to convert all output arrays to pandas, otherwise returns raw NumPy arrays. If None, converts only if there is at least one pandas object among them. If sequence, applies to each argument. to_frame :\u2002 bool Whether to convert all Series to DataFrames. align_index :\u2002 bool Whether to align index of pandas objects using multi-index. Pass None to use the default. align_columns :\u2002 bool Whether to align columns of pandas objects using multi-index. Pass None to use the default. index_from :\u2002 any Broadcasting rule for index. Pass None to use the default. columns_from :\u2002 any Broadcasting rule for columns. Pass None to use the default. require_kwargs :\u2002 dict or list of dict Keyword arguments passed to np.require . If sequence, applies to each argument. keep_raw :\u2002 bool or list of bool Whether to keep the unbroadcasted version of the array. Only makes sure that the array can be broadcast to the target shape. If sequence, applies to each argument. return_meta :\u2002 bool Whether to also return new shape, index and columns. **kwargs Keyword arguments passed to broadcast_index() . For defaults, see broadcasting in settings . Usage Without broadcasting index and columns: >>> import numpy as np >>> import pandas as pd >>> from vectorbt.base.reshape_fns import broadcast >>> v = 0 >>> a = np . array ([ 1 , 2 , 3 ]) >>> sr = pd . Series ([ 1 , 2 , 3 ], index = pd . Index ([ 'x' , 'y' , 'z' ]), name = 'a' ) >>> df = pd . DataFrame ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]], ... index = pd . Index ([ 'x2' , 'y2' , 'z2' ]), ... columns = pd . Index ([ 'a2' , 'b2' , 'c2' ])) >>> for i in broadcast ( ... v , a , sr , df , ... index_from = 'keep' , ... columns_from = 'keep' , ... ): print ( i ) 0 1 2 0 0 0 0 1 0 0 0 2 0 0 0 0 1 2 0 1 2 3 1 1 2 3 2 1 2 3 a a a x 1 1 1 y 2 2 2 z 3 3 3 a2 b2 c2 x2 1 2 3 y2 4 5 6 z2 7 8 9 Taking new index and columns from position: >>> for i in broadcast ( ... v , a , sr , df , ... index_from = 2 , ... columns_from = 3 ... ): print ( i ) a2 b2 c2 x 0 0 0 y 0 0 0 z 0 0 0 a2 b2 c2 x 1 2 3 y 1 2 3 z 1 2 3 a2 b2 c2 x 1 1 1 y 2 2 2 z 3 3 3 a2 b2 c2 x 1 2 3 y 4 5 6 z 7 8 9 Broadcasting index and columns through stacking: >>> for i in broadcast ( ... v , a , sr , df , ... index_from = 'stack' , ... columns_from = 'stack' ... ): print ( i ) a2 b2 c2 x x2 0 0 0 y y2 0 0 0 z z2 0 0 0 a2 b2 c2 x x2 1 2 3 y y2 1 2 3 z z2 1 2 3 a2 b2 c2 x x2 1 1 1 y y2 2 2 2 z z2 3 3 3 a2 b2 c2 x x2 1 2 3 y y2 4 5 6 z z2 7 8 9 Setting index and columns manually: >>> for i in broadcast ( ... v , a , sr , df , ... index_from = [ 'a' , 'b' , 'c' ], ... columns_from = [ 'd' , 'e' , 'f' ] ... ): print ( i ) d e f a 0 0 0 b 0 0 0 c 0 0 0 d e f a 1 2 3 b 1 2 3 c 1 2 3 d e f a 1 1 1 b 2 2 2 c 3 3 3 d e f a 1 2 3 b 4 5 6 c 7 8 9 broadcast_index function \u00b6 broadcast_index ( args , to_shape , index_from = None , axis = 0 , ignore_sr_names = None , ** kwargs ) Produce a broadcast index/columns. Args args :\u2002 list of array_like Array-like objects. to_shape :\u2002 tuple of int Target shape. index_from :\u2002 any Broadcasting rule for this index/these columns. Accepts the following values: 'keep' or None - keep the original index/columns of the objects in args 'stack' - stack different indexes/columns using stack_indexes() 'strict' - ensure that all pandas objects have the same index/columns 'reset' - reset any index/columns (they become a simple range) integer - use the index/columns of the i-th object in args everything else will be converted to pd.Index axis :\u2002 int Set to 0 for index and 1 for columns. ignore_sr_names :\u2002 bool Whether to ignore Series names if they are in conflict. Conflicting Series names are those that are different but not None. **kwargs Keyword arguments passed to stack_indexes() . For defaults, see broadcasting in settings . Note Series names are treated as columns with a single element but without a name. If a column level without a name loses its meaning, better to convert Series to DataFrames with one column prior to broadcasting. If the name of a Series is not that important, better to drop it altogether by setting it to None. broadcast_to function \u00b6 broadcast_to ( arg1 , arg2 , to_pd = None , index_from = None , columns_from = None , ** kwargs ) Broadcast arg1 to arg2 . Pass None to index_from / columns_from to use index/columns of the second argument. Keyword arguments **kwargs are passed to broadcast() . Usage >>> import numpy as np >>> import pandas as pd >>> from vectorbt.base.reshape_fns import broadcast_to >>> a = np . array ([ 1 , 2 , 3 ]) >>> sr = pd . Series ([ 4 , 5 , 6 ], index = pd . Index ([ 'x' , 'y' , 'z' ]), name = 'a' ) >>> broadcast_to ( a , sr ) x 1 y 2 z 3 Name: a, dtype: int64 >>> broadcast_to ( sr , a ) array([4, 5, 6]) broadcast_to_array_of function \u00b6 broadcast_to_array_of ( arg1 , arg2 ) Broadcast arg1 to the shape (1, *arg2.shape) . arg1 must be either a scalar, a 1-dim array, or have 1 dimension more than arg2 . Usage >>> import numpy as np >>> from vectorbt.base.reshape_fns import broadcast_to_array_of >>> broadcast_to_array_of ([ 0.1 , 0.2 ], np . empty (( 2 , 2 ))) [[[0.1 0.1] [0.1 0.1]] [[0.2 0.2] [0.2 0.2]]] broadcast_to_axis_of function \u00b6 broadcast_to_axis_of ( arg1 , arg2 , axis , require_kwargs = None ) Broadcast arg1 to an axis of arg2 . If arg2 has less dimensions than requested, will broadcast arg1 to a single number. For other keyword arguments, see broadcast() . flex_choose_i_and_col_nb function \u00b6 flex_choose_i_and_col_nb ( a , flex_2d = True ) Choose selection index and column based on the array's shape. Instead of expensive broadcasting, keep the original shape and do indexing in a smart way. A nice feature of this is that it has almost no memory footprint and can broadcast in any direction infinitely. Call it once before using flex_select_nb() . if flex_2d is True, 1-dim array will correspond to columns, otherwise to rows. flex_select_auto_nb function \u00b6 flex_select_auto_nb ( a , i , col , flex_2d = True ) Combines flex_choose_i_and_col_nb() and flex_select_nb() . flex_select_nb function \u00b6 flex_select_nb ( a , i , col , flex_i , flex_col , flex_2d = True ) Select element of a as if it has been broadcast. get_multiindex_series function \u00b6 get_multiindex_series ( arg ) Get Series with a multi-index. If DataFrame has been passed, should at maximum have one row or column. make_symmetric function \u00b6 make_symmetric ( arg , sort = True ) Make arg symmetric. The index and columns of the resulting DataFrame will be identical. Requires the index and columns to have the same number of levels. Pass sort=False if index and columns should not be sorted, but concatenated and get duplicates removed. Usage >>> import pandas as pd >>> from vectorbt.base.reshape_fns import make_symmetric >>> df = pd . DataFrame ([[ 1 , 2 ], [ 3 , 4 ]], index = [ 'a' , 'b' ], columns = [ 'c' , 'd' ]) >>> make_symmetric ( df ) a b c d a NaN NaN 1.0 2.0 b NaN NaN 3.0 4.0 c 1.0 3.0 NaN NaN d 2.0 4.0 NaN NaN repeat function \u00b6 repeat ( arg , n , axis = 1 , raw = False ) Repeat each element in arg n times along the specified axis. soft_to_ndim function \u00b6 soft_to_ndim ( arg , ndim , raw = False ) Try to softly bring arg to the specified number of dimensions ndim (max 2). tile function \u00b6 tile ( arg , n , axis = 1 , raw = False ) Repeat the whole arg n times along the specified axis. to_1d function \u00b6 to_1d ( arg , raw = False ) Reshape argument to one dimension. If raw is True, returns NumPy array. If 2-dim, will collapse along axis 1 (i.e., DataFrame with one column to Series). to_2d function \u00b6 to_2d ( arg , raw = False , expand_axis = 1 ) Reshape argument to two dimensions. If raw is True, returns NumPy array. If 1-dim, will expand along axis 1 (i.e., Series to DataFrame with one column). to_any_array function \u00b6 to_any_array ( arg , raw = False ) Convert any array-like object to an array. Pandas objects are kept as-is. to_dict function \u00b6 to_dict ( arg , orient = 'dict' ) Convert object to dict. to_pd_array function \u00b6 to_pd_array ( arg ) Convert any array-like object to a pandas object. unstack_to_array function \u00b6 unstack_to_array ( arg , levels = None ) Reshape arg based on its multi-index into a multi-dimensional array. Use levels to specify what index levels to unstack and in which order. Usage >>> import pandas as pd >>> from vectorbt.base.reshape_fns import unstack_to_array >>> index = pd . MultiIndex . from_arrays ( ... [[ 1 , 1 , 2 , 2 ], [ 3 , 4 , 3 , 4 ], [ 'a' , 'b' , 'c' , 'd' ]]) >>> sr = pd . Series ([ 1 , 2 , 3 , 4 ], index = index ) >>> unstack_to_array ( sr ) . shape (2, 2, 4) >>> unstack_to_array ( sr ) [[[ 1. nan nan nan] [nan 2. nan nan]] [[nan nan 3. nan] [nan nan nan 4.]]] >>> unstack_to_array ( sr , levels = ( 2 , 0 )) [[ 1. nan] [ 2. nan] [nan 3.] [nan 4.]] unstack_to_df function \u00b6 unstack_to_df ( arg , index_levels = None , column_levels = None , symmetric = False , sort = True ) Reshape arg based on its multi-index into a DataFrame. Use index_levels to specify what index levels will form new index, and column_levels for new columns. Set symmetric to True to make DataFrame symmetric. Usage >>> import pandas as pd >>> from vectorbt.base.reshape_fns import unstack_to_df >>> index = pd . MultiIndex . from_arrays ( ... [[ 1 , 1 , 2 , 2 ], [ 3 , 4 , 3 , 4 ], [ 'a' , 'b' , 'c' , 'd' ]], ... names = [ 'x' , 'y' , 'z' ]) >>> sr = pd . Series ([ 1 , 2 , 3 , 4 ], index = index ) >>> unstack_to_df ( sr , index_levels = ( 0 , 1 ), column_levels = 2 ) z a b c d x y 1 3 1.0 NaN NaN NaN 1 4 NaN 2.0 NaN NaN 2 3 NaN NaN 3.0 NaN 2 4 NaN NaN NaN 4.0 wrap_broadcasted function \u00b6 wrap_broadcasted ( old_arg , new_arg , is_pd = False , new_index = None , new_columns = None ) If the newly brodcasted array was originally a pandas object, make it pandas object again and assign it the newly broadcast index/columns.","title":"reshape_fns"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns","text":"Functions for reshaping arrays. Reshape functions transform a pandas object/NumPy array in some way, such as tiling, broadcasting, and unstacking.","title":"vectorbt.base.reshape_fns"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.IndexFromLike","text":"Any object that can be coerced into a index_from argument.","title":"IndexFromLike"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.broadcast","text":"broadcast ( * args , to_shape = None , to_pd = None , to_frame = None , align_index = None , align_columns = None , index_from = None , columns_from = None , require_kwargs = None , keep_raw = False , return_meta = False , ** kwargs ) Bring any array-like object in args to the same shape by using NumPy broadcasting. See Broadcasting . Can broadcast pandas objects by broadcasting their index/columns with broadcast_index() . Args *args :\u2002 array_like Array-like objects. to_shape :\u2002 tuple of int Target shape. If set, will broadcast every element in args to to_shape . to_pd :\u2002 bool or list of bool Whether to convert all output arrays to pandas, otherwise returns raw NumPy arrays. If None, converts only if there is at least one pandas object among them. If sequence, applies to each argument. to_frame :\u2002 bool Whether to convert all Series to DataFrames. align_index :\u2002 bool Whether to align index of pandas objects using multi-index. Pass None to use the default. align_columns :\u2002 bool Whether to align columns of pandas objects using multi-index. Pass None to use the default. index_from :\u2002 any Broadcasting rule for index. Pass None to use the default. columns_from :\u2002 any Broadcasting rule for columns. Pass None to use the default. require_kwargs :\u2002 dict or list of dict Keyword arguments passed to np.require . If sequence, applies to each argument. keep_raw :\u2002 bool or list of bool Whether to keep the unbroadcasted version of the array. Only makes sure that the array can be broadcast to the target shape. If sequence, applies to each argument. return_meta :\u2002 bool Whether to also return new shape, index and columns. **kwargs Keyword arguments passed to broadcast_index() . For defaults, see broadcasting in settings . Usage Without broadcasting index and columns: >>> import numpy as np >>> import pandas as pd >>> from vectorbt.base.reshape_fns import broadcast >>> v = 0 >>> a = np . array ([ 1 , 2 , 3 ]) >>> sr = pd . Series ([ 1 , 2 , 3 ], index = pd . Index ([ 'x' , 'y' , 'z' ]), name = 'a' ) >>> df = pd . DataFrame ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]], ... index = pd . Index ([ 'x2' , 'y2' , 'z2' ]), ... columns = pd . Index ([ 'a2' , 'b2' , 'c2' ])) >>> for i in broadcast ( ... v , a , sr , df , ... index_from = 'keep' , ... columns_from = 'keep' , ... ): print ( i ) 0 1 2 0 0 0 0 1 0 0 0 2 0 0 0 0 1 2 0 1 2 3 1 1 2 3 2 1 2 3 a a a x 1 1 1 y 2 2 2 z 3 3 3 a2 b2 c2 x2 1 2 3 y2 4 5 6 z2 7 8 9 Taking new index and columns from position: >>> for i in broadcast ( ... v , a , sr , df , ... index_from = 2 , ... columns_from = 3 ... ): print ( i ) a2 b2 c2 x 0 0 0 y 0 0 0 z 0 0 0 a2 b2 c2 x 1 2 3 y 1 2 3 z 1 2 3 a2 b2 c2 x 1 1 1 y 2 2 2 z 3 3 3 a2 b2 c2 x 1 2 3 y 4 5 6 z 7 8 9 Broadcasting index and columns through stacking: >>> for i in broadcast ( ... v , a , sr , df , ... index_from = 'stack' , ... columns_from = 'stack' ... ): print ( i ) a2 b2 c2 x x2 0 0 0 y y2 0 0 0 z z2 0 0 0 a2 b2 c2 x x2 1 2 3 y y2 1 2 3 z z2 1 2 3 a2 b2 c2 x x2 1 1 1 y y2 2 2 2 z z2 3 3 3 a2 b2 c2 x x2 1 2 3 y y2 4 5 6 z z2 7 8 9 Setting index and columns manually: >>> for i in broadcast ( ... v , a , sr , df , ... index_from = [ 'a' , 'b' , 'c' ], ... columns_from = [ 'd' , 'e' , 'f' ] ... ): print ( i ) d e f a 0 0 0 b 0 0 0 c 0 0 0 d e f a 1 2 3 b 1 2 3 c 1 2 3 d e f a 1 1 1 b 2 2 2 c 3 3 3 d e f a 1 2 3 b 4 5 6 c 7 8 9","title":"broadcast()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.broadcast_index","text":"broadcast_index ( args , to_shape , index_from = None , axis = 0 , ignore_sr_names = None , ** kwargs ) Produce a broadcast index/columns. Args args :\u2002 list of array_like Array-like objects. to_shape :\u2002 tuple of int Target shape. index_from :\u2002 any Broadcasting rule for this index/these columns. Accepts the following values: 'keep' or None - keep the original index/columns of the objects in args 'stack' - stack different indexes/columns using stack_indexes() 'strict' - ensure that all pandas objects have the same index/columns 'reset' - reset any index/columns (they become a simple range) integer - use the index/columns of the i-th object in args everything else will be converted to pd.Index axis :\u2002 int Set to 0 for index and 1 for columns. ignore_sr_names :\u2002 bool Whether to ignore Series names if they are in conflict. Conflicting Series names are those that are different but not None. **kwargs Keyword arguments passed to stack_indexes() . For defaults, see broadcasting in settings . Note Series names are treated as columns with a single element but without a name. If a column level without a name loses its meaning, better to convert Series to DataFrames with one column prior to broadcasting. If the name of a Series is not that important, better to drop it altogether by setting it to None.","title":"broadcast_index()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.broadcast_to","text":"broadcast_to ( arg1 , arg2 , to_pd = None , index_from = None , columns_from = None , ** kwargs ) Broadcast arg1 to arg2 . Pass None to index_from / columns_from to use index/columns of the second argument. Keyword arguments **kwargs are passed to broadcast() . Usage >>> import numpy as np >>> import pandas as pd >>> from vectorbt.base.reshape_fns import broadcast_to >>> a = np . array ([ 1 , 2 , 3 ]) >>> sr = pd . Series ([ 4 , 5 , 6 ], index = pd . Index ([ 'x' , 'y' , 'z' ]), name = 'a' ) >>> broadcast_to ( a , sr ) x 1 y 2 z 3 Name: a, dtype: int64 >>> broadcast_to ( sr , a ) array([4, 5, 6])","title":"broadcast_to()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.broadcast_to_array_of","text":"broadcast_to_array_of ( arg1 , arg2 ) Broadcast arg1 to the shape (1, *arg2.shape) . arg1 must be either a scalar, a 1-dim array, or have 1 dimension more than arg2 . Usage >>> import numpy as np >>> from vectorbt.base.reshape_fns import broadcast_to_array_of >>> broadcast_to_array_of ([ 0.1 , 0.2 ], np . empty (( 2 , 2 ))) [[[0.1 0.1] [0.1 0.1]] [[0.2 0.2] [0.2 0.2]]]","title":"broadcast_to_array_of()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.broadcast_to_axis_of","text":"broadcast_to_axis_of ( arg1 , arg2 , axis , require_kwargs = None ) Broadcast arg1 to an axis of arg2 . If arg2 has less dimensions than requested, will broadcast arg1 to a single number. For other keyword arguments, see broadcast() .","title":"broadcast_to_axis_of()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.flex_choose_i_and_col_nb","text":"flex_choose_i_and_col_nb ( a , flex_2d = True ) Choose selection index and column based on the array's shape. Instead of expensive broadcasting, keep the original shape and do indexing in a smart way. A nice feature of this is that it has almost no memory footprint and can broadcast in any direction infinitely. Call it once before using flex_select_nb() . if flex_2d is True, 1-dim array will correspond to columns, otherwise to rows.","title":"flex_choose_i_and_col_nb()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.flex_select_auto_nb","text":"flex_select_auto_nb ( a , i , col , flex_2d = True ) Combines flex_choose_i_and_col_nb() and flex_select_nb() .","title":"flex_select_auto_nb()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.flex_select_nb","text":"flex_select_nb ( a , i , col , flex_i , flex_col , flex_2d = True ) Select element of a as if it has been broadcast.","title":"flex_select_nb()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.get_multiindex_series","text":"get_multiindex_series ( arg ) Get Series with a multi-index. If DataFrame has been passed, should at maximum have one row or column.","title":"get_multiindex_series()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.make_symmetric","text":"make_symmetric ( arg , sort = True ) Make arg symmetric. The index and columns of the resulting DataFrame will be identical. Requires the index and columns to have the same number of levels. Pass sort=False if index and columns should not be sorted, but concatenated and get duplicates removed. Usage >>> import pandas as pd >>> from vectorbt.base.reshape_fns import make_symmetric >>> df = pd . DataFrame ([[ 1 , 2 ], [ 3 , 4 ]], index = [ 'a' , 'b' ], columns = [ 'c' , 'd' ]) >>> make_symmetric ( df ) a b c d a NaN NaN 1.0 2.0 b NaN NaN 3.0 4.0 c 1.0 3.0 NaN NaN d 2.0 4.0 NaN NaN","title":"make_symmetric()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.repeat","text":"repeat ( arg , n , axis = 1 , raw = False ) Repeat each element in arg n times along the specified axis.","title":"repeat()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.soft_to_ndim","text":"soft_to_ndim ( arg , ndim , raw = False ) Try to softly bring arg to the specified number of dimensions ndim (max 2).","title":"soft_to_ndim()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.tile","text":"tile ( arg , n , axis = 1 , raw = False ) Repeat the whole arg n times along the specified axis.","title":"tile()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.to_1d","text":"to_1d ( arg , raw = False ) Reshape argument to one dimension. If raw is True, returns NumPy array. If 2-dim, will collapse along axis 1 (i.e., DataFrame with one column to Series).","title":"to_1d()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.to_2d","text":"to_2d ( arg , raw = False , expand_axis = 1 ) Reshape argument to two dimensions. If raw is True, returns NumPy array. If 1-dim, will expand along axis 1 (i.e., Series to DataFrame with one column).","title":"to_2d()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.to_any_array","text":"to_any_array ( arg , raw = False ) Convert any array-like object to an array. Pandas objects are kept as-is.","title":"to_any_array()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.to_dict","text":"to_dict ( arg , orient = 'dict' ) Convert object to dict.","title":"to_dict()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.to_pd_array","text":"to_pd_array ( arg ) Convert any array-like object to a pandas object.","title":"to_pd_array()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.unstack_to_array","text":"unstack_to_array ( arg , levels = None ) Reshape arg based on its multi-index into a multi-dimensional array. Use levels to specify what index levels to unstack and in which order. Usage >>> import pandas as pd >>> from vectorbt.base.reshape_fns import unstack_to_array >>> index = pd . MultiIndex . from_arrays ( ... [[ 1 , 1 , 2 , 2 ], [ 3 , 4 , 3 , 4 ], [ 'a' , 'b' , 'c' , 'd' ]]) >>> sr = pd . Series ([ 1 , 2 , 3 , 4 ], index = index ) >>> unstack_to_array ( sr ) . shape (2, 2, 4) >>> unstack_to_array ( sr ) [[[ 1. nan nan nan] [nan 2. nan nan]] [[nan nan 3. nan] [nan nan nan 4.]]] >>> unstack_to_array ( sr , levels = ( 2 , 0 )) [[ 1. nan] [ 2. nan] [nan 3.] [nan 4.]]","title":"unstack_to_array()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.unstack_to_df","text":"unstack_to_df ( arg , index_levels = None , column_levels = None , symmetric = False , sort = True ) Reshape arg based on its multi-index into a DataFrame. Use index_levels to specify what index levels will form new index, and column_levels for new columns. Set symmetric to True to make DataFrame symmetric. Usage >>> import pandas as pd >>> from vectorbt.base.reshape_fns import unstack_to_df >>> index = pd . MultiIndex . from_arrays ( ... [[ 1 , 1 , 2 , 2 ], [ 3 , 4 , 3 , 4 ], [ 'a' , 'b' , 'c' , 'd' ]], ... names = [ 'x' , 'y' , 'z' ]) >>> sr = pd . Series ([ 1 , 2 , 3 , 4 ], index = index ) >>> unstack_to_df ( sr , index_levels = ( 0 , 1 ), column_levels = 2 ) z a b c d x y 1 3 1.0 NaN NaN NaN 1 4 NaN 2.0 NaN NaN 2 3 NaN NaN 3.0 NaN 2 4 NaN NaN NaN 4.0","title":"unstack_to_df()"},{"location":"api/base/reshape_fns/#vectorbt.base.reshape_fns.wrap_broadcasted","text":"wrap_broadcasted ( old_arg , new_arg , is_pd = False , new_index = None , new_columns = None ) If the newly brodcasted array was originally a pandas object, make it pandas object again and assign it the newly broadcast index/columns.","title":"wrap_broadcasted()"},{"location":"api/data/","text":"data package \u00b6 Modules for working with data sources. Sub-modules \u00b6 vectorbt.data.base vectorbt.data.custom vectorbt.data.updater","title":"data"},{"location":"api/data/#vectorbt.data","text":"Modules for working with data sources.","title":"vectorbt.data"},{"location":"api/data/#sub-modules","text":"vectorbt.data.base vectorbt.data.custom vectorbt.data.updater","title":"Sub-modules"},{"location":"api/data/base/","text":"base module \u00b6 Base data class. Class Data allows storing, downloading, updating, and managing data. It stores data as a dictionary of Series/DataFrames keyed by symbol, and makes sure that all pandas objects have the same index and columns by aligning them. Downloading \u00b6 Data can be downloaded by overriding the Data.download_symbol() class method. What Data does under the hood is iterating over all symbols and calling this method. Let's create a simple data class RandomData that generates price based on random returns with provided mean and standard deviation: >>> import numpy as np >>> import pandas as pd >>> import vectorbt as vbt >>> class RandomData ( vbt . Data ): ... @classmethod ... def download_symbol ( cls , symbol , mean = 0. , stdev = 0.1 , start_value = 100 , ... start_dt = '2021-01-01' , end_dt = '2021-01-10' ): ... index = pd . date_range ( start_dt , end_dt ) ... rand_returns = np . random . normal ( mean , stdev , size = len ( index )) ... rand_price = start_value + np . cumprod ( rand_returns + 1 ) ... return pd . Series ( rand_price , index = index ) >>> rand_data = RandomData . download ([ 'RANDNX1' , 'RANDNX2' ]) >>> rand_data . get () symbol RANDNX1 RANDNX2 2021-01-01 101.042956 100.920462 2021-01-02 100.987327 100.956455 2021-01-03 101.022333 100.955128 2021-01-04 101.084243 100.791793 2021-01-05 101.158619 100.781000 2021-01-06 101.172688 100.786198 2021-01-07 101.311609 100.848192 2021-01-08 101.331841 100.861500 2021-01-09 101.440530 100.944935 2021-01-10 101.585689 100.993223 To provide different keyword arguments for different symbols, we can use symbol_dict : >>> start_value = vbt . symbol_dict ({ 'RANDNX2' : 200 }) >>> rand_data = RandomData . download ([ 'RANDNX1' , 'RANDNX2' ], start_value = start_value ) >>> rand_data . get () symbol RANDNX1 RANDNX2 2021-01-01 101.083324 200.886078 2021-01-02 101.113405 200.791934 2021-01-03 101.169194 200.852877 2021-01-04 101.164033 200.820111 2021-01-05 101.326248 201.060448 2021-01-06 101.394482 200.876984 2021-01-07 101.494227 200.845519 2021-01-08 101.422012 200.963474 2021-01-09 101.493162 200.790369 2021-01-10 101.606052 200.752296 In case two symbols have different index or columns, they are automatically aligned based on missing_index and missing_columns respectively (see data in settings ): >>> start_dt = vbt . symbol_dict ({ 'RANDNX2' : '2021-01-03' }) >>> end_dt = vbt . symbol_dict ({ 'RANDNX2' : '2021-01-07' }) >>> rand_data = RandomData . download ( ... [ 'RANDNX1' , 'RANDNX2' ], start_value = start_value , ... start_dt = start_dt , end_dt = end_dt ) >>> rand_data . get () symbol RANDNX1 RANDNX2 2021-01-01 101.028054 NaN 2021-01-02 101.032090 NaN 2021-01-03 101.038531 200.936283 2021-01-04 101.068265 200.926764 2021-01-05 100.878492 200.898898 2021-01-06 100.857444 200.922368 2021-01-07 100.933123 200.987094 2021-01-08 100.938034 NaN 2021-01-09 101.044736 NaN 2021-01-10 101.098133 NaN Updating \u00b6 Updating can be implemented by overriding the Data.update_symbol() instance method, which takes the same arguments as Data.download_symbol() . In contrast to the download method, the update method is an instance method and can access the data downloaded earlier. It can also access the keyword arguments initially passed to the download method, accessible under Data.download_kwargs . Those arguments can be used as default arguments and overriden by arguments passed directly to the update method, using merge_dicts() . Let's define an update method that updates the latest data point and adds two news data points. Note that updating data always returns a new Data instance. >>> from datetime import timedelta >>> from vectorbt.utils.config import merge_dicts >>> class RandomData ( vbt . Data ): ... @classmethod ... def download_symbol ( cls , symbol , mean = 0. , stdev = 0.1 , start_value = 100 , ... start_dt = '2021-01-01' , end_dt = '2021-01-10' ): ... index = pd . date_range ( start_dt , end_dt ) ... rand_returns = np . random . normal ( mean , stdev , size = len ( index )) ... rand_price = start_value + np . cumprod ( rand_returns + 1 ) ... return pd . Series ( rand_price , index = index ) ... ... def update_symbol ( self , symbol , ** kwargs ): ... download_kwargs = self . select_symbol_kwargs ( symbol , self . download_kwargs ) ... download_kwargs [ 'start_dt' ] = self . data [ symbol ] . index [ - 1 ] ... download_kwargs [ 'end_dt' ] = download_kwargs [ 'start_dt' ] + timedelta ( days = 2 ) ... kwargs = merge_dicts ( download_kwargs , kwargs ) ... return self . download_symbol ( symbol , ** kwargs ) >>> rand_data = RandomData . download ([ 'RANDNX1' , 'RANDNX2' ], end_dt = '2021-01-05' ) >>> rand_data . get () symbol RANDNX1 RANDNX2 2021-01-01 100.956601 100.970865 2021-01-02 100.919011 100.987026 2021-01-03 101.062733 100.835376 2021-01-04 100.960535 100.820817 2021-01-05 100.834387 100.866549 >>> rand_data = rand_data . update () >>> rand_data . get () symbol RANDNX1 RANDNX2 2021-01-01 100.956601 100.970865 2021-01-02 100.919011 100.987026 2021-01-03 101.062733 100.835376 2021-01-04 100.960535 100.820817 2021-01-05 101.011255 100.887049 < updated from here 2021-01-06 101.004149 100.808410 2021-01-07 101.023673 100.714583 >>> rand_data = rand_data . update () >>> rand_data . get () symbol RANDNX1 RANDNX2 2021-01-01 100.956601 100.970865 2021-01-02 100.919011 100.987026 2021-01-03 101.062733 100.835376 2021-01-04 100.960535 100.820817 2021-01-05 101.011255 100.887049 2021-01-06 101.004149 100.808410 2021-01-07 100.883400 100.874922 < updated from here 2021-01-08 101.011738 100.780188 2021-01-09 100.912639 100.934014 Merging \u00b6 You can merge symbols from different Data instances either by subclassing Data and defining custom download and update methods, or by manually merging their data dicts into one data dict and passing it to the Data.from_data() class method. >>> rand_data1 = RandomData . download ( 'RANDNX1' , mean = 0.2 ) >>> rand_data2 = RandomData . download ( 'RANDNX2' , start_value = 200 , start_dt = '2021-01-05' ) >>> merged_data = vbt . Data . from_data ( vbt . merge_dicts ( rand_data1 . data , rand_data2 . data )) >>> merged_data . get () symbol RANDNX1 RANDNX2 2021-01-01 101.160718 NaN 2021-01-02 101.421020 NaN 2021-01-03 101.959176 NaN 2021-01-04 102.076670 NaN 2021-01-05 102.447234 200.916198 2021-01-06 103.195187 201.033907 2021-01-07 103.595915 200.908229 2021-01-08 104.332550 201.000497 2021-01-09 105.159708 201.019157 2021-01-10 106.729495 200.910210 Indexing \u00b6 Like any other class subclassing Wrapping , we can do pandas indexing on a Data instance, which forwards indexing operation to each Series/DataFrame: >>> rand_data . loc [ '2021-01-07' : '2021-01-09' ] <__main__.RandomData at 0x7fdba4e36198> >>> rand_data . loc [ '2021-01-07' : '2021-01-09' ] . get () symbol RANDNX1 RANDNX2 2021-01-07 100.883400 100.874922 2021-01-08 101.011738 100.780188 2021-01-09 100.912639 100.934014 Saving and loading \u00b6 Like any other class subclassing Pickleable , we can save a Data instance to the disk with Pickleable.save() and load it with Pickleable.load() : >>> rand_data . save ( 'rand_data' ) >>> rand_data = RandomData . load ( 'rand_data' ) >>> rand_data . get () symbol RANDNX1 RANDNX2 2021-01-01 100.956601 100.970865 2021-01-02 100.919011 100.987026 2021-01-03 101.062733 100.835376 2021-01-04 100.960535 100.820817 2021-01-05 101.011255 100.887049 2021-01-06 101.004149 100.808410 2021-01-07 100.883400 100.874922 2021-01-08 101.011738 100.780188 2021-01-09 100.912639 100.934014 Stats \u00b6 Hint See StatsBuilderMixin.stats() and Data.metrics . >>> rand_data = RandomData . download ([ 'RANDNX1' , 'RANDNX2' ]) >>> rand_data . stats ( column = 'a' ) Start 2021-01-01 00:00:00+00:00 End 2021-01-10 00:00:00+00:00 Period 10 days 00:00:00 Total Symbols 2 Null Counts: RANDNX1 0 Null Counts: RANDNX2 0 dtype: object StatsBuilderMixin.stats() also supports (re-)grouping: >>> rand_data . stats ( group_by = True ) Start 2021-01-01 00:00:00+00:00 End 2021-01-10 00:00:00+00:00 Period 10 days 00:00:00 Total Symbols 2 Null Counts: RANDNX1 0 Null Counts: RANDNX2 0 Name: group, dtype: object Plots \u00b6 Hint See PlotsBuilderMixin.plots() and Data.subplots . Data class has a single subplot based on Data.plot() : >>> rand_data . plots ( settings = dict ( base = 100 )) . show_svg () Data class \u00b6 Class that downloads, updates, and manages data coming from a data source. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.config Wrapping.iloc Wrapping.indexing_kwargs Wrapping.loc Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Wrapping.self_aliases Wrapping.wrapper Wrapping.writeable_attrs Subclasses AlpacaData BinanceData CCXTData SyntheticData YFData align_columns class method \u00b6 Data . align_columns ( data , missing = 'raise' ) Align data to have the same columns. See Data.align_index() for missing . align_index class method \u00b6 Data . align_index ( data , missing = 'nan' ) Align data to have the same index. The argument missing accepts the following values: 'nan': set missing data points to NaN 'drop': remove missing data points 'raise': raise an error concat method \u00b6 Data . concat ( level_name = 'symbol' ) Return a dict of Series/DataFrames with symbols as columns, keyed by column name. data property \u00b6 Data dictionary keyed by symbol. download class method \u00b6 Data . download ( symbols , tz_localize = None , tz_convert = None , missing_index = None , missing_columns = None , wrapper_kwargs = None , ** kwargs ) Download data using Data.download_symbol() . Args symbols :\u2002 hashable or sequence of hashable One or multiple symbols. Note Tuple is considered as a single symbol (since hashable). tz_localize :\u2002 any See Data.from_data() . tz_convert :\u2002 any See Data.from_data() . missing_index :\u2002 str See Data.from_data() . missing_columns :\u2002 str See Data.from_data() . wrapper_kwargs :\u2002 dict See Data.from_data() . **kwargs Passed to Data.download_symbol() . If two symbols require different keyword arguments, pass symbol_dict for each argument. download_kwargs property \u00b6 Keyword arguments initially passed to Data.download_symbol() . download_symbol class method \u00b6 Data . download_symbol ( symbol , ** kwargs ) Abstract method to download a symbol. from_data class method \u00b6 Data . from_data ( data , tz_localize = None , tz_convert = None , missing_index = None , missing_columns = None , wrapper_kwargs = None , ** kwargs ) Create a new Data instance from (aligned) data. Args data :\u2002 dict Dictionary of array-like objects keyed by symbol. tz_localize :\u2002 timezone_like If the index is tz-naive, convert to a timezone. See to_timezone() . tz_convert :\u2002 timezone_like Convert the index from one timezone to another. See to_timezone() . missing_index :\u2002 str See Data.align_index() . missing_columns :\u2002 str See Data.align_columns() . wrapper_kwargs :\u2002 dict Keyword arguments passed to ArrayWrapper . **kwargs Keyword arguments passed to the __init__ method. For defaults, see data in settings . get method \u00b6 Data . get ( column = None , ** kwargs ) Get column data. If one symbol, returns data for that symbol. If multiple symbols, performs concatenation first and returns a DataFrame if one column and a tuple of DataFrames if a list of columns passed. indexing_func method \u00b6 Data . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on Data . metrics class variable \u00b6 Metrics supported by Data . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function Data.<lambda> at 0x7facbb5b0d90>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function Data.<lambda> at 0x7facbb5b0e18>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function Data.<lambda> at 0x7facbb5b0ea0>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"total_symbols\" : { \"title\" : \"Total Symbols\" , \"calc_func\" : \"<function Data.<lambda> at 0x7facbb5b0f28>\" , \"agg_func\" : null , \"tags\" : \"data\" }, \"null_counts\" : { \"title\" : \"Null Counts\" , \"calc_func\" : \"<function Data.<lambda> at 0x7facbb5bb048>\" , \"tags\" : \"data\" } } ) Returns Data._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable Data._metrics . missing_columns property \u00b6 missing_columns initially passed to Data.download_symbol() . missing_index property \u00b6 missing_index initially passed to Data.download_symbol() . plot method \u00b6 Data . plot ( column = None , base = None , ** kwargs ) Plot orders. Args column :\u2002 str Name of the column to plot. base :\u2002 float Rebase all series of a column to a given intial base. Note The column should contain prices. kwargs :\u2002 dict Keyword arguments passed to GenericAccessor.plot() . Usage >>> import vectorbt as vbt >>> start = '2021-01-01 UTC' # crypto is in UTC >>> end = '2021-06-01 UTC' >>> data = vbt . YFData . download ([ 'BTC-USD' , 'ETH-USD' , 'ADA-USD' ], start = start , end = end ) >>> data . plot ( column = 'Close' , base = 1 ) plots_defaults property \u00b6 Defaults for PlotsBuilderMixin.plots() . Merges PlotsBuilderMixin.plots_defaults and data.plots from settings . select_symbol_kwargs class method \u00b6 Data . select_symbol_kwargs ( symbol , kwargs ) Select keyword arguments belonging to symbol . stats_defaults property \u00b6 Defaults for StatsBuilderMixin.stats() . Merges StatsBuilderMixin.stats_defaults and data.stats from settings . subplots class variable \u00b6 Subplots supported by Data . Co nf ig( { \"plot\" : { \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"pass_add_trace_kwargs\" : true , \"tags\" : \"data\" } } ) Returns Data._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable Data._subplots . symbols property \u00b6 List of symbols. tz_convert property \u00b6 tz_convert initially passed to Data.download_symbol() . tz_localize property \u00b6 tz_localize initially passed to Data.download_symbol() . update method \u00b6 Data . update ( ** kwargs ) Update the data using Data.update_symbol() . Args **kwargs Passed to Data.update_symbol() . If two symbols require different keyword arguments, pass symbol_dict for each argument. Note Returns a new Data instance. update_symbol method \u00b6 Data . update_symbol ( symbol , ** kwargs ) Abstract method to update a symbol. MetaData class \u00b6 Meta class that exposes a read-only class property StatsBuilderMixin.metrics . Superclasses MetaPlotsBuilderMixin MetaStatsBuilderMixin builtins.type Inherited members MetaPlotsBuilderMixin.subplots MetaStatsBuilderMixin.metrics symbol_dict class \u00b6 Dict that contains symbols as keys. Superclasses builtins.dict","title":"base"},{"location":"api/data/base/#vectorbt.data.base","text":"Base data class. Class Data allows storing, downloading, updating, and managing data. It stores data as a dictionary of Series/DataFrames keyed by symbol, and makes sure that all pandas objects have the same index and columns by aligning them.","title":"vectorbt.data.base"},{"location":"api/data/base/#downloading","text":"Data can be downloaded by overriding the Data.download_symbol() class method. What Data does under the hood is iterating over all symbols and calling this method. Let's create a simple data class RandomData that generates price based on random returns with provided mean and standard deviation: >>> import numpy as np >>> import pandas as pd >>> import vectorbt as vbt >>> class RandomData ( vbt . Data ): ... @classmethod ... def download_symbol ( cls , symbol , mean = 0. , stdev = 0.1 , start_value = 100 , ... start_dt = '2021-01-01' , end_dt = '2021-01-10' ): ... index = pd . date_range ( start_dt , end_dt ) ... rand_returns = np . random . normal ( mean , stdev , size = len ( index )) ... rand_price = start_value + np . cumprod ( rand_returns + 1 ) ... return pd . Series ( rand_price , index = index ) >>> rand_data = RandomData . download ([ 'RANDNX1' , 'RANDNX2' ]) >>> rand_data . get () symbol RANDNX1 RANDNX2 2021-01-01 101.042956 100.920462 2021-01-02 100.987327 100.956455 2021-01-03 101.022333 100.955128 2021-01-04 101.084243 100.791793 2021-01-05 101.158619 100.781000 2021-01-06 101.172688 100.786198 2021-01-07 101.311609 100.848192 2021-01-08 101.331841 100.861500 2021-01-09 101.440530 100.944935 2021-01-10 101.585689 100.993223 To provide different keyword arguments for different symbols, we can use symbol_dict : >>> start_value = vbt . symbol_dict ({ 'RANDNX2' : 200 }) >>> rand_data = RandomData . download ([ 'RANDNX1' , 'RANDNX2' ], start_value = start_value ) >>> rand_data . get () symbol RANDNX1 RANDNX2 2021-01-01 101.083324 200.886078 2021-01-02 101.113405 200.791934 2021-01-03 101.169194 200.852877 2021-01-04 101.164033 200.820111 2021-01-05 101.326248 201.060448 2021-01-06 101.394482 200.876984 2021-01-07 101.494227 200.845519 2021-01-08 101.422012 200.963474 2021-01-09 101.493162 200.790369 2021-01-10 101.606052 200.752296 In case two symbols have different index or columns, they are automatically aligned based on missing_index and missing_columns respectively (see data in settings ): >>> start_dt = vbt . symbol_dict ({ 'RANDNX2' : '2021-01-03' }) >>> end_dt = vbt . symbol_dict ({ 'RANDNX2' : '2021-01-07' }) >>> rand_data = RandomData . download ( ... [ 'RANDNX1' , 'RANDNX2' ], start_value = start_value , ... start_dt = start_dt , end_dt = end_dt ) >>> rand_data . get () symbol RANDNX1 RANDNX2 2021-01-01 101.028054 NaN 2021-01-02 101.032090 NaN 2021-01-03 101.038531 200.936283 2021-01-04 101.068265 200.926764 2021-01-05 100.878492 200.898898 2021-01-06 100.857444 200.922368 2021-01-07 100.933123 200.987094 2021-01-08 100.938034 NaN 2021-01-09 101.044736 NaN 2021-01-10 101.098133 NaN","title":"Downloading"},{"location":"api/data/base/#updating","text":"Updating can be implemented by overriding the Data.update_symbol() instance method, which takes the same arguments as Data.download_symbol() . In contrast to the download method, the update method is an instance method and can access the data downloaded earlier. It can also access the keyword arguments initially passed to the download method, accessible under Data.download_kwargs . Those arguments can be used as default arguments and overriden by arguments passed directly to the update method, using merge_dicts() . Let's define an update method that updates the latest data point and adds two news data points. Note that updating data always returns a new Data instance. >>> from datetime import timedelta >>> from vectorbt.utils.config import merge_dicts >>> class RandomData ( vbt . Data ): ... @classmethod ... def download_symbol ( cls , symbol , mean = 0. , stdev = 0.1 , start_value = 100 , ... start_dt = '2021-01-01' , end_dt = '2021-01-10' ): ... index = pd . date_range ( start_dt , end_dt ) ... rand_returns = np . random . normal ( mean , stdev , size = len ( index )) ... rand_price = start_value + np . cumprod ( rand_returns + 1 ) ... return pd . Series ( rand_price , index = index ) ... ... def update_symbol ( self , symbol , ** kwargs ): ... download_kwargs = self . select_symbol_kwargs ( symbol , self . download_kwargs ) ... download_kwargs [ 'start_dt' ] = self . data [ symbol ] . index [ - 1 ] ... download_kwargs [ 'end_dt' ] = download_kwargs [ 'start_dt' ] + timedelta ( days = 2 ) ... kwargs = merge_dicts ( download_kwargs , kwargs ) ... return self . download_symbol ( symbol , ** kwargs ) >>> rand_data = RandomData . download ([ 'RANDNX1' , 'RANDNX2' ], end_dt = '2021-01-05' ) >>> rand_data . get () symbol RANDNX1 RANDNX2 2021-01-01 100.956601 100.970865 2021-01-02 100.919011 100.987026 2021-01-03 101.062733 100.835376 2021-01-04 100.960535 100.820817 2021-01-05 100.834387 100.866549 >>> rand_data = rand_data . update () >>> rand_data . get () symbol RANDNX1 RANDNX2 2021-01-01 100.956601 100.970865 2021-01-02 100.919011 100.987026 2021-01-03 101.062733 100.835376 2021-01-04 100.960535 100.820817 2021-01-05 101.011255 100.887049 < updated from here 2021-01-06 101.004149 100.808410 2021-01-07 101.023673 100.714583 >>> rand_data = rand_data . update () >>> rand_data . get () symbol RANDNX1 RANDNX2 2021-01-01 100.956601 100.970865 2021-01-02 100.919011 100.987026 2021-01-03 101.062733 100.835376 2021-01-04 100.960535 100.820817 2021-01-05 101.011255 100.887049 2021-01-06 101.004149 100.808410 2021-01-07 100.883400 100.874922 < updated from here 2021-01-08 101.011738 100.780188 2021-01-09 100.912639 100.934014","title":"Updating"},{"location":"api/data/base/#merging","text":"You can merge symbols from different Data instances either by subclassing Data and defining custom download and update methods, or by manually merging their data dicts into one data dict and passing it to the Data.from_data() class method. >>> rand_data1 = RandomData . download ( 'RANDNX1' , mean = 0.2 ) >>> rand_data2 = RandomData . download ( 'RANDNX2' , start_value = 200 , start_dt = '2021-01-05' ) >>> merged_data = vbt . Data . from_data ( vbt . merge_dicts ( rand_data1 . data , rand_data2 . data )) >>> merged_data . get () symbol RANDNX1 RANDNX2 2021-01-01 101.160718 NaN 2021-01-02 101.421020 NaN 2021-01-03 101.959176 NaN 2021-01-04 102.076670 NaN 2021-01-05 102.447234 200.916198 2021-01-06 103.195187 201.033907 2021-01-07 103.595915 200.908229 2021-01-08 104.332550 201.000497 2021-01-09 105.159708 201.019157 2021-01-10 106.729495 200.910210","title":"Merging"},{"location":"api/data/base/#indexing","text":"Like any other class subclassing Wrapping , we can do pandas indexing on a Data instance, which forwards indexing operation to each Series/DataFrame: >>> rand_data . loc [ '2021-01-07' : '2021-01-09' ] <__main__.RandomData at 0x7fdba4e36198> >>> rand_data . loc [ '2021-01-07' : '2021-01-09' ] . get () symbol RANDNX1 RANDNX2 2021-01-07 100.883400 100.874922 2021-01-08 101.011738 100.780188 2021-01-09 100.912639 100.934014","title":"Indexing"},{"location":"api/data/base/#saving-and-loading","text":"Like any other class subclassing Pickleable , we can save a Data instance to the disk with Pickleable.save() and load it with Pickleable.load() : >>> rand_data . save ( 'rand_data' ) >>> rand_data = RandomData . load ( 'rand_data' ) >>> rand_data . get () symbol RANDNX1 RANDNX2 2021-01-01 100.956601 100.970865 2021-01-02 100.919011 100.987026 2021-01-03 101.062733 100.835376 2021-01-04 100.960535 100.820817 2021-01-05 101.011255 100.887049 2021-01-06 101.004149 100.808410 2021-01-07 100.883400 100.874922 2021-01-08 101.011738 100.780188 2021-01-09 100.912639 100.934014","title":"Saving and loading"},{"location":"api/data/base/#stats","text":"Hint See StatsBuilderMixin.stats() and Data.metrics . >>> rand_data = RandomData . download ([ 'RANDNX1' , 'RANDNX2' ]) >>> rand_data . stats ( column = 'a' ) Start 2021-01-01 00:00:00+00:00 End 2021-01-10 00:00:00+00:00 Period 10 days 00:00:00 Total Symbols 2 Null Counts: RANDNX1 0 Null Counts: RANDNX2 0 dtype: object StatsBuilderMixin.stats() also supports (re-)grouping: >>> rand_data . stats ( group_by = True ) Start 2021-01-01 00:00:00+00:00 End 2021-01-10 00:00:00+00:00 Period 10 days 00:00:00 Total Symbols 2 Null Counts: RANDNX1 0 Null Counts: RANDNX2 0 Name: group, dtype: object","title":"Stats"},{"location":"api/data/base/#plots","text":"Hint See PlotsBuilderMixin.plots() and Data.subplots . Data class has a single subplot based on Data.plot() : >>> rand_data . plots ( settings = dict ( base = 100 )) . show_svg ()","title":"Plots"},{"location":"api/data/base/#vectorbt.data.base.Data","text":"Class that downloads, updates, and manages data coming from a data source. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.config Wrapping.iloc Wrapping.indexing_kwargs Wrapping.loc Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Wrapping.self_aliases Wrapping.wrapper Wrapping.writeable_attrs Subclasses AlpacaData BinanceData CCXTData SyntheticData YFData","title":"Data"},{"location":"api/data/base/#vectorbt.data.base.Data.align_columns","text":"Data . align_columns ( data , missing = 'raise' ) Align data to have the same columns. See Data.align_index() for missing .","title":"align_columns()"},{"location":"api/data/base/#vectorbt.data.base.Data.align_index","text":"Data . align_index ( data , missing = 'nan' ) Align data to have the same index. The argument missing accepts the following values: 'nan': set missing data points to NaN 'drop': remove missing data points 'raise': raise an error","title":"align_index()"},{"location":"api/data/base/#vectorbt.data.base.Data.concat","text":"Data . concat ( level_name = 'symbol' ) Return a dict of Series/DataFrames with symbols as columns, keyed by column name.","title":"concat()"},{"location":"api/data/base/#vectorbt.data.base.Data.data","text":"Data dictionary keyed by symbol.","title":"data"},{"location":"api/data/base/#vectorbt.data.base.Data.download","text":"Data . download ( symbols , tz_localize = None , tz_convert = None , missing_index = None , missing_columns = None , wrapper_kwargs = None , ** kwargs ) Download data using Data.download_symbol() . Args symbols :\u2002 hashable or sequence of hashable One or multiple symbols. Note Tuple is considered as a single symbol (since hashable). tz_localize :\u2002 any See Data.from_data() . tz_convert :\u2002 any See Data.from_data() . missing_index :\u2002 str See Data.from_data() . missing_columns :\u2002 str See Data.from_data() . wrapper_kwargs :\u2002 dict See Data.from_data() . **kwargs Passed to Data.download_symbol() . If two symbols require different keyword arguments, pass symbol_dict for each argument.","title":"download()"},{"location":"api/data/base/#vectorbt.data.base.Data.download_kwargs","text":"Keyword arguments initially passed to Data.download_symbol() .","title":"download_kwargs"},{"location":"api/data/base/#vectorbt.data.base.Data.download_symbol","text":"Data . download_symbol ( symbol , ** kwargs ) Abstract method to download a symbol.","title":"download_symbol()"},{"location":"api/data/base/#vectorbt.data.base.Data.from_data","text":"Data . from_data ( data , tz_localize = None , tz_convert = None , missing_index = None , missing_columns = None , wrapper_kwargs = None , ** kwargs ) Create a new Data instance from (aligned) data. Args data :\u2002 dict Dictionary of array-like objects keyed by symbol. tz_localize :\u2002 timezone_like If the index is tz-naive, convert to a timezone. See to_timezone() . tz_convert :\u2002 timezone_like Convert the index from one timezone to another. See to_timezone() . missing_index :\u2002 str See Data.align_index() . missing_columns :\u2002 str See Data.align_columns() . wrapper_kwargs :\u2002 dict Keyword arguments passed to ArrayWrapper . **kwargs Keyword arguments passed to the __init__ method. For defaults, see data in settings .","title":"from_data()"},{"location":"api/data/base/#vectorbt.data.base.Data.get","text":"Data . get ( column = None , ** kwargs ) Get column data. If one symbol, returns data for that symbol. If multiple symbols, performs concatenation first and returns a DataFrame if one column and a tuple of DataFrames if a list of columns passed.","title":"get()"},{"location":"api/data/base/#vectorbt.data.base.Data.indexing_func","text":"Data . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on Data .","title":"indexing_func()"},{"location":"api/data/base/#vectorbt.data.base.Data.metrics","text":"Metrics supported by Data . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function Data.<lambda> at 0x7facbb5b0d90>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function Data.<lambda> at 0x7facbb5b0e18>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function Data.<lambda> at 0x7facbb5b0ea0>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"total_symbols\" : { \"title\" : \"Total Symbols\" , \"calc_func\" : \"<function Data.<lambda> at 0x7facbb5b0f28>\" , \"agg_func\" : null , \"tags\" : \"data\" }, \"null_counts\" : { \"title\" : \"Null Counts\" , \"calc_func\" : \"<function Data.<lambda> at 0x7facbb5bb048>\" , \"tags\" : \"data\" } } ) Returns Data._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable Data._metrics .","title":"metrics"},{"location":"api/data/base/#vectorbt.data.base.Data.missing_columns","text":"missing_columns initially passed to Data.download_symbol() .","title":"missing_columns"},{"location":"api/data/base/#vectorbt.data.base.Data.missing_index","text":"missing_index initially passed to Data.download_symbol() .","title":"missing_index"},{"location":"api/data/base/#vectorbt.data.base.Data.plot","text":"Data . plot ( column = None , base = None , ** kwargs ) Plot orders. Args column :\u2002 str Name of the column to plot. base :\u2002 float Rebase all series of a column to a given intial base. Note The column should contain prices. kwargs :\u2002 dict Keyword arguments passed to GenericAccessor.plot() . Usage >>> import vectorbt as vbt >>> start = '2021-01-01 UTC' # crypto is in UTC >>> end = '2021-06-01 UTC' >>> data = vbt . YFData . download ([ 'BTC-USD' , 'ETH-USD' , 'ADA-USD' ], start = start , end = end ) >>> data . plot ( column = 'Close' , base = 1 )","title":"plot()"},{"location":"api/data/base/#vectorbt.data.base.Data.plots_defaults","text":"Defaults for PlotsBuilderMixin.plots() . Merges PlotsBuilderMixin.plots_defaults and data.plots from settings .","title":"plots_defaults"},{"location":"api/data/base/#vectorbt.data.base.Data.select_symbol_kwargs","text":"Data . select_symbol_kwargs ( symbol , kwargs ) Select keyword arguments belonging to symbol .","title":"select_symbol_kwargs()"},{"location":"api/data/base/#vectorbt.data.base.Data.stats_defaults","text":"Defaults for StatsBuilderMixin.stats() . Merges StatsBuilderMixin.stats_defaults and data.stats from settings .","title":"stats_defaults"},{"location":"api/data/base/#vectorbt.data.base.Data.subplots","text":"Subplots supported by Data . Co nf ig( { \"plot\" : { \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"pass_add_trace_kwargs\" : true , \"tags\" : \"data\" } } ) Returns Data._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable Data._subplots .","title":"subplots"},{"location":"api/data/base/#vectorbt.data.base.Data.symbols","text":"List of symbols.","title":"symbols"},{"location":"api/data/base/#vectorbt.data.base.Data.tz_convert","text":"tz_convert initially passed to Data.download_symbol() .","title":"tz_convert"},{"location":"api/data/base/#vectorbt.data.base.Data.tz_localize","text":"tz_localize initially passed to Data.download_symbol() .","title":"tz_localize"},{"location":"api/data/base/#vectorbt.data.base.Data.update","text":"Data . update ( ** kwargs ) Update the data using Data.update_symbol() . Args **kwargs Passed to Data.update_symbol() . If two symbols require different keyword arguments, pass symbol_dict for each argument. Note Returns a new Data instance.","title":"update()"},{"location":"api/data/base/#vectorbt.data.base.Data.update_symbol","text":"Data . update_symbol ( symbol , ** kwargs ) Abstract method to update a symbol.","title":"update_symbol()"},{"location":"api/data/base/#vectorbt.data.base.MetaData","text":"Meta class that exposes a read-only class property StatsBuilderMixin.metrics . Superclasses MetaPlotsBuilderMixin MetaStatsBuilderMixin builtins.type Inherited members MetaPlotsBuilderMixin.subplots MetaStatsBuilderMixin.metrics","title":"MetaData"},{"location":"api/data/base/#vectorbt.data.base.symbol_dict","text":"Dict that contains symbols as keys. Superclasses builtins.dict","title":"symbol_dict"},{"location":"api/data/custom/","text":"custom module \u00b6 Custom data classes that subclass Data . generate_gbm_paths function \u00b6 generate_gbm_paths ( S0 , mu , sigma , T , M , I , seed = None ) Generate using Geometric Brownian Motion (GBM). See https://stackoverflow.com/a/45036114/8141780. AlpacaData class \u00b6 Data for data coming from alpaca-trade-api . Sign up for Alpaca API keys under https://app.alpaca.markets/signup. Usage Fetch the 1-minute data of the last 2 hours, wait 1 minute, and update: >>> import vectorbt as vbt >>> alpaca_data = vbt . AlpacaData . download ( ... \"AAPL\" , ... start = '2 hours ago UTC' , ... end = '15 minutes ago UTC' , ... interval = '1m' ... ) >>> alpaca_data . get () Open High Low Close Volume timestamp 2021-12-27 14:04:00+00:00 177.0500 177.0500 177.0500 177.0500 1967 2021-12-27 14:05:00+00:00 177.0500 177.0500 177.0300 177.0500 3218 2021-12-27 14:06:00+00:00 177.0400 177.0400 177.0400 177.0400 873 ... ... ... ... ... ... 2021-12-27 15:46:00+00:00 177.9500 178.0000 177.8289 177.8850 162778 2021-12-27 15:47:00+00:00 177.8810 177.9600 177.8400 177.9515 123284 2021-12-27 15:48:00+00:00 177.9600 178.0500 177.9600 178.0100 159700 [105 rows x 5 columns] >>> import time >>> time . sleep ( 60 ) >>> alpaca_data = alpaca_data . update () >>> alpaca_data . get () Open High Low Close Volume timestamp 2021-12-27 14:04:00+00:00 177.0500 177.0500 177.0500 177.0500 1967 2021-12-27 14:05:00+00:00 177.0500 177.0500 177.0300 177.0500 3218 2021-12-27 14:06:00+00:00 177.0400 177.0400 177.0400 177.0400 873 ... ... ... ... ... ... 2021-12-27 15:47:00+00:00 177.8810 177.9600 177.8400 177.9515 123284 2021-12-27 15:48:00+00:00 177.9600 178.0500 177.9600 178.0100 159700 2021-12-27 15:49:00+00:00 178.0100 178.0700 177.9700 178.0650 185037 [106 rows x 5 columns] Superclasses AttrResolver Configured Data Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Data.align_columns() Data.align_index() Data.concat() Data.config Data.data Data.download() Data.download_kwargs Data.from_data() Data.get() Data.iloc Data.indexing_func() Data.indexing_kwargs Data.loc Data.missing_columns Data.missing_index Data.plot() Data.plots_defaults Data.select_symbol_kwargs() Data.self_aliases Data.stats_defaults Data.symbols Data.tz_convert Data.tz_localize Data.update() Data.wrapper Data.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() download_symbol class method \u00b6 AlpacaData . download_symbol ( symbol , timeframe = '1d' , start = 0 , end = 'now UTC' , adjustment = 'all' , limit = 500 , exchange = 'CBSE' , ** kwargs ) Download the symbol. Args symbol :\u2002 str Symbol. timeframe :\u2002 str Timeframe of data. Must be integer multiple of 'm' (minute), 'h' (hour) or 'd' (day). i.e. '15m'. See https://alpaca.markets/data. Note Data from the latest 15 minutes is not available with a free data plan. start :\u2002 any Start datetime. See to_tzaware_datetime() . end :\u2002 any End datetime. See to_tzaware_datetime() . adjustment :\u2002 str Specifies the corporate action adjustment for the stocks. Allowed are raw , split , dividend or all . limit :\u2002 int The maximum number of returned items. exchange :\u2002 str For crypto symbols. Which exchange you wish to retrieve data from. Allowed are FTX , ERSX , CBSE For defaults, see data.alpaca in settings . update_symbol method \u00b6 AlpacaData . update_symbol ( symbol , ** kwargs ) Update the symbol. **kwargs will override keyword arguments passed to AlpacaData.download_symbol() . BinanceData class \u00b6 Data for data coming from python-binance . Usage Fetch the 1-minute data of the last 2 hours, wait 1 minute, and update: >>> import vectorbt as vbt >>> binance_data = vbt . BinanceData . download ( ... \"BTCUSDT\" , ... start = '2 hours ago UTC' , ... end = 'now UTC' , ... interval = '1m' ... ) >>> binance_data . get () 2021-05-02 14:47:20.478000+00:00 - 2021-05-02 16:47:00+00:00: : 1it [00:00, 3.42it/s] Open High Low Close Volume \\ Open time 2021-05-02 14:48:00+00:00 56867.44 56913.57 56857.40 56913.56 28.709976 2021-05-02 14:49:00+00:00 56913.56 56913.57 56845.94 56888.00 19.734841 2021-05-02 14:50:00+00:00 56888.00 56947.32 56879.78 56934.71 23.150163 ... ... ... ... ... ... 2021-05-02 16:45:00+00:00 56664.13 56666.77 56641.11 56644.03 40.852719 2021-05-02 16:46:00+00:00 56644.02 56663.43 56605.17 56605.18 27.573654 2021-05-02 16:47:00+00:00 56605.18 56657.55 56605.17 56627.12 7.719933 Close time Quote volume \\ Open time 2021-05-02 14:48:00+00:00 2021-05-02 14:48:59.999000+00:00 1.633534e+06 2021-05-02 14:49:00+00:00 2021-05-02 14:49:59.999000+00:00 1.122519e+06 2021-05-02 14:50:00+00:00 2021-05-02 14:50:59.999000+00:00 1.317969e+06 ... ... ... 2021-05-02 16:45:00+00:00 2021-05-02 16:45:59.999000+00:00 2.314579e+06 2021-05-02 16:46:00+00:00 2021-05-02 16:46:59.999000+00:00 1.561548e+06 2021-05-02 16:47:00+00:00 2021-05-02 16:47:59.999000+00:00 4.371848e+05 Number of trades Taker base volume \\ Open time 2021-05-02 14:48:00+00:00 991 13.771152 2021-05-02 14:49:00+00:00 816 5.981942 2021-05-02 14:50:00+00:00 1086 10.813757 ... ... ... 2021-05-02 16:45:00+00:00 1006 18.106933 2021-05-02 16:46:00+00:00 916 14.869411 2021-05-02 16:47:00+00:00 353 3.903321 Taker quote volume Open time 2021-05-02 14:48:00+00:00 7.835391e+05 2021-05-02 14:49:00+00:00 3.402170e+05 2021-05-02 14:50:00+00:00 6.156418e+05 ... ... 2021-05-02 16:45:00+00:00 1.025892e+06 2021-05-02 16:46:00+00:00 8.421173e+05 2021-05-02 16:47:00+00:00 2.210323e+05 [120 rows x 10 columns] >>> import time >>> time . sleep ( 60 ) >>> binance_data = binance_data . update () >>> binance_data . get () Open High Low Close Volume \\ Open time 2021-05-02 14:48:00+00:00 56867.44 56913.57 56857.40 56913.56 28.709976 2021-05-02 14:49:00+00:00 56913.56 56913.57 56845.94 56888.00 19.734841 2021-05-02 14:50:00+00:00 56888.00 56947.32 56879.78 56934.71 23.150163 ... ... ... ... ... ... 2021-05-02 16:46:00+00:00 56644.02 56663.43 56605.17 56605.18 27.573654 2021-05-02 16:47:00+00:00 56605.18 56657.55 56605.17 56625.76 14.615437 2021-05-02 16:48:00+00:00 56625.75 56643.60 56614.32 56623.01 5.895843 Close time Quote volume \\ Open time 2021-05-02 14:48:00+00:00 2021-05-02 14:48:59.999000+00:00 1.633534e+06 2021-05-02 14:49:00+00:00 2021-05-02 14:49:59.999000+00:00 1.122519e+06 2021-05-02 14:50:00+00:00 2021-05-02 14:50:59.999000+00:00 1.317969e+06 ... ... ... 2021-05-02 16:46:00+00:00 2021-05-02 16:46:59.999000+00:00 1.561548e+06 2021-05-02 16:47:00+00:00 2021-05-02 16:47:59.999000+00:00 8.276017e+05 2021-05-02 16:48:00+00:00 2021-05-02 16:48:59.999000+00:00 3.338702e+05 Number of trades Taker base volume \\ Open time 2021-05-02 14:48:00+00:00 991 13.771152 2021-05-02 14:49:00+00:00 816 5.981942 2021-05-02 14:50:00+00:00 1086 10.813757 ... ... ... 2021-05-02 16:46:00+00:00 916 14.869411 2021-05-02 16:47:00+00:00 912 7.778489 2021-05-02 16:48:00+00:00 308 2.358130 Taker quote volume Open time 2021-05-02 14:48:00+00:00 7.835391e+05 2021-05-02 14:49:00+00:00 3.402170e+05 2021-05-02 14:50:00+00:00 6.156418e+05 ... ... 2021-05-02 16:46:00+00:00 8.421173e+05 2021-05-02 16:47:00+00:00 4.404362e+05 2021-05-02 16:48:00+00:00 1.335474e+05 [121 rows x 10 columns] Superclasses AttrResolver Configured Data Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Data.align_columns() Data.align_index() Data.concat() Data.config Data.data Data.download_kwargs Data.from_data() Data.get() Data.iloc Data.indexing_func() Data.indexing_kwargs Data.loc Data.missing_columns Data.missing_index Data.plot() Data.plots_defaults Data.select_symbol_kwargs() Data.self_aliases Data.stats_defaults Data.symbols Data.tz_convert Data.tz_localize Data.update() Data.wrapper Data.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() download class method \u00b6 BinanceData . download ( symbols , client = None , ** kwargs ) Override Data.download() to instantiate a Binance client. download_symbol class method \u00b6 BinanceData . download_symbol ( symbol , client = None , interval = '1d' , start = 0 , end = 'now UTC' , delay = 500 , limit = 500 , show_progress = True , tqdm_kwargs = None ) Download the symbol. Args symbol :\u2002 str Symbol. client :\u2002 binance.client.Client Binance client of type binance.client.Client . interval :\u2002 str Kline interval. See binance.enums . start :\u2002 any Start datetime. See to_tzaware_datetime() . end :\u2002 any End datetime. See to_tzaware_datetime() . delay :\u2002 float Time to sleep after each request (in milliseconds). limit :\u2002 int The maximum number of returned items. show_progress :\u2002 bool Whether to show the progress bar. tqdm_kwargs :\u2002 dict Keyword arguments passed to tqdm . For defaults, see data.binance in settings . update_symbol method \u00b6 BinanceData . update_symbol ( symbol , ** kwargs ) Update the symbol. **kwargs will override keyword arguments passed to BinanceData.download_symbol() . CCXTData class \u00b6 Data for data coming from ccxt . Usage Fetch the 1-minute data of the last 2 hours, wait 1 minute, and update: >>> import vectorbt as vbt >>> ccxt_data = vbt . CCXTData . download ( ... \"BTC/USDT\" , ... start = '2 hours ago UTC' , ... end = 'now UTC' , ... timeframe = '1m' ... ) >>> ccxt_data . get () 2021-05-02 14:50:26.305000+00:00 - 2021-05-02 16:50:00+00:00: : 1it [00:00, 1.96it/s] Open High Low Close Volume Open time 2021-05-02 14:51:00+00:00 56934.70 56964.59 56910.00 56948.99 22.158319 2021-05-02 14:52:00+00:00 56948.99 56999.00 56940.04 56977.62 46.958464 2021-05-02 14:53:00+00:00 56977.61 56987.09 56882.98 56885.42 27.752200 ... ... ... ... ... ... 2021-05-02 16:48:00+00:00 56625.75 56643.60 56595.47 56596.01 15.452510 2021-05-02 16:49:00+00:00 56596.00 56664.14 56596.00 56640.35 12.777475 2021-05-02 16:50:00+00:00 56640.35 56675.82 56640.35 56670.65 6.882321 [120 rows x 5 columns] >>> import time >>> time . sleep ( 60 ) >>> ccxt_data = ccxt_data . update () >>> ccxt_data . get () Open High Low Close Volume Open time 2021-05-02 14:51:00+00:00 56934.70 56964.59 56910.00 56948.99 22.158319 2021-05-02 14:52:00+00:00 56948.99 56999.00 56940.04 56977.62 46.958464 2021-05-02 14:53:00+00:00 56977.61 56987.09 56882.98 56885.42 27.752200 ... ... ... ... ... ... 2021-05-02 16:49:00+00:00 56596.00 56664.14 56596.00 56640.35 12.777475 2021-05-02 16:50:00+00:00 56640.35 56689.99 56640.35 56678.33 14.610231 2021-05-02 16:51:00+00:00 56678.33 56688.99 56636.89 56653.42 11.647158 [121 rows x 5 columns] Superclasses AttrResolver Configured Data Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Data.align_columns() Data.align_index() Data.concat() Data.config Data.data Data.download() Data.download_kwargs Data.from_data() Data.get() Data.iloc Data.indexing_func() Data.indexing_kwargs Data.loc Data.missing_columns Data.missing_index Data.plot() Data.plots_defaults Data.select_symbol_kwargs() Data.self_aliases Data.stats_defaults Data.symbols Data.tz_convert Data.tz_localize Data.update() Data.wrapper Data.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() download_symbol class method \u00b6 CCXTData . download_symbol ( symbol , exchange = 'binance' , config = None , timeframe = '1d' , start = 0 , end = 'now UTC' , delay = None , limit = 500 , retries = 3 , show_progress = True , params = None , tqdm_kwargs = None ) Download the symbol. Args symbol :\u2002 str Symbol. exchange :\u2002 str or object Exchange identifier or an exchange object of type ccxt.base.exchange.Exchange . config :\u2002 dict Config passed to the exchange upon instantiation. Will raise an exception if exchange has been already instantiated. timeframe :\u2002 str Timeframe supported by the exchange. start :\u2002 any Start datetime. See to_tzaware_datetime() . end :\u2002 any End datetime. See to_tzaware_datetime() . delay :\u2002 float Time to sleep after each request (in milliseconds). Note Use only if enableRateLimit is not set. limit :\u2002 int The maximum number of returned items. retries :\u2002 int The number of retries on failure to fetch data. show_progress :\u2002 bool Whether to show the progress bar. tqdm_kwargs :\u2002 dict Keyword arguments passed to tqdm . params :\u2002 dict Exchange-specific key-value parameters. For defaults, see data.ccxt in settings . update_symbol method \u00b6 CCXTData . update_symbol ( symbol , ** kwargs ) Update the symbol. **kwargs will override keyword arguments passed to CCXTData.download_symbol() . GBMData class \u00b6 SyntheticData for data generated using Geometric Brownian Motion (GBM). Usage See the example under BinanceData . >>> import vectorbt as vbt >>> gbm_data = vbt . GBMData . download ( 'GBM' , start = '2 hours ago' , end = 'now' , freq = '1min' , seed = 42 ) >>> gbm_data . get () 2021-05-02 14:14:15.182089+00:00 102.386605 2021-05-02 14:15:15.182089+00:00 101.554203 2021-05-02 14:16:15.182089+00:00 104.765771 ... ... 2021-05-02 16:12:15.182089+00:00 51.614839 2021-05-02 16:13:15.182089+00:00 53.525376 2021-05-02 16:14:15.182089+00:00 55.615250 Freq: T, Length: 121, dtype: float64 >>> import time >>> time . sleep ( 60 ) >>> gbm_data = gbm_data . update () >>> gbm_data . get () 2021-05-02 14:14:15.182089+00:00 102.386605 2021-05-02 14:15:15.182089+00:00 101.554203 2021-05-02 14:16:15.182089+00:00 104.765771 ... ... 2021-05-02 16:13:15.182089+00:00 53.525376 2021-05-02 16:14:15.182089+00:00 51.082220 2021-05-02 16:15:15.182089+00:00 54.725304 Freq: T, Length: 122, dtype: float64 Superclasses AttrResolver Configured Data Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin SyntheticData Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Data.align_columns() Data.align_index() Data.concat() Data.download() Data.from_data() Data.get() Data.indexing_func() Data.plot() Data.select_symbol_kwargs() Data.update() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() SyntheticData.config SyntheticData.data SyntheticData.download_kwargs SyntheticData.download_symbol() SyntheticData.iloc SyntheticData.indexing_kwargs SyntheticData.loc SyntheticData.missing_columns SyntheticData.missing_index SyntheticData.plots_defaults SyntheticData.self_aliases SyntheticData.stats_defaults SyntheticData.symbols SyntheticData.tz_convert SyntheticData.tz_localize SyntheticData.wrapper SyntheticData.writeable_attrs Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() generate_symbol class method \u00b6 GBMData . generate_symbol ( symbol , index , S0 = 100.0 , mu = 0.0 , sigma = 0.05 , T = None , I = 1 , seed = None ) Generate the symbol using generate_gbm_paths() . Args symbol :\u2002 str Symbol. index :\u2002 pd.Index Pandas index. S0 :\u2002 float Value at time 0. Does not appear as the first value in the output data. mu :\u2002 float Drift, or mean of the percentage change. sigma :\u2002 float Standard deviation of the percentage change. T :\u2002 int Number of time steps. Defaults to the length of index . I :\u2002 int Number of generated paths (columns in our case). seed :\u2002 int Set seed to make the results deterministic. update_symbol method \u00b6 GBMData . update_symbol ( symbol , ** kwargs ) Update the symbol. **kwargs will override keyword arguments passed to SyntheticData.download_symbol() . SyntheticData class \u00b6 Data for synthetically generated data. Superclasses AttrResolver Configured Data Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Data.align_columns() Data.align_index() Data.concat() Data.config Data.data Data.download() Data.download_kwargs Data.from_data() Data.get() Data.iloc Data.indexing_func() Data.indexing_kwargs Data.loc Data.missing_columns Data.missing_index Data.plot() Data.plots_defaults Data.select_symbol_kwargs() Data.self_aliases Data.stats_defaults Data.symbols Data.tz_convert Data.tz_localize Data.update() Data.wrapper Data.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses GBMData download_symbol class method \u00b6 SyntheticData . download_symbol ( symbol , start = 0 , end = 'now' , freq = None , date_range_kwargs = None , ** kwargs ) Download the symbol. Generates datetime index and passes it to SyntheticData.generate_symbol() to fill the Series/DataFrame with generated data. generate_symbol class method \u00b6 SyntheticData . generate_symbol ( symbol , index , ** kwargs ) Abstract method to generate a symbol. update_symbol method \u00b6 SyntheticData . update_symbol ( symbol , ** kwargs ) Update the symbol. **kwargs will override keyword arguments passed to SyntheticData.download_symbol() . YFData class \u00b6 Data for data coming from yfinance . Stocks are usually in the timezone \"+0500\" and cryptocurrencies in UTC. Warning Data coming from Yahoo is not the most stable data out there. Yahoo may manipulate data how they want, add noise, return missing data points (see volume in the example below), etc. It's only used in vectorbt for demonstration purposes. Usage Fetch the business day except the last 5 minutes of trading data, and then update with the missing 5 minutes: >>> import vectorbt as vbt >>> yf_data = vbt . YFData . download ( ... \"TSLA\" , ... start = '2021-04-12 09:30:00 -0400' , ... end = '2021-04-12 09:35:00 -0400' , ... interval = '1m' ... ) >>> yf_data . get ()) Open High Low Close \\ Datetime 2021-04-12 13:30:00+00:00 685.080017 685.679993 684.765015 685.679993 2021-04-12 13:31:00+00:00 684.625000 686.500000 684.010010 685.500000 2021-04-12 13:32:00+00:00 685.646790 686.820007 683.190002 686.455017 2021-04-12 13:33:00+00:00 686.455017 687.000000 685.000000 685.565002 2021-04-12 13:34:00+00:00 685.690002 686.400024 683.200012 683.715027 Volume Dividends Stock Splits Datetime 2021-04-12 13:30:00+00:00 0 0 0 2021-04-12 13:31:00+00:00 152276 0 0 2021-04-12 13:32:00+00:00 168363 0 0 2021-04-12 13:33:00+00:00 129607 0 0 2021-04-12 13:34:00+00:00 134620 0 0 >>> yf_data = yf_data . update ( end = '2021-04-12 09:40:00 -0400' ) >>> yf_data . get () Open High Low Close \\ Datetime 2021-04-12 13:30:00+00:00 685.080017 685.679993 684.765015 685.679993 2021-04-12 13:31:00+00:00 684.625000 686.500000 684.010010 685.500000 2021-04-12 13:32:00+00:00 685.646790 686.820007 683.190002 686.455017 2021-04-12 13:33:00+00:00 686.455017 687.000000 685.000000 685.565002 2021-04-12 13:34:00+00:00 685.690002 686.400024 683.200012 683.715027 2021-04-12 13:35:00+00:00 683.604980 684.340027 682.760071 684.135010 2021-04-12 13:36:00+00:00 684.130005 686.640015 683.333984 686.563904 2021-04-12 13:37:00+00:00 686.530029 688.549988 686.000000 686.635010 2021-04-12 13:38:00+00:00 686.593201 689.500000 686.409973 688.179993 2021-04-12 13:39:00+00:00 688.500000 689.347595 687.710022 688.070007 Volume Dividends Stock Splits Datetime 2021-04-12 13:30:00+00:00 0 0 0 2021-04-12 13:31:00+00:00 152276 0 0 2021-04-12 13:32:00+00:00 168363 0 0 2021-04-12 13:33:00+00:00 129607 0 0 2021-04-12 13:34:00+00:00 0 0 0 2021-04-12 13:35:00+00:00 110500 0 0 2021-04-12 13:36:00+00:00 148384 0 0 2021-04-12 13:37:00+00:00 243851 0 0 2021-04-12 13:38:00+00:00 203569 0 0 2021-04-12 13:39:00+00:00 93308 0 0 Superclasses AttrResolver Configured Data Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Data.align_columns() Data.align_index() Data.concat() Data.config Data.data Data.download() Data.download_kwargs Data.from_data() Data.get() Data.iloc Data.indexing_func() Data.indexing_kwargs Data.loc Data.missing_columns Data.missing_index Data.plot() Data.plots_defaults Data.select_symbol_kwargs() Data.self_aliases Data.stats_defaults Data.symbols Data.tz_convert Data.tz_localize Data.update() Data.wrapper Data.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() download_symbol class method \u00b6 YFData . download_symbol ( symbol , period = 'max' , start = None , end = None , ** kwargs ) Download the symbol. Args symbol :\u2002 str Symbol. period :\u2002 str Period. start :\u2002 any Start datetime. See to_tzaware_datetime() . end :\u2002 any End datetime. See to_tzaware_datetime() . **kwargs Keyword arguments passed to yfinance.base.TickerBase.history . update_symbol method \u00b6 YFData . update_symbol ( symbol , ** kwargs ) Update the symbol. **kwargs will override keyword arguments passed to YFData.download_symbol() .","title":"custom"},{"location":"api/data/custom/#vectorbt.data.custom","text":"Custom data classes that subclass Data .","title":"vectorbt.data.custom"},{"location":"api/data/custom/#vectorbt.data.custom.generate_gbm_paths","text":"generate_gbm_paths ( S0 , mu , sigma , T , M , I , seed = None ) Generate using Geometric Brownian Motion (GBM). See https://stackoverflow.com/a/45036114/8141780.","title":"generate_gbm_paths()"},{"location":"api/data/custom/#vectorbt.data.custom.AlpacaData","text":"Data for data coming from alpaca-trade-api . Sign up for Alpaca API keys under https://app.alpaca.markets/signup. Usage Fetch the 1-minute data of the last 2 hours, wait 1 minute, and update: >>> import vectorbt as vbt >>> alpaca_data = vbt . AlpacaData . download ( ... \"AAPL\" , ... start = '2 hours ago UTC' , ... end = '15 minutes ago UTC' , ... interval = '1m' ... ) >>> alpaca_data . get () Open High Low Close Volume timestamp 2021-12-27 14:04:00+00:00 177.0500 177.0500 177.0500 177.0500 1967 2021-12-27 14:05:00+00:00 177.0500 177.0500 177.0300 177.0500 3218 2021-12-27 14:06:00+00:00 177.0400 177.0400 177.0400 177.0400 873 ... ... ... ... ... ... 2021-12-27 15:46:00+00:00 177.9500 178.0000 177.8289 177.8850 162778 2021-12-27 15:47:00+00:00 177.8810 177.9600 177.8400 177.9515 123284 2021-12-27 15:48:00+00:00 177.9600 178.0500 177.9600 178.0100 159700 [105 rows x 5 columns] >>> import time >>> time . sleep ( 60 ) >>> alpaca_data = alpaca_data . update () >>> alpaca_data . get () Open High Low Close Volume timestamp 2021-12-27 14:04:00+00:00 177.0500 177.0500 177.0500 177.0500 1967 2021-12-27 14:05:00+00:00 177.0500 177.0500 177.0300 177.0500 3218 2021-12-27 14:06:00+00:00 177.0400 177.0400 177.0400 177.0400 873 ... ... ... ... ... ... 2021-12-27 15:47:00+00:00 177.8810 177.9600 177.8400 177.9515 123284 2021-12-27 15:48:00+00:00 177.9600 178.0500 177.9600 178.0100 159700 2021-12-27 15:49:00+00:00 178.0100 178.0700 177.9700 178.0650 185037 [106 rows x 5 columns] Superclasses AttrResolver Configured Data Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Data.align_columns() Data.align_index() Data.concat() Data.config Data.data Data.download() Data.download_kwargs Data.from_data() Data.get() Data.iloc Data.indexing_func() Data.indexing_kwargs Data.loc Data.missing_columns Data.missing_index Data.plot() Data.plots_defaults Data.select_symbol_kwargs() Data.self_aliases Data.stats_defaults Data.symbols Data.tz_convert Data.tz_localize Data.update() Data.wrapper Data.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"AlpacaData"},{"location":"api/data/custom/#vectorbt.data.custom.AlpacaData.download_symbol","text":"AlpacaData . download_symbol ( symbol , timeframe = '1d' , start = 0 , end = 'now UTC' , adjustment = 'all' , limit = 500 , exchange = 'CBSE' , ** kwargs ) Download the symbol. Args symbol :\u2002 str Symbol. timeframe :\u2002 str Timeframe of data. Must be integer multiple of 'm' (minute), 'h' (hour) or 'd' (day). i.e. '15m'. See https://alpaca.markets/data. Note Data from the latest 15 minutes is not available with a free data plan. start :\u2002 any Start datetime. See to_tzaware_datetime() . end :\u2002 any End datetime. See to_tzaware_datetime() . adjustment :\u2002 str Specifies the corporate action adjustment for the stocks. Allowed are raw , split , dividend or all . limit :\u2002 int The maximum number of returned items. exchange :\u2002 str For crypto symbols. Which exchange you wish to retrieve data from. Allowed are FTX , ERSX , CBSE For defaults, see data.alpaca in settings .","title":"download_symbol()"},{"location":"api/data/custom/#vectorbt.data.custom.AlpacaData.update_symbol","text":"AlpacaData . update_symbol ( symbol , ** kwargs ) Update the symbol. **kwargs will override keyword arguments passed to AlpacaData.download_symbol() .","title":"update_symbol()"},{"location":"api/data/custom/#vectorbt.data.custom.BinanceData","text":"Data for data coming from python-binance . Usage Fetch the 1-minute data of the last 2 hours, wait 1 minute, and update: >>> import vectorbt as vbt >>> binance_data = vbt . BinanceData . download ( ... \"BTCUSDT\" , ... start = '2 hours ago UTC' , ... end = 'now UTC' , ... interval = '1m' ... ) >>> binance_data . get () 2021-05-02 14:47:20.478000+00:00 - 2021-05-02 16:47:00+00:00: : 1it [00:00, 3.42it/s] Open High Low Close Volume \\ Open time 2021-05-02 14:48:00+00:00 56867.44 56913.57 56857.40 56913.56 28.709976 2021-05-02 14:49:00+00:00 56913.56 56913.57 56845.94 56888.00 19.734841 2021-05-02 14:50:00+00:00 56888.00 56947.32 56879.78 56934.71 23.150163 ... ... ... ... ... ... 2021-05-02 16:45:00+00:00 56664.13 56666.77 56641.11 56644.03 40.852719 2021-05-02 16:46:00+00:00 56644.02 56663.43 56605.17 56605.18 27.573654 2021-05-02 16:47:00+00:00 56605.18 56657.55 56605.17 56627.12 7.719933 Close time Quote volume \\ Open time 2021-05-02 14:48:00+00:00 2021-05-02 14:48:59.999000+00:00 1.633534e+06 2021-05-02 14:49:00+00:00 2021-05-02 14:49:59.999000+00:00 1.122519e+06 2021-05-02 14:50:00+00:00 2021-05-02 14:50:59.999000+00:00 1.317969e+06 ... ... ... 2021-05-02 16:45:00+00:00 2021-05-02 16:45:59.999000+00:00 2.314579e+06 2021-05-02 16:46:00+00:00 2021-05-02 16:46:59.999000+00:00 1.561548e+06 2021-05-02 16:47:00+00:00 2021-05-02 16:47:59.999000+00:00 4.371848e+05 Number of trades Taker base volume \\ Open time 2021-05-02 14:48:00+00:00 991 13.771152 2021-05-02 14:49:00+00:00 816 5.981942 2021-05-02 14:50:00+00:00 1086 10.813757 ... ... ... 2021-05-02 16:45:00+00:00 1006 18.106933 2021-05-02 16:46:00+00:00 916 14.869411 2021-05-02 16:47:00+00:00 353 3.903321 Taker quote volume Open time 2021-05-02 14:48:00+00:00 7.835391e+05 2021-05-02 14:49:00+00:00 3.402170e+05 2021-05-02 14:50:00+00:00 6.156418e+05 ... ... 2021-05-02 16:45:00+00:00 1.025892e+06 2021-05-02 16:46:00+00:00 8.421173e+05 2021-05-02 16:47:00+00:00 2.210323e+05 [120 rows x 10 columns] >>> import time >>> time . sleep ( 60 ) >>> binance_data = binance_data . update () >>> binance_data . get () Open High Low Close Volume \\ Open time 2021-05-02 14:48:00+00:00 56867.44 56913.57 56857.40 56913.56 28.709976 2021-05-02 14:49:00+00:00 56913.56 56913.57 56845.94 56888.00 19.734841 2021-05-02 14:50:00+00:00 56888.00 56947.32 56879.78 56934.71 23.150163 ... ... ... ... ... ... 2021-05-02 16:46:00+00:00 56644.02 56663.43 56605.17 56605.18 27.573654 2021-05-02 16:47:00+00:00 56605.18 56657.55 56605.17 56625.76 14.615437 2021-05-02 16:48:00+00:00 56625.75 56643.60 56614.32 56623.01 5.895843 Close time Quote volume \\ Open time 2021-05-02 14:48:00+00:00 2021-05-02 14:48:59.999000+00:00 1.633534e+06 2021-05-02 14:49:00+00:00 2021-05-02 14:49:59.999000+00:00 1.122519e+06 2021-05-02 14:50:00+00:00 2021-05-02 14:50:59.999000+00:00 1.317969e+06 ... ... ... 2021-05-02 16:46:00+00:00 2021-05-02 16:46:59.999000+00:00 1.561548e+06 2021-05-02 16:47:00+00:00 2021-05-02 16:47:59.999000+00:00 8.276017e+05 2021-05-02 16:48:00+00:00 2021-05-02 16:48:59.999000+00:00 3.338702e+05 Number of trades Taker base volume \\ Open time 2021-05-02 14:48:00+00:00 991 13.771152 2021-05-02 14:49:00+00:00 816 5.981942 2021-05-02 14:50:00+00:00 1086 10.813757 ... ... ... 2021-05-02 16:46:00+00:00 916 14.869411 2021-05-02 16:47:00+00:00 912 7.778489 2021-05-02 16:48:00+00:00 308 2.358130 Taker quote volume Open time 2021-05-02 14:48:00+00:00 7.835391e+05 2021-05-02 14:49:00+00:00 3.402170e+05 2021-05-02 14:50:00+00:00 6.156418e+05 ... ... 2021-05-02 16:46:00+00:00 8.421173e+05 2021-05-02 16:47:00+00:00 4.404362e+05 2021-05-02 16:48:00+00:00 1.335474e+05 [121 rows x 10 columns] Superclasses AttrResolver Configured Data Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Data.align_columns() Data.align_index() Data.concat() Data.config Data.data Data.download_kwargs Data.from_data() Data.get() Data.iloc Data.indexing_func() Data.indexing_kwargs Data.loc Data.missing_columns Data.missing_index Data.plot() Data.plots_defaults Data.select_symbol_kwargs() Data.self_aliases Data.stats_defaults Data.symbols Data.tz_convert Data.tz_localize Data.update() Data.wrapper Data.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"BinanceData"},{"location":"api/data/custom/#vectorbt.data.custom.BinanceData.download","text":"BinanceData . download ( symbols , client = None , ** kwargs ) Override Data.download() to instantiate a Binance client.","title":"download()"},{"location":"api/data/custom/#vectorbt.data.custom.BinanceData.download_symbol","text":"BinanceData . download_symbol ( symbol , client = None , interval = '1d' , start = 0 , end = 'now UTC' , delay = 500 , limit = 500 , show_progress = True , tqdm_kwargs = None ) Download the symbol. Args symbol :\u2002 str Symbol. client :\u2002 binance.client.Client Binance client of type binance.client.Client . interval :\u2002 str Kline interval. See binance.enums . start :\u2002 any Start datetime. See to_tzaware_datetime() . end :\u2002 any End datetime. See to_tzaware_datetime() . delay :\u2002 float Time to sleep after each request (in milliseconds). limit :\u2002 int The maximum number of returned items. show_progress :\u2002 bool Whether to show the progress bar. tqdm_kwargs :\u2002 dict Keyword arguments passed to tqdm . For defaults, see data.binance in settings .","title":"download_symbol()"},{"location":"api/data/custom/#vectorbt.data.custom.BinanceData.update_symbol","text":"BinanceData . update_symbol ( symbol , ** kwargs ) Update the symbol. **kwargs will override keyword arguments passed to BinanceData.download_symbol() .","title":"update_symbol()"},{"location":"api/data/custom/#vectorbt.data.custom.CCXTData","text":"Data for data coming from ccxt . Usage Fetch the 1-minute data of the last 2 hours, wait 1 minute, and update: >>> import vectorbt as vbt >>> ccxt_data = vbt . CCXTData . download ( ... \"BTC/USDT\" , ... start = '2 hours ago UTC' , ... end = 'now UTC' , ... timeframe = '1m' ... ) >>> ccxt_data . get () 2021-05-02 14:50:26.305000+00:00 - 2021-05-02 16:50:00+00:00: : 1it [00:00, 1.96it/s] Open High Low Close Volume Open time 2021-05-02 14:51:00+00:00 56934.70 56964.59 56910.00 56948.99 22.158319 2021-05-02 14:52:00+00:00 56948.99 56999.00 56940.04 56977.62 46.958464 2021-05-02 14:53:00+00:00 56977.61 56987.09 56882.98 56885.42 27.752200 ... ... ... ... ... ... 2021-05-02 16:48:00+00:00 56625.75 56643.60 56595.47 56596.01 15.452510 2021-05-02 16:49:00+00:00 56596.00 56664.14 56596.00 56640.35 12.777475 2021-05-02 16:50:00+00:00 56640.35 56675.82 56640.35 56670.65 6.882321 [120 rows x 5 columns] >>> import time >>> time . sleep ( 60 ) >>> ccxt_data = ccxt_data . update () >>> ccxt_data . get () Open High Low Close Volume Open time 2021-05-02 14:51:00+00:00 56934.70 56964.59 56910.00 56948.99 22.158319 2021-05-02 14:52:00+00:00 56948.99 56999.00 56940.04 56977.62 46.958464 2021-05-02 14:53:00+00:00 56977.61 56987.09 56882.98 56885.42 27.752200 ... ... ... ... ... ... 2021-05-02 16:49:00+00:00 56596.00 56664.14 56596.00 56640.35 12.777475 2021-05-02 16:50:00+00:00 56640.35 56689.99 56640.35 56678.33 14.610231 2021-05-02 16:51:00+00:00 56678.33 56688.99 56636.89 56653.42 11.647158 [121 rows x 5 columns] Superclasses AttrResolver Configured Data Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Data.align_columns() Data.align_index() Data.concat() Data.config Data.data Data.download() Data.download_kwargs Data.from_data() Data.get() Data.iloc Data.indexing_func() Data.indexing_kwargs Data.loc Data.missing_columns Data.missing_index Data.plot() Data.plots_defaults Data.select_symbol_kwargs() Data.self_aliases Data.stats_defaults Data.symbols Data.tz_convert Data.tz_localize Data.update() Data.wrapper Data.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"CCXTData"},{"location":"api/data/custom/#vectorbt.data.custom.CCXTData.download_symbol","text":"CCXTData . download_symbol ( symbol , exchange = 'binance' , config = None , timeframe = '1d' , start = 0 , end = 'now UTC' , delay = None , limit = 500 , retries = 3 , show_progress = True , params = None , tqdm_kwargs = None ) Download the symbol. Args symbol :\u2002 str Symbol. exchange :\u2002 str or object Exchange identifier or an exchange object of type ccxt.base.exchange.Exchange . config :\u2002 dict Config passed to the exchange upon instantiation. Will raise an exception if exchange has been already instantiated. timeframe :\u2002 str Timeframe supported by the exchange. start :\u2002 any Start datetime. See to_tzaware_datetime() . end :\u2002 any End datetime. See to_tzaware_datetime() . delay :\u2002 float Time to sleep after each request (in milliseconds). Note Use only if enableRateLimit is not set. limit :\u2002 int The maximum number of returned items. retries :\u2002 int The number of retries on failure to fetch data. show_progress :\u2002 bool Whether to show the progress bar. tqdm_kwargs :\u2002 dict Keyword arguments passed to tqdm . params :\u2002 dict Exchange-specific key-value parameters. For defaults, see data.ccxt in settings .","title":"download_symbol()"},{"location":"api/data/custom/#vectorbt.data.custom.CCXTData.update_symbol","text":"CCXTData . update_symbol ( symbol , ** kwargs ) Update the symbol. **kwargs will override keyword arguments passed to CCXTData.download_symbol() .","title":"update_symbol()"},{"location":"api/data/custom/#vectorbt.data.custom.GBMData","text":"SyntheticData for data generated using Geometric Brownian Motion (GBM). Usage See the example under BinanceData . >>> import vectorbt as vbt >>> gbm_data = vbt . GBMData . download ( 'GBM' , start = '2 hours ago' , end = 'now' , freq = '1min' , seed = 42 ) >>> gbm_data . get () 2021-05-02 14:14:15.182089+00:00 102.386605 2021-05-02 14:15:15.182089+00:00 101.554203 2021-05-02 14:16:15.182089+00:00 104.765771 ... ... 2021-05-02 16:12:15.182089+00:00 51.614839 2021-05-02 16:13:15.182089+00:00 53.525376 2021-05-02 16:14:15.182089+00:00 55.615250 Freq: T, Length: 121, dtype: float64 >>> import time >>> time . sleep ( 60 ) >>> gbm_data = gbm_data . update () >>> gbm_data . get () 2021-05-02 14:14:15.182089+00:00 102.386605 2021-05-02 14:15:15.182089+00:00 101.554203 2021-05-02 14:16:15.182089+00:00 104.765771 ... ... 2021-05-02 16:13:15.182089+00:00 53.525376 2021-05-02 16:14:15.182089+00:00 51.082220 2021-05-02 16:15:15.182089+00:00 54.725304 Freq: T, Length: 122, dtype: float64 Superclasses AttrResolver Configured Data Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin SyntheticData Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Data.align_columns() Data.align_index() Data.concat() Data.download() Data.from_data() Data.get() Data.indexing_func() Data.plot() Data.select_symbol_kwargs() Data.update() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() SyntheticData.config SyntheticData.data SyntheticData.download_kwargs SyntheticData.download_symbol() SyntheticData.iloc SyntheticData.indexing_kwargs SyntheticData.loc SyntheticData.missing_columns SyntheticData.missing_index SyntheticData.plots_defaults SyntheticData.self_aliases SyntheticData.stats_defaults SyntheticData.symbols SyntheticData.tz_convert SyntheticData.tz_localize SyntheticData.wrapper SyntheticData.writeable_attrs Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"GBMData"},{"location":"api/data/custom/#vectorbt.data.custom.GBMData.generate_symbol","text":"GBMData . generate_symbol ( symbol , index , S0 = 100.0 , mu = 0.0 , sigma = 0.05 , T = None , I = 1 , seed = None ) Generate the symbol using generate_gbm_paths() . Args symbol :\u2002 str Symbol. index :\u2002 pd.Index Pandas index. S0 :\u2002 float Value at time 0. Does not appear as the first value in the output data. mu :\u2002 float Drift, or mean of the percentage change. sigma :\u2002 float Standard deviation of the percentage change. T :\u2002 int Number of time steps. Defaults to the length of index . I :\u2002 int Number of generated paths (columns in our case). seed :\u2002 int Set seed to make the results deterministic.","title":"generate_symbol()"},{"location":"api/data/custom/#vectorbt.data.custom.GBMData.update_symbol","text":"GBMData . update_symbol ( symbol , ** kwargs ) Update the symbol. **kwargs will override keyword arguments passed to SyntheticData.download_symbol() .","title":"update_symbol()"},{"location":"api/data/custom/#vectorbt.data.custom.SyntheticData","text":"Data for synthetically generated data. Superclasses AttrResolver Configured Data Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Data.align_columns() Data.align_index() Data.concat() Data.config Data.data Data.download() Data.download_kwargs Data.from_data() Data.get() Data.iloc Data.indexing_func() Data.indexing_kwargs Data.loc Data.missing_columns Data.missing_index Data.plot() Data.plots_defaults Data.select_symbol_kwargs() Data.self_aliases Data.stats_defaults Data.symbols Data.tz_convert Data.tz_localize Data.update() Data.wrapper Data.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses GBMData","title":"SyntheticData"},{"location":"api/data/custom/#vectorbt.data.custom.SyntheticData.download_symbol","text":"SyntheticData . download_symbol ( symbol , start = 0 , end = 'now' , freq = None , date_range_kwargs = None , ** kwargs ) Download the symbol. Generates datetime index and passes it to SyntheticData.generate_symbol() to fill the Series/DataFrame with generated data.","title":"download_symbol()"},{"location":"api/data/custom/#vectorbt.data.custom.SyntheticData.generate_symbol","text":"SyntheticData . generate_symbol ( symbol , index , ** kwargs ) Abstract method to generate a symbol.","title":"generate_symbol()"},{"location":"api/data/custom/#vectorbt.data.custom.SyntheticData.update_symbol","text":"SyntheticData . update_symbol ( symbol , ** kwargs ) Update the symbol. **kwargs will override keyword arguments passed to SyntheticData.download_symbol() .","title":"update_symbol()"},{"location":"api/data/custom/#vectorbt.data.custom.YFData","text":"Data for data coming from yfinance . Stocks are usually in the timezone \"+0500\" and cryptocurrencies in UTC. Warning Data coming from Yahoo is not the most stable data out there. Yahoo may manipulate data how they want, add noise, return missing data points (see volume in the example below), etc. It's only used in vectorbt for demonstration purposes. Usage Fetch the business day except the last 5 minutes of trading data, and then update with the missing 5 minutes: >>> import vectorbt as vbt >>> yf_data = vbt . YFData . download ( ... \"TSLA\" , ... start = '2021-04-12 09:30:00 -0400' , ... end = '2021-04-12 09:35:00 -0400' , ... interval = '1m' ... ) >>> yf_data . get ()) Open High Low Close \\ Datetime 2021-04-12 13:30:00+00:00 685.080017 685.679993 684.765015 685.679993 2021-04-12 13:31:00+00:00 684.625000 686.500000 684.010010 685.500000 2021-04-12 13:32:00+00:00 685.646790 686.820007 683.190002 686.455017 2021-04-12 13:33:00+00:00 686.455017 687.000000 685.000000 685.565002 2021-04-12 13:34:00+00:00 685.690002 686.400024 683.200012 683.715027 Volume Dividends Stock Splits Datetime 2021-04-12 13:30:00+00:00 0 0 0 2021-04-12 13:31:00+00:00 152276 0 0 2021-04-12 13:32:00+00:00 168363 0 0 2021-04-12 13:33:00+00:00 129607 0 0 2021-04-12 13:34:00+00:00 134620 0 0 >>> yf_data = yf_data . update ( end = '2021-04-12 09:40:00 -0400' ) >>> yf_data . get () Open High Low Close \\ Datetime 2021-04-12 13:30:00+00:00 685.080017 685.679993 684.765015 685.679993 2021-04-12 13:31:00+00:00 684.625000 686.500000 684.010010 685.500000 2021-04-12 13:32:00+00:00 685.646790 686.820007 683.190002 686.455017 2021-04-12 13:33:00+00:00 686.455017 687.000000 685.000000 685.565002 2021-04-12 13:34:00+00:00 685.690002 686.400024 683.200012 683.715027 2021-04-12 13:35:00+00:00 683.604980 684.340027 682.760071 684.135010 2021-04-12 13:36:00+00:00 684.130005 686.640015 683.333984 686.563904 2021-04-12 13:37:00+00:00 686.530029 688.549988 686.000000 686.635010 2021-04-12 13:38:00+00:00 686.593201 689.500000 686.409973 688.179993 2021-04-12 13:39:00+00:00 688.500000 689.347595 687.710022 688.070007 Volume Dividends Stock Splits Datetime 2021-04-12 13:30:00+00:00 0 0 0 2021-04-12 13:31:00+00:00 152276 0 0 2021-04-12 13:32:00+00:00 168363 0 0 2021-04-12 13:33:00+00:00 129607 0 0 2021-04-12 13:34:00+00:00 0 0 0 2021-04-12 13:35:00+00:00 110500 0 0 2021-04-12 13:36:00+00:00 148384 0 0 2021-04-12 13:37:00+00:00 243851 0 0 2021-04-12 13:38:00+00:00 203569 0 0 2021-04-12 13:39:00+00:00 93308 0 0 Superclasses AttrResolver Configured Data Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Data.align_columns() Data.align_index() Data.concat() Data.config Data.data Data.download() Data.download_kwargs Data.from_data() Data.get() Data.iloc Data.indexing_func() Data.indexing_kwargs Data.loc Data.missing_columns Data.missing_index Data.plot() Data.plots_defaults Data.select_symbol_kwargs() Data.self_aliases Data.stats_defaults Data.symbols Data.tz_convert Data.tz_localize Data.update() Data.wrapper Data.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"YFData"},{"location":"api/data/custom/#vectorbt.data.custom.YFData.download_symbol","text":"YFData . download_symbol ( symbol , period = 'max' , start = None , end = None , ** kwargs ) Download the symbol. Args symbol :\u2002 str Symbol. period :\u2002 str Period. start :\u2002 any Start datetime. See to_tzaware_datetime() . end :\u2002 any End datetime. See to_tzaware_datetime() . **kwargs Keyword arguments passed to yfinance.base.TickerBase.history .","title":"download_symbol()"},{"location":"api/data/custom/#vectorbt.data.custom.YFData.update_symbol","text":"YFData . update_symbol ( symbol , ** kwargs ) Update the symbol. **kwargs will override keyword arguments passed to YFData.download_symbol() .","title":"update_symbol()"},{"location":"api/data/updater/","text":"updater module \u00b6 Class for scheduling data updates. DataUpdater class \u00b6 Class for scheduling data updates. Usage Update in the foreground: >>> import vectorbt as vbt >>> class MyDataUpdater ( vbt . DataUpdater ): ... def __init__ ( self , * args , ** kwargs ): ... super () . __init__ ( * args , ** kwargs ) ... self . update_count = 0 ... ... def update ( self , count_limit = None ): ... prev_index_len = len ( self . data . wrapper . index ) ... super () . update () ... new_index_len = len ( self . data . wrapper . index ) ... print ( f \"Data updated with { new_index_len - prev_index_len } data points\" ) ... self . update_count += 1 ... if count_limit is not None and self . update_count >= count_limit : ... raise vbt . CancelledError >>> data = vbt . GBMData . download ( 'SYMBOL' , start = '1 minute ago' , freq = '1s' ) >>> my_updater = MyDataUpdater ( data ) >>> my_updater . update_every ( count_limit = 10 ) Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points >>> my_updater . data . get () 2021-05-02 16:53:51.755347+00:00 96.830482 2021-05-02 16:53:52.755347+00:00 94.481883 2021-05-02 16:53:53.755347+00:00 94.327835 2021-05-02 16:53:54.755347+00:00 90.178038 2021-05-02 16:53:55.755347+00:00 88.260168 ... 2021-05-02 16:54:57.755347+00:00 99.342590 2021-05-02 16:54:58.755347+00:00 94.872893 2021-05-02 16:54:59.755347+00:00 93.212823 2021-05-02 16:55:00.755347+00:00 95.199882 2021-05-02 16:55:01.755347+00:00 93.070532 Freq: S, Length: 71, dtype: float64 Update in the background: >>> my_updater = MyDataUpdater ( my_updater . data ) >>> my_updater . update_every ( in_background = True , count_limit = 10 ) Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points >>> my_updater . data . get () 2021-05-02 16:53:51.755347+00:00 96.830482 2021-05-02 16:53:52.755347+00:00 94.481883 2021-05-02 16:53:53.755347+00:00 94.327835 2021-05-02 16:53:54.755347+00:00 90.178038 2021-05-02 16:53:55.755347+00:00 88.260168 ... 2021-05-02 16:55:07.755347+00:00 94.502885 2021-05-02 16:55:08.755347+00:00 94.823707 2021-05-02 16:55:09.755347+00:00 92.570025 2021-05-02 16:55:10.755347+00:00 84.239018 2021-05-02 16:55:11.755347+00:00 81.294486 Freq: S, Length: 81, dtype: float64 Superclasses Configured Documented Pickleable Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() data property \u00b6 Data instance. See Data . schedule_manager property \u00b6 Schedule manager instance. See ScheduleManager . update method \u00b6 DataUpdater . update ( ** kwargs ) Method that updates data. Override to do pre- and postprocessing. To stop this method from running again, raise CancelledError . update_every method \u00b6 DataUpdater . update_every ( * args , to = None , tags = None , in_background = False , start_kwargs = None , ** kwargs ) Schedule DataUpdater.update() . For *args , to and tags , see ScheduleManager.every() . If in_background is set to True, starts in the background as an asyncio task. The task can be stopped with ScheduleManager.stop() . **kwargs are passed to DataUpdater.update() .","title":"updater"},{"location":"api/data/updater/#vectorbt.data.updater","text":"Class for scheduling data updates.","title":"vectorbt.data.updater"},{"location":"api/data/updater/#vectorbt.data.updater.DataUpdater","text":"Class for scheduling data updates. Usage Update in the foreground: >>> import vectorbt as vbt >>> class MyDataUpdater ( vbt . DataUpdater ): ... def __init__ ( self , * args , ** kwargs ): ... super () . __init__ ( * args , ** kwargs ) ... self . update_count = 0 ... ... def update ( self , count_limit = None ): ... prev_index_len = len ( self . data . wrapper . index ) ... super () . update () ... new_index_len = len ( self . data . wrapper . index ) ... print ( f \"Data updated with { new_index_len - prev_index_len } data points\" ) ... self . update_count += 1 ... if count_limit is not None and self . update_count >= count_limit : ... raise vbt . CancelledError >>> data = vbt . GBMData . download ( 'SYMBOL' , start = '1 minute ago' , freq = '1s' ) >>> my_updater = MyDataUpdater ( data ) >>> my_updater . update_every ( count_limit = 10 ) Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points >>> my_updater . data . get () 2021-05-02 16:53:51.755347+00:00 96.830482 2021-05-02 16:53:52.755347+00:00 94.481883 2021-05-02 16:53:53.755347+00:00 94.327835 2021-05-02 16:53:54.755347+00:00 90.178038 2021-05-02 16:53:55.755347+00:00 88.260168 ... 2021-05-02 16:54:57.755347+00:00 99.342590 2021-05-02 16:54:58.755347+00:00 94.872893 2021-05-02 16:54:59.755347+00:00 93.212823 2021-05-02 16:55:00.755347+00:00 95.199882 2021-05-02 16:55:01.755347+00:00 93.070532 Freq: S, Length: 71, dtype: float64 Update in the background: >>> my_updater = MyDataUpdater ( my_updater . data ) >>> my_updater . update_every ( in_background = True , count_limit = 10 ) Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points Data updated with 1 data points >>> my_updater . data . get () 2021-05-02 16:53:51.755347+00:00 96.830482 2021-05-02 16:53:52.755347+00:00 94.481883 2021-05-02 16:53:53.755347+00:00 94.327835 2021-05-02 16:53:54.755347+00:00 90.178038 2021-05-02 16:53:55.755347+00:00 88.260168 ... 2021-05-02 16:55:07.755347+00:00 94.502885 2021-05-02 16:55:08.755347+00:00 94.823707 2021-05-02 16:55:09.755347+00:00 92.570025 2021-05-02 16:55:10.755347+00:00 84.239018 2021-05-02 16:55:11.755347+00:00 81.294486 Freq: S, Length: 81, dtype: float64 Superclasses Configured Documented Pickleable Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save()","title":"DataUpdater"},{"location":"api/data/updater/#vectorbt.data.updater.DataUpdater.data","text":"Data instance. See Data .","title":"data"},{"location":"api/data/updater/#vectorbt.data.updater.DataUpdater.schedule_manager","text":"Schedule manager instance. See ScheduleManager .","title":"schedule_manager"},{"location":"api/data/updater/#vectorbt.data.updater.DataUpdater.update","text":"DataUpdater . update ( ** kwargs ) Method that updates data. Override to do pre- and postprocessing. To stop this method from running again, raise CancelledError .","title":"update()"},{"location":"api/data/updater/#vectorbt.data.updater.DataUpdater.update_every","text":"DataUpdater . update_every ( * args , to = None , tags = None , in_background = False , start_kwargs = None , ** kwargs ) Schedule DataUpdater.update() . For *args , to and tags , see ScheduleManager.every() . If in_background is set to True, starts in the background as an asyncio task. The task can be stopped with ScheduleManager.stop() . **kwargs are passed to DataUpdater.update() .","title":"update_every()"},{"location":"api/generic/","text":"generic package \u00b6 Modules for working with any time series. In contrast to the vectorbt.base sub-package, focuses on the data itself. Sub-modules \u00b6 vectorbt.generic.accessors vectorbt.generic.decorators vectorbt.generic.drawdowns vectorbt.generic.enums vectorbt.generic.nb vectorbt.generic.plots_builder vectorbt.generic.plotting vectorbt.generic.ranges vectorbt.generic.splitters vectorbt.generic.stats_builder","title":"generic"},{"location":"api/generic/#vectorbt.generic","text":"Modules for working with any time series. In contrast to the vectorbt.base sub-package, focuses on the data itself.","title":"vectorbt.generic"},{"location":"api/generic/#sub-modules","text":"vectorbt.generic.accessors vectorbt.generic.decorators vectorbt.generic.drawdowns vectorbt.generic.enums vectorbt.generic.nb vectorbt.generic.plots_builder vectorbt.generic.plotting vectorbt.generic.ranges vectorbt.generic.splitters vectorbt.generic.stats_builder","title":"Sub-modules"},{"location":"api/generic/accessors/","text":"accessors module \u00b6 Custom pandas accessors for generic data. Methods can be accessed as follows: GenericSRAccessor -> pd.Series.vbt.* GenericDFAccessor -> pd.DataFrame.vbt.* >>> import pandas as pd >>> import vectorbt as vbt >>> # vectorbt.generic.accessors.GenericAccessor.rolling_mean >>> pd . Series ([ 1 , 2 , 3 , 4 ]) . vbt . rolling_mean ( 2 ) 0 NaN 1 1.5 2 2.5 3 3.5 dtype: float64 The accessors inherit vectorbt.base.accessors and are inherited by more specialized accessors, such as vectorbt.signals.accessors and vectorbt.returns.accessors . Note Grouping is only supported by the methods that accept the group_by argument. Accessors do not utilize caching. Run for the examples below >>> import vectorbt as vbt >>> import numpy as np >>> import pandas as pd >>> from numba import njit >>> from datetime import datetime , timedelta >>> df = pd . DataFrame ({ ... 'a' : [ 1 , 2 , 3 , 4 , 5 ], ... 'b' : [ 5 , 4 , 3 , 2 , 1 ], ... 'c' : [ 1 , 2 , 3 , 2 , 1 ] ... }, index = pd . Index ([ ... datetime ( 2020 , 1 , 1 ), ... datetime ( 2020 , 1 , 2 ), ... datetime ( 2020 , 1 , 3 ), ... datetime ( 2020 , 1 , 4 ), ... datetime ( 2020 , 1 , 5 ) ... ])) >>> df a b c 2020-01-01 1 5 1 2020-01-02 2 4 2 2020-01-03 3 3 3 2020-01-04 4 2 2 2020-01-05 5 1 1 >>> index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 10 )] >>> sr = pd . Series ( np . arange ( len ( index )), index = index ) >>> sr 2020-01-01 0 2020-01-02 1 2020-01-03 2 2020-01-04 3 2020-01-05 4 2020-01-06 5 2020-01-07 6 2020-01-08 7 2020-01-09 8 2020-01-10 9 dtype: int64 Stats \u00b6 Hint See StatsBuilderMixin.stats() and GenericAccessor.metrics . >>> df2 = pd . DataFrame ({ ... 'a' : [ np . nan , 2 , 3 ], ... 'b' : [ 4 , np . nan , 5 ], ... 'c' : [ 6 , 7 , np . nan ] ... }, index = [ 'x' , 'y' , 'z' ]) >>> df2 . vbt ( freq = 'd' ) . stats ( column = 'a' ) Start x End z Period 3 days 00:00:00 Count 2 Mean 2.5 Std 0.707107 Min 2.0 Median 2.5 Max 3.0 Min Index y Max Index z Name: a, dtype: object Mapping \u00b6 Mapping can be set both in GenericAccessor (preferred) and StatsBuilderMixin.stats() : >>> mapping = { x : 'test_' + str ( x ) for x in pd . unique ( df2 . values . flatten ())} >>> df2 . vbt ( freq = 'd' , mapping = mapping ) . stats ( column = 'a' ) Start x End z Period 3 days 00:00:00 Count 2 Value Counts: test_2.0 1 Value Counts: test_3.0 1 Value Counts: test_4.0 0 Value Counts: test_5.0 0 Value Counts: test_6.0 0 Value Counts: test_7.0 0 Value Counts: test_nan 1 Name: a, dtype: object >>> df2 . vbt ( freq = 'd' ) . stats ( column = 'a' , settings = dict ( mapping = mapping )) UserWarning: Changing the mapping will create a copy of this object. Consider setting it upon object creation to re-use existing cache. Start x End z Period 3 days 00:00:00 Count 2 Value Counts: test_2.0 1 Value Counts: test_3.0 1 Value Counts: test_4.0 0 Value Counts: test_5.0 0 Value Counts: test_6.0 0 Value Counts: test_7.0 0 Value Counts: test_nan 1 Name: a, dtype: object Selecting a column before calling stats will consider uniques from this column only: >>> df2 [ 'a' ] . vbt ( freq = 'd' , mapping = mapping ) . stats () Start x End z Period 3 days 00:00:00 Count 2 Value Counts: test_2.0 1 Value Counts: test_3.0 1 Value Counts: test_nan 1 Name: a, dtype: object To include all keys from mapping , pass incl_all_keys=True : df2['a'].vbt(freq='d', mapping=mapping).stats(settings=dict(incl_all_keys=True)) Start x End z Period 3 days 00:00:00 Count 2 Value Counts: test_2.0 1 Value Counts: test_3.0 1 Value Counts: test_4.0 0 Value Counts: test_5.0 0 Value Counts: test_6.0 0 Value Counts: test_7.0 0 Value Counts: test_nan 1 Name: a, dtype: object `GenericAccessor.stats` also supports (re-)grouping: ```pycon >>> df2.vbt(freq='d').stats(column=0, group_by=[0, 0, 1]) Start x End z Period 3 days 00:00:00 Count 4 Mean 3.5 Std 1.290994 Min 2.0 Median 3.5 Max 5.0 Min Index y Max Index z Name: 0, dtype: object Plots \u00b6 Hint See PlotsBuilderMixin.plots() and GenericAccessor.subplots . GenericAccessor class has a single subplot based on GenericAccessor.plot() : >>> df2 . vbt . plots () nb_config Config \u00b6 Config of Numba methods to be added to GenericAccessor . Co nf ig( { \"shuffle\" : { \"func\" : \"CPUDispatcher(<function shuffle_nb at 0x7facbadd8d08>)\" , \"path\" : \"vectorbt.generic.nb.shuffle_nb\" }, \"fillna\" : { \"func\" : \"CPUDispatcher(<function fillna_nb at 0x7facbadd8840>)\" , \"path\" : \"vectorbt.generic.nb.fillna_nb\" }, \"bshift\" : { \"func\" : \"CPUDispatcher(<function bshift_nb at 0x7facbaf40a60>)\" , \"path\" : \"vectorbt.generic.nb.bshift_nb\" }, \"fshift\" : { \"func\" : \"CPUDispatcher(<function fshift_nb at 0x7facbaf4b048>)\" , \"path\" : \"vectorbt.generic.nb.fshift_nb\" }, \"diff\" : { \"func\" : \"CPUDispatcher(<function diff_nb at 0x7facbaf4b598>)\" , \"path\" : \"vectorbt.generic.nb.diff_nb\" }, \"pct_change\" : { \"func\" : \"CPUDispatcher(<function pct_change_nb at 0x7facbaf4bae8>)\" , \"path\" : \"vectorbt.generic.nb.pct_change_nb\" }, \"bfill\" : { \"func\" : \"CPUDispatcher(<function bfill_nb at 0x7facbaf550d0>)\" , \"path\" : \"vectorbt.generic.nb.bfill_nb\" }, \"ffill\" : { \"func\" : \"CPUDispatcher(<function ffill_nb at 0x7facbaf55620>)\" , \"path\" : \"vectorbt.generic.nb.ffill_nb\" }, \"cumsum\" : { \"func\" : \"CPUDispatcher(<function nancumsum_nb at 0x7facbaf55b70>)\" , \"path\" : \"vectorbt.generic.nb.nancumsum_nb\" }, \"cumprod\" : { \"func\" : \"CPUDispatcher(<function nancumprod_nb at 0x7facbaf55e18>)\" , \"path\" : \"vectorbt.generic.nb.nancumprod_nb\" }, \"rolling_min\" : { \"func\" : \"CPUDispatcher(<function rolling_min_nb at 0x7facbaf659d8>)\" , \"path\" : \"vectorbt.generic.nb.rolling_min_nb\" }, \"rolling_max\" : { \"func\" : \"CPUDispatcher(<function rolling_max_nb at 0x7facbaf65f28>)\" , \"path\" : \"vectorbt.generic.nb.rolling_max_nb\" }, \"rolling_mean\" : { \"func\" : \"CPUDispatcher(<function rolling_mean_nb at 0x7facbaf6b510>)\" , \"path\" : \"vectorbt.generic.nb.rolling_mean_nb\" }, \"expanding_min\" : { \"func\" : \"CPUDispatcher(<function expanding_min_nb at 0x7facbaf71ae8>)\" , \"path\" : \"vectorbt.generic.nb.expanding_min_nb\" }, \"expanding_max\" : { \"func\" : \"CPUDispatcher(<function expanding_max_nb at 0x7facbaf780d0>)\" , \"path\" : \"vectorbt.generic.nb.expanding_max_nb\" }, \"expanding_mean\" : { \"func\" : \"CPUDispatcher(<function expanding_mean_nb at 0x7facbaf78620>)\" , \"path\" : \"vectorbt.generic.nb.expanding_mean_nb\" }, \"product\" : { \"func\" : \"CPUDispatcher(<function nanprod_nb at 0x7facbaf558c8>)\" , \"is_reducing\" : true , \"path\" : \"vectorbt.generic.nb.nanprod_nb\" } } ) transform_config Config \u00b6 Config of transform methods to be added to GenericAccessor . Co nf ig( { \"binarize\" : { \"transformer\" : \"<class 'sklearn.preprocessing._data.Binarizer'>\" , \"docstring\" : \"See `sklearn.preprocessing.Binarizer`.\" }, \"minmax_scale\" : { \"transformer\" : \"<class 'sklearn.preprocessing._data.MinMaxScaler'>\" , \"docstring\" : \"See `sklearn.preprocessing.MinMaxScaler`.\" }, \"maxabs_scale\" : { \"transformer\" : \"<class 'sklearn.preprocessing._data.MaxAbsScaler'>\" , \"docstring\" : \"See `sklearn.preprocessing.MaxAbsScaler`.\" }, \"normalize\" : { \"transformer\" : \"<class 'sklearn.preprocessing._data.Normalizer'>\" , \"docstring\" : \"See `sklearn.preprocessing.Normalizer`.\" }, \"robust_scale\" : { \"transformer\" : \"<class 'sklearn.preprocessing._data.RobustScaler'>\" , \"docstring\" : \"See `sklearn.preprocessing.RobustScaler`.\" }, \"scale\" : { \"transformer\" : \"<class 'sklearn.preprocessing._data.StandardScaler'>\" , \"docstring\" : \"See `sklearn.preprocessing.StandardScaler`.\" }, \"quantile_transform\" : { \"transformer\" : \"<class 'sklearn.preprocessing._data.QuantileTransformer'>\" , \"docstring\" : \"See `sklearn.preprocessing.QuantileTransformer`.\" }, \"power_transform\" : { \"transformer\" : \"<class 'sklearn.preprocessing._data.PowerTransformer'>\" , \"docstring\" : \"See `sklearn.preprocessing.PowerTransformer`.\" } } ) GenericAccessor class \u00b6 Accessor on top of data of any type. For both, Series and DataFrames. Accessible through pd.Series.vbt and pd.DataFrame.vbt . Superclasses AttrResolver BaseAccessor Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.config BaseAccessor.df_accessor_cls BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.iloc BaseAccessor.indexing_func() BaseAccessor.indexing_kwargs BaseAccessor.loc BaseAccessor.make_symmetric() BaseAccessor.obj BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.self_aliases BaseAccessor.sr_accessor_cls BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() BaseAccessor.wrapper BaseAccessor.writeable_attrs Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses GenericDFAccessor GenericSRAccessor ReturnsAccessor SignalsAccessor apply_along_axis method \u00b6 GenericAccessor . apply_along_axis ( apply_func_nb , * args , axis = 0 , wrap_kwargs = None ) Apply a function apply_func_nb along an axis. apply_and_reduce method \u00b6 GenericAccessor . apply_and_reduce ( apply_func_nb , reduce_func_nb , apply_args = None , reduce_args = None , wrap_kwargs = None ) See apply_and_reduce_nb() . Usage >>> greater_nb = njit ( lambda col , a : a [ a > 2 ]) >>> mean_nb = njit ( lambda col , a : np . nanmean ( a )) >>> df . vbt . apply_and_reduce ( greater_nb , mean_nb ) a 4.0 b 4.0 c 3.0 dtype: float64 apply_mapping method \u00b6 GenericAccessor . apply_mapping ( ** kwargs ) See apply_mapping() . applymap method \u00b6 GenericAccessor . applymap ( apply_func_nb , * args , wrap_kwargs = None ) See applymap_nb() . Usage >>> multiply_nb = njit ( lambda i , col , a : a ** 2 ) >>> df . vbt . applymap ( multiply_nb ) a b c 2020-01-01 1.0 25.0 1.0 2020-01-02 4.0 16.0 4.0 2020-01-03 9.0 9.0 9.0 2020-01-04 16.0 4.0 4.0 2020-01-05 25.0 1.0 1.0 barplot method \u00b6 GenericAccessor . barplot ( trace_names = None , x_labels = None , return_fig = True , ** kwargs ) Create Bar and return the figure. Usage >>> df . vbt . barplot () bfill method \u00b6 GenericAccessor . bfill ( * , wrap_kwargs = None ) See bfill_nb() . binarize method \u00b6 GenericAccessor . binarize ( * , threshold = 0.0 , copy = True , ** kwargs ) See sklearn.preprocessing.Binarizer . boxplot method \u00b6 GenericAccessor . boxplot ( trace_names = None , group_by = None , return_fig = True , ** kwargs ) Create Box and return the figure. Usage >>> df . vbt . boxplot () bshift method \u00b6 GenericAccessor . bshift ( n = 1 , fill_value = nan , * , wrap_kwargs = None ) See bshift_nb() . count method \u00b6 GenericAccessor . count ( group_by = None , wrap_kwargs = None ) Return count of non-NaN elements. crossed_above method \u00b6 GenericAccessor . crossed_above ( other , wait = 0 , broadcast_kwargs = None , wrap_kwargs = None ) Generate crossover above another array. See crossed_above_nb() . Usage >>> df [ 'b' ] . vbt . crossed_above ( df [ 'c' ]) 2020-01-01 False 2020-01-02 False 2020-01-03 False 2020-01-04 False 2020-01-05 False dtype: bool >>> df [ 'a' ] . vbt . crossed_above ( df [ 'b' ]) 2020-01-01 False 2020-01-02 False 2020-01-03 False 2020-01-04 True 2020-01-05 False dtype: bool >>> df [ 'a' ] . vbt . crossed_above ( df [ 'b' ], wait = 1 ) 2020-01-01 False 2020-01-02 False 2020-01-03 False 2020-01-04 False 2020-01-05 True dtype: bool crossed_below method \u00b6 GenericAccessor . crossed_below ( other , wait = 0 , broadcast_kwargs = None , wrap_kwargs = None ) Generate crossover below another array. See crossed_above_nb() but in reversed order. cumprod method \u00b6 GenericAccessor . cumprod ( * , wrap_kwargs = None ) See nancumprod_nb() . cumsum method \u00b6 GenericAccessor . cumsum ( * , wrap_kwargs = None ) See nancumsum_nb() . describe method \u00b6 GenericAccessor . describe ( percentiles = None , ddof = 1 , group_by = None , wrap_kwargs = None ) See describe_reduce_nb() . For percentiles , see pd.DataFrame.describe . Usage >>> df . vbt . describe () a b c count 5.000000 5.000000 5.00000 mean 3.000000 3.000000 1.80000 std 1.581139 1.581139 0.83666 min 1.000000 1.000000 1.00000 25% 2.000000 2.000000 1.00000 50% 3.000000 3.000000 2.00000 75% 4.000000 4.000000 2.00000 max 5.000000 5.000000 3.00000 diff method \u00b6 GenericAccessor . diff ( n = 1 , * , wrap_kwargs = None ) See diff_nb() . drawdown method \u00b6 GenericAccessor . drawdown ( wrap_kwargs = None ) Drawdown series. drawdowns property \u00b6 GenericAccessor.get_drawdowns() with default arguments. ewm_mean method \u00b6 GenericAccessor . ewm_mean ( span , minp = 0 , adjust = True , wrap_kwargs = None ) See ewm_mean_nb() . ewm_std method \u00b6 GenericAccessor . ewm_std ( span , minp = 0 , adjust = True , ddof = 1 , wrap_kwargs = None ) See ewm_std_nb() . expanding_apply method \u00b6 GenericAccessor . expanding_apply ( apply_func_nb , * args , minp = 1 , on_matrix = False , wrap_kwargs = None ) See expanding_apply_nb() and expanding_matrix_apply_nb() for on_matrix=True . Usage >>> mean_nb = njit ( lambda i , col , a : np . nanmean ( a )) >>> df . vbt . expanding_apply ( mean_nb ) a b c 2020-01-01 1.0 5.0 1.0 2020-01-02 1.5 4.5 1.5 2020-01-03 2.0 4.0 2.0 2020-01-04 2.5 3.5 2.0 2020-01-05 3.0 3.0 1.8 >>> mean_matrix_nb = njit ( lambda i , a : np . nanmean ( a )) >>> df . vbt . expanding_apply ( mean_matrix_nb , on_matrix = True ) a b c 2020-01-01 2.333333 2.333333 2.333333 2020-01-02 2.500000 2.500000 2.500000 2020-01-03 2.666667 2.666667 2.666667 2020-01-04 2.666667 2.666667 2.666667 2020-01-05 2.600000 2.600000 2.600000 expanding_max method \u00b6 GenericAccessor . expanding_max ( minp = 1 , * , wrap_kwargs = None ) See expanding_max_nb() . expanding_mean method \u00b6 GenericAccessor . expanding_mean ( minp = 1 , * , wrap_kwargs = None ) See expanding_mean_nb() . expanding_min method \u00b6 GenericAccessor . expanding_min ( minp = 1 , * , wrap_kwargs = None ) See expanding_min_nb() . expanding_split method \u00b6 GenericAccessor . expanding_split ( ** kwargs ) Split using GenericAccessor.split() on ExpandingSplitter . Usage >>> train_set , valid_set , test_set = sr . vbt . expanding_split ( ... n = 5 , set_lens = ( 1 , 1 ), min_len = 3 , left_to_right = False ) >>> train_set [ 0 ] split_idx 0 1 2 3 4 5 6 7 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 NaN 1.0 1.0 1.0 1.0 1.0 1.0 1 2 NaN NaN 2.0 2.0 2.0 2.0 2.0 2 3 NaN NaN NaN 3.0 3.0 3.0 3.0 3 4 NaN NaN NaN NaN 4.0 4.0 4.0 4 5 NaN NaN NaN NaN NaN 5.0 5.0 5 6 NaN NaN NaN NaN NaN NaN 6.0 6 7 NaN NaN NaN NaN NaN NaN NaN 7 >>> valid_set [ 0 ] split_idx 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 8 >>> test_set [ 0 ] split_idx 0 1 2 3 4 5 6 7 0 2 3 4 5 6 7 8 9 >>> sr . vbt . expanding_split ( ... set_lens = ( 1 , 1 ), min_len = 3 , left_to_right = False , ... plot = True , trace_names = [ 'train' , 'valid' , 'test' ]) expanding_std method \u00b6 GenericAccessor . expanding_std ( minp = 1 , ddof = 1 , wrap_kwargs = None ) See expanding_std_nb() . ffill method \u00b6 GenericAccessor . ffill ( * , wrap_kwargs = None ) See ffill_nb() . fillna method \u00b6 GenericAccessor . fillna ( value , * , wrap_kwargs = None ) See fillna_nb() . filter method \u00b6 GenericAccessor . filter ( filter_func_nb , * args , wrap_kwargs = None ) See filter_nb() . Usage >>> greater_nb = njit ( lambda i , col , a : a > 2 ) >>> df . vbt . filter ( greater_nb ) a b c 2020-01-01 NaN 5.0 NaN 2020-01-02 NaN 4.0 NaN 2020-01-03 3.0 3.0 3.0 2020-01-04 4.0 NaN NaN 2020-01-05 5.0 NaN NaN fshift method \u00b6 GenericAccessor . fshift ( n = 1 , fill_value = nan , * , wrap_kwargs = None ) See fshift_nb() . get_drawdowns method \u00b6 GenericAccessor . get_drawdowns ( wrapper_kwargs = None , ** kwargs ) Generate drawdown records. See Drawdowns . get_ranges method \u00b6 GenericAccessor . get_ranges ( wrapper_kwargs = None , ** kwargs ) Generate range records. See Ranges . groupby_apply method \u00b6 GenericAccessor . groupby_apply ( by , apply_func_nb , * args , on_matrix = False , wrap_kwargs = None , ** kwargs ) See groupby_apply_nb() and groupby_matrix_apply_nb() for on_matrix=True . For by , see pd.DataFrame.groupby . Usage >>> mean_nb = njit ( lambda i , col , a : np . nanmean ( a )) >>> df . vbt . groupby_apply ([ 1 , 1 , 2 , 2 , 3 ], mean_nb ) a b c 1 1.5 4.5 1.5 2 3.5 2.5 2.5 3 5.0 1.0 1.0 >>> mean_matrix_nb = njit ( lambda i , a : np . nanmean ( a )) >>> df . vbt . groupby_apply ([ 1 , 1 , 2 , 2 , 3 ], mean_matrix_nb , on_matrix = True ) a b c 1 2.500000 2.500000 2.500000 2 2.833333 2.833333 2.833333 3 2.333333 2.333333 2.333333 histplot method \u00b6 GenericAccessor . histplot ( trace_names = None , group_by = None , return_fig = True , ** kwargs ) Create Histogram and return the figure. Usage >>> df . vbt . histplot () idxmax method \u00b6 GenericAccessor . idxmax ( group_by = None , order = 'C' , wrap_kwargs = None ) Return labeled index of max of non-NaN elements. idxmin method \u00b6 GenericAccessor . idxmin ( group_by = None , order = 'C' , wrap_kwargs = None ) Return labeled index of min of non-NaN elements. lineplot method \u00b6 GenericAccessor . lineplot ( ** kwargs ) GenericAccessor.plot() with 'lines' mode. Usage >>> df . vbt . lineplot () mapping property \u00b6 Mapping. max method \u00b6 GenericAccessor . max ( group_by = None , wrap_kwargs = None ) Return max of non-NaN elements. maxabs_scale method \u00b6 GenericAccessor . maxabs_scale ( * , copy = True , ** kwargs ) See sklearn.preprocessing.MaxAbsScaler . mean method \u00b6 GenericAccessor . mean ( group_by = None , wrap_kwargs = None ) Return mean of non-NaN elements. median method \u00b6 GenericAccessor . median ( group_by = None , wrap_kwargs = None ) Return median of non-NaN elements. metrics class variable \u00b6 Metrics supported by GenericAccessor . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function GenericAccessor.<lambda> at 0x7facbcf4df28>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function GenericAccessor.<lambda> at 0x7facbcf50048>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function GenericAccessor.<lambda> at 0x7facbcf500d0>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"count\" : { \"title\" : \"Count\" , \"calc_func\" : \"count\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"describe\" ] }, \"mean\" : { \"title\" : \"Mean\" , \"calc_func\" : \"mean\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"describe\" ] }, \"std\" : { \"title\" : \"Std\" , \"calc_func\" : \"std\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"describe\" ] }, \"min\" : { \"title\" : \"Min\" , \"calc_func\" : \"min\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"describe\" ] }, \"median\" : { \"title\" : \"Median\" , \"calc_func\" : \"median\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"describe\" ] }, \"max\" : { \"title\" : \"Max\" , \"calc_func\" : \"max\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"describe\" ] }, \"idx_min\" : { \"title\" : \"Min Index\" , \"calc_func\" : \"idxmin\" , \"agg_func\" : null , \"inv_check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"index\" ] }, \"idx_max\" : { \"title\" : \"Max Index\" , \"calc_func\" : \"idxmax\" , \"agg_func\" : null , \"inv_check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"index\" ] }, \"value_counts\" : { \"title\" : \"Value Counts\" , \"calc_func\" : \"<function GenericAccessor.<lambda> at 0x7facbcf50158>\" , \"resolve_value_counts\" : true , \"check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"value_counts\" ] } } ) Returns GenericAccessor._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable GenericAccessor._metrics . min method \u00b6 GenericAccessor . min ( group_by = None , wrap_kwargs = None ) Return min of non-NaN elements. minmax_scale method \u00b6 GenericAccessor . minmax_scale ( feature_range = ( 0 , 1 ), * , copy = True , clip = False , ** kwargs ) See sklearn.preprocessing.MinMaxScaler . normalize method \u00b6 GenericAccessor . normalize ( norm = 'l2' , * , copy = True , ** kwargs ) See sklearn.preprocessing.Normalizer . pct_change method \u00b6 GenericAccessor . pct_change ( n = 1 , * , wrap_kwargs = None ) See pct_change_nb() . plot method \u00b6 GenericAccessor . plot ( trace_names = None , x_labels = None , return_fig = True , ** kwargs ) Create Scatter and return the figure. Usage >>> df . vbt . plot () plots_defaults property \u00b6 Defaults for PlotsBuilderMixin.plots() . Merges PlotsBuilderMixin.plots_defaults and generic.plots from settings . power_transform method \u00b6 GenericAccessor . power_transform ( method = 'yeo-johnson' , * , standardize = True , copy = True , ** kwargs ) See sklearn.preprocessing.PowerTransformer . product method \u00b6 GenericAccessor . product ( * , wrap_kwargs = None ) See nanprod_nb() . quantile_transform method \u00b6 GenericAccessor . quantile_transform ( * , n_quantiles = 1000 , output_distribution = 'uniform' , ignore_implicit_zeros = False , subsample = 100000 , random_state = None , copy = True , ** kwargs ) See sklearn.preprocessing.QuantileTransformer . range_split method \u00b6 GenericAccessor . range_split ( ** kwargs ) Split using GenericAccessor.split() on RangeSplitter . Usage >>> range_df , range_indexes = sr . vbt . range_split ( n = 2 ) >>> range_df split_idx 0 1 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 >>> range_indexes [DatetimeIndex(['2020-01-01', ..., '2020-01-05'], dtype='datetime64[ns]', name='split_0'), DatetimeIndex(['2020-01-06', ..., '2020-01-10'], dtype='datetime64[ns]', name='split_1')] >>> range_df , range_indexes = sr . vbt . range_split ( range_len = 4 ) >>> range_df split_idx 0 1 2 3 4 5 6 0 0 1 2 3 4 5 6 1 1 2 3 4 5 6 7 2 2 3 4 5 6 7 8 3 3 4 5 6 7 8 9 >>> range_indexes [DatetimeIndex(['2020-01-01', ..., '2020-01-04'], dtype='datetime64[ns]', name='split_0'), DatetimeIndex(['2020-01-02', ..., '2020-01-05'], dtype='datetime64[ns]', name='split_1'), DatetimeIndex(['2020-01-03', ..., '2020-01-06'], dtype='datetime64[ns]', name='split_2'), DatetimeIndex(['2020-01-04', ..., '2020-01-07'], dtype='datetime64[ns]', name='split_3'), DatetimeIndex(['2020-01-05', ..., '2020-01-08'], dtype='datetime64[ns]', name='split_4'), DatetimeIndex(['2020-01-06', ..., '2020-01-09'], dtype='datetime64[ns]', name='split_5'), DatetimeIndex(['2020-01-07', ..., '2020-01-10'], dtype='datetime64[ns]', name='split_6')] >>> range_df , range_indexes = sr . vbt . range_split ( start_idxs = [ 0 , 2 ], end_idxs = [ 5 , 7 ]) >>> range_df split_idx 0 1 0 0 2 1 1 3 2 2 4 3 3 5 4 4 6 5 5 7 >>> range_indexes [DatetimeIndex(['2020-01-01', ..., '2020-01-06'], dtype='datetime64[ns]', name='split_0'), DatetimeIndex(['2020-01-03', ..., '2020-01-08'], dtype='datetime64[ns]', name='split_1')] >>> range_df , range_indexes = sr . vbt . range_split ( start_idxs = [ 0 ], end_idxs = [ 2 , 3 , 4 ]) >>> range_df split_idx 0 1 2 0 0.0 0.0 0 1 1.0 1.0 1 2 2.0 2.0 2 3 NaN 3.0 3 4 NaN NaN 4 >>> range_indexes [DatetimeIndex(['2020-01-01', ..., '2020-01-03'], dtype='datetime64[ns]', name='split_0'), DatetimeIndex(['2020-01-01', ..., '2020-01-04'], dtype='datetime64[ns]', name='split_1'), DatetimeIndex(['2020-01-01', ..., '2020-01-05'], dtype='datetime64[ns]', name='split_2')] >>> range_df , range_indexes = sr . vbt . range_split ( ... start_idxs = pd . Index ([ '2020-01-01' , '2020-01-02' ]), ... end_idxs = pd . Index ([ '2020-01-04' , '2020-01-05' ]) ... ) >>> range_df split_idx 0 1 0 0 1 1 1 2 2 2 3 3 3 4 >>> range_indexes [DatetimeIndex(['2020-01-01', ..., '2020-01-04'], dtype='datetime64[ns]', name='split_0'), DatetimeIndex(['2020-01-02', ..., '2020-01-05'], dtype='datetime64[ns]', name='split_1')] >>> sr.vbt.range_split( ... start_idxs=pd.Index(['2020-01-01', '2020-01-02', '2020-01-01']), ... end_idxs=pd.Index(['2020-01-08', '2020-01-04', '2020-01-07']), ... plot=True ... ) ranges property \u00b6 GenericAccessor.get_ranges() with default arguments. rebase method \u00b6 GenericAccessor . rebase ( base , wrap_kwargs = None ) Rebase all series to a given intial base. This makes comparing/plotting different series together easier. Will forward and backward fill NaN values. reduce method \u00b6 GenericAccessor . reduce ( reduce_func_nb , * args , returns_array = False , returns_idx = False , flatten = False , order = 'C' , to_index = True , group_by = None , wrap_kwargs = None ) Reduce by column. See flat_reduce_grouped_to_array_nb() if grouped, returns_array is True and flatten is True. See flat_reduce_grouped_nb() if grouped, returns_array is False and flatten is True. See reduce_grouped_to_array_nb() if grouped, returns_array is True and flatten is False. See reduce_grouped_nb() if grouped, returns_array is False and flatten is False. See reduce_to_array_nb() if not grouped and returns_array is True. See reduce_nb() if not grouped and returns_array is False. Set returns_idx to True if values returned by reduce_func_nb are indices/positions. Set to_index to False to return raw positions instead of labels. Usage >>> mean_nb = njit ( lambda col , a : np . nanmean ( a )) >>> df . vbt . reduce ( mean_nb ) a 3.0 b 3.0 c 1.8 dtype: float64 >>> argmax_nb = njit ( lambda col , a : np . argmax ( a )) >>> df . vbt . reduce ( argmax_nb , returns_idx = True ) a 2020-01-05 b 2020-01-01 c 2020-01-03 dtype: datetime64[ns] >>> argmax_nb = njit ( lambda col , a : np . argmax ( a )) >>> df . vbt . reduce ( argmax_nb , returns_idx = True , to_index = False ) a 4 b 0 c 2 dtype: int64 >>> min_max_nb = njit ( lambda col , a : np . array ([ np . nanmin ( a ), np . nanmax ( a )])) >>> df . vbt . reduce ( min_max_nb , returns_array = True , wrap_kwargs = dict ( name_or_index = [ 'min' , 'max' ])) a b c min 1.0 1.0 1.0 max 5.0 5.0 3.0 >>> group_by = pd . Series ([ 'first' , 'first' , 'second' ], name = 'group' ) >>> df . vbt . reduce ( mean_nb , group_by = group_by ) group first 3.0 second 1.8 dtype: float64 >>> df . vbt . reduce ( min_max_nb , name_or_index = [ 'min' , 'max' ], ... returns_array = True , group_by = group_by ) group first second min 1.0 1.0 max 5.0 3.0 resample_apply method \u00b6 GenericAccessor . resample_apply ( freq , apply_func_nb , * args , on_matrix = False , wrap_kwargs = None , ** kwargs ) See groupby_apply_nb() and groupby_matrix_apply_nb() for on_matrix=True . For freq , see pd.DataFrame.resample . Usage >>> mean_nb = njit ( lambda i , col , a : np . nanmean ( a )) >>> df . vbt . resample_apply ( '2d' , mean_nb ) a b c 2020-01-01 1.5 4.5 1.5 2020-01-03 3.5 2.5 2.5 2020-01-05 5.0 1.0 1.0 >>> mean_matrix_nb = njit ( lambda i , a : np . nanmean ( a )) >>> df . vbt . resample_apply ( '2d' , mean_matrix_nb , on_matrix = True ) a b c 2020-01-01 2.500000 2.500000 2.500000 2020-01-03 2.833333 2.833333 2.833333 2020-01-05 2.333333 2.333333 2.333333 resolve_self method \u00b6 GenericAccessor . resolve_self ( cond_kwargs = None , custom_arg_names = None , impacts_caching = True , silence_warnings = False ) Resolve self. See Wrapping.resolve_self() . Creates a copy of this instance mapping is different in cond_kwargs . robust_scale method \u00b6 GenericAccessor . robust_scale ( * , with_centering = True , with_scaling = True , quantile_range = ( 25.0 , 75.0 ), copy = True , unit_variance = False , ** kwargs ) See sklearn.preprocessing.RobustScaler . rolling_apply method \u00b6 GenericAccessor . rolling_apply ( window , apply_func_nb , * args , minp = None , on_matrix = False , wrap_kwargs = None ) See rolling_apply_nb() and rolling_matrix_apply_nb() for on_matrix=True . Usage >>> mean_nb = njit ( lambda i , col , a : np . nanmean ( a )) >>> df . vbt . rolling_apply ( 3 , mean_nb ) a b c 2020-01-01 1.0 5.0 1.000000 2020-01-02 1.5 4.5 1.500000 2020-01-03 2.0 4.0 2.000000 2020-01-04 3.0 3.0 2.333333 2020-01-05 4.0 2.0 2.000000 >>> mean_matrix_nb = njit ( lambda i , a : np . nanmean ( a )) >>> df . vbt . rolling_apply ( 3 , mean_matrix_nb , on_matrix = True ) a b c 2020-01-01 2.333333 2.333333 2.333333 2020-01-02 2.500000 2.500000 2.500000 2020-01-03 2.666667 2.666667 2.666667 2020-01-04 2.777778 2.777778 2.777778 2020-01-05 2.666667 2.666667 2.666667 rolling_max method \u00b6 GenericAccessor . rolling_max ( window , minp = None , * , wrap_kwargs = None ) See rolling_max_nb() . rolling_mean method \u00b6 GenericAccessor . rolling_mean ( window , minp = None , * , wrap_kwargs = None ) See rolling_mean_nb() . rolling_min method \u00b6 GenericAccessor . rolling_min ( window , minp = None , * , wrap_kwargs = None ) See rolling_min_nb() . rolling_split method \u00b6 GenericAccessor . rolling_split ( ** kwargs ) Split using GenericAccessor.split() on RollingSplitter . Usage >>> train_set , valid_set , test_set = sr . vbt . rolling_split ( ... window_len = 5 , set_lens = ( 1 , 1 ), left_to_right = False ) >>> train_set [ 0 ] split_idx 0 1 2 3 4 5 0 0 1 2 3 4 5 1 1 2 3 4 5 6 2 2 3 4 5 6 7 >>> valid_set [ 0 ] split_idx 0 1 2 3 4 5 0 3 4 5 6 7 8 >>> test_set [ 0 ] split_idx 0 1 2 3 4 5 0 4 5 6 7 8 9 >>> sr . vbt . rolling_split ( ... window_len = 5 , set_lens = ( 1 , 1 ), left_to_right = False , ... plot = True , trace_names = [ 'train' , 'valid' , 'test' ]) rolling_std method \u00b6 GenericAccessor . rolling_std ( window , minp = None , ddof = 1 , wrap_kwargs = None ) See rolling_std_nb() . scale method \u00b6 GenericAccessor . scale ( * , copy = True , with_mean = True , with_std = True , ** kwargs ) See sklearn.preprocessing.StandardScaler . scatterplot method \u00b6 GenericAccessor . scatterplot ( ** kwargs ) GenericAccessor.plot() with 'markers' mode. Usage >>> df . vbt . scatterplot () shuffle method \u00b6 GenericAccessor . shuffle ( seed = None , * , wrap_kwargs = None ) See shuffle_nb() . split method \u00b6 GenericAccessor . split ( splitter , stack_kwargs = None , keys = None , plot = False , trace_names = None , heatmap_kwargs = None , ** kwargs ) Split using a splitter. Returns a tuple of tuples, each corresponding to a set and composed of a dataframe and split indexes. A splitter can be any class instance that has split method, ideally subclassing sklearn.model_selection.BaseCrossValidator or BaseSplitter . heatmap_kwargs are passed to Heatmap if plot is True, can be a dictionary or a list per set, for example, to set trace name for each set ('train', 'test', etc.). **kwargs are passed to the split method. Note The datetime-like format of the index will be lost as result of this operation. Make sure to store the index metadata such as frequency information beforehand. Usage >>> from sklearn.model_selection import TimeSeriesSplit >>> splitter = TimeSeriesSplit ( n_splits = 3 ) >>> ( train_df , train_indexes ), ( test_df , test_indexes ) = sr . vbt . split ( splitter ) >>> train_df split_idx 0 1 2 0 0.0 0.0 0 1 1.0 1.0 1 2 2.0 2.0 2 3 3.0 3.0 3 4 NaN 4.0 4 5 NaN 5.0 5 6 NaN NaN 6 7 NaN NaN 7 >>> train_indexes [DatetimeIndex(['2020-01-01', ..., '2020-01-04'], dtype='datetime64[ns]', name='split_0'), DatetimeIndex(['2020-01-01', ..., '2020-01-06'], dtype='datetime64[ns]', name='split_1'), DatetimeIndex(['2020-01-01', ..., '2020-01-08'], dtype='datetime64[ns]', name='split_2')] >>> test_df split_idx 0 1 2 0 4 6 8 1 5 7 9 >>> test_indexes [DatetimeIndex(['2020-01-05', '2020-01-06'], dtype='datetime64[ns]', name='split_0'), DatetimeIndex(['2020-01-07', '2020-01-08'], dtype='datetime64[ns]', name='split_1'), DatetimeIndex(['2020-01-09', '2020-01-10'], dtype='datetime64[ns]', name='split_2')] >>> sr . vbt . split ( splitter , plot = True , trace_names = [ 'train' , 'test' ]) stats_defaults property \u00b6 Defaults for StatsBuilderMixin.stats() . Merges StatsBuilderMixin.stats_defaults and generic.stats from settings . std method \u00b6 GenericAccessor . std ( ddof = 1 , group_by = None , wrap_kwargs = None ) Return standard deviation of non-NaN elements. subplots class variable \u00b6 Subplots supported by GenericAccessor . Co nf ig( { \"plot\" : { \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"pass_trace_names\" : false , \"tags\" : \"generic\" } } ) Returns GenericAccessor._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable GenericAccessor._subplots . sum method \u00b6 GenericAccessor . sum ( group_by = None , wrap_kwargs = None ) Return sum of non-NaN elements. to_mapped method \u00b6 GenericAccessor . to_mapped ( dropna = True , dtype = None , group_by = None , ** kwargs ) Convert this object into an instance of MappedArray . to_returns method \u00b6 GenericAccessor . to_returns ( ** kwargs ) Get returns of this object. transform method \u00b6 GenericAccessor . transform ( transformer , wrap_kwargs = None , ** kwargs ) Transform using a transformer. A transformer can be any class instance that has transform and fit_transform methods, ideally subclassing sklearn.base.TransformerMixin and sklearn.base.BaseEstimator . Will fit transformer if not fitted. **kwargs are passed to the transform or fit_transform method. Usage >>> from sklearn.preprocessing import MinMaxScaler >>> df . vbt . transform ( MinMaxScaler (( - 1 , 1 ))) a b c 2020-01-01 -1.0 1.0 -1.0 2020-01-02 -0.5 0.5 0.0 2020-01-03 0.0 0.0 1.0 2020-01-04 0.5 -0.5 0.0 2020-01-05 1.0 -1.0 -1.0 >>> fitted_scaler = MinMaxScaler (( - 1 , 1 )) . fit ( np . array ([[ 2 ], [ 4 ]])) >>> df . vbt . transform ( fitted_scaler ) a b c 2020-01-01 -2.0 2.0 -2.0 2020-01-02 -1.0 1.0 -1.0 2020-01-03 0.0 0.0 0.0 2020-01-04 1.0 -1.0 -1.0 2020-01-05 2.0 -2.0 -2.0 value_counts method \u00b6 GenericAccessor . value_counts ( normalize = False , sort_uniques = True , sort = False , ascending = False , dropna = False , group_by = None , mapping = None , incl_all_keys = False , wrap_kwargs = None , ** kwargs ) Return a Series/DataFrame containing counts of unique values. Enable normalize flag to return the relative frequencies of the unique values. Enable sort_uniques flag to sort uniques. Enable sort flag to sort by frequencies. Enable ascending flag to sort in ascending order. Enable dropna flag to exclude counts of NaN. Enable incl_all_keys to include all mapping keys, no only those that are present in the array. Mapping will be applied using apply_mapping() with **kwargs . zscore method \u00b6 GenericAccessor . zscore ( ** kwargs ) Compute z-score using sklearn.preprocessing.StandardScaler . GenericDFAccessor class \u00b6 Accessor on top of data of any type. For DataFrames only. Accessible through pd.DataFrame.vbt . Superclasses AttrResolver BaseAccessor BaseDFAccessor Configured Documented GenericAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.bshift() GenericAccessor.config GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.df_accessor_cls GenericAccessor.diff() GenericAccessor.drawdown() GenericAccessor.drawdowns GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.fshift() GenericAccessor.get_drawdowns() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.iloc GenericAccessor.indexing_kwargs GenericAccessor.lineplot() GenericAccessor.loc GenericAccessor.mapping GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.obj GenericAccessor.pct_change() GenericAccessor.plot() GenericAccessor.plots_defaults GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.ranges GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.resolve_self() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.self_aliases GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.sr_accessor_cls GenericAccessor.stats_defaults GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.wrapper GenericAccessor.writeable_attrs GenericAccessor.zscore() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses OHLCVDFAccessor ReturnsDFAccessor SignalsDFAccessor Vbt_DFAccessor flatten_grouped method \u00b6 GenericDFAccessor . flatten_grouped ( group_by = None , order = 'C' , wrap_kwargs = None ) Flatten each group of columns. See flatten_grouped_nb() . If all groups have the same length, see flatten_uniform_grouped_nb() . Warning Make sure that the distribution of group lengths is close to uniform, otherwise groups with less columns will be filled with NaN and needlessly occupy memory. Usage >>> group_by = pd . Series ([ 'first' , 'first' , 'second' ], name = 'group' ) >>> df . vbt . flatten_grouped ( group_by = group_by , order = 'C' ) group first second 2020-01-01 1.0 1.0 2020-01-01 5.0 NaN 2020-01-02 2.0 2.0 2020-01-02 4.0 NaN 2020-01-03 3.0 3.0 2020-01-03 3.0 NaN 2020-01-04 4.0 2.0 2020-01-04 2.0 NaN 2020-01-05 5.0 1.0 2020-01-05 1.0 NaN >>> df . vbt . flatten_grouped ( group_by = group_by , order = 'F' ) group first second 2020-01-01 1.0 1.0 2020-01-02 2.0 2.0 2020-01-03 3.0 3.0 2020-01-04 4.0 2.0 2020-01-05 5.0 1.0 2020-01-01 5.0 NaN 2020-01-02 4.0 NaN 2020-01-03 3.0 NaN 2020-01-04 2.0 NaN 2020-01-05 1.0 NaN heatmap method \u00b6 GenericDFAccessor . heatmap ( x_labels = None , y_labels = None , return_fig = True , ** kwargs ) Create Heatmap and return the figure. Usage >>> df = pd . DataFrame ([ ... [ 0 , np . nan , np . nan ], ... [ np . nan , 1 , np . nan ], ... [ np . nan , np . nan , 2 ] ... ]) >>> df . vbt . heatmap () squeeze_grouped method \u00b6 GenericDFAccessor . squeeze_grouped ( squeeze_func_nb , * args , group_by = None , wrap_kwargs = None ) Squeeze each group of columns into a single column. See squeeze_grouped_nb() . Usage >>> group_by = pd . Series ([ 'first' , 'first' , 'second' ], name = 'group' ) >>> mean_squeeze_nb = njit ( lambda i , group , a : np . nanmean ( a )) >>> df . vbt . squeeze_grouped ( mean_squeeze_nb , group_by = group_by ) group first second 2020-01-01 3.0 1.0 2020-01-02 3.0 2.0 2020-01-03 3.0 3.0 2020-01-04 3.0 2.0 2020-01-05 3.0 1.0 ts_heatmap method \u00b6 GenericDFAccessor . ts_heatmap ( is_y_category = True , ** kwargs ) Heatmap of time-series data. GenericSRAccessor class \u00b6 Accessor on top of data of any type. For Series only. Accessible through pd.Series.vbt . Superclasses AttrResolver BaseAccessor BaseSRAccessor Configured Documented GenericAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.bshift() GenericAccessor.config GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.df_accessor_cls GenericAccessor.diff() GenericAccessor.drawdown() GenericAccessor.drawdowns GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.fshift() GenericAccessor.get_drawdowns() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.iloc GenericAccessor.indexing_kwargs GenericAccessor.lineplot() GenericAccessor.loc GenericAccessor.mapping GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.obj GenericAccessor.pct_change() GenericAccessor.plot() GenericAccessor.plots_defaults GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.ranges GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.resolve_self() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.self_aliases GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.sr_accessor_cls GenericAccessor.stats_defaults GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.wrapper GenericAccessor.writeable_attrs GenericAccessor.zscore() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses ReturnsSRAccessor SignalsSRAccessor Vbt_SRAccessor flatten_grouped method \u00b6 GenericSRAccessor . flatten_grouped ( group_by = None , order = 'C' , wrap_kwargs = None ) Flatten each group of elements. Based on GenericDFAccessor.flatten_grouped() . heatmap method \u00b6 GenericSRAccessor . heatmap ( x_level = None , y_level = None , symmetric = False , sort = True , x_labels = None , y_labels = None , slider_level = None , active = 0 , slider_labels = None , return_fig = True , fig = None , ** kwargs ) Create a heatmap figure based on object's multi-index and values. If index is not a multi-index, converts Series into a DataFrame and calls GenericDFAccessor.heatmap() . If multi-index contains more than two levels or you want them in specific order, pass x_level and y_level , each ( int if index or str if name) corresponding to an axis of the heatmap. Optionally, pass slider_level to use a level as a slider. Creates Heatmap and returns the figure. Usage >>> multi_index = pd . MultiIndex . from_tuples ([ ... ( 1 , 1 ), ... ( 2 , 2 ), ... ( 3 , 3 ) ... ]) >>> sr = pd . Series ( np . arange ( len ( multi_index )), index = multi_index ) >>> sr 1 1 0 2 2 1 3 3 2 dtype: int64 >>> sr . vbt . heatmap () Using one level as a slider: >>> multi_index = pd . MultiIndex . from_tuples ([ ... ( 1 , 1 , 1 ), ... ( 1 , 2 , 2 ), ... ( 1 , 3 , 3 ), ... ( 2 , 3 , 3 ), ... ( 2 , 2 , 2 ), ... ( 2 , 1 , 1 ) ... ]) >>> sr = pd . Series ( np . arange ( len ( multi_index )), index = multi_index ) >>> sr 1 1 1 0 2 2 1 3 3 2 2 3 3 3 2 2 4 1 1 5 dtype: int64 >>> sr . vbt . heatmap ( slider_level = 0 ) overlay_with_heatmap method \u00b6 GenericSRAccessor . overlay_with_heatmap ( other , trace_kwargs = None , heatmap_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot Series as a line and overlays it with a heatmap. Args other :\u2002 array_like Second array. Will broadcast. trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter . heatmap_kwargs :\u2002 dict Keyword arguments passed to GenericDFAccessor.heatmap() . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> df [ 'a' ] . vbt . overlay_with_heatmap ( df [ 'b' ]) plot_against method \u00b6 GenericSRAccessor . plot_against ( other , trace_kwargs = None , other_trace_kwargs = None , pos_trace_kwargs = None , neg_trace_kwargs = None , hidden_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot Series as a line against another line. Args other :\u2002 array_like Second array. Will broadcast. trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter . other_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for other . Set to 'hidden' to hide. pos_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for positive line. neg_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for negative line. hidden_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for hidden lines. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> df [ 'a' ] . vbt . plot_against ( df [ 'b' ]) qqplot method \u00b6 GenericSRAccessor . qqplot ( sparams = (), dist = 'norm' , plot_line = True , line_shape_kwargs = None , xref = 'x' , yref = 'y' , fig = None , ** kwargs ) Plot probability plot using scipy.stats.probplot . **kwargs are passed to GenericAccessor.scatterplot() . Usage >>> pd . Series ( np . random . standard_normal ( 100 )) . vbt . qqplot () squeeze_grouped method \u00b6 GenericSRAccessor . squeeze_grouped ( squeeze_func_nb , * args , group_by = None , wrap_kwargs = None ) Squeeze each group of elements into a single element. Based on GenericDFAccessor.squeeze_grouped() . ts_heatmap method \u00b6 GenericSRAccessor . ts_heatmap ( ** kwargs ) Heatmap of time-series data. volume method \u00b6 GenericSRAccessor . volume ( x_level = None , y_level = None , z_level = None , x_labels = None , y_labels = None , z_labels = None , slider_level = None , slider_labels = None , active = 0 , scene_name = 'scene' , fillna = None , fig = None , return_fig = True , ** kwargs ) Create a 3D volume figure based on object's multi-index and values. If multi-index contains more than three levels or you want them in specific order, pass x_level , y_level , and z_level , each ( int if index or str if name) corresponding to an axis of the volume. Optionally, pass slider_level to use a level as a slider. Creates Volume and returns the figure. Usage >>> multi_index = pd . MultiIndex . from_tuples ([ ... ( 1 , 1 , 1 ), ... ( 2 , 2 , 2 ), ... ( 3 , 3 , 3 ) ... ]) >>> sr = pd . Series ( np . arange ( len ( multi_index )), index = multi_index ) >>> sr 1 1 1 0 2 2 2 1 3 3 3 2 dtype: int64 >>> sr . vbt . volume () . show () MetaGenericAccessor class \u00b6 Meta class that exposes a read-only class property StatsBuilderMixin.metrics . Superclasses MetaPlotsBuilderMixin MetaStatsBuilderMixin builtins.type Inherited members MetaPlotsBuilderMixin.subplots MetaStatsBuilderMixin.metrics TransformerT class \u00b6 Base class for protocol classes. Protocol classes are defined as:: class Proto(Protocol): def meth(self) -> int: ... Such classes are primarily used with static type checkers that recognize structural subtyping (static duck-typing), for example:: class C def meth(self) -> int: return 0 def func(x: Proto) -> int: return x.meth() func(C()) # Passes static type check See PEP 544 for details. Protocol classes decorated with @typing_extensions .runtime act as simple-minded runtime protocol that checks only the presence of given attributes, ignoring their type signatures. Protocol classes can be generic, they are defined as:: class GenProto(Protocol[T]): def meth(self) -> T: ... Superclasses typing_extensions.Protocol fit_transform method \u00b6 TransformerT . fit_transform ( * args , ** kwargs ) transform method \u00b6 TransformerT . transform ( * args , ** kwargs )","title":"accessors"},{"location":"api/generic/accessors/#vectorbt.generic.accessors","text":"Custom pandas accessors for generic data. Methods can be accessed as follows: GenericSRAccessor -> pd.Series.vbt.* GenericDFAccessor -> pd.DataFrame.vbt.* >>> import pandas as pd >>> import vectorbt as vbt >>> # vectorbt.generic.accessors.GenericAccessor.rolling_mean >>> pd . Series ([ 1 , 2 , 3 , 4 ]) . vbt . rolling_mean ( 2 ) 0 NaN 1 1.5 2 2.5 3 3.5 dtype: float64 The accessors inherit vectorbt.base.accessors and are inherited by more specialized accessors, such as vectorbt.signals.accessors and vectorbt.returns.accessors . Note Grouping is only supported by the methods that accept the group_by argument. Accessors do not utilize caching. Run for the examples below >>> import vectorbt as vbt >>> import numpy as np >>> import pandas as pd >>> from numba import njit >>> from datetime import datetime , timedelta >>> df = pd . DataFrame ({ ... 'a' : [ 1 , 2 , 3 , 4 , 5 ], ... 'b' : [ 5 , 4 , 3 , 2 , 1 ], ... 'c' : [ 1 , 2 , 3 , 2 , 1 ] ... }, index = pd . Index ([ ... datetime ( 2020 , 1 , 1 ), ... datetime ( 2020 , 1 , 2 ), ... datetime ( 2020 , 1 , 3 ), ... datetime ( 2020 , 1 , 4 ), ... datetime ( 2020 , 1 , 5 ) ... ])) >>> df a b c 2020-01-01 1 5 1 2020-01-02 2 4 2 2020-01-03 3 3 3 2020-01-04 4 2 2 2020-01-05 5 1 1 >>> index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 10 )] >>> sr = pd . Series ( np . arange ( len ( index )), index = index ) >>> sr 2020-01-01 0 2020-01-02 1 2020-01-03 2 2020-01-04 3 2020-01-05 4 2020-01-06 5 2020-01-07 6 2020-01-08 7 2020-01-09 8 2020-01-10 9 dtype: int64","title":"vectorbt.generic.accessors"},{"location":"api/generic/accessors/#stats","text":"Hint See StatsBuilderMixin.stats() and GenericAccessor.metrics . >>> df2 = pd . DataFrame ({ ... 'a' : [ np . nan , 2 , 3 ], ... 'b' : [ 4 , np . nan , 5 ], ... 'c' : [ 6 , 7 , np . nan ] ... }, index = [ 'x' , 'y' , 'z' ]) >>> df2 . vbt ( freq = 'd' ) . stats ( column = 'a' ) Start x End z Period 3 days 00:00:00 Count 2 Mean 2.5 Std 0.707107 Min 2.0 Median 2.5 Max 3.0 Min Index y Max Index z Name: a, dtype: object","title":"Stats"},{"location":"api/generic/accessors/#mapping","text":"Mapping can be set both in GenericAccessor (preferred) and StatsBuilderMixin.stats() : >>> mapping = { x : 'test_' + str ( x ) for x in pd . unique ( df2 . values . flatten ())} >>> df2 . vbt ( freq = 'd' , mapping = mapping ) . stats ( column = 'a' ) Start x End z Period 3 days 00:00:00 Count 2 Value Counts: test_2.0 1 Value Counts: test_3.0 1 Value Counts: test_4.0 0 Value Counts: test_5.0 0 Value Counts: test_6.0 0 Value Counts: test_7.0 0 Value Counts: test_nan 1 Name: a, dtype: object >>> df2 . vbt ( freq = 'd' ) . stats ( column = 'a' , settings = dict ( mapping = mapping )) UserWarning: Changing the mapping will create a copy of this object. Consider setting it upon object creation to re-use existing cache. Start x End z Period 3 days 00:00:00 Count 2 Value Counts: test_2.0 1 Value Counts: test_3.0 1 Value Counts: test_4.0 0 Value Counts: test_5.0 0 Value Counts: test_6.0 0 Value Counts: test_7.0 0 Value Counts: test_nan 1 Name: a, dtype: object Selecting a column before calling stats will consider uniques from this column only: >>> df2 [ 'a' ] . vbt ( freq = 'd' , mapping = mapping ) . stats () Start x End z Period 3 days 00:00:00 Count 2 Value Counts: test_2.0 1 Value Counts: test_3.0 1 Value Counts: test_nan 1 Name: a, dtype: object To include all keys from mapping , pass incl_all_keys=True : df2['a'].vbt(freq='d', mapping=mapping).stats(settings=dict(incl_all_keys=True)) Start x End z Period 3 days 00:00:00 Count 2 Value Counts: test_2.0 1 Value Counts: test_3.0 1 Value Counts: test_4.0 0 Value Counts: test_5.0 0 Value Counts: test_6.0 0 Value Counts: test_7.0 0 Value Counts: test_nan 1 Name: a, dtype: object `GenericAccessor.stats` also supports (re-)grouping: ```pycon >>> df2.vbt(freq='d').stats(column=0, group_by=[0, 0, 1]) Start x End z Period 3 days 00:00:00 Count 4 Mean 3.5 Std 1.290994 Min 2.0 Median 3.5 Max 5.0 Min Index y Max Index z Name: 0, dtype: object","title":"Mapping"},{"location":"api/generic/accessors/#plots","text":"Hint See PlotsBuilderMixin.plots() and GenericAccessor.subplots . GenericAccessor class has a single subplot based on GenericAccessor.plot() : >>> df2 . vbt . plots ()","title":"Plots"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.nb_config","text":"Config of Numba methods to be added to GenericAccessor . Co nf ig( { \"shuffle\" : { \"func\" : \"CPUDispatcher(<function shuffle_nb at 0x7facbadd8d08>)\" , \"path\" : \"vectorbt.generic.nb.shuffle_nb\" }, \"fillna\" : { \"func\" : \"CPUDispatcher(<function fillna_nb at 0x7facbadd8840>)\" , \"path\" : \"vectorbt.generic.nb.fillna_nb\" }, \"bshift\" : { \"func\" : \"CPUDispatcher(<function bshift_nb at 0x7facbaf40a60>)\" , \"path\" : \"vectorbt.generic.nb.bshift_nb\" }, \"fshift\" : { \"func\" : \"CPUDispatcher(<function fshift_nb at 0x7facbaf4b048>)\" , \"path\" : \"vectorbt.generic.nb.fshift_nb\" }, \"diff\" : { \"func\" : \"CPUDispatcher(<function diff_nb at 0x7facbaf4b598>)\" , \"path\" : \"vectorbt.generic.nb.diff_nb\" }, \"pct_change\" : { \"func\" : \"CPUDispatcher(<function pct_change_nb at 0x7facbaf4bae8>)\" , \"path\" : \"vectorbt.generic.nb.pct_change_nb\" }, \"bfill\" : { \"func\" : \"CPUDispatcher(<function bfill_nb at 0x7facbaf550d0>)\" , \"path\" : \"vectorbt.generic.nb.bfill_nb\" }, \"ffill\" : { \"func\" : \"CPUDispatcher(<function ffill_nb at 0x7facbaf55620>)\" , \"path\" : \"vectorbt.generic.nb.ffill_nb\" }, \"cumsum\" : { \"func\" : \"CPUDispatcher(<function nancumsum_nb at 0x7facbaf55b70>)\" , \"path\" : \"vectorbt.generic.nb.nancumsum_nb\" }, \"cumprod\" : { \"func\" : \"CPUDispatcher(<function nancumprod_nb at 0x7facbaf55e18>)\" , \"path\" : \"vectorbt.generic.nb.nancumprod_nb\" }, \"rolling_min\" : { \"func\" : \"CPUDispatcher(<function rolling_min_nb at 0x7facbaf659d8>)\" , \"path\" : \"vectorbt.generic.nb.rolling_min_nb\" }, \"rolling_max\" : { \"func\" : \"CPUDispatcher(<function rolling_max_nb at 0x7facbaf65f28>)\" , \"path\" : \"vectorbt.generic.nb.rolling_max_nb\" }, \"rolling_mean\" : { \"func\" : \"CPUDispatcher(<function rolling_mean_nb at 0x7facbaf6b510>)\" , \"path\" : \"vectorbt.generic.nb.rolling_mean_nb\" }, \"expanding_min\" : { \"func\" : \"CPUDispatcher(<function expanding_min_nb at 0x7facbaf71ae8>)\" , \"path\" : \"vectorbt.generic.nb.expanding_min_nb\" }, \"expanding_max\" : { \"func\" : \"CPUDispatcher(<function expanding_max_nb at 0x7facbaf780d0>)\" , \"path\" : \"vectorbt.generic.nb.expanding_max_nb\" }, \"expanding_mean\" : { \"func\" : \"CPUDispatcher(<function expanding_mean_nb at 0x7facbaf78620>)\" , \"path\" : \"vectorbt.generic.nb.expanding_mean_nb\" }, \"product\" : { \"func\" : \"CPUDispatcher(<function nanprod_nb at 0x7facbaf558c8>)\" , \"is_reducing\" : true , \"path\" : \"vectorbt.generic.nb.nanprod_nb\" } } )","title":"nb_config"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.transform_config","text":"Config of transform methods to be added to GenericAccessor . Co nf ig( { \"binarize\" : { \"transformer\" : \"<class 'sklearn.preprocessing._data.Binarizer'>\" , \"docstring\" : \"See `sklearn.preprocessing.Binarizer`.\" }, \"minmax_scale\" : { \"transformer\" : \"<class 'sklearn.preprocessing._data.MinMaxScaler'>\" , \"docstring\" : \"See `sklearn.preprocessing.MinMaxScaler`.\" }, \"maxabs_scale\" : { \"transformer\" : \"<class 'sklearn.preprocessing._data.MaxAbsScaler'>\" , \"docstring\" : \"See `sklearn.preprocessing.MaxAbsScaler`.\" }, \"normalize\" : { \"transformer\" : \"<class 'sklearn.preprocessing._data.Normalizer'>\" , \"docstring\" : \"See `sklearn.preprocessing.Normalizer`.\" }, \"robust_scale\" : { \"transformer\" : \"<class 'sklearn.preprocessing._data.RobustScaler'>\" , \"docstring\" : \"See `sklearn.preprocessing.RobustScaler`.\" }, \"scale\" : { \"transformer\" : \"<class 'sklearn.preprocessing._data.StandardScaler'>\" , \"docstring\" : \"See `sklearn.preprocessing.StandardScaler`.\" }, \"quantile_transform\" : { \"transformer\" : \"<class 'sklearn.preprocessing._data.QuantileTransformer'>\" , \"docstring\" : \"See `sklearn.preprocessing.QuantileTransformer`.\" }, \"power_transform\" : { \"transformer\" : \"<class 'sklearn.preprocessing._data.PowerTransformer'>\" , \"docstring\" : \"See `sklearn.preprocessing.PowerTransformer`.\" } } )","title":"transform_config"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor","text":"Accessor on top of data of any type. For both, Series and DataFrames. Accessible through pd.Series.vbt and pd.DataFrame.vbt . Superclasses AttrResolver BaseAccessor Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.config BaseAccessor.df_accessor_cls BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.iloc BaseAccessor.indexing_func() BaseAccessor.indexing_kwargs BaseAccessor.loc BaseAccessor.make_symmetric() BaseAccessor.obj BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.self_aliases BaseAccessor.sr_accessor_cls BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() BaseAccessor.wrapper BaseAccessor.writeable_attrs Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses GenericDFAccessor GenericSRAccessor ReturnsAccessor SignalsAccessor","title":"GenericAccessor"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.apply_along_axis","text":"GenericAccessor . apply_along_axis ( apply_func_nb , * args , axis = 0 , wrap_kwargs = None ) Apply a function apply_func_nb along an axis.","title":"apply_along_axis()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.apply_and_reduce","text":"GenericAccessor . apply_and_reduce ( apply_func_nb , reduce_func_nb , apply_args = None , reduce_args = None , wrap_kwargs = None ) See apply_and_reduce_nb() . Usage >>> greater_nb = njit ( lambda col , a : a [ a > 2 ]) >>> mean_nb = njit ( lambda col , a : np . nanmean ( a )) >>> df . vbt . apply_and_reduce ( greater_nb , mean_nb ) a 4.0 b 4.0 c 3.0 dtype: float64","title":"apply_and_reduce()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.apply_mapping","text":"GenericAccessor . apply_mapping ( ** kwargs ) See apply_mapping() .","title":"apply_mapping()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.applymap","text":"GenericAccessor . applymap ( apply_func_nb , * args , wrap_kwargs = None ) See applymap_nb() . Usage >>> multiply_nb = njit ( lambda i , col , a : a ** 2 ) >>> df . vbt . applymap ( multiply_nb ) a b c 2020-01-01 1.0 25.0 1.0 2020-01-02 4.0 16.0 4.0 2020-01-03 9.0 9.0 9.0 2020-01-04 16.0 4.0 4.0 2020-01-05 25.0 1.0 1.0","title":"applymap()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.barplot","text":"GenericAccessor . barplot ( trace_names = None , x_labels = None , return_fig = True , ** kwargs ) Create Bar and return the figure. Usage >>> df . vbt . barplot ()","title":"barplot()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.bfill","text":"GenericAccessor . bfill ( * , wrap_kwargs = None ) See bfill_nb() .","title":"bfill()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.binarize","text":"GenericAccessor . binarize ( * , threshold = 0.0 , copy = True , ** kwargs ) See sklearn.preprocessing.Binarizer .","title":"binarize()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.boxplot","text":"GenericAccessor . boxplot ( trace_names = None , group_by = None , return_fig = True , ** kwargs ) Create Box and return the figure. Usage >>> df . vbt . boxplot ()","title":"boxplot()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.bshift","text":"GenericAccessor . bshift ( n = 1 , fill_value = nan , * , wrap_kwargs = None ) See bshift_nb() .","title":"bshift()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.count","text":"GenericAccessor . count ( group_by = None , wrap_kwargs = None ) Return count of non-NaN elements.","title":"count()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.crossed_above","text":"GenericAccessor . crossed_above ( other , wait = 0 , broadcast_kwargs = None , wrap_kwargs = None ) Generate crossover above another array. See crossed_above_nb() . Usage >>> df [ 'b' ] . vbt . crossed_above ( df [ 'c' ]) 2020-01-01 False 2020-01-02 False 2020-01-03 False 2020-01-04 False 2020-01-05 False dtype: bool >>> df [ 'a' ] . vbt . crossed_above ( df [ 'b' ]) 2020-01-01 False 2020-01-02 False 2020-01-03 False 2020-01-04 True 2020-01-05 False dtype: bool >>> df [ 'a' ] . vbt . crossed_above ( df [ 'b' ], wait = 1 ) 2020-01-01 False 2020-01-02 False 2020-01-03 False 2020-01-04 False 2020-01-05 True dtype: bool","title":"crossed_above()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.crossed_below","text":"GenericAccessor . crossed_below ( other , wait = 0 , broadcast_kwargs = None , wrap_kwargs = None ) Generate crossover below another array. See crossed_above_nb() but in reversed order.","title":"crossed_below()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.cumprod","text":"GenericAccessor . cumprod ( * , wrap_kwargs = None ) See nancumprod_nb() .","title":"cumprod()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.cumsum","text":"GenericAccessor . cumsum ( * , wrap_kwargs = None ) See nancumsum_nb() .","title":"cumsum()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.describe","text":"GenericAccessor . describe ( percentiles = None , ddof = 1 , group_by = None , wrap_kwargs = None ) See describe_reduce_nb() . For percentiles , see pd.DataFrame.describe . Usage >>> df . vbt . describe () a b c count 5.000000 5.000000 5.00000 mean 3.000000 3.000000 1.80000 std 1.581139 1.581139 0.83666 min 1.000000 1.000000 1.00000 25% 2.000000 2.000000 1.00000 50% 3.000000 3.000000 2.00000 75% 4.000000 4.000000 2.00000 max 5.000000 5.000000 3.00000","title":"describe()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.diff","text":"GenericAccessor . diff ( n = 1 , * , wrap_kwargs = None ) See diff_nb() .","title":"diff()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.drawdown","text":"GenericAccessor . drawdown ( wrap_kwargs = None ) Drawdown series.","title":"drawdown()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.drawdowns","text":"GenericAccessor.get_drawdowns() with default arguments.","title":"drawdowns"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.ewm_mean","text":"GenericAccessor . ewm_mean ( span , minp = 0 , adjust = True , wrap_kwargs = None ) See ewm_mean_nb() .","title":"ewm_mean()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.ewm_std","text":"GenericAccessor . ewm_std ( span , minp = 0 , adjust = True , ddof = 1 , wrap_kwargs = None ) See ewm_std_nb() .","title":"ewm_std()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.expanding_apply","text":"GenericAccessor . expanding_apply ( apply_func_nb , * args , minp = 1 , on_matrix = False , wrap_kwargs = None ) See expanding_apply_nb() and expanding_matrix_apply_nb() for on_matrix=True . Usage >>> mean_nb = njit ( lambda i , col , a : np . nanmean ( a )) >>> df . vbt . expanding_apply ( mean_nb ) a b c 2020-01-01 1.0 5.0 1.0 2020-01-02 1.5 4.5 1.5 2020-01-03 2.0 4.0 2.0 2020-01-04 2.5 3.5 2.0 2020-01-05 3.0 3.0 1.8 >>> mean_matrix_nb = njit ( lambda i , a : np . nanmean ( a )) >>> df . vbt . expanding_apply ( mean_matrix_nb , on_matrix = True ) a b c 2020-01-01 2.333333 2.333333 2.333333 2020-01-02 2.500000 2.500000 2.500000 2020-01-03 2.666667 2.666667 2.666667 2020-01-04 2.666667 2.666667 2.666667 2020-01-05 2.600000 2.600000 2.600000","title":"expanding_apply()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.expanding_max","text":"GenericAccessor . expanding_max ( minp = 1 , * , wrap_kwargs = None ) See expanding_max_nb() .","title":"expanding_max()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.expanding_mean","text":"GenericAccessor . expanding_mean ( minp = 1 , * , wrap_kwargs = None ) See expanding_mean_nb() .","title":"expanding_mean()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.expanding_min","text":"GenericAccessor . expanding_min ( minp = 1 , * , wrap_kwargs = None ) See expanding_min_nb() .","title":"expanding_min()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.expanding_split","text":"GenericAccessor . expanding_split ( ** kwargs ) Split using GenericAccessor.split() on ExpandingSplitter . Usage >>> train_set , valid_set , test_set = sr . vbt . expanding_split ( ... n = 5 , set_lens = ( 1 , 1 ), min_len = 3 , left_to_right = False ) >>> train_set [ 0 ] split_idx 0 1 2 3 4 5 6 7 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 NaN 1.0 1.0 1.0 1.0 1.0 1.0 1 2 NaN NaN 2.0 2.0 2.0 2.0 2.0 2 3 NaN NaN NaN 3.0 3.0 3.0 3.0 3 4 NaN NaN NaN NaN 4.0 4.0 4.0 4 5 NaN NaN NaN NaN NaN 5.0 5.0 5 6 NaN NaN NaN NaN NaN NaN 6.0 6 7 NaN NaN NaN NaN NaN NaN NaN 7 >>> valid_set [ 0 ] split_idx 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 8 >>> test_set [ 0 ] split_idx 0 1 2 3 4 5 6 7 0 2 3 4 5 6 7 8 9 >>> sr . vbt . expanding_split ( ... set_lens = ( 1 , 1 ), min_len = 3 , left_to_right = False , ... plot = True , trace_names = [ 'train' , 'valid' , 'test' ])","title":"expanding_split()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.expanding_std","text":"GenericAccessor . expanding_std ( minp = 1 , ddof = 1 , wrap_kwargs = None ) See expanding_std_nb() .","title":"expanding_std()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.ffill","text":"GenericAccessor . ffill ( * , wrap_kwargs = None ) See ffill_nb() .","title":"ffill()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.fillna","text":"GenericAccessor . fillna ( value , * , wrap_kwargs = None ) See fillna_nb() .","title":"fillna()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.filter","text":"GenericAccessor . filter ( filter_func_nb , * args , wrap_kwargs = None ) See filter_nb() . Usage >>> greater_nb = njit ( lambda i , col , a : a > 2 ) >>> df . vbt . filter ( greater_nb ) a b c 2020-01-01 NaN 5.0 NaN 2020-01-02 NaN 4.0 NaN 2020-01-03 3.0 3.0 3.0 2020-01-04 4.0 NaN NaN 2020-01-05 5.0 NaN NaN","title":"filter()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.fshift","text":"GenericAccessor . fshift ( n = 1 , fill_value = nan , * , wrap_kwargs = None ) See fshift_nb() .","title":"fshift()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.get_drawdowns","text":"GenericAccessor . get_drawdowns ( wrapper_kwargs = None , ** kwargs ) Generate drawdown records. See Drawdowns .","title":"get_drawdowns()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.get_ranges","text":"GenericAccessor . get_ranges ( wrapper_kwargs = None , ** kwargs ) Generate range records. See Ranges .","title":"get_ranges()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.groupby_apply","text":"GenericAccessor . groupby_apply ( by , apply_func_nb , * args , on_matrix = False , wrap_kwargs = None , ** kwargs ) See groupby_apply_nb() and groupby_matrix_apply_nb() for on_matrix=True . For by , see pd.DataFrame.groupby . Usage >>> mean_nb = njit ( lambda i , col , a : np . nanmean ( a )) >>> df . vbt . groupby_apply ([ 1 , 1 , 2 , 2 , 3 ], mean_nb ) a b c 1 1.5 4.5 1.5 2 3.5 2.5 2.5 3 5.0 1.0 1.0 >>> mean_matrix_nb = njit ( lambda i , a : np . nanmean ( a )) >>> df . vbt . groupby_apply ([ 1 , 1 , 2 , 2 , 3 ], mean_matrix_nb , on_matrix = True ) a b c 1 2.500000 2.500000 2.500000 2 2.833333 2.833333 2.833333 3 2.333333 2.333333 2.333333","title":"groupby_apply()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.histplot","text":"GenericAccessor . histplot ( trace_names = None , group_by = None , return_fig = True , ** kwargs ) Create Histogram and return the figure. Usage >>> df . vbt . histplot ()","title":"histplot()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.idxmax","text":"GenericAccessor . idxmax ( group_by = None , order = 'C' , wrap_kwargs = None ) Return labeled index of max of non-NaN elements.","title":"idxmax()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.idxmin","text":"GenericAccessor . idxmin ( group_by = None , order = 'C' , wrap_kwargs = None ) Return labeled index of min of non-NaN elements.","title":"idxmin()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.lineplot","text":"GenericAccessor . lineplot ( ** kwargs ) GenericAccessor.plot() with 'lines' mode. Usage >>> df . vbt . lineplot ()","title":"lineplot()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.mapping","text":"Mapping.","title":"mapping"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.max","text":"GenericAccessor . max ( group_by = None , wrap_kwargs = None ) Return max of non-NaN elements.","title":"max()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.maxabs_scale","text":"GenericAccessor . maxabs_scale ( * , copy = True , ** kwargs ) See sklearn.preprocessing.MaxAbsScaler .","title":"maxabs_scale()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.mean","text":"GenericAccessor . mean ( group_by = None , wrap_kwargs = None ) Return mean of non-NaN elements.","title":"mean()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.median","text":"GenericAccessor . median ( group_by = None , wrap_kwargs = None ) Return median of non-NaN elements.","title":"median()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.metrics","text":"Metrics supported by GenericAccessor . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function GenericAccessor.<lambda> at 0x7facbcf4df28>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function GenericAccessor.<lambda> at 0x7facbcf50048>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function GenericAccessor.<lambda> at 0x7facbcf500d0>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"count\" : { \"title\" : \"Count\" , \"calc_func\" : \"count\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"describe\" ] }, \"mean\" : { \"title\" : \"Mean\" , \"calc_func\" : \"mean\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"describe\" ] }, \"std\" : { \"title\" : \"Std\" , \"calc_func\" : \"std\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"describe\" ] }, \"min\" : { \"title\" : \"Min\" , \"calc_func\" : \"min\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"describe\" ] }, \"median\" : { \"title\" : \"Median\" , \"calc_func\" : \"median\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"describe\" ] }, \"max\" : { \"title\" : \"Max\" , \"calc_func\" : \"max\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"describe\" ] }, \"idx_min\" : { \"title\" : \"Min Index\" , \"calc_func\" : \"idxmin\" , \"agg_func\" : null , \"inv_check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"index\" ] }, \"idx_max\" : { \"title\" : \"Max Index\" , \"calc_func\" : \"idxmax\" , \"agg_func\" : null , \"inv_check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"index\" ] }, \"value_counts\" : { \"title\" : \"Value Counts\" , \"calc_func\" : \"<function GenericAccessor.<lambda> at 0x7facbcf50158>\" , \"resolve_value_counts\" : true , \"check_has_mapping\" : true , \"tags\" : [ \"generic\" , \"value_counts\" ] } } ) Returns GenericAccessor._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable GenericAccessor._metrics .","title":"metrics"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.min","text":"GenericAccessor . min ( group_by = None , wrap_kwargs = None ) Return min of non-NaN elements.","title":"min()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.minmax_scale","text":"GenericAccessor . minmax_scale ( feature_range = ( 0 , 1 ), * , copy = True , clip = False , ** kwargs ) See sklearn.preprocessing.MinMaxScaler .","title":"minmax_scale()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.normalize","text":"GenericAccessor . normalize ( norm = 'l2' , * , copy = True , ** kwargs ) See sklearn.preprocessing.Normalizer .","title":"normalize()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.pct_change","text":"GenericAccessor . pct_change ( n = 1 , * , wrap_kwargs = None ) See pct_change_nb() .","title":"pct_change()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.plot","text":"GenericAccessor . plot ( trace_names = None , x_labels = None , return_fig = True , ** kwargs ) Create Scatter and return the figure. Usage >>> df . vbt . plot ()","title":"plot()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.plots_defaults","text":"Defaults for PlotsBuilderMixin.plots() . Merges PlotsBuilderMixin.plots_defaults and generic.plots from settings .","title":"plots_defaults"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.power_transform","text":"GenericAccessor . power_transform ( method = 'yeo-johnson' , * , standardize = True , copy = True , ** kwargs ) See sklearn.preprocessing.PowerTransformer .","title":"power_transform()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.product","text":"GenericAccessor . product ( * , wrap_kwargs = None ) See nanprod_nb() .","title":"product()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.quantile_transform","text":"GenericAccessor . quantile_transform ( * , n_quantiles = 1000 , output_distribution = 'uniform' , ignore_implicit_zeros = False , subsample = 100000 , random_state = None , copy = True , ** kwargs ) See sklearn.preprocessing.QuantileTransformer .","title":"quantile_transform()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.range_split","text":"GenericAccessor . range_split ( ** kwargs ) Split using GenericAccessor.split() on RangeSplitter . Usage >>> range_df , range_indexes = sr . vbt . range_split ( n = 2 ) >>> range_df split_idx 0 1 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 >>> range_indexes [DatetimeIndex(['2020-01-01', ..., '2020-01-05'], dtype='datetime64[ns]', name='split_0'), DatetimeIndex(['2020-01-06', ..., '2020-01-10'], dtype='datetime64[ns]', name='split_1')] >>> range_df , range_indexes = sr . vbt . range_split ( range_len = 4 ) >>> range_df split_idx 0 1 2 3 4 5 6 0 0 1 2 3 4 5 6 1 1 2 3 4 5 6 7 2 2 3 4 5 6 7 8 3 3 4 5 6 7 8 9 >>> range_indexes [DatetimeIndex(['2020-01-01', ..., '2020-01-04'], dtype='datetime64[ns]', name='split_0'), DatetimeIndex(['2020-01-02', ..., '2020-01-05'], dtype='datetime64[ns]', name='split_1'), DatetimeIndex(['2020-01-03', ..., '2020-01-06'], dtype='datetime64[ns]', name='split_2'), DatetimeIndex(['2020-01-04', ..., '2020-01-07'], dtype='datetime64[ns]', name='split_3'), DatetimeIndex(['2020-01-05', ..., '2020-01-08'], dtype='datetime64[ns]', name='split_4'), DatetimeIndex(['2020-01-06', ..., '2020-01-09'], dtype='datetime64[ns]', name='split_5'), DatetimeIndex(['2020-01-07', ..., '2020-01-10'], dtype='datetime64[ns]', name='split_6')] >>> range_df , range_indexes = sr . vbt . range_split ( start_idxs = [ 0 , 2 ], end_idxs = [ 5 , 7 ]) >>> range_df split_idx 0 1 0 0 2 1 1 3 2 2 4 3 3 5 4 4 6 5 5 7 >>> range_indexes [DatetimeIndex(['2020-01-01', ..., '2020-01-06'], dtype='datetime64[ns]', name='split_0'), DatetimeIndex(['2020-01-03', ..., '2020-01-08'], dtype='datetime64[ns]', name='split_1')] >>> range_df , range_indexes = sr . vbt . range_split ( start_idxs = [ 0 ], end_idxs = [ 2 , 3 , 4 ]) >>> range_df split_idx 0 1 2 0 0.0 0.0 0 1 1.0 1.0 1 2 2.0 2.0 2 3 NaN 3.0 3 4 NaN NaN 4 >>> range_indexes [DatetimeIndex(['2020-01-01', ..., '2020-01-03'], dtype='datetime64[ns]', name='split_0'), DatetimeIndex(['2020-01-01', ..., '2020-01-04'], dtype='datetime64[ns]', name='split_1'), DatetimeIndex(['2020-01-01', ..., '2020-01-05'], dtype='datetime64[ns]', name='split_2')] >>> range_df , range_indexes = sr . vbt . range_split ( ... start_idxs = pd . Index ([ '2020-01-01' , '2020-01-02' ]), ... end_idxs = pd . Index ([ '2020-01-04' , '2020-01-05' ]) ... ) >>> range_df split_idx 0 1 0 0 1 1 1 2 2 2 3 3 3 4 >>> range_indexes [DatetimeIndex(['2020-01-01', ..., '2020-01-04'], dtype='datetime64[ns]', name='split_0'), DatetimeIndex(['2020-01-02', ..., '2020-01-05'], dtype='datetime64[ns]', name='split_1')] >>> sr.vbt.range_split( ... start_idxs=pd.Index(['2020-01-01', '2020-01-02', '2020-01-01']), ... end_idxs=pd.Index(['2020-01-08', '2020-01-04', '2020-01-07']), ... plot=True ... )","title":"range_split()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.ranges","text":"GenericAccessor.get_ranges() with default arguments.","title":"ranges"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.rebase","text":"GenericAccessor . rebase ( base , wrap_kwargs = None ) Rebase all series to a given intial base. This makes comparing/plotting different series together easier. Will forward and backward fill NaN values.","title":"rebase()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.reduce","text":"GenericAccessor . reduce ( reduce_func_nb , * args , returns_array = False , returns_idx = False , flatten = False , order = 'C' , to_index = True , group_by = None , wrap_kwargs = None ) Reduce by column. See flat_reduce_grouped_to_array_nb() if grouped, returns_array is True and flatten is True. See flat_reduce_grouped_nb() if grouped, returns_array is False and flatten is True. See reduce_grouped_to_array_nb() if grouped, returns_array is True and flatten is False. See reduce_grouped_nb() if grouped, returns_array is False and flatten is False. See reduce_to_array_nb() if not grouped and returns_array is True. See reduce_nb() if not grouped and returns_array is False. Set returns_idx to True if values returned by reduce_func_nb are indices/positions. Set to_index to False to return raw positions instead of labels. Usage >>> mean_nb = njit ( lambda col , a : np . nanmean ( a )) >>> df . vbt . reduce ( mean_nb ) a 3.0 b 3.0 c 1.8 dtype: float64 >>> argmax_nb = njit ( lambda col , a : np . argmax ( a )) >>> df . vbt . reduce ( argmax_nb , returns_idx = True ) a 2020-01-05 b 2020-01-01 c 2020-01-03 dtype: datetime64[ns] >>> argmax_nb = njit ( lambda col , a : np . argmax ( a )) >>> df . vbt . reduce ( argmax_nb , returns_idx = True , to_index = False ) a 4 b 0 c 2 dtype: int64 >>> min_max_nb = njit ( lambda col , a : np . array ([ np . nanmin ( a ), np . nanmax ( a )])) >>> df . vbt . reduce ( min_max_nb , returns_array = True , wrap_kwargs = dict ( name_or_index = [ 'min' , 'max' ])) a b c min 1.0 1.0 1.0 max 5.0 5.0 3.0 >>> group_by = pd . Series ([ 'first' , 'first' , 'second' ], name = 'group' ) >>> df . vbt . reduce ( mean_nb , group_by = group_by ) group first 3.0 second 1.8 dtype: float64 >>> df . vbt . reduce ( min_max_nb , name_or_index = [ 'min' , 'max' ], ... returns_array = True , group_by = group_by ) group first second min 1.0 1.0 max 5.0 3.0","title":"reduce()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.resample_apply","text":"GenericAccessor . resample_apply ( freq , apply_func_nb , * args , on_matrix = False , wrap_kwargs = None , ** kwargs ) See groupby_apply_nb() and groupby_matrix_apply_nb() for on_matrix=True . For freq , see pd.DataFrame.resample . Usage >>> mean_nb = njit ( lambda i , col , a : np . nanmean ( a )) >>> df . vbt . resample_apply ( '2d' , mean_nb ) a b c 2020-01-01 1.5 4.5 1.5 2020-01-03 3.5 2.5 2.5 2020-01-05 5.0 1.0 1.0 >>> mean_matrix_nb = njit ( lambda i , a : np . nanmean ( a )) >>> df . vbt . resample_apply ( '2d' , mean_matrix_nb , on_matrix = True ) a b c 2020-01-01 2.500000 2.500000 2.500000 2020-01-03 2.833333 2.833333 2.833333 2020-01-05 2.333333 2.333333 2.333333","title":"resample_apply()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.resolve_self","text":"GenericAccessor . resolve_self ( cond_kwargs = None , custom_arg_names = None , impacts_caching = True , silence_warnings = False ) Resolve self. See Wrapping.resolve_self() . Creates a copy of this instance mapping is different in cond_kwargs .","title":"resolve_self()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.robust_scale","text":"GenericAccessor . robust_scale ( * , with_centering = True , with_scaling = True , quantile_range = ( 25.0 , 75.0 ), copy = True , unit_variance = False , ** kwargs ) See sklearn.preprocessing.RobustScaler .","title":"robust_scale()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.rolling_apply","text":"GenericAccessor . rolling_apply ( window , apply_func_nb , * args , minp = None , on_matrix = False , wrap_kwargs = None ) See rolling_apply_nb() and rolling_matrix_apply_nb() for on_matrix=True . Usage >>> mean_nb = njit ( lambda i , col , a : np . nanmean ( a )) >>> df . vbt . rolling_apply ( 3 , mean_nb ) a b c 2020-01-01 1.0 5.0 1.000000 2020-01-02 1.5 4.5 1.500000 2020-01-03 2.0 4.0 2.000000 2020-01-04 3.0 3.0 2.333333 2020-01-05 4.0 2.0 2.000000 >>> mean_matrix_nb = njit ( lambda i , a : np . nanmean ( a )) >>> df . vbt . rolling_apply ( 3 , mean_matrix_nb , on_matrix = True ) a b c 2020-01-01 2.333333 2.333333 2.333333 2020-01-02 2.500000 2.500000 2.500000 2020-01-03 2.666667 2.666667 2.666667 2020-01-04 2.777778 2.777778 2.777778 2020-01-05 2.666667 2.666667 2.666667","title":"rolling_apply()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.rolling_max","text":"GenericAccessor . rolling_max ( window , minp = None , * , wrap_kwargs = None ) See rolling_max_nb() .","title":"rolling_max()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.rolling_mean","text":"GenericAccessor . rolling_mean ( window , minp = None , * , wrap_kwargs = None ) See rolling_mean_nb() .","title":"rolling_mean()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.rolling_min","text":"GenericAccessor . rolling_min ( window , minp = None , * , wrap_kwargs = None ) See rolling_min_nb() .","title":"rolling_min()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.rolling_split","text":"GenericAccessor . rolling_split ( ** kwargs ) Split using GenericAccessor.split() on RollingSplitter . Usage >>> train_set , valid_set , test_set = sr . vbt . rolling_split ( ... window_len = 5 , set_lens = ( 1 , 1 ), left_to_right = False ) >>> train_set [ 0 ] split_idx 0 1 2 3 4 5 0 0 1 2 3 4 5 1 1 2 3 4 5 6 2 2 3 4 5 6 7 >>> valid_set [ 0 ] split_idx 0 1 2 3 4 5 0 3 4 5 6 7 8 >>> test_set [ 0 ] split_idx 0 1 2 3 4 5 0 4 5 6 7 8 9 >>> sr . vbt . rolling_split ( ... window_len = 5 , set_lens = ( 1 , 1 ), left_to_right = False , ... plot = True , trace_names = [ 'train' , 'valid' , 'test' ])","title":"rolling_split()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.rolling_std","text":"GenericAccessor . rolling_std ( window , minp = None , ddof = 1 , wrap_kwargs = None ) See rolling_std_nb() .","title":"rolling_std()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.scale","text":"GenericAccessor . scale ( * , copy = True , with_mean = True , with_std = True , ** kwargs ) See sklearn.preprocessing.StandardScaler .","title":"scale()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.scatterplot","text":"GenericAccessor . scatterplot ( ** kwargs ) GenericAccessor.plot() with 'markers' mode. Usage >>> df . vbt . scatterplot ()","title":"scatterplot()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.shuffle","text":"GenericAccessor . shuffle ( seed = None , * , wrap_kwargs = None ) See shuffle_nb() .","title":"shuffle()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.split","text":"GenericAccessor . split ( splitter , stack_kwargs = None , keys = None , plot = False , trace_names = None , heatmap_kwargs = None , ** kwargs ) Split using a splitter. Returns a tuple of tuples, each corresponding to a set and composed of a dataframe and split indexes. A splitter can be any class instance that has split method, ideally subclassing sklearn.model_selection.BaseCrossValidator or BaseSplitter . heatmap_kwargs are passed to Heatmap if plot is True, can be a dictionary or a list per set, for example, to set trace name for each set ('train', 'test', etc.). **kwargs are passed to the split method. Note The datetime-like format of the index will be lost as result of this operation. Make sure to store the index metadata such as frequency information beforehand. Usage >>> from sklearn.model_selection import TimeSeriesSplit >>> splitter = TimeSeriesSplit ( n_splits = 3 ) >>> ( train_df , train_indexes ), ( test_df , test_indexes ) = sr . vbt . split ( splitter ) >>> train_df split_idx 0 1 2 0 0.0 0.0 0 1 1.0 1.0 1 2 2.0 2.0 2 3 3.0 3.0 3 4 NaN 4.0 4 5 NaN 5.0 5 6 NaN NaN 6 7 NaN NaN 7 >>> train_indexes [DatetimeIndex(['2020-01-01', ..., '2020-01-04'], dtype='datetime64[ns]', name='split_0'), DatetimeIndex(['2020-01-01', ..., '2020-01-06'], dtype='datetime64[ns]', name='split_1'), DatetimeIndex(['2020-01-01', ..., '2020-01-08'], dtype='datetime64[ns]', name='split_2')] >>> test_df split_idx 0 1 2 0 4 6 8 1 5 7 9 >>> test_indexes [DatetimeIndex(['2020-01-05', '2020-01-06'], dtype='datetime64[ns]', name='split_0'), DatetimeIndex(['2020-01-07', '2020-01-08'], dtype='datetime64[ns]', name='split_1'), DatetimeIndex(['2020-01-09', '2020-01-10'], dtype='datetime64[ns]', name='split_2')] >>> sr . vbt . split ( splitter , plot = True , trace_names = [ 'train' , 'test' ])","title":"split()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.stats_defaults","text":"Defaults for StatsBuilderMixin.stats() . Merges StatsBuilderMixin.stats_defaults and generic.stats from settings .","title":"stats_defaults"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.std","text":"GenericAccessor . std ( ddof = 1 , group_by = None , wrap_kwargs = None ) Return standard deviation of non-NaN elements.","title":"std()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.subplots","text":"Subplots supported by GenericAccessor . Co nf ig( { \"plot\" : { \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"pass_trace_names\" : false , \"tags\" : \"generic\" } } ) Returns GenericAccessor._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable GenericAccessor._subplots .","title":"subplots"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.sum","text":"GenericAccessor . sum ( group_by = None , wrap_kwargs = None ) Return sum of non-NaN elements.","title":"sum()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.to_mapped","text":"GenericAccessor . to_mapped ( dropna = True , dtype = None , group_by = None , ** kwargs ) Convert this object into an instance of MappedArray .","title":"to_mapped()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.to_returns","text":"GenericAccessor . to_returns ( ** kwargs ) Get returns of this object.","title":"to_returns()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.transform","text":"GenericAccessor . transform ( transformer , wrap_kwargs = None , ** kwargs ) Transform using a transformer. A transformer can be any class instance that has transform and fit_transform methods, ideally subclassing sklearn.base.TransformerMixin and sklearn.base.BaseEstimator . Will fit transformer if not fitted. **kwargs are passed to the transform or fit_transform method. Usage >>> from sklearn.preprocessing import MinMaxScaler >>> df . vbt . transform ( MinMaxScaler (( - 1 , 1 ))) a b c 2020-01-01 -1.0 1.0 -1.0 2020-01-02 -0.5 0.5 0.0 2020-01-03 0.0 0.0 1.0 2020-01-04 0.5 -0.5 0.0 2020-01-05 1.0 -1.0 -1.0 >>> fitted_scaler = MinMaxScaler (( - 1 , 1 )) . fit ( np . array ([[ 2 ], [ 4 ]])) >>> df . vbt . transform ( fitted_scaler ) a b c 2020-01-01 -2.0 2.0 -2.0 2020-01-02 -1.0 1.0 -1.0 2020-01-03 0.0 0.0 0.0 2020-01-04 1.0 -1.0 -1.0 2020-01-05 2.0 -2.0 -2.0","title":"transform()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.value_counts","text":"GenericAccessor . value_counts ( normalize = False , sort_uniques = True , sort = False , ascending = False , dropna = False , group_by = None , mapping = None , incl_all_keys = False , wrap_kwargs = None , ** kwargs ) Return a Series/DataFrame containing counts of unique values. Enable normalize flag to return the relative frequencies of the unique values. Enable sort_uniques flag to sort uniques. Enable sort flag to sort by frequencies. Enable ascending flag to sort in ascending order. Enable dropna flag to exclude counts of NaN. Enable incl_all_keys to include all mapping keys, no only those that are present in the array. Mapping will be applied using apply_mapping() with **kwargs .","title":"value_counts()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericAccessor.zscore","text":"GenericAccessor . zscore ( ** kwargs ) Compute z-score using sklearn.preprocessing.StandardScaler .","title":"zscore()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericDFAccessor","text":"Accessor on top of data of any type. For DataFrames only. Accessible through pd.DataFrame.vbt . Superclasses AttrResolver BaseAccessor BaseDFAccessor Configured Documented GenericAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.bshift() GenericAccessor.config GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.df_accessor_cls GenericAccessor.diff() GenericAccessor.drawdown() GenericAccessor.drawdowns GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.fshift() GenericAccessor.get_drawdowns() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.iloc GenericAccessor.indexing_kwargs GenericAccessor.lineplot() GenericAccessor.loc GenericAccessor.mapping GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.obj GenericAccessor.pct_change() GenericAccessor.plot() GenericAccessor.plots_defaults GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.ranges GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.resolve_self() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.self_aliases GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.sr_accessor_cls GenericAccessor.stats_defaults GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.wrapper GenericAccessor.writeable_attrs GenericAccessor.zscore() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses OHLCVDFAccessor ReturnsDFAccessor SignalsDFAccessor Vbt_DFAccessor","title":"GenericDFAccessor"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericDFAccessor.flatten_grouped","text":"GenericDFAccessor . flatten_grouped ( group_by = None , order = 'C' , wrap_kwargs = None ) Flatten each group of columns. See flatten_grouped_nb() . If all groups have the same length, see flatten_uniform_grouped_nb() . Warning Make sure that the distribution of group lengths is close to uniform, otherwise groups with less columns will be filled with NaN and needlessly occupy memory. Usage >>> group_by = pd . Series ([ 'first' , 'first' , 'second' ], name = 'group' ) >>> df . vbt . flatten_grouped ( group_by = group_by , order = 'C' ) group first second 2020-01-01 1.0 1.0 2020-01-01 5.0 NaN 2020-01-02 2.0 2.0 2020-01-02 4.0 NaN 2020-01-03 3.0 3.0 2020-01-03 3.0 NaN 2020-01-04 4.0 2.0 2020-01-04 2.0 NaN 2020-01-05 5.0 1.0 2020-01-05 1.0 NaN >>> df . vbt . flatten_grouped ( group_by = group_by , order = 'F' ) group first second 2020-01-01 1.0 1.0 2020-01-02 2.0 2.0 2020-01-03 3.0 3.0 2020-01-04 4.0 2.0 2020-01-05 5.0 1.0 2020-01-01 5.0 NaN 2020-01-02 4.0 NaN 2020-01-03 3.0 NaN 2020-01-04 2.0 NaN 2020-01-05 1.0 NaN","title":"flatten_grouped()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericDFAccessor.heatmap","text":"GenericDFAccessor . heatmap ( x_labels = None , y_labels = None , return_fig = True , ** kwargs ) Create Heatmap and return the figure. Usage >>> df = pd . DataFrame ([ ... [ 0 , np . nan , np . nan ], ... [ np . nan , 1 , np . nan ], ... [ np . nan , np . nan , 2 ] ... ]) >>> df . vbt . heatmap ()","title":"heatmap()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericDFAccessor.squeeze_grouped","text":"GenericDFAccessor . squeeze_grouped ( squeeze_func_nb , * args , group_by = None , wrap_kwargs = None ) Squeeze each group of columns into a single column. See squeeze_grouped_nb() . Usage >>> group_by = pd . Series ([ 'first' , 'first' , 'second' ], name = 'group' ) >>> mean_squeeze_nb = njit ( lambda i , group , a : np . nanmean ( a )) >>> df . vbt . squeeze_grouped ( mean_squeeze_nb , group_by = group_by ) group first second 2020-01-01 3.0 1.0 2020-01-02 3.0 2.0 2020-01-03 3.0 3.0 2020-01-04 3.0 2.0 2020-01-05 3.0 1.0","title":"squeeze_grouped()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericDFAccessor.ts_heatmap","text":"GenericDFAccessor . ts_heatmap ( is_y_category = True , ** kwargs ) Heatmap of time-series data.","title":"ts_heatmap()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericSRAccessor","text":"Accessor on top of data of any type. For Series only. Accessible through pd.Series.vbt . Superclasses AttrResolver BaseAccessor BaseSRAccessor Configured Documented GenericAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.bshift() GenericAccessor.config GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.df_accessor_cls GenericAccessor.diff() GenericAccessor.drawdown() GenericAccessor.drawdowns GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.fshift() GenericAccessor.get_drawdowns() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.iloc GenericAccessor.indexing_kwargs GenericAccessor.lineplot() GenericAccessor.loc GenericAccessor.mapping GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.obj GenericAccessor.pct_change() GenericAccessor.plot() GenericAccessor.plots_defaults GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.ranges GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.resolve_self() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.self_aliases GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.sr_accessor_cls GenericAccessor.stats_defaults GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.wrapper GenericAccessor.writeable_attrs GenericAccessor.zscore() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses ReturnsSRAccessor SignalsSRAccessor Vbt_SRAccessor","title":"GenericSRAccessor"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericSRAccessor.flatten_grouped","text":"GenericSRAccessor . flatten_grouped ( group_by = None , order = 'C' , wrap_kwargs = None ) Flatten each group of elements. Based on GenericDFAccessor.flatten_grouped() .","title":"flatten_grouped()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericSRAccessor.heatmap","text":"GenericSRAccessor . heatmap ( x_level = None , y_level = None , symmetric = False , sort = True , x_labels = None , y_labels = None , slider_level = None , active = 0 , slider_labels = None , return_fig = True , fig = None , ** kwargs ) Create a heatmap figure based on object's multi-index and values. If index is not a multi-index, converts Series into a DataFrame and calls GenericDFAccessor.heatmap() . If multi-index contains more than two levels or you want them in specific order, pass x_level and y_level , each ( int if index or str if name) corresponding to an axis of the heatmap. Optionally, pass slider_level to use a level as a slider. Creates Heatmap and returns the figure. Usage >>> multi_index = pd . MultiIndex . from_tuples ([ ... ( 1 , 1 ), ... ( 2 , 2 ), ... ( 3 , 3 ) ... ]) >>> sr = pd . Series ( np . arange ( len ( multi_index )), index = multi_index ) >>> sr 1 1 0 2 2 1 3 3 2 dtype: int64 >>> sr . vbt . heatmap () Using one level as a slider: >>> multi_index = pd . MultiIndex . from_tuples ([ ... ( 1 , 1 , 1 ), ... ( 1 , 2 , 2 ), ... ( 1 , 3 , 3 ), ... ( 2 , 3 , 3 ), ... ( 2 , 2 , 2 ), ... ( 2 , 1 , 1 ) ... ]) >>> sr = pd . Series ( np . arange ( len ( multi_index )), index = multi_index ) >>> sr 1 1 1 0 2 2 1 3 3 2 2 3 3 3 2 2 4 1 1 5 dtype: int64 >>> sr . vbt . heatmap ( slider_level = 0 )","title":"heatmap()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericSRAccessor.overlay_with_heatmap","text":"GenericSRAccessor . overlay_with_heatmap ( other , trace_kwargs = None , heatmap_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot Series as a line and overlays it with a heatmap. Args other :\u2002 array_like Second array. Will broadcast. trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter . heatmap_kwargs :\u2002 dict Keyword arguments passed to GenericDFAccessor.heatmap() . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> df [ 'a' ] . vbt . overlay_with_heatmap ( df [ 'b' ])","title":"overlay_with_heatmap()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericSRAccessor.plot_against","text":"GenericSRAccessor . plot_against ( other , trace_kwargs = None , other_trace_kwargs = None , pos_trace_kwargs = None , neg_trace_kwargs = None , hidden_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot Series as a line against another line. Args other :\u2002 array_like Second array. Will broadcast. trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter . other_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for other . Set to 'hidden' to hide. pos_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for positive line. neg_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for negative line. hidden_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for hidden lines. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> df [ 'a' ] . vbt . plot_against ( df [ 'b' ])","title":"plot_against()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericSRAccessor.qqplot","text":"GenericSRAccessor . qqplot ( sparams = (), dist = 'norm' , plot_line = True , line_shape_kwargs = None , xref = 'x' , yref = 'y' , fig = None , ** kwargs ) Plot probability plot using scipy.stats.probplot . **kwargs are passed to GenericAccessor.scatterplot() . Usage >>> pd . Series ( np . random . standard_normal ( 100 )) . vbt . qqplot ()","title":"qqplot()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericSRAccessor.squeeze_grouped","text":"GenericSRAccessor . squeeze_grouped ( squeeze_func_nb , * args , group_by = None , wrap_kwargs = None ) Squeeze each group of elements into a single element. Based on GenericDFAccessor.squeeze_grouped() .","title":"squeeze_grouped()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericSRAccessor.ts_heatmap","text":"GenericSRAccessor . ts_heatmap ( ** kwargs ) Heatmap of time-series data.","title":"ts_heatmap()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.GenericSRAccessor.volume","text":"GenericSRAccessor . volume ( x_level = None , y_level = None , z_level = None , x_labels = None , y_labels = None , z_labels = None , slider_level = None , slider_labels = None , active = 0 , scene_name = 'scene' , fillna = None , fig = None , return_fig = True , ** kwargs ) Create a 3D volume figure based on object's multi-index and values. If multi-index contains more than three levels or you want them in specific order, pass x_level , y_level , and z_level , each ( int if index or str if name) corresponding to an axis of the volume. Optionally, pass slider_level to use a level as a slider. Creates Volume and returns the figure. Usage >>> multi_index = pd . MultiIndex . from_tuples ([ ... ( 1 , 1 , 1 ), ... ( 2 , 2 , 2 ), ... ( 3 , 3 , 3 ) ... ]) >>> sr = pd . Series ( np . arange ( len ( multi_index )), index = multi_index ) >>> sr 1 1 1 0 2 2 2 1 3 3 3 2 dtype: int64 >>> sr . vbt . volume () . show ()","title":"volume()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.MetaGenericAccessor","text":"Meta class that exposes a read-only class property StatsBuilderMixin.metrics . Superclasses MetaPlotsBuilderMixin MetaStatsBuilderMixin builtins.type Inherited members MetaPlotsBuilderMixin.subplots MetaStatsBuilderMixin.metrics","title":"MetaGenericAccessor"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.TransformerT","text":"Base class for protocol classes. Protocol classes are defined as:: class Proto(Protocol): def meth(self) -> int: ... Such classes are primarily used with static type checkers that recognize structural subtyping (static duck-typing), for example:: class C def meth(self) -> int: return 0 def func(x: Proto) -> int: return x.meth() func(C()) # Passes static type check See PEP 544 for details. Protocol classes decorated with @typing_extensions .runtime act as simple-minded runtime protocol that checks only the presence of given attributes, ignoring their type signatures. Protocol classes can be generic, they are defined as:: class GenProto(Protocol[T]): def meth(self) -> T: ... Superclasses typing_extensions.Protocol","title":"TransformerT"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.TransformerT.fit_transform","text":"TransformerT . fit_transform ( * args , ** kwargs )","title":"fit_transform()"},{"location":"api/generic/accessors/#vectorbt.generic.accessors.TransformerT.transform","text":"TransformerT . transform ( * args , ** kwargs )","title":"transform()"},{"location":"api/generic/decorators/","text":"decorators module \u00b6 Class and function decorators. attach_nb_methods function \u00b6 attach_nb_methods ( config ) Class decorator to add Numba methods. config should contain target method names (keys) and dictionaries (values) with the following keys: func : Function that should be wrapped. The first argument should expect a 2-dim array. is_reducing : Whether the function is reducing. Defaults to False. path : Path to the function for documentation. Defaults to func.__name__ . replace_signature : Whether to replace the target signature with the source signature. Defaults to True. wrap_kwargs : Default keyword arguments for wrapping. Will be merged with the dict supplied by the user. Defaults to dict(name_or_index=target_name) for reducing functions. The class should be a subclass of Wrapping . attach_transform_methods function \u00b6 attach_transform_methods ( config ) Class decorator to add transformation methods. config should contain target method names (keys) and dictionaries (values) with the following keys: transformer : Transformer class/object. docstring : Method docstring. Defaults to \"See {transformer}.__name__ .\". replace_signature : Whether to replace the target signature. Defaults to True. The class should be a subclass of GenericAccessor .","title":"decorators"},{"location":"api/generic/decorators/#vectorbt.generic.decorators","text":"Class and function decorators.","title":"vectorbt.generic.decorators"},{"location":"api/generic/decorators/#vectorbt.generic.decorators.attach_nb_methods","text":"attach_nb_methods ( config ) Class decorator to add Numba methods. config should contain target method names (keys) and dictionaries (values) with the following keys: func : Function that should be wrapped. The first argument should expect a 2-dim array. is_reducing : Whether the function is reducing. Defaults to False. path : Path to the function for documentation. Defaults to func.__name__ . replace_signature : Whether to replace the target signature with the source signature. Defaults to True. wrap_kwargs : Default keyword arguments for wrapping. Will be merged with the dict supplied by the user. Defaults to dict(name_or_index=target_name) for reducing functions. The class should be a subclass of Wrapping .","title":"attach_nb_methods()"},{"location":"api/generic/decorators/#vectorbt.generic.decorators.attach_transform_methods","text":"attach_transform_methods ( config ) Class decorator to add transformation methods. config should contain target method names (keys) and dictionaries (values) with the following keys: transformer : Transformer class/object. docstring : Method docstring. Defaults to \"See {transformer}.__name__ .\". replace_signature : Whether to replace the target signature. Defaults to True. The class should be a subclass of GenericAccessor .","title":"attach_transform_methods()"},{"location":"api/generic/drawdowns/","text":"drawdowns module \u00b6 Base class for working with drawdown records. Drawdown records capture information on drawdowns. Since drawdowns are ranges, they subclass Ranges . Warning Drawdowns return both recovered AND active drawdowns, which may skew your performance results. To only consider recovered drawdowns, you should explicitly query recovered attribute. Using Drawdowns.from_ts() , you can generate drawdown records for any time series and analyze them right away. >>> import vectorbt as vbt >>> import numpy as np >>> import pandas as pd >>> start = '2019-10-01 UTC' # crypto is in UTC >>> end = '2020-01-01 UTC' >>> price = vbt . YFData . download ( 'BTC-USD' , start = start , end = end ) . get ( 'Close' ) >>> price = price . rename ( None ) >>> drawdowns = vbt . Drawdowns . from_ts ( price , wrapper_kwargs = dict ( freq = 'd' )) >>> drawdowns . records_readable Drawdown Id Column Peak Timestamp Start Timestamp \\ 0 0 0 2019-10-02 00:00:00+00:00 2019-10-03 00:00:00+00:00 1 1 0 2019-10-09 00:00:00+00:00 2019-10-10 00:00:00+00:00 2 2 0 2019-10-27 00:00:00+00:00 2019-10-28 00:00:00+00:00 Valley Timestamp End Timestamp Peak Value \\ 0 2019-10-06 00:00:00+00:00 2019-10-09 00:00:00+00:00 8393.041992 1 2019-10-24 00:00:00+00:00 2019-10-25 00:00:00+00:00 8595.740234 2 2019-12-17 00:00:00+00:00 2020-01-01 00:00:00+00:00 9551.714844 Valley Value End Value Status 0 7988.155762 8595.740234 Recovered 1 7493.488770 8660.700195 Recovered 2 6640.515137 7200.174316 Active >>> drawdowns . duration . max ( wrap_kwargs = dict ( to_timedelta = True )) Timedelta('66 days 00:00:00') From accessors \u00b6 Moreover, all generic accessors have a property drawdowns and a method get_drawdowns : >>> # vectorbt.generic.accessors.GenericAccessor.drawdowns.coverage >>> price . vbt . drawdowns . coverage () 0.9354838709677419 Stats \u00b6 Hint See StatsBuilderMixin.stats() and Drawdowns.metrics . >>> df = pd . DataFrame ({ ... 'a' : [ 1 , 2 , 1 , 3 , 2 ], ... 'b' : [ 2 , 3 , 1 , 2 , 1 ] ... }) >>> drawdowns = df . vbt ( freq = 'd' ) . drawdowns >>> drawdowns [ 'a' ] . stats () Start 0 End 4 Period 5 days 00:00:00 Coverage [%] 40.0 Total Records 2 Total Recovered Drawdowns 1 Total Active Drawdowns 1 Active Drawdown [%] 33.333333 Active Duration 1 days 00:00:00 Active Recovery [%] 0.0 Active Recovery Return [%] 0.0 Active Recovery Duration 0 days 00:00:00 Max Drawdown [%] 50.0 Avg Drawdown [%] 50.0 Max Drawdown Duration 1 days 00:00:00 Avg Drawdown Duration 1 days 00:00:00 Max Recovery Return [%] 200.0 Avg Recovery Return [%] 200.0 Max Recovery Duration 1 days 00:00:00 Avg Recovery Duration 1 days 00:00:00 Avg Recovery Duration Ratio 1.0 Name: a, dtype: object By default, the metrics max_dd , avg_dd , max_dd_duration , and avg_dd_duration do not include active drawdowns. To change that, pass incl_active=True : >>> drawdowns [ 'a' ] . stats ( settings = dict ( incl_active = True )) Start 0 End 4 Period 5 days 00:00:00 Coverage [%] 40.0 Total Records 2 Total Recovered Drawdowns 1 Total Active Drawdowns 1 Active Drawdown [%] 33.333333 Active Duration 1 days 00:00:00 Active Recovery [%] 0.0 Active Recovery Return [%] 0.0 Active Recovery Duration 0 days 00:00:00 Max Drawdown [%] 50.0 Avg Drawdown [%] 41.666667 Max Drawdown Duration 1 days 00:00:00 Avg Drawdown Duration 1 days 00:00:00 Max Recovery Return [%] 200.0 Avg Recovery Return [%] 200.0 Max Recovery Duration 1 days 00:00:00 Avg Recovery Duration 1 days 00:00:00 Avg Recovery Duration Ratio 1.0 Name: a, dtype: object StatsBuilderMixin.stats() also supports (re-)grouping: >>> drawdowns [ 'a' ] . stats ( group_by = True ) UserWarning: Metric 'active_dd' does not support grouped data UserWarning: Metric 'active_duration' does not support grouped data UserWarning: Metric 'active_recovery' does not support grouped data UserWarning: Metric 'active_recovery_return' does not support grouped data UserWarning: Metric 'active_recovery_duration' does not support grouped data Start 0 End 4 Period 5 days 00:00:00 Coverage [%] 40.0 Total Records 2 Total Recovered Drawdowns 1 Total Active Drawdowns 1 Max Drawdown [%] 50.0 Avg Drawdown [%] 50.0 Max Drawdown Duration 1 days 00:00:00 Avg Drawdown Duration 1 days 00:00:00 Max Recovery Return [%] 200.0 Avg Recovery Return [%] 200.0 Max Recovery Duration 1 days 00:00:00 Avg Recovery Duration 1 days 00:00:00 Avg Recovery Duration Ratio 1.0 Name: group, dtype: object Plots \u00b6 Hint See PlotsBuilderMixin.plots() and Drawdowns.subplots . Drawdowns class has a single subplot based on Drawdowns.plot() : >>> drawdowns [ 'a' ] . plots () dd_attach_field_config Config \u00b6 Config of fields to be attached to Drawdowns . Co nf ig( { \"status\" : { \"attach_filters\" : true } } ) dd_field_config Config \u00b6 Field config for Drawdowns . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"col\" : \"int64\" , \"peak_idx\" : \"int64\" , \"start_idx\" : \"int64\" , \"valley_idx\" : \"int64\" , \"end_idx\" : \"int64\" , \"peak_val\" : \"float64\" , \"valley_val\" : \"float64\" , \"end_val\" : \"float64\" , \"status\" : \"int64\" }, \"settings\" : { \"id\" : { \"title\" : \"Drawdown Id\" }, \"peak_idx\" : { \"title\" : \"Peak Timestamp\" , \"mapping\" : \"index\" }, \"valley_idx\" : { \"title\" : \"Valley Timestamp\" , \"mapping\" : \"index\" }, \"peak_val\" : { \"title\" : \"Peak Value\" }, \"valley_val\" : { \"title\" : \"Valley Value\" }, \"end_val\" : { \"title\" : \"End Value\" }, \"status\" : { \"mapping\" : { \"Active\" : 0 , \"Recovered\" : 1 } } } } ) Drawdowns class \u00b6 Extends Ranges for working with drawdown records. Requires records_arr to have all fields defined in drawdown_dt . Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin Ranges Records RecordsWithFields StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() Ranges.avg_duration() Ranges.closed Ranges.col Ranges.col_arr Ranges.col_mapper Ranges.config Ranges.coverage() Ranges.duration Ranges.end_idx Ranges.id Ranges.id_arr Ranges.idx_arr Ranges.iloc Ranges.indexing_kwargs Ranges.loc Ranges.max_duration() Ranges.open Ranges.records Ranges.records_arr Ranges.records_readable Ranges.self_aliases Ranges.start_idx Ranges.status Ranges.to_mask() Ranges.ts Ranges.values Ranges.wrapper Ranges.writeable_attrs Records.apply() Records.apply_mask() Records.build_field_config_doc() Records.count() Records.get_apply_mapping_arr() Records.get_by_col_idxs() Records.get_field_arr() Records.get_field_mapping() Records.get_field_name() Records.get_field_setting() Records.get_field_title() Records.get_map_field() Records.get_map_field_to_index() Records.indexing_func_meta() Records.is_sorted() Records.map() Records.map_array() Records.map_field() Records.override_field_config_doc() Records.replace() Records.sort() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() active method \u00b6 Records filtered by status == 0 . active_drawdown method \u00b6 Drawdowns . active_drawdown ( group_by = None , wrap_kwargs = None ) Drawdown of the last active drawdown only. Does not support grouping. active_duration method \u00b6 Drawdowns . active_duration ( group_by = None , wrap_kwargs = None , ** kwargs ) Duration of the last active drawdown only. Does not support grouping. active_recovery method \u00b6 Drawdowns . active_recovery ( group_by = None , wrap_kwargs = None ) Recovery of the last active drawdown only. Does not support grouping. active_recovery_duration method \u00b6 Drawdowns . active_recovery_duration ( group_by = None , wrap_kwargs = None , ** kwargs ) Recovery duration of the last active drawdown only. Does not support grouping. active_recovery_return method \u00b6 Drawdowns . active_recovery_return ( group_by = None , wrap_kwargs = None , ** kwargs ) Recovery return of the last active drawdown only. Does not support grouping. avg_drawdown method \u00b6 Drawdowns . avg_drawdown ( group_by = None , wrap_kwargs = None , ** kwargs ) Average drawdown (ADD). Based on Drawdowns.drawdown . avg_recovery_return method \u00b6 Drawdowns . avg_recovery_return ( group_by = None , wrap_kwargs = None , ** kwargs ) Average recovery return. Based on Drawdowns.recovery_return . decline_duration method \u00b6 See dd_decline_duration_nb() . Takes into account both recovered and active drawdowns. drawdown method \u00b6 See dd_drawdown_nb() . Takes into account both recovered and active drawdowns. end_val method \u00b6 Mapped array of the field end_val . field_config class variable \u00b6 Field config of Drawdowns . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"col\" : \"int64\" , \"peak_idx\" : \"int64\" , \"start_idx\" : \"int64\" , \"valley_idx\" : \"int64\" , \"end_idx\" : \"int64\" , \"peak_val\" : \"float64\" , \"valley_val\" : \"float64\" , \"end_val\" : \"float64\" , \"status\" : \"int64\" }, \"settings\" : { \"id\" : { \"name\" : \"id\" , \"title\" : \"Drawdown Id\" }, \"col\" : { \"name\" : \"col\" , \"title\" : \"Column\" , \"mapping\" : \"columns\" }, \"idx\" : { \"name\" : \"end_idx\" , \"title\" : \"Timestamp\" , \"mapping\" : \"index\" }, \"start_idx\" : { \"title\" : \"Start Timestamp\" , \"mapping\" : \"index\" }, \"end_idx\" : { \"title\" : \"End Timestamp\" , \"mapping\" : \"index\" }, \"status\" : { \"title\" : \"Status\" , \"mapping\" : { \"Active\" : 0 , \"Recovered\" : 1 } }, \"peak_idx\" : { \"title\" : \"Peak Timestamp\" , \"mapping\" : \"index\" }, \"valley_idx\" : { \"title\" : \"Valley Timestamp\" , \"mapping\" : \"index\" }, \"peak_val\" : { \"title\" : \"Peak Value\" }, \"valley_val\" : { \"title\" : \"Valley Value\" }, \"end_val\" : { \"title\" : \"End Value\" } } } ) from_ts class method \u00b6 Drawdowns . from_ts ( ts , attach_ts = True , wrapper_kwargs = None , ** kwargs ) Build Drawdowns from time series ts . **kwargs will be passed to Drawdowns . indexing_func method \u00b6 Drawdowns . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on Drawdowns . max_drawdown method \u00b6 Drawdowns . max_drawdown ( group_by = None , wrap_kwargs = None , ** kwargs ) Maximum drawdown (MDD). Based on Drawdowns.drawdown . max_recovery_return method \u00b6 Drawdowns . max_recovery_return ( group_by = None , wrap_kwargs = None , ** kwargs ) Maximum recovery return. Based on Drawdowns.recovery_return . metrics class variable \u00b6 Metrics supported by Drawdowns . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881d4e18>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881d4ea0>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881d4f28>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"coverage\" : { \"title\" : \"Coverage [%]\" , \"calc_func\" : \"coverage\" , \"post_calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881c9048>\" , \"tags\" : [ \"ranges\" , \"duration\" ] }, \"total_records\" : { \"title\" : \"Total Records\" , \"calc_func\" : \"count\" , \"tags\" : \"records\" }, \"total_recovered\" : { \"title\" : \"Total Recovered Drawdowns\" , \"calc_func\" : \"recovered.count\" , \"tags\" : \"drawdowns\" }, \"total_active\" : { \"title\" : \"Total Active Drawdowns\" , \"calc_func\" : \"active.count\" , \"tags\" : \"drawdowns\" }, \"active_dd\" : { \"title\" : \"Active Drawdown [%]\" , \"calc_func\" : \"active_drawdown\" , \"post_calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881c90d0>\" , \"check_is_not_grouped\" : true , \"tags\" : [ \"drawdowns\" , \"active\" ] }, \"active_duration\" : { \"title\" : \"Active Duration\" , \"calc_func\" : \"active_duration\" , \"fill_wrap_kwargs\" : true , \"check_is_not_grouped\" : true , \"tags\" : [ \"drawdowns\" , \"active\" , \"duration\" ] }, \"active_recovery\" : { \"title\" : \"Active Recovery [%]\" , \"calc_func\" : \"active_recovery\" , \"post_calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881c9158>\" , \"check_is_not_grouped\" : true , \"tags\" : [ \"drawdowns\" , \"active\" ] }, \"active_recovery_return\" : { \"title\" : \"Active Recovery Return [%]\" , \"calc_func\" : \"active_recovery_return\" , \"post_calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881c91e0>\" , \"check_is_not_grouped\" : true , \"tags\" : [ \"drawdowns\" , \"active\" ] }, \"active_recovery_duration\" : { \"title\" : \"Active Recovery Duration\" , \"calc_func\" : \"active_recovery_duration\" , \"fill_wrap_kwargs\" : true , \"check_is_not_grouped\" : true , \"tags\" : [ \"drawdowns\" , \"active\" , \"duration\" ] }, \"max_dd\" : { \"title\" : \"Max Drawdown [%]\" , \"calc_func\" : \"RepEval(expression=\\\"'max_drawdown' if incl_active else 'recovered.max_drawdown'\\\", mapping={})\" , \"post_calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881c9268>\" , \"tags\" : \"RepEval(expression=\\\"['drawdowns'] if incl_active else ['drawdowns', 'recovered']\\\", mapping={})\" }, \"avg_dd\" : { \"title\" : \"Avg Drawdown [%]\" , \"calc_func\" : \"RepEval(expression=\\\"'avg_drawdown' if incl_active else 'recovered.avg_drawdown'\\\", mapping={})\" , \"post_calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881c92f0>\" , \"tags\" : \"RepEval(expression=\\\"['drawdowns'] if incl_active else ['drawdowns', 'recovered']\\\", mapping={})\" }, \"max_dd_duration\" : { \"title\" : \"Max Drawdown Duration\" , \"calc_func\" : \"RepEval(expression=\\\"'max_duration' if incl_active else 'recovered.max_duration'\\\", mapping={})\" , \"fill_wrap_kwargs\" : true , \"tags\" : \"RepEval(expression=\\\"['drawdowns', 'duration'] if incl_active else ['drawdowns', 'recovered', 'duration']\\\", mapping={})\" }, \"avg_dd_duration\" : { \"title\" : \"Avg Drawdown Duration\" , \"calc_func\" : \"RepEval(expression=\\\"'avg_duration' if incl_active else 'recovered.avg_duration'\\\", mapping={})\" , \"fill_wrap_kwargs\" : true , \"tags\" : \"RepEval(expression=\\\"['drawdowns', 'duration'] if incl_active else ['drawdowns', 'recovered', 'duration']\\\", mapping={})\" }, \"max_return\" : { \"title\" : \"Max Recovery Return [%]\" , \"calc_func\" : \"recovered.recovery_return.max\" , \"post_calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881c9378>\" , \"tags\" : [ \"drawdowns\" , \"recovered\" ] }, \"avg_return\" : { \"title\" : \"Avg Recovery Return [%]\" , \"calc_func\" : \"recovered.recovery_return.mean\" , \"post_calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881c9400>\" , \"tags\" : [ \"drawdowns\" , \"recovered\" ] }, \"max_recovery_duration\" : { \"title\" : \"Max Recovery Duration\" , \"calc_func\" : \"recovered.recovery_duration.max\" , \"apply_to_timedelta\" : true , \"tags\" : [ \"drawdowns\" , \"recovered\" , \"duration\" ] }, \"avg_recovery_duration\" : { \"title\" : \"Avg Recovery Duration\" , \"calc_func\" : \"recovered.recovery_duration.mean\" , \"apply_to_timedelta\" : true , \"tags\" : [ \"drawdowns\" , \"recovered\" , \"duration\" ] }, \"recovery_duration_ratio\" : { \"title\" : \"Avg Recovery Duration Ratio\" , \"calc_func\" : \"recovered.recovery_duration_ratio.mean\" , \"tags\" : [ \"drawdowns\" , \"recovered\" ] } } ) Returns Drawdowns._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable Drawdowns._metrics . peak_idx method \u00b6 Mapped array of the field peak_idx . peak_val method \u00b6 Mapped array of the field peak_val . plot method \u00b6 Drawdowns . plot ( column = None , top_n = 5 , plot_zones = True , ts_trace_kwargs = None , peak_trace_kwargs = None , valley_trace_kwargs = None , recovery_trace_kwargs = None , active_trace_kwargs = None , decline_shape_kwargs = None , recovery_shape_kwargs = None , active_shape_kwargs = None , add_trace_kwargs = None , xref = 'x' , yref = 'y' , fig = None , ** layout_kwargs ) Plot drawdowns. Args column :\u2002 str Name of the column to plot. top_n :\u2002 int Filter top N drawdown records by maximum drawdown. plot_zones :\u2002 bool Whether to plot zones. ts_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for Drawdowns.ts . peak_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for peak values. valley_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for valley values. recovery_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for recovery values. active_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for active recovery values. decline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for decline zones. recovery_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for recovery zones. active_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for active recovery zones. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import vectorbt as vbt >>> from datetime import datetime , timedelta >>> import pandas as pd >>> price = pd . Series ([ 1 , 2 , 1 , 2 , 3 , 2 , 1 , 2 ], name = 'Price' ) >>> price . index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( len ( price ))] >>> vbt . Drawdowns . from_ts ( price , wrapper_kwargs = dict ( freq = '1 day' )) . plot () plots_defaults property \u00b6 Defaults for PlotsBuilderMixin.plots() . Merges Ranges.plots_defaults and drawdowns.plots from settings . recovered method \u00b6 Records filtered by status == 1 . recovery_duration method \u00b6 See dd_recovery_duration_nb() . A value higher than 1 means the recovery was slower than the decline. Takes into account both recovered and active drawdowns. recovery_duration_ratio method \u00b6 See dd_recovery_duration_ratio_nb() . Takes into account both recovered and active drawdowns. recovery_return method \u00b6 See dd_recovery_return_nb() . Takes into account both recovered and active drawdowns. stats_defaults property \u00b6 Defaults for StatsBuilderMixin.stats() . Merges Ranges.stats_defaults and drawdowns.stats from settings . subplots class variable \u00b6 Subplots supported by Drawdowns . Co nf ig( { \"plot\" : { \"title\" : \"Drawdowns\" , \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"tags\" : \"drawdowns\" } } ) Returns Drawdowns._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable Drawdowns._subplots . valley_idx method \u00b6 Mapped array of the field valley_idx . valley_val method \u00b6 Mapped array of the field valley_val .","title":"drawdowns"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns","text":"Base class for working with drawdown records. Drawdown records capture information on drawdowns. Since drawdowns are ranges, they subclass Ranges . Warning Drawdowns return both recovered AND active drawdowns, which may skew your performance results. To only consider recovered drawdowns, you should explicitly query recovered attribute. Using Drawdowns.from_ts() , you can generate drawdown records for any time series and analyze them right away. >>> import vectorbt as vbt >>> import numpy as np >>> import pandas as pd >>> start = '2019-10-01 UTC' # crypto is in UTC >>> end = '2020-01-01 UTC' >>> price = vbt . YFData . download ( 'BTC-USD' , start = start , end = end ) . get ( 'Close' ) >>> price = price . rename ( None ) >>> drawdowns = vbt . Drawdowns . from_ts ( price , wrapper_kwargs = dict ( freq = 'd' )) >>> drawdowns . records_readable Drawdown Id Column Peak Timestamp Start Timestamp \\ 0 0 0 2019-10-02 00:00:00+00:00 2019-10-03 00:00:00+00:00 1 1 0 2019-10-09 00:00:00+00:00 2019-10-10 00:00:00+00:00 2 2 0 2019-10-27 00:00:00+00:00 2019-10-28 00:00:00+00:00 Valley Timestamp End Timestamp Peak Value \\ 0 2019-10-06 00:00:00+00:00 2019-10-09 00:00:00+00:00 8393.041992 1 2019-10-24 00:00:00+00:00 2019-10-25 00:00:00+00:00 8595.740234 2 2019-12-17 00:00:00+00:00 2020-01-01 00:00:00+00:00 9551.714844 Valley Value End Value Status 0 7988.155762 8595.740234 Recovered 1 7493.488770 8660.700195 Recovered 2 6640.515137 7200.174316 Active >>> drawdowns . duration . max ( wrap_kwargs = dict ( to_timedelta = True )) Timedelta('66 days 00:00:00')","title":"vectorbt.generic.drawdowns"},{"location":"api/generic/drawdowns/#from-accessors","text":"Moreover, all generic accessors have a property drawdowns and a method get_drawdowns : >>> # vectorbt.generic.accessors.GenericAccessor.drawdowns.coverage >>> price . vbt . drawdowns . coverage () 0.9354838709677419","title":"From accessors"},{"location":"api/generic/drawdowns/#stats","text":"Hint See StatsBuilderMixin.stats() and Drawdowns.metrics . >>> df = pd . DataFrame ({ ... 'a' : [ 1 , 2 , 1 , 3 , 2 ], ... 'b' : [ 2 , 3 , 1 , 2 , 1 ] ... }) >>> drawdowns = df . vbt ( freq = 'd' ) . drawdowns >>> drawdowns [ 'a' ] . stats () Start 0 End 4 Period 5 days 00:00:00 Coverage [%] 40.0 Total Records 2 Total Recovered Drawdowns 1 Total Active Drawdowns 1 Active Drawdown [%] 33.333333 Active Duration 1 days 00:00:00 Active Recovery [%] 0.0 Active Recovery Return [%] 0.0 Active Recovery Duration 0 days 00:00:00 Max Drawdown [%] 50.0 Avg Drawdown [%] 50.0 Max Drawdown Duration 1 days 00:00:00 Avg Drawdown Duration 1 days 00:00:00 Max Recovery Return [%] 200.0 Avg Recovery Return [%] 200.0 Max Recovery Duration 1 days 00:00:00 Avg Recovery Duration 1 days 00:00:00 Avg Recovery Duration Ratio 1.0 Name: a, dtype: object By default, the metrics max_dd , avg_dd , max_dd_duration , and avg_dd_duration do not include active drawdowns. To change that, pass incl_active=True : >>> drawdowns [ 'a' ] . stats ( settings = dict ( incl_active = True )) Start 0 End 4 Period 5 days 00:00:00 Coverage [%] 40.0 Total Records 2 Total Recovered Drawdowns 1 Total Active Drawdowns 1 Active Drawdown [%] 33.333333 Active Duration 1 days 00:00:00 Active Recovery [%] 0.0 Active Recovery Return [%] 0.0 Active Recovery Duration 0 days 00:00:00 Max Drawdown [%] 50.0 Avg Drawdown [%] 41.666667 Max Drawdown Duration 1 days 00:00:00 Avg Drawdown Duration 1 days 00:00:00 Max Recovery Return [%] 200.0 Avg Recovery Return [%] 200.0 Max Recovery Duration 1 days 00:00:00 Avg Recovery Duration 1 days 00:00:00 Avg Recovery Duration Ratio 1.0 Name: a, dtype: object StatsBuilderMixin.stats() also supports (re-)grouping: >>> drawdowns [ 'a' ] . stats ( group_by = True ) UserWarning: Metric 'active_dd' does not support grouped data UserWarning: Metric 'active_duration' does not support grouped data UserWarning: Metric 'active_recovery' does not support grouped data UserWarning: Metric 'active_recovery_return' does not support grouped data UserWarning: Metric 'active_recovery_duration' does not support grouped data Start 0 End 4 Period 5 days 00:00:00 Coverage [%] 40.0 Total Records 2 Total Recovered Drawdowns 1 Total Active Drawdowns 1 Max Drawdown [%] 50.0 Avg Drawdown [%] 50.0 Max Drawdown Duration 1 days 00:00:00 Avg Drawdown Duration 1 days 00:00:00 Max Recovery Return [%] 200.0 Avg Recovery Return [%] 200.0 Max Recovery Duration 1 days 00:00:00 Avg Recovery Duration 1 days 00:00:00 Avg Recovery Duration Ratio 1.0 Name: group, dtype: object","title":"Stats"},{"location":"api/generic/drawdowns/#plots","text":"Hint See PlotsBuilderMixin.plots() and Drawdowns.subplots . Drawdowns class has a single subplot based on Drawdowns.plot() : >>> drawdowns [ 'a' ] . plots ()","title":"Plots"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.dd_attach_field_config","text":"Config of fields to be attached to Drawdowns . Co nf ig( { \"status\" : { \"attach_filters\" : true } } )","title":"dd_attach_field_config"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.dd_field_config","text":"Field config for Drawdowns . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"col\" : \"int64\" , \"peak_idx\" : \"int64\" , \"start_idx\" : \"int64\" , \"valley_idx\" : \"int64\" , \"end_idx\" : \"int64\" , \"peak_val\" : \"float64\" , \"valley_val\" : \"float64\" , \"end_val\" : \"float64\" , \"status\" : \"int64\" }, \"settings\" : { \"id\" : { \"title\" : \"Drawdown Id\" }, \"peak_idx\" : { \"title\" : \"Peak Timestamp\" , \"mapping\" : \"index\" }, \"valley_idx\" : { \"title\" : \"Valley Timestamp\" , \"mapping\" : \"index\" }, \"peak_val\" : { \"title\" : \"Peak Value\" }, \"valley_val\" : { \"title\" : \"Valley Value\" }, \"end_val\" : { \"title\" : \"End Value\" }, \"status\" : { \"mapping\" : { \"Active\" : 0 , \"Recovered\" : 1 } } } } )","title":"dd_field_config"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns","text":"Extends Ranges for working with drawdown records. Requires records_arr to have all fields defined in drawdown_dt . Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin Ranges Records RecordsWithFields StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() Ranges.avg_duration() Ranges.closed Ranges.col Ranges.col_arr Ranges.col_mapper Ranges.config Ranges.coverage() Ranges.duration Ranges.end_idx Ranges.id Ranges.id_arr Ranges.idx_arr Ranges.iloc Ranges.indexing_kwargs Ranges.loc Ranges.max_duration() Ranges.open Ranges.records Ranges.records_arr Ranges.records_readable Ranges.self_aliases Ranges.start_idx Ranges.status Ranges.to_mask() Ranges.ts Ranges.values Ranges.wrapper Ranges.writeable_attrs Records.apply() Records.apply_mask() Records.build_field_config_doc() Records.count() Records.get_apply_mapping_arr() Records.get_by_col_idxs() Records.get_field_arr() Records.get_field_mapping() Records.get_field_name() Records.get_field_setting() Records.get_field_title() Records.get_map_field() Records.get_map_field_to_index() Records.indexing_func_meta() Records.is_sorted() Records.map() Records.map_array() Records.map_field() Records.override_field_config_doc() Records.replace() Records.sort() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"Drawdowns"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.active","text":"Records filtered by status == 0 .","title":"active"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.active_drawdown","text":"Drawdowns . active_drawdown ( group_by = None , wrap_kwargs = None ) Drawdown of the last active drawdown only. Does not support grouping.","title":"active_drawdown()"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.active_duration","text":"Drawdowns . active_duration ( group_by = None , wrap_kwargs = None , ** kwargs ) Duration of the last active drawdown only. Does not support grouping.","title":"active_duration()"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.active_recovery","text":"Drawdowns . active_recovery ( group_by = None , wrap_kwargs = None ) Recovery of the last active drawdown only. Does not support grouping.","title":"active_recovery()"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.active_recovery_duration","text":"Drawdowns . active_recovery_duration ( group_by = None , wrap_kwargs = None , ** kwargs ) Recovery duration of the last active drawdown only. Does not support grouping.","title":"active_recovery_duration()"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.active_recovery_return","text":"Drawdowns . active_recovery_return ( group_by = None , wrap_kwargs = None , ** kwargs ) Recovery return of the last active drawdown only. Does not support grouping.","title":"active_recovery_return()"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.avg_drawdown","text":"Drawdowns . avg_drawdown ( group_by = None , wrap_kwargs = None , ** kwargs ) Average drawdown (ADD). Based on Drawdowns.drawdown .","title":"avg_drawdown()"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.avg_recovery_return","text":"Drawdowns . avg_recovery_return ( group_by = None , wrap_kwargs = None , ** kwargs ) Average recovery return. Based on Drawdowns.recovery_return .","title":"avg_recovery_return()"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.decline_duration","text":"See dd_decline_duration_nb() . Takes into account both recovered and active drawdowns.","title":"decline_duration"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.drawdown","text":"See dd_drawdown_nb() . Takes into account both recovered and active drawdowns.","title":"drawdown"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.end_val","text":"Mapped array of the field end_val .","title":"end_val"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.field_config","text":"Field config of Drawdowns . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"col\" : \"int64\" , \"peak_idx\" : \"int64\" , \"start_idx\" : \"int64\" , \"valley_idx\" : \"int64\" , \"end_idx\" : \"int64\" , \"peak_val\" : \"float64\" , \"valley_val\" : \"float64\" , \"end_val\" : \"float64\" , \"status\" : \"int64\" }, \"settings\" : { \"id\" : { \"name\" : \"id\" , \"title\" : \"Drawdown Id\" }, \"col\" : { \"name\" : \"col\" , \"title\" : \"Column\" , \"mapping\" : \"columns\" }, \"idx\" : { \"name\" : \"end_idx\" , \"title\" : \"Timestamp\" , \"mapping\" : \"index\" }, \"start_idx\" : { \"title\" : \"Start Timestamp\" , \"mapping\" : \"index\" }, \"end_idx\" : { \"title\" : \"End Timestamp\" , \"mapping\" : \"index\" }, \"status\" : { \"title\" : \"Status\" , \"mapping\" : { \"Active\" : 0 , \"Recovered\" : 1 } }, \"peak_idx\" : { \"title\" : \"Peak Timestamp\" , \"mapping\" : \"index\" }, \"valley_idx\" : { \"title\" : \"Valley Timestamp\" , \"mapping\" : \"index\" }, \"peak_val\" : { \"title\" : \"Peak Value\" }, \"valley_val\" : { \"title\" : \"Valley Value\" }, \"end_val\" : { \"title\" : \"End Value\" } } } )","title":"field_config"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.from_ts","text":"Drawdowns . from_ts ( ts , attach_ts = True , wrapper_kwargs = None , ** kwargs ) Build Drawdowns from time series ts . **kwargs will be passed to Drawdowns .","title":"from_ts()"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.indexing_func","text":"Drawdowns . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on Drawdowns .","title":"indexing_func()"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.max_drawdown","text":"Drawdowns . max_drawdown ( group_by = None , wrap_kwargs = None , ** kwargs ) Maximum drawdown (MDD). Based on Drawdowns.drawdown .","title":"max_drawdown()"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.max_recovery_return","text":"Drawdowns . max_recovery_return ( group_by = None , wrap_kwargs = None , ** kwargs ) Maximum recovery return. Based on Drawdowns.recovery_return .","title":"max_recovery_return()"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.metrics","text":"Metrics supported by Drawdowns . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881d4e18>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881d4ea0>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881d4f28>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"coverage\" : { \"title\" : \"Coverage [%]\" , \"calc_func\" : \"coverage\" , \"post_calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881c9048>\" , \"tags\" : [ \"ranges\" , \"duration\" ] }, \"total_records\" : { \"title\" : \"Total Records\" , \"calc_func\" : \"count\" , \"tags\" : \"records\" }, \"total_recovered\" : { \"title\" : \"Total Recovered Drawdowns\" , \"calc_func\" : \"recovered.count\" , \"tags\" : \"drawdowns\" }, \"total_active\" : { \"title\" : \"Total Active Drawdowns\" , \"calc_func\" : \"active.count\" , \"tags\" : \"drawdowns\" }, \"active_dd\" : { \"title\" : \"Active Drawdown [%]\" , \"calc_func\" : \"active_drawdown\" , \"post_calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881c90d0>\" , \"check_is_not_grouped\" : true , \"tags\" : [ \"drawdowns\" , \"active\" ] }, \"active_duration\" : { \"title\" : \"Active Duration\" , \"calc_func\" : \"active_duration\" , \"fill_wrap_kwargs\" : true , \"check_is_not_grouped\" : true , \"tags\" : [ \"drawdowns\" , \"active\" , \"duration\" ] }, \"active_recovery\" : { \"title\" : \"Active Recovery [%]\" , \"calc_func\" : \"active_recovery\" , \"post_calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881c9158>\" , \"check_is_not_grouped\" : true , \"tags\" : [ \"drawdowns\" , \"active\" ] }, \"active_recovery_return\" : { \"title\" : \"Active Recovery Return [%]\" , \"calc_func\" : \"active_recovery_return\" , \"post_calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881c91e0>\" , \"check_is_not_grouped\" : true , \"tags\" : [ \"drawdowns\" , \"active\" ] }, \"active_recovery_duration\" : { \"title\" : \"Active Recovery Duration\" , \"calc_func\" : \"active_recovery_duration\" , \"fill_wrap_kwargs\" : true , \"check_is_not_grouped\" : true , \"tags\" : [ \"drawdowns\" , \"active\" , \"duration\" ] }, \"max_dd\" : { \"title\" : \"Max Drawdown [%]\" , \"calc_func\" : \"RepEval(expression=\\\"'max_drawdown' if incl_active else 'recovered.max_drawdown'\\\", mapping={})\" , \"post_calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881c9268>\" , \"tags\" : \"RepEval(expression=\\\"['drawdowns'] if incl_active else ['drawdowns', 'recovered']\\\", mapping={})\" }, \"avg_dd\" : { \"title\" : \"Avg Drawdown [%]\" , \"calc_func\" : \"RepEval(expression=\\\"'avg_drawdown' if incl_active else 'recovered.avg_drawdown'\\\", mapping={})\" , \"post_calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881c92f0>\" , \"tags\" : \"RepEval(expression=\\\"['drawdowns'] if incl_active else ['drawdowns', 'recovered']\\\", mapping={})\" }, \"max_dd_duration\" : { \"title\" : \"Max Drawdown Duration\" , \"calc_func\" : \"RepEval(expression=\\\"'max_duration' if incl_active else 'recovered.max_duration'\\\", mapping={})\" , \"fill_wrap_kwargs\" : true , \"tags\" : \"RepEval(expression=\\\"['drawdowns', 'duration'] if incl_active else ['drawdowns', 'recovered', 'duration']\\\", mapping={})\" }, \"avg_dd_duration\" : { \"title\" : \"Avg Drawdown Duration\" , \"calc_func\" : \"RepEval(expression=\\\"'avg_duration' if incl_active else 'recovered.avg_duration'\\\", mapping={})\" , \"fill_wrap_kwargs\" : true , \"tags\" : \"RepEval(expression=\\\"['drawdowns', 'duration'] if incl_active else ['drawdowns', 'recovered', 'duration']\\\", mapping={})\" }, \"max_return\" : { \"title\" : \"Max Recovery Return [%]\" , \"calc_func\" : \"recovered.recovery_return.max\" , \"post_calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881c9378>\" , \"tags\" : [ \"drawdowns\" , \"recovered\" ] }, \"avg_return\" : { \"title\" : \"Avg Recovery Return [%]\" , \"calc_func\" : \"recovered.recovery_return.mean\" , \"post_calc_func\" : \"<function Drawdowns.<lambda> at 0x7fac881c9400>\" , \"tags\" : [ \"drawdowns\" , \"recovered\" ] }, \"max_recovery_duration\" : { \"title\" : \"Max Recovery Duration\" , \"calc_func\" : \"recovered.recovery_duration.max\" , \"apply_to_timedelta\" : true , \"tags\" : [ \"drawdowns\" , \"recovered\" , \"duration\" ] }, \"avg_recovery_duration\" : { \"title\" : \"Avg Recovery Duration\" , \"calc_func\" : \"recovered.recovery_duration.mean\" , \"apply_to_timedelta\" : true , \"tags\" : [ \"drawdowns\" , \"recovered\" , \"duration\" ] }, \"recovery_duration_ratio\" : { \"title\" : \"Avg Recovery Duration Ratio\" , \"calc_func\" : \"recovered.recovery_duration_ratio.mean\" , \"tags\" : [ \"drawdowns\" , \"recovered\" ] } } ) Returns Drawdowns._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable Drawdowns._metrics .","title":"metrics"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.peak_idx","text":"Mapped array of the field peak_idx .","title":"peak_idx"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.peak_val","text":"Mapped array of the field peak_val .","title":"peak_val"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.plot","text":"Drawdowns . plot ( column = None , top_n = 5 , plot_zones = True , ts_trace_kwargs = None , peak_trace_kwargs = None , valley_trace_kwargs = None , recovery_trace_kwargs = None , active_trace_kwargs = None , decline_shape_kwargs = None , recovery_shape_kwargs = None , active_shape_kwargs = None , add_trace_kwargs = None , xref = 'x' , yref = 'y' , fig = None , ** layout_kwargs ) Plot drawdowns. Args column :\u2002 str Name of the column to plot. top_n :\u2002 int Filter top N drawdown records by maximum drawdown. plot_zones :\u2002 bool Whether to plot zones. ts_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for Drawdowns.ts . peak_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for peak values. valley_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for valley values. recovery_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for recovery values. active_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for active recovery values. decline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for decline zones. recovery_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for recovery zones. active_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for active recovery zones. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import vectorbt as vbt >>> from datetime import datetime , timedelta >>> import pandas as pd >>> price = pd . Series ([ 1 , 2 , 1 , 2 , 3 , 2 , 1 , 2 ], name = 'Price' ) >>> price . index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( len ( price ))] >>> vbt . Drawdowns . from_ts ( price , wrapper_kwargs = dict ( freq = '1 day' )) . plot ()","title":"plot()"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.plots_defaults","text":"Defaults for PlotsBuilderMixin.plots() . Merges Ranges.plots_defaults and drawdowns.plots from settings .","title":"plots_defaults"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.recovered","text":"Records filtered by status == 1 .","title":"recovered"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.recovery_duration","text":"See dd_recovery_duration_nb() . A value higher than 1 means the recovery was slower than the decline. Takes into account both recovered and active drawdowns.","title":"recovery_duration"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.recovery_duration_ratio","text":"See dd_recovery_duration_ratio_nb() . Takes into account both recovered and active drawdowns.","title":"recovery_duration_ratio"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.recovery_return","text":"See dd_recovery_return_nb() . Takes into account both recovered and active drawdowns.","title":"recovery_return"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.stats_defaults","text":"Defaults for StatsBuilderMixin.stats() . Merges Ranges.stats_defaults and drawdowns.stats from settings .","title":"stats_defaults"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.subplots","text":"Subplots supported by Drawdowns . Co nf ig( { \"plot\" : { \"title\" : \"Drawdowns\" , \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"tags\" : \"drawdowns\" } } ) Returns Drawdowns._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable Drawdowns._subplots .","title":"subplots"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.valley_idx","text":"Mapped array of the field valley_idx .","title":"valley_idx"},{"location":"api/generic/drawdowns/#vectorbt.generic.drawdowns.Drawdowns.valley_val","text":"Mapped array of the field valley_val .","title":"valley_val"},{"location":"api/generic/enums/","text":"enums module \u00b6 Named tuples and enumerated types. Defines enums and other schemas for vectorbt.generic . DrawdownStatus DrawdownStatusT \u00b6 Drawdown status. { \"Active\" : 0 , \"Recovered\" : 1 } RangeStatus RangeStatusT \u00b6 Range status. { \"Open\" : 0 , \"Closed\" : 1 } drawdown_dt dtype[void] \u00b6 np.dtype of drawdown records. { \"id\" : \"int64\" , \"col\" : \"int64\" , \"peak_idx\" : \"int64\" , \"start_idx\" : \"int64\" , \"valley_idx\" : \"int64\" , \"end_idx\" : \"int64\" , \"peak_val\" : \"float64\" , \"valley_val\" : \"float64\" , \"end_val\" : \"float64\" , \"status\" : \"int64\" } range_dt dtype[void] \u00b6 np.dtype of range records. { \"id\" : \"int64\" , \"col\" : \"int64\" , \"start_idx\" : \"int64\" , \"end_idx\" : \"int64\" , \"status\" : \"int64\" }","title":"enums"},{"location":"api/generic/enums/#vectorbt.generic.enums","text":"Named tuples and enumerated types. Defines enums and other schemas for vectorbt.generic .","title":"vectorbt.generic.enums"},{"location":"api/generic/enums/#vectorbt.generic.enums.DrawdownStatus","text":"Drawdown status. { \"Active\" : 0 , \"Recovered\" : 1 }","title":"DrawdownStatus"},{"location":"api/generic/enums/#vectorbt.generic.enums.RangeStatus","text":"Range status. { \"Open\" : 0 , \"Closed\" : 1 }","title":"RangeStatus"},{"location":"api/generic/enums/#vectorbt.generic.enums.drawdown_dt","text":"np.dtype of drawdown records. { \"id\" : \"int64\" , \"col\" : \"int64\" , \"peak_idx\" : \"int64\" , \"start_idx\" : \"int64\" , \"valley_idx\" : \"int64\" , \"end_idx\" : \"int64\" , \"peak_val\" : \"float64\" , \"valley_val\" : \"float64\" , \"end_val\" : \"float64\" , \"status\" : \"int64\" }","title":"drawdown_dt"},{"location":"api/generic/enums/#vectorbt.generic.enums.range_dt","text":"np.dtype of range records. { \"id\" : \"int64\" , \"col\" : \"int64\" , \"start_idx\" : \"int64\" , \"end_idx\" : \"int64\" , \"status\" : \"int64\" }","title":"range_dt"},{"location":"api/generic/nb/","text":"nb module \u00b6 Numba-compiled functions. Provides an arsenal of Numba-compiled functions that are used by accessors and in many other parts of the backtesting pipeline, such as technical indicators. These only accept NumPy arrays and other Numba-compatible types. The module can be accessed directly via vbt.nb . >>> import numpy as np >>> import vectorbt as vbt >>> # vectorbt.generic.nb.rolling_mean_1d_nb >>> vbt . nb . rolling_mean_1d_nb ( np . array ([ 1 , 2 , 3 , 4 ]), 2 ) array([nan, 1.5, 2.5, 3.5]) Note vectorbt treats matrices as first-class citizens and expects input arrays to be 2-dim, unless function has suffix _1d or is meant to be input to another function. Data is processed along index (axis 0). Rolling functions with minp=None have min_periods set to the window size. All functions passed as argument should be Numba-compiled. any_squeeze_nb function \u00b6 any_squeeze_nb ( col , group , a ) Return any (ignores NaNs) of a group. apply_and_reduce_nb function \u00b6 apply_and_reduce_nb ( a , apply_func_nb , apply_args , reduce_func_nb , reduce_args ) Apply apply_func_nb on each column and reduce into a single value using reduce_func_nb . apply_func_nb should accept index of the column, the column itself, and *apply_args . Should return an array. reduce_func_nb should accept index of the column, the array of results from apply_func_nb for that column, and *reduce_args . Should return a single value. apply_nb function \u00b6 apply_nb ( a , apply_func_nb , * args ) Apply function on each column. apply_func_nb should accept index of the column, the array, and *args . Should return a single value or an array of shape a.shape[1] . applymap_nb function \u00b6 applymap_nb ( a , map_func_nb , * args ) Map non-NA elements element-wise using map_func_nb . map_func_nb should accept index of the row, index of the column, the element itself, and *args . Should return a single value. argmax_reduce_nb function \u00b6 argmax_reduce_nb ( col , a ) Return position of max. argmin_reduce_nb function \u00b6 argmin_reduce_nb ( col , a ) Return position of min. bfill_1d_nb function \u00b6 bfill_1d_nb ( a ) Fill NaNs by propagating first valid observation backward. Numba equivalent to pd.Series(a).fillna(method='bfill') . Warning This operation looks ahead. bfill_nb function \u00b6 bfill_nb ( a ) 2-dim version of bfill_1d_nb() . bshift_1d_nb function \u00b6 bshift_1d_nb ( a , n = 1 , fill_value = nan ) Shift backward by n positions. Numba equivalent to pd.Series(a).shift(n) . Warning This operation looks ahead. bshift_nb function \u00b6 bshift_nb ( a , n = 1 , fill_value = nan ) 2-dim version of bshift_1d_nb() . count_reduce_nb function \u00b6 count_reduce_nb ( col , a ) Return count (ignores NaNs). crossed_above_1d_nb function \u00b6 crossed_above_1d_nb ( arr1 , arr2 , wait = 0 ) Get the crossover of the first array going above the second array. crossed_above_nb function \u00b6 crossed_above_nb ( arr1 , arr2 , wait = 0 ) 2-dim version of crossed_above_1d_nb() . dd_decline_duration_nb function \u00b6 dd_decline_duration_nb ( start_idx_arr , valley_idx_arr ) Return the duration of the peak-to-valley phase of each drawdown record. dd_drawdown_nb function \u00b6 dd_drawdown_nb ( peak_val_arr , valley_val_arr ) Return the drawdown of each drawdown record. dd_recovery_duration_nb function \u00b6 dd_recovery_duration_nb ( valley_idx_arr , end_idx_arr ) Return the duration of the valley-to-recovery phase of each drawdown record. dd_recovery_duration_ratio_nb function \u00b6 dd_recovery_duration_ratio_nb ( start_idx_arr , valley_idx_arr , end_idx_arr ) Return the ratio of the recovery duration to the decline duration of each drawdown record. dd_recovery_return_nb function \u00b6 dd_recovery_return_nb ( valley_val_arr , end_val_arr ) Return the recovery return of each drawdown record. describe_reduce_nb function \u00b6 describe_reduce_nb ( col , a , perc , ddof ) Return descriptive statistics (ignores NaNs). Numba equivalent to pd.Series(a).describe(perc) . diff_1d_nb function \u00b6 diff_1d_nb ( a , n = 1 ) Return the 1-th discrete difference. Numba equivalent to pd.Series(a).diff() . diff_nb function \u00b6 diff_nb ( a , n = 1 ) 2-dim version of diff_1d_nb() . ewm_mean_1d_nb function \u00b6 ewm_mean_1d_nb ( a , span , minp = 0 , adjust = False ) Return exponential weighted average. Numba equivalent to pd.Series(a).ewm(span=span, min_periods=minp, adjust=adjust).mean() . Adaptation of pd._libs.window.aggregations.window_aggregations.ewma with default arguments. ewm_mean_nb function \u00b6 ewm_mean_nb ( a , span , minp = 0 , adjust = False ) 2-dim version of ewm_mean_1d_nb() . ewm_std_1d_nb function \u00b6 ewm_std_1d_nb ( a , span , minp = 0 , adjust = False , ddof = 0 ) Return exponential weighted standard deviation. Numba equivalent to pd.Series(a).ewm(span=span, min_periods=minp).std(ddof=ddof) . Adaptation of pd._libs.window.aggregations.window_aggregations.ewmcov with default arguments. ewm_std_nb function \u00b6 ewm_std_nb ( a , span , minp = 0 , adjust = False , ddof = 0 ) 2-dim version of ewm_std_1d_nb() . expanding_apply_nb function \u00b6 expanding_apply_nb ( a , minp , apply_func_nb , * args ) Expanding version of rolling_apply_nb() . expanding_matrix_apply_nb function \u00b6 expanding_matrix_apply_nb ( a , minp , apply_func_nb , * args ) Expanding version of rolling_matrix_apply_nb() . expanding_max_1d_nb function \u00b6 expanding_max_1d_nb ( a , minp = 1 ) Return expanding max. Numba equivalent to pd.Series(a).expanding(min_periods=minp).max() . expanding_max_nb function \u00b6 expanding_max_nb ( a , minp = 1 ) 2-dim version of expanding_max_1d_nb() . expanding_mean_1d_nb function \u00b6 expanding_mean_1d_nb ( a , minp = 1 ) Return expanding mean. Numba equivalent to pd.Series(a).expanding(min_periods=minp).mean() . expanding_mean_nb function \u00b6 expanding_mean_nb ( a , minp = 1 ) 2-dim version of expanding_mean_1d_nb() . expanding_min_1d_nb function \u00b6 expanding_min_1d_nb ( a , minp = 1 ) Return expanding min. Numba equivalent to pd.Series(a).expanding(min_periods=minp).min() . expanding_min_nb function \u00b6 expanding_min_nb ( a , minp = 1 ) 2-dim version of expanding_min_1d_nb() . expanding_std_1d_nb function \u00b6 expanding_std_1d_nb ( a , minp = 1 , ddof = 0 ) Return expanding standard deviation. Numba equivalent to pd.Series(a).expanding(min_periods=minp).std(ddof=ddof) . expanding_std_nb function \u00b6 expanding_std_nb ( a , minp = 1 , ddof = 0 ) 2-dim version of expanding_std_1d_nb() . ffill_1d_nb function \u00b6 ffill_1d_nb ( a ) Fill NaNs by propagating last valid observation forward. Numba equivalent to pd.Series(a).fillna(method='ffill') . ffill_nb function \u00b6 ffill_nb ( a ) 2-dim version of ffill_1d_nb() . fillna_1d_nb function \u00b6 fillna_1d_nb ( a , value ) Replace NaNs with value. Numba equivalent to pd.Series(a).fillna(value) . fillna_nb function \u00b6 fillna_nb ( a , value ) 2-dim version of fillna_1d_nb() . filter_nb function \u00b6 filter_nb ( a , filter_func_nb , * args ) Filter non-NA elements elementwise using filter_func_nb . The filtered out elements will become NA. filter_func_nb should accept index of the row, index of the column, the element itself, and *args . Should return a bool. find_ranges_nb function \u00b6 find_ranges_nb ( ts , gap_value ) Find ranges and store their information as records to an array. Usage Find ranges in time series: >>> import numpy as np >>> import pandas as pd >>> from vectorbt.generic.nb import find_ranges_nb >>> ts = np . asarray ([ ... [ np . nan , np . nan , np . nan , np . nan ], ... [ 2 , np . nan , np . nan , np . nan ], ... [ 3 , 3 , np . nan , np . nan ], ... [ np . nan , 4 , 4 , np . nan ], ... [ 5 , np . nan , 5 , 5 ], ... [ 6 , 6 , np . nan , 6 ] ... ]) >>> records = find_ranges_nb ( ts , np . nan ) >>> pd . DataFrame . from_records ( records ) id col start_idx end_idx 0 0 0 1 3 1 1 0 4 6 2 2 1 2 4 3 3 1 5 6 4 4 2 3 5 5 5 3 4 6 flat_reduce_grouped_nb function \u00b6 flat_reduce_grouped_nb ( a , group_lens , in_c_order , reduce_func_nb , * args ) Same as reduce_grouped_nb() but passes flattened array. flat_reduce_grouped_to_array_nb function \u00b6 flat_reduce_grouped_to_array_nb ( a , group_lens , in_c_order , reduce_func_nb , * args ) Same as reduce_grouped_to_array_nb() but passes flattened 1D array. flatten_forder_nb function \u00b6 flatten_forder_nb ( a ) Flatten a in F order. flatten_grouped_nb function \u00b6 flatten_grouped_nb ( a , group_lens , in_c_order ) Flatten each group of columns. flatten_uniform_grouped_nb function \u00b6 flatten_uniform_grouped_nb ( a , group_lens , in_c_order ) Flatten each group of columns of the same length. fshift_1d_nb function \u00b6 fshift_1d_nb ( a , n = 1 , fill_value = nan ) Shift forward by n positions. Numba equivalent to pd.Series(a).shift(n) . fshift_nb function \u00b6 fshift_nb ( a , n = 1 , fill_value = nan ) 2-dim version of fshift_1d_nb() . get_drawdowns_nb function \u00b6 get_drawdowns_nb ( ts ) Fill drawdown records by analyzing a time series. Usage >>> import numpy as np >>> import pandas as pd >>> from vectorbt.generic.nb import get_drawdowns_nb >>> ts = np . asarray ([ ... [ 1 , 5 , 1 , 3 ], ... [ 2 , 4 , 2 , 2 ], ... [ 3 , 3 , 3 , 1 ], ... [ 4 , 2 , 2 , 2 ], ... [ 5 , 1 , 1 , 3 ] ... ]) >>> records = get_drawdowns_nb ( ts ) >>> pd . DataFrame . from_records ( records ) id col peak_idx start_idx valley_idx end_idx peak_val valley_val \\ 0 0 1 0 1 4 4 5.0 1.0 1 1 2 2 3 4 4 3.0 1.0 2 2 3 0 1 2 4 3.0 1.0 end_val status 0 1.0 0 1 1.0 0 2 3.0 1 groupby_apply_nb function \u00b6 groupby_apply_nb ( a , groups , apply_func_nb , * args ) Provide group-by calculations. groups should be a dictionary, where each key is an index that points to an element in the new array where a group-by result will be stored, while the value should be an array of indices in a to apply apply_func_nb on. apply_func_nb should accept indices of the group, index of the column, the array, and *args . Should return a single value. groupby_matrix_apply_nb function \u00b6 groupby_matrix_apply_nb ( a , groups , apply_func_nb , * args ) groupby_apply_nb() with apply_func_nb being applied on all columns at once. apply_func_nb should accept indices of the group, the 2-dim array, and *args . Should return a single value or an array of shape a.shape[1] . max_reduce_nb function \u00b6 max_reduce_nb ( col , a ) Return max (ignores NaNs). max_squeeze_nb function \u00b6 max_squeeze_nb ( col , group , a ) Return max (ignores NaNs) of a group. mean_reduce_nb function \u00b6 mean_reduce_nb ( col , a ) Return mean (ignores NaNs). median_reduce_nb function \u00b6 median_reduce_nb ( col , a ) Return median (ignores NaNs). min_reduce_nb function \u00b6 min_reduce_nb ( col , a ) Return min (ignores NaNs). min_squeeze_nb function \u00b6 min_squeeze_nb ( col , group , a ) Return min (ignores NaNs) of a group. nancnt_nb function \u00b6 nancnt_nb ( a ) Compute count while ignoring NaNs. nancumprod_nb function \u00b6 nancumprod_nb ( a ) Numba-equivalent of np.nancumprod along axis 0. nancumsum_nb function \u00b6 nancumsum_nb ( a ) Numba-equivalent of np.nancumsum along axis 0. nanmax_nb function \u00b6 nanmax_nb ( a ) Numba-equivalent of np.nanmax along axis 0. nanmean_nb function \u00b6 nanmean_nb ( a ) Numba-equivalent of np.nanmean along axis 0. nanmedian_nb function \u00b6 nanmedian_nb ( a ) Numba-equivalent of np.nanmedian along axis 0. nanmin_nb function \u00b6 nanmin_nb ( a ) Numba-equivalent of np.nanmin along axis 0. nanprod_nb function \u00b6 nanprod_nb ( a ) Numba-equivalent of np.nanprod along axis 0. nanstd_1d_nb function \u00b6 nanstd_1d_nb ( a , ddof = 0 ) Numba-equivalent of np.nanstd . nanstd_nb function \u00b6 nanstd_nb ( a , ddof = 0 ) 2-dim version of nanstd_1d_nb() . nansum_nb function \u00b6 nansum_nb ( a ) Numba-equivalent of np.nansum along axis 0. nth_index_reduce_nb function \u00b6 nth_index_reduce_nb ( col , a , n ) Return index of n-th element. nth_reduce_nb function \u00b6 nth_reduce_nb ( col , a , n ) Return n-th element. pct_change_1d_nb function \u00b6 pct_change_1d_nb ( a , n = 1 ) Return the percentage change. Numba equivalent to pd.Series(a).pct_change() . pct_change_nb function \u00b6 pct_change_nb ( a , n = 1 ) 2-dim version of pct_change_1d_nb() . range_coverage_nb function \u00b6 range_coverage_nb ( start_idx_arr , end_idx_arr , status_arr , col_map , index_lens , overlapping = False , normalize = False ) Get coverage of range records. Set overlapping to True to get the number of overlapping steps. Set normalize to True to get the number of steps in relation either to the total number of steps (when overlapping=False ) or to the number of covered steps (when overlapping=True ). range_duration_nb function \u00b6 range_duration_nb ( start_idx_arr , end_idx_arr , status_arr ) Get duration of each duration record. ranges_to_mask_nb function \u00b6 ranges_to_mask_nb ( start_idx_arr , end_idx_arr , status_arr , col_map , index_len ) Convert ranges to 2-dim mask. reduce_grouped_nb function \u00b6 reduce_grouped_nb ( a , group_lens , reduce_func_nb , * args ) Reduce each group of columns into a single value using reduce_func_nb . reduce_func_nb should accept index of the group, the array of row values, and *args . Should return a single value. reduce_grouped_to_array_nb function \u00b6 reduce_grouped_to_array_nb ( a , group_lens , reduce_func_nb , * args ) Reduce each group of columns into an array of values using reduce_func_nb . reduce_func_nb same as for reduce_grouped_nb() but should return an array. Note Output of reduce_func_nb should be strictly homogeneous. reduce_nb function \u00b6 reduce_nb ( a , reduce_func_nb , * args ) Reduce each column into a single value using reduce_func_nb . reduce_func_nb should accept index of the column, the array, and *args . Should return a single value. reduce_to_array_nb function \u00b6 reduce_to_array_nb ( a , reduce_func_nb , * args ) Reduce each column into an array of values using reduce_func_nb . reduce_func_nb same as for reduce_nb() but should return an array. Note Output of reduce_func_nb should be strictly homogeneous. rolling_apply_nb function \u00b6 rolling_apply_nb ( a , window , minp , apply_func_nb , * args ) Provide rolling window calculations. apply_func_nb should accept index of the row, index of the column, the array, and *args . Should return a single value. rolling_matrix_apply_nb function \u00b6 rolling_matrix_apply_nb ( a , window , minp , apply_func_nb , * args ) rolling_apply_nb() with apply_func_nb being applied on all columns at once. apply_func_nb should accept index of the row, the 2-dim array, and *args . Should return a single value or an array of shape a.shape[1] . rolling_max_1d_nb function \u00b6 rolling_max_1d_nb ( a , window , minp = None ) Return rolling max. Numba equivalent to pd.Series(a).rolling(window, min_periods=minp).max() . rolling_max_nb function \u00b6 rolling_max_nb ( a , window , minp = None ) 2-dim version of rolling_max_1d_nb() . rolling_mean_1d_nb function \u00b6 rolling_mean_1d_nb ( a , window , minp = None ) Return rolling mean. Numba equivalent to pd.Series(a).rolling(window, min_periods=minp).mean() . rolling_mean_nb function \u00b6 rolling_mean_nb ( a , window , minp = None ) 2-dim version of rolling_mean_1d_nb() . rolling_min_1d_nb function \u00b6 rolling_min_1d_nb ( a , window , minp = None ) Return rolling min. Numba equivalent to pd.Series(a).rolling(window, min_periods=minp).min() . rolling_min_nb function \u00b6 rolling_min_nb ( a , window , minp = None ) 2-dim version of rolling_min_1d_nb() . rolling_std_1d_nb function \u00b6 rolling_std_1d_nb ( a , window , minp = None , ddof = 0 ) Return rolling standard deviation. Numba equivalent to pd.Series(a).rolling(window, min_periods=minp).std(ddof=ddof) . rolling_std_nb function \u00b6 rolling_std_nb ( a , window , minp = None , ddof = 0 ) 2-dim version of rolling_std_1d_nb() . row_apply_nb function \u00b6 row_apply_nb ( a , apply_func_nb , * args ) Apply function on each row. apply_func_nb should accept index of the row, the array, and *args . Should return a single value or an array of shape a.shape[1] . set_by_mask_1d_nb function \u00b6 set_by_mask_1d_nb ( a , mask , value ) Set each element to a value by boolean mask. set_by_mask_mult_1d_nb function \u00b6 set_by_mask_mult_1d_nb ( a , mask , values ) Set each element in one array to the corresponding element in another by boolean mask. values should be of the same shape as in a . set_by_mask_mult_nb function \u00b6 set_by_mask_mult_nb ( a , mask , values ) 2-dim version of set_by_mask_mult_1d_nb() . set_by_mask_nb function \u00b6 set_by_mask_nb ( a , mask , value ) 2-dim version of set_by_mask_1d_nb() . shuffle_1d_nb function \u00b6 shuffle_1d_nb ( a , seed = None ) Shuffle each column in a . Specify seed to make output deterministic. shuffle_nb function \u00b6 shuffle_nb ( a , seed = None ) 2-dim version of shuffle_1d_nb() . squeeze_grouped_nb function \u00b6 squeeze_grouped_nb ( a , group_lens , squeeze_func_nb , * args ) Squeeze each group of columns into a single column using squeeze_func_nb . squeeze_func_nb should accept index of the row, index of the group, the array, and *args . Should return a single value. std_reduce_nb function \u00b6 std_reduce_nb ( col , a , ddof ) Return std (ignores NaNs). sum_reduce_nb function \u00b6 sum_reduce_nb ( col , a ) Return sum (ignores NaNs). sum_squeeze_nb function \u00b6 sum_squeeze_nb ( col , group , a ) Return sum (ignores NaNs) of a group. value_counts_nb function \u00b6 value_counts_nb ( codes , n_uniques , group_lens ) Return value counts per column/group.","title":"nb"},{"location":"api/generic/nb/#vectorbt.generic.nb","text":"Numba-compiled functions. Provides an arsenal of Numba-compiled functions that are used by accessors and in many other parts of the backtesting pipeline, such as technical indicators. These only accept NumPy arrays and other Numba-compatible types. The module can be accessed directly via vbt.nb . >>> import numpy as np >>> import vectorbt as vbt >>> # vectorbt.generic.nb.rolling_mean_1d_nb >>> vbt . nb . rolling_mean_1d_nb ( np . array ([ 1 , 2 , 3 , 4 ]), 2 ) array([nan, 1.5, 2.5, 3.5]) Note vectorbt treats matrices as first-class citizens and expects input arrays to be 2-dim, unless function has suffix _1d or is meant to be input to another function. Data is processed along index (axis 0). Rolling functions with minp=None have min_periods set to the window size. All functions passed as argument should be Numba-compiled.","title":"vectorbt.generic.nb"},{"location":"api/generic/nb/#vectorbt.generic.nb.any_squeeze_nb","text":"any_squeeze_nb ( col , group , a ) Return any (ignores NaNs) of a group.","title":"any_squeeze_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.apply_and_reduce_nb","text":"apply_and_reduce_nb ( a , apply_func_nb , apply_args , reduce_func_nb , reduce_args ) Apply apply_func_nb on each column and reduce into a single value using reduce_func_nb . apply_func_nb should accept index of the column, the column itself, and *apply_args . Should return an array. reduce_func_nb should accept index of the column, the array of results from apply_func_nb for that column, and *reduce_args . Should return a single value.","title":"apply_and_reduce_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.apply_nb","text":"apply_nb ( a , apply_func_nb , * args ) Apply function on each column. apply_func_nb should accept index of the column, the array, and *args . Should return a single value or an array of shape a.shape[1] .","title":"apply_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.applymap_nb","text":"applymap_nb ( a , map_func_nb , * args ) Map non-NA elements element-wise using map_func_nb . map_func_nb should accept index of the row, index of the column, the element itself, and *args . Should return a single value.","title":"applymap_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.argmax_reduce_nb","text":"argmax_reduce_nb ( col , a ) Return position of max.","title":"argmax_reduce_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.argmin_reduce_nb","text":"argmin_reduce_nb ( col , a ) Return position of min.","title":"argmin_reduce_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.bfill_1d_nb","text":"bfill_1d_nb ( a ) Fill NaNs by propagating first valid observation backward. Numba equivalent to pd.Series(a).fillna(method='bfill') . Warning This operation looks ahead.","title":"bfill_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.bfill_nb","text":"bfill_nb ( a ) 2-dim version of bfill_1d_nb() .","title":"bfill_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.bshift_1d_nb","text":"bshift_1d_nb ( a , n = 1 , fill_value = nan ) Shift backward by n positions. Numba equivalent to pd.Series(a).shift(n) . Warning This operation looks ahead.","title":"bshift_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.bshift_nb","text":"bshift_nb ( a , n = 1 , fill_value = nan ) 2-dim version of bshift_1d_nb() .","title":"bshift_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.count_reduce_nb","text":"count_reduce_nb ( col , a ) Return count (ignores NaNs).","title":"count_reduce_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.crossed_above_1d_nb","text":"crossed_above_1d_nb ( arr1 , arr2 , wait = 0 ) Get the crossover of the first array going above the second array.","title":"crossed_above_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.crossed_above_nb","text":"crossed_above_nb ( arr1 , arr2 , wait = 0 ) 2-dim version of crossed_above_1d_nb() .","title":"crossed_above_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.dd_decline_duration_nb","text":"dd_decline_duration_nb ( start_idx_arr , valley_idx_arr ) Return the duration of the peak-to-valley phase of each drawdown record.","title":"dd_decline_duration_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.dd_drawdown_nb","text":"dd_drawdown_nb ( peak_val_arr , valley_val_arr ) Return the drawdown of each drawdown record.","title":"dd_drawdown_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.dd_recovery_duration_nb","text":"dd_recovery_duration_nb ( valley_idx_arr , end_idx_arr ) Return the duration of the valley-to-recovery phase of each drawdown record.","title":"dd_recovery_duration_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.dd_recovery_duration_ratio_nb","text":"dd_recovery_duration_ratio_nb ( start_idx_arr , valley_idx_arr , end_idx_arr ) Return the ratio of the recovery duration to the decline duration of each drawdown record.","title":"dd_recovery_duration_ratio_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.dd_recovery_return_nb","text":"dd_recovery_return_nb ( valley_val_arr , end_val_arr ) Return the recovery return of each drawdown record.","title":"dd_recovery_return_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.describe_reduce_nb","text":"describe_reduce_nb ( col , a , perc , ddof ) Return descriptive statistics (ignores NaNs). Numba equivalent to pd.Series(a).describe(perc) .","title":"describe_reduce_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.diff_1d_nb","text":"diff_1d_nb ( a , n = 1 ) Return the 1-th discrete difference. Numba equivalent to pd.Series(a).diff() .","title":"diff_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.diff_nb","text":"diff_nb ( a , n = 1 ) 2-dim version of diff_1d_nb() .","title":"diff_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.ewm_mean_1d_nb","text":"ewm_mean_1d_nb ( a , span , minp = 0 , adjust = False ) Return exponential weighted average. Numba equivalent to pd.Series(a).ewm(span=span, min_periods=minp, adjust=adjust).mean() . Adaptation of pd._libs.window.aggregations.window_aggregations.ewma with default arguments.","title":"ewm_mean_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.ewm_mean_nb","text":"ewm_mean_nb ( a , span , minp = 0 , adjust = False ) 2-dim version of ewm_mean_1d_nb() .","title":"ewm_mean_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.ewm_std_1d_nb","text":"ewm_std_1d_nb ( a , span , minp = 0 , adjust = False , ddof = 0 ) Return exponential weighted standard deviation. Numba equivalent to pd.Series(a).ewm(span=span, min_periods=minp).std(ddof=ddof) . Adaptation of pd._libs.window.aggregations.window_aggregations.ewmcov with default arguments.","title":"ewm_std_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.ewm_std_nb","text":"ewm_std_nb ( a , span , minp = 0 , adjust = False , ddof = 0 ) 2-dim version of ewm_std_1d_nb() .","title":"ewm_std_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.expanding_apply_nb","text":"expanding_apply_nb ( a , minp , apply_func_nb , * args ) Expanding version of rolling_apply_nb() .","title":"expanding_apply_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.expanding_matrix_apply_nb","text":"expanding_matrix_apply_nb ( a , minp , apply_func_nb , * args ) Expanding version of rolling_matrix_apply_nb() .","title":"expanding_matrix_apply_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.expanding_max_1d_nb","text":"expanding_max_1d_nb ( a , minp = 1 ) Return expanding max. Numba equivalent to pd.Series(a).expanding(min_periods=minp).max() .","title":"expanding_max_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.expanding_max_nb","text":"expanding_max_nb ( a , minp = 1 ) 2-dim version of expanding_max_1d_nb() .","title":"expanding_max_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.expanding_mean_1d_nb","text":"expanding_mean_1d_nb ( a , minp = 1 ) Return expanding mean. Numba equivalent to pd.Series(a).expanding(min_periods=minp).mean() .","title":"expanding_mean_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.expanding_mean_nb","text":"expanding_mean_nb ( a , minp = 1 ) 2-dim version of expanding_mean_1d_nb() .","title":"expanding_mean_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.expanding_min_1d_nb","text":"expanding_min_1d_nb ( a , minp = 1 ) Return expanding min. Numba equivalent to pd.Series(a).expanding(min_periods=minp).min() .","title":"expanding_min_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.expanding_min_nb","text":"expanding_min_nb ( a , minp = 1 ) 2-dim version of expanding_min_1d_nb() .","title":"expanding_min_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.expanding_std_1d_nb","text":"expanding_std_1d_nb ( a , minp = 1 , ddof = 0 ) Return expanding standard deviation. Numba equivalent to pd.Series(a).expanding(min_periods=minp).std(ddof=ddof) .","title":"expanding_std_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.expanding_std_nb","text":"expanding_std_nb ( a , minp = 1 , ddof = 0 ) 2-dim version of expanding_std_1d_nb() .","title":"expanding_std_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.ffill_1d_nb","text":"ffill_1d_nb ( a ) Fill NaNs by propagating last valid observation forward. Numba equivalent to pd.Series(a).fillna(method='ffill') .","title":"ffill_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.ffill_nb","text":"ffill_nb ( a ) 2-dim version of ffill_1d_nb() .","title":"ffill_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.fillna_1d_nb","text":"fillna_1d_nb ( a , value ) Replace NaNs with value. Numba equivalent to pd.Series(a).fillna(value) .","title":"fillna_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.fillna_nb","text":"fillna_nb ( a , value ) 2-dim version of fillna_1d_nb() .","title":"fillna_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.filter_nb","text":"filter_nb ( a , filter_func_nb , * args ) Filter non-NA elements elementwise using filter_func_nb . The filtered out elements will become NA. filter_func_nb should accept index of the row, index of the column, the element itself, and *args . Should return a bool.","title":"filter_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.find_ranges_nb","text":"find_ranges_nb ( ts , gap_value ) Find ranges and store their information as records to an array. Usage Find ranges in time series: >>> import numpy as np >>> import pandas as pd >>> from vectorbt.generic.nb import find_ranges_nb >>> ts = np . asarray ([ ... [ np . nan , np . nan , np . nan , np . nan ], ... [ 2 , np . nan , np . nan , np . nan ], ... [ 3 , 3 , np . nan , np . nan ], ... [ np . nan , 4 , 4 , np . nan ], ... [ 5 , np . nan , 5 , 5 ], ... [ 6 , 6 , np . nan , 6 ] ... ]) >>> records = find_ranges_nb ( ts , np . nan ) >>> pd . DataFrame . from_records ( records ) id col start_idx end_idx 0 0 0 1 3 1 1 0 4 6 2 2 1 2 4 3 3 1 5 6 4 4 2 3 5 5 5 3 4 6","title":"find_ranges_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.flat_reduce_grouped_nb","text":"flat_reduce_grouped_nb ( a , group_lens , in_c_order , reduce_func_nb , * args ) Same as reduce_grouped_nb() but passes flattened array.","title":"flat_reduce_grouped_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.flat_reduce_grouped_to_array_nb","text":"flat_reduce_grouped_to_array_nb ( a , group_lens , in_c_order , reduce_func_nb , * args ) Same as reduce_grouped_to_array_nb() but passes flattened 1D array.","title":"flat_reduce_grouped_to_array_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.flatten_forder_nb","text":"flatten_forder_nb ( a ) Flatten a in F order.","title":"flatten_forder_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.flatten_grouped_nb","text":"flatten_grouped_nb ( a , group_lens , in_c_order ) Flatten each group of columns.","title":"flatten_grouped_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.flatten_uniform_grouped_nb","text":"flatten_uniform_grouped_nb ( a , group_lens , in_c_order ) Flatten each group of columns of the same length.","title":"flatten_uniform_grouped_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.fshift_1d_nb","text":"fshift_1d_nb ( a , n = 1 , fill_value = nan ) Shift forward by n positions. Numba equivalent to pd.Series(a).shift(n) .","title":"fshift_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.fshift_nb","text":"fshift_nb ( a , n = 1 , fill_value = nan ) 2-dim version of fshift_1d_nb() .","title":"fshift_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.get_drawdowns_nb","text":"get_drawdowns_nb ( ts ) Fill drawdown records by analyzing a time series. Usage >>> import numpy as np >>> import pandas as pd >>> from vectorbt.generic.nb import get_drawdowns_nb >>> ts = np . asarray ([ ... [ 1 , 5 , 1 , 3 ], ... [ 2 , 4 , 2 , 2 ], ... [ 3 , 3 , 3 , 1 ], ... [ 4 , 2 , 2 , 2 ], ... [ 5 , 1 , 1 , 3 ] ... ]) >>> records = get_drawdowns_nb ( ts ) >>> pd . DataFrame . from_records ( records ) id col peak_idx start_idx valley_idx end_idx peak_val valley_val \\ 0 0 1 0 1 4 4 5.0 1.0 1 1 2 2 3 4 4 3.0 1.0 2 2 3 0 1 2 4 3.0 1.0 end_val status 0 1.0 0 1 1.0 0 2 3.0 1","title":"get_drawdowns_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.groupby_apply_nb","text":"groupby_apply_nb ( a , groups , apply_func_nb , * args ) Provide group-by calculations. groups should be a dictionary, where each key is an index that points to an element in the new array where a group-by result will be stored, while the value should be an array of indices in a to apply apply_func_nb on. apply_func_nb should accept indices of the group, index of the column, the array, and *args . Should return a single value.","title":"groupby_apply_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.groupby_matrix_apply_nb","text":"groupby_matrix_apply_nb ( a , groups , apply_func_nb , * args ) groupby_apply_nb() with apply_func_nb being applied on all columns at once. apply_func_nb should accept indices of the group, the 2-dim array, and *args . Should return a single value or an array of shape a.shape[1] .","title":"groupby_matrix_apply_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.max_reduce_nb","text":"max_reduce_nb ( col , a ) Return max (ignores NaNs).","title":"max_reduce_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.max_squeeze_nb","text":"max_squeeze_nb ( col , group , a ) Return max (ignores NaNs) of a group.","title":"max_squeeze_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.mean_reduce_nb","text":"mean_reduce_nb ( col , a ) Return mean (ignores NaNs).","title":"mean_reduce_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.median_reduce_nb","text":"median_reduce_nb ( col , a ) Return median (ignores NaNs).","title":"median_reduce_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.min_reduce_nb","text":"min_reduce_nb ( col , a ) Return min (ignores NaNs).","title":"min_reduce_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.min_squeeze_nb","text":"min_squeeze_nb ( col , group , a ) Return min (ignores NaNs) of a group.","title":"min_squeeze_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.nancnt_nb","text":"nancnt_nb ( a ) Compute count while ignoring NaNs.","title":"nancnt_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.nancumprod_nb","text":"nancumprod_nb ( a ) Numba-equivalent of np.nancumprod along axis 0.","title":"nancumprod_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.nancumsum_nb","text":"nancumsum_nb ( a ) Numba-equivalent of np.nancumsum along axis 0.","title":"nancumsum_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.nanmax_nb","text":"nanmax_nb ( a ) Numba-equivalent of np.nanmax along axis 0.","title":"nanmax_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.nanmean_nb","text":"nanmean_nb ( a ) Numba-equivalent of np.nanmean along axis 0.","title":"nanmean_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.nanmedian_nb","text":"nanmedian_nb ( a ) Numba-equivalent of np.nanmedian along axis 0.","title":"nanmedian_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.nanmin_nb","text":"nanmin_nb ( a ) Numba-equivalent of np.nanmin along axis 0.","title":"nanmin_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.nanprod_nb","text":"nanprod_nb ( a ) Numba-equivalent of np.nanprod along axis 0.","title":"nanprod_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.nanstd_1d_nb","text":"nanstd_1d_nb ( a , ddof = 0 ) Numba-equivalent of np.nanstd .","title":"nanstd_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.nanstd_nb","text":"nanstd_nb ( a , ddof = 0 ) 2-dim version of nanstd_1d_nb() .","title":"nanstd_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.nansum_nb","text":"nansum_nb ( a ) Numba-equivalent of np.nansum along axis 0.","title":"nansum_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.nth_index_reduce_nb","text":"nth_index_reduce_nb ( col , a , n ) Return index of n-th element.","title":"nth_index_reduce_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.nth_reduce_nb","text":"nth_reduce_nb ( col , a , n ) Return n-th element.","title":"nth_reduce_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.pct_change_1d_nb","text":"pct_change_1d_nb ( a , n = 1 ) Return the percentage change. Numba equivalent to pd.Series(a).pct_change() .","title":"pct_change_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.pct_change_nb","text":"pct_change_nb ( a , n = 1 ) 2-dim version of pct_change_1d_nb() .","title":"pct_change_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.range_coverage_nb","text":"range_coverage_nb ( start_idx_arr , end_idx_arr , status_arr , col_map , index_lens , overlapping = False , normalize = False ) Get coverage of range records. Set overlapping to True to get the number of overlapping steps. Set normalize to True to get the number of steps in relation either to the total number of steps (when overlapping=False ) or to the number of covered steps (when overlapping=True ).","title":"range_coverage_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.range_duration_nb","text":"range_duration_nb ( start_idx_arr , end_idx_arr , status_arr ) Get duration of each duration record.","title":"range_duration_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.ranges_to_mask_nb","text":"ranges_to_mask_nb ( start_idx_arr , end_idx_arr , status_arr , col_map , index_len ) Convert ranges to 2-dim mask.","title":"ranges_to_mask_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.reduce_grouped_nb","text":"reduce_grouped_nb ( a , group_lens , reduce_func_nb , * args ) Reduce each group of columns into a single value using reduce_func_nb . reduce_func_nb should accept index of the group, the array of row values, and *args . Should return a single value.","title":"reduce_grouped_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.reduce_grouped_to_array_nb","text":"reduce_grouped_to_array_nb ( a , group_lens , reduce_func_nb , * args ) Reduce each group of columns into an array of values using reduce_func_nb . reduce_func_nb same as for reduce_grouped_nb() but should return an array. Note Output of reduce_func_nb should be strictly homogeneous.","title":"reduce_grouped_to_array_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.reduce_nb","text":"reduce_nb ( a , reduce_func_nb , * args ) Reduce each column into a single value using reduce_func_nb . reduce_func_nb should accept index of the column, the array, and *args . Should return a single value.","title":"reduce_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.reduce_to_array_nb","text":"reduce_to_array_nb ( a , reduce_func_nb , * args ) Reduce each column into an array of values using reduce_func_nb . reduce_func_nb same as for reduce_nb() but should return an array. Note Output of reduce_func_nb should be strictly homogeneous.","title":"reduce_to_array_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.rolling_apply_nb","text":"rolling_apply_nb ( a , window , minp , apply_func_nb , * args ) Provide rolling window calculations. apply_func_nb should accept index of the row, index of the column, the array, and *args . Should return a single value.","title":"rolling_apply_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.rolling_matrix_apply_nb","text":"rolling_matrix_apply_nb ( a , window , minp , apply_func_nb , * args ) rolling_apply_nb() with apply_func_nb being applied on all columns at once. apply_func_nb should accept index of the row, the 2-dim array, and *args . Should return a single value or an array of shape a.shape[1] .","title":"rolling_matrix_apply_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.rolling_max_1d_nb","text":"rolling_max_1d_nb ( a , window , minp = None ) Return rolling max. Numba equivalent to pd.Series(a).rolling(window, min_periods=minp).max() .","title":"rolling_max_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.rolling_max_nb","text":"rolling_max_nb ( a , window , minp = None ) 2-dim version of rolling_max_1d_nb() .","title":"rolling_max_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.rolling_mean_1d_nb","text":"rolling_mean_1d_nb ( a , window , minp = None ) Return rolling mean. Numba equivalent to pd.Series(a).rolling(window, min_periods=minp).mean() .","title":"rolling_mean_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.rolling_mean_nb","text":"rolling_mean_nb ( a , window , minp = None ) 2-dim version of rolling_mean_1d_nb() .","title":"rolling_mean_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.rolling_min_1d_nb","text":"rolling_min_1d_nb ( a , window , minp = None ) Return rolling min. Numba equivalent to pd.Series(a).rolling(window, min_periods=minp).min() .","title":"rolling_min_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.rolling_min_nb","text":"rolling_min_nb ( a , window , minp = None ) 2-dim version of rolling_min_1d_nb() .","title":"rolling_min_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.rolling_std_1d_nb","text":"rolling_std_1d_nb ( a , window , minp = None , ddof = 0 ) Return rolling standard deviation. Numba equivalent to pd.Series(a).rolling(window, min_periods=minp).std(ddof=ddof) .","title":"rolling_std_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.rolling_std_nb","text":"rolling_std_nb ( a , window , minp = None , ddof = 0 ) 2-dim version of rolling_std_1d_nb() .","title":"rolling_std_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.row_apply_nb","text":"row_apply_nb ( a , apply_func_nb , * args ) Apply function on each row. apply_func_nb should accept index of the row, the array, and *args . Should return a single value or an array of shape a.shape[1] .","title":"row_apply_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.set_by_mask_1d_nb","text":"set_by_mask_1d_nb ( a , mask , value ) Set each element to a value by boolean mask.","title":"set_by_mask_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.set_by_mask_mult_1d_nb","text":"set_by_mask_mult_1d_nb ( a , mask , values ) Set each element in one array to the corresponding element in another by boolean mask. values should be of the same shape as in a .","title":"set_by_mask_mult_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.set_by_mask_mult_nb","text":"set_by_mask_mult_nb ( a , mask , values ) 2-dim version of set_by_mask_mult_1d_nb() .","title":"set_by_mask_mult_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.set_by_mask_nb","text":"set_by_mask_nb ( a , mask , value ) 2-dim version of set_by_mask_1d_nb() .","title":"set_by_mask_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.shuffle_1d_nb","text":"shuffle_1d_nb ( a , seed = None ) Shuffle each column in a . Specify seed to make output deterministic.","title":"shuffle_1d_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.shuffle_nb","text":"shuffle_nb ( a , seed = None ) 2-dim version of shuffle_1d_nb() .","title":"shuffle_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.squeeze_grouped_nb","text":"squeeze_grouped_nb ( a , group_lens , squeeze_func_nb , * args ) Squeeze each group of columns into a single column using squeeze_func_nb . squeeze_func_nb should accept index of the row, index of the group, the array, and *args . Should return a single value.","title":"squeeze_grouped_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.std_reduce_nb","text":"std_reduce_nb ( col , a , ddof ) Return std (ignores NaNs).","title":"std_reduce_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.sum_reduce_nb","text":"sum_reduce_nb ( col , a ) Return sum (ignores NaNs).","title":"sum_reduce_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.sum_squeeze_nb","text":"sum_squeeze_nb ( col , group , a ) Return sum (ignores NaNs) of a group.","title":"sum_squeeze_nb()"},{"location":"api/generic/nb/#vectorbt.generic.nb.value_counts_nb","text":"value_counts_nb ( codes , n_uniques , group_lens ) Return value counts per column/group.","title":"value_counts_nb()"},{"location":"api/generic/plots_builder/","text":"plots_builder module \u00b6 Mixin for building plots out of subplots. MetaPlotsBuilderMixin class \u00b6 Meta class that exposes a read-only class property PlotsBuilderMixin.subplots . Superclasses builtins.type Subclasses MetaData MetaGenericAccessor MetaIndicatorBase MetaMappedArray MetaPortfolio MetaRecords subplots property \u00b6 Subplots supported by PlotsBuilderMixin.plots() . PlotsBuilderMixin class \u00b6 Mixin that implements PlotsBuilderMixin.plots() . Required to be a subclass of Wrapping . Subclasses Data GenericAccessor IndicatorBase MappedArray Portfolio Records build_subplots_doc class method \u00b6 PlotsBuilderMixin . build_subplots_doc ( source_cls = None ) Build subplots documentation. override_subplots_doc class method \u00b6 PlotsBuilderMixin . override_subplots_doc ( __pdoc__ , source_cls = None ) Call this method on each subclass that overrides subplots . plots method \u00b6 PlotsBuilderMixin . plots ( subplots = None , tags = None , column = None , group_by = None , silence_warnings = None , template_mapping = None , settings = None , filters = None , subplot_settings = None , show_titles = None , hide_id_labels = None , group_id_labels = None , make_subplots_kwargs = None , ** layout_kwargs ) Plot various parts of this object. Args subplots :\u2002 str , tuple , iterable , or dict Subplots to plot. Each element can be either: a subplot name (see keys in PlotsBuilderMixin.subplots ) a tuple of a subplot name and a settings dict as in PlotsBuilderMixin.subplots . The settings dict can contain the following keys: title : Title of the subplot. Defaults to the name. plot_func (required): Plotting function for custom subplots. Should write the supplied figure fig in-place and can return anything (it won't be used). xaxis_kwargs : Layout keyword arguments for the x-axis. Defaults to dict(title='Index') . yaxis_kwargs : Layout keyword arguments for the y-axis. Defaults to empty dict. tags , check_{filter} , inv_check_{filter} , resolve_plot_func , pass_{arg} , resolve_path_{arg} , resolve_{arg} and template_mapping : The same as in StatsBuilderMixin for calc_func . Any other keyword argument that overrides the settings or is passed directly to plot_func . If resolve_plot_func is True, the plotting function may \"request\" any of the following arguments by accepting them or if pass_{arg} was found in the settings dict: Each of AttrResolver.self_aliases : original object (ungrouped, with no column selected) group_by : won't be passed if it was used in resolving the first attribute of plot_func specified as a path, use pass_group_by=True to pass anyway column subplot_name trace_names : list with the subplot name, can't be used in templates add_trace_kwargs : dict with subplot row and column index xref yref xaxis yaxis x_domain y_domain fig silence_warnings Any argument from settings Any attribute of this object if it meant to be resolved (see AttrResolver.resolve_attr() ) Note Layout-related resolution arguments such as add_trace_kwargs are unavailable before filtering and thus cannot be used in any templates but can still be overridden. Pass subplots='all' to plot all supported subplots. tags :\u2002 str or iterable See tags in StatsBuilderMixin . column :\u2002 str See column in StatsBuilderMixin . group_by :\u2002 any See group_by in StatsBuilderMixin . silence_warnings :\u2002 bool See silence_warnings in StatsBuilderMixin . template_mapping :\u2002 mapping See template_mapping in StatsBuilderMixin . Applied on settings , make_subplots_kwargs , and layout_kwargs , and then on each subplot settings. filters :\u2002 dict See filters in StatsBuilderMixin . settings :\u2002 dict See settings in StatsBuilderMixin . subplot_settings :\u2002 dict See metric_settings in StatsBuilderMixin . show_titles :\u2002 bool Whether to show the title of each subplot. hide_id_labels :\u2002 bool Whether to hide identical legend labels. Two labels are identical if their name, marker style and line style match. group_id_labels :\u2002 bool Whether to group identical legend labels. make_subplots_kwargs :\u2002 dict Keyword arguments passed to plotly.subplots.make_subplots . **layout_kwargs Keyword arguments used to update the layout of the figure. Note PlotsBuilderMixin and StatsBuilderMixin are very similar. Some artifacts follow the same concept, just named differently: plots_defaults vs stats_defaults subplots vs metrics subplot_settings vs metric_settings See further notes under StatsBuilderMixin . Usage See vectorbt.portfolio.base for examples. plots_defaults property \u00b6 Defaults for PlotsBuilderMixin.plots() . See plots_builder in settings . subplots class variable \u00b6 Subplots supported by PlotsBuilderMixin . Co nf ig( {} ) Returns PlotsBuilderMixin._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable PlotsBuilderMixin._subplots . writeable_attrs property \u00b6 Set of writeable attributes that will be saved/copied along with the config.","title":"plots_builder"},{"location":"api/generic/plots_builder/#vectorbt.generic.plots_builder","text":"Mixin for building plots out of subplots.","title":"vectorbt.generic.plots_builder"},{"location":"api/generic/plots_builder/#vectorbt.generic.plots_builder.MetaPlotsBuilderMixin","text":"Meta class that exposes a read-only class property PlotsBuilderMixin.subplots . Superclasses builtins.type Subclasses MetaData MetaGenericAccessor MetaIndicatorBase MetaMappedArray MetaPortfolio MetaRecords","title":"MetaPlotsBuilderMixin"},{"location":"api/generic/plots_builder/#vectorbt.generic.plots_builder.MetaPlotsBuilderMixin.subplots","text":"Subplots supported by PlotsBuilderMixin.plots() .","title":"subplots"},{"location":"api/generic/plots_builder/#vectorbt.generic.plots_builder.PlotsBuilderMixin","text":"Mixin that implements PlotsBuilderMixin.plots() . Required to be a subclass of Wrapping . Subclasses Data GenericAccessor IndicatorBase MappedArray Portfolio Records","title":"PlotsBuilderMixin"},{"location":"api/generic/plots_builder/#vectorbt.generic.plots_builder.PlotsBuilderMixin.build_subplots_doc","text":"PlotsBuilderMixin . build_subplots_doc ( source_cls = None ) Build subplots documentation.","title":"build_subplots_doc()"},{"location":"api/generic/plots_builder/#vectorbt.generic.plots_builder.PlotsBuilderMixin.override_subplots_doc","text":"PlotsBuilderMixin . override_subplots_doc ( __pdoc__ , source_cls = None ) Call this method on each subclass that overrides subplots .","title":"override_subplots_doc()"},{"location":"api/generic/plots_builder/#vectorbt.generic.plots_builder.PlotsBuilderMixin.plots","text":"PlotsBuilderMixin . plots ( subplots = None , tags = None , column = None , group_by = None , silence_warnings = None , template_mapping = None , settings = None , filters = None , subplot_settings = None , show_titles = None , hide_id_labels = None , group_id_labels = None , make_subplots_kwargs = None , ** layout_kwargs ) Plot various parts of this object. Args subplots :\u2002 str , tuple , iterable , or dict Subplots to plot. Each element can be either: a subplot name (see keys in PlotsBuilderMixin.subplots ) a tuple of a subplot name and a settings dict as in PlotsBuilderMixin.subplots . The settings dict can contain the following keys: title : Title of the subplot. Defaults to the name. plot_func (required): Plotting function for custom subplots. Should write the supplied figure fig in-place and can return anything (it won't be used). xaxis_kwargs : Layout keyword arguments for the x-axis. Defaults to dict(title='Index') . yaxis_kwargs : Layout keyword arguments for the y-axis. Defaults to empty dict. tags , check_{filter} , inv_check_{filter} , resolve_plot_func , pass_{arg} , resolve_path_{arg} , resolve_{arg} and template_mapping : The same as in StatsBuilderMixin for calc_func . Any other keyword argument that overrides the settings or is passed directly to plot_func . If resolve_plot_func is True, the plotting function may \"request\" any of the following arguments by accepting them or if pass_{arg} was found in the settings dict: Each of AttrResolver.self_aliases : original object (ungrouped, with no column selected) group_by : won't be passed if it was used in resolving the first attribute of plot_func specified as a path, use pass_group_by=True to pass anyway column subplot_name trace_names : list with the subplot name, can't be used in templates add_trace_kwargs : dict with subplot row and column index xref yref xaxis yaxis x_domain y_domain fig silence_warnings Any argument from settings Any attribute of this object if it meant to be resolved (see AttrResolver.resolve_attr() ) Note Layout-related resolution arguments such as add_trace_kwargs are unavailable before filtering and thus cannot be used in any templates but can still be overridden. Pass subplots='all' to plot all supported subplots. tags :\u2002 str or iterable See tags in StatsBuilderMixin . column :\u2002 str See column in StatsBuilderMixin . group_by :\u2002 any See group_by in StatsBuilderMixin . silence_warnings :\u2002 bool See silence_warnings in StatsBuilderMixin . template_mapping :\u2002 mapping See template_mapping in StatsBuilderMixin . Applied on settings , make_subplots_kwargs , and layout_kwargs , and then on each subplot settings. filters :\u2002 dict See filters in StatsBuilderMixin . settings :\u2002 dict See settings in StatsBuilderMixin . subplot_settings :\u2002 dict See metric_settings in StatsBuilderMixin . show_titles :\u2002 bool Whether to show the title of each subplot. hide_id_labels :\u2002 bool Whether to hide identical legend labels. Two labels are identical if their name, marker style and line style match. group_id_labels :\u2002 bool Whether to group identical legend labels. make_subplots_kwargs :\u2002 dict Keyword arguments passed to plotly.subplots.make_subplots . **layout_kwargs Keyword arguments used to update the layout of the figure. Note PlotsBuilderMixin and StatsBuilderMixin are very similar. Some artifacts follow the same concept, just named differently: plots_defaults vs stats_defaults subplots vs metrics subplot_settings vs metric_settings See further notes under StatsBuilderMixin . Usage See vectorbt.portfolio.base for examples.","title":"plots()"},{"location":"api/generic/plots_builder/#vectorbt.generic.plots_builder.PlotsBuilderMixin.plots_defaults","text":"Defaults for PlotsBuilderMixin.plots() . See plots_builder in settings .","title":"plots_defaults"},{"location":"api/generic/plots_builder/#vectorbt.generic.plots_builder.PlotsBuilderMixin.subplots","text":"Subplots supported by PlotsBuilderMixin . Co nf ig( {} ) Returns PlotsBuilderMixin._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable PlotsBuilderMixin._subplots .","title":"subplots"},{"location":"api/generic/plots_builder/#vectorbt.generic.plots_builder.PlotsBuilderMixin.writeable_attrs","text":"Set of writeable attributes that will be saved/copied along with the config.","title":"writeable_attrs"},{"location":"api/generic/plotting/","text":"plotting module \u00b6 Base plotting functions. Provides functions for visualizing data in an efficient and convenient way. Each creates a figure widget that is compatible with ipywidgets and enables interactive data visualization in Jupyter Notebook and JupyterLab environments. For more details on using Plotly, see Getting Started with Plotly in Python . The module can be accessed directly via vbt.plotting . Warning In case of errors, it won't be visible in the notebook cell, but in the logs. clean_labels function \u00b6 clean_labels ( labels ) Clean labels. Plotly doesn't support multi-indexes. Bar class \u00b6 Class with an initialization config. All subclasses of Configured are initialized using Config , which makes it easier to pickle. Settings are defined under configured in settings . Warning If any attribute has been overwritten that isn't listed in Configured.writeable_attrs , or if any Configured.__init__ argument depends upon global defaults, their values won't be copied over. Make sure to pass them explicitly to make the saved & loaded / copied instance resilient to changes in globals. Create a bar plot. Args data :\u2002 array_like Data in any format that can be converted to NumPy. Must be of shape ( x_labels , trace_names ). trace_names :\u2002 str or list of str Trace names, corresponding to columns in pandas. x_labels :\u2002 array_like X-axis labels, corresponding to index in pandas. trace_kwargs :\u2002 dict or list of dict Keyword arguments passed to plotly.graph_objects.Bar . Can be specified per trace as a sequence of dicts. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import vectorbt as vbt >>> bar = vbt . plotting . Bar ( ... data = [[ 1 , 2 ], [ 3 , 4 ]], ... trace_names = [ 'a' , 'b' ], ... x_labels = [ 'x' , 'y' ] ... ) >>> bar . fig Superclasses Configured Documented Pickleable TraceUpdater Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() TraceUpdater.fig TraceUpdater.traces update method \u00b6 Bar . update ( data ) Update the trace data. Usage >>> bar . update ([[ 2 , 1 ], [ 4 , 3 ]]) >>> bar . fig Box class \u00b6 Class with an initialization config. All subclasses of Configured are initialized using Config , which makes it easier to pickle. Settings are defined under configured in settings . Warning If any attribute has been overwritten that isn't listed in Configured.writeable_attrs , or if any Configured.__init__ argument depends upon global defaults, their values won't be copied over. Make sure to pass them explicitly to make the saved & loaded / copied instance resilient to changes in globals. Create a box plot. For keyword arguments, see Histogram . Usage >>> import vectorbt as vbt >>> box = vbt . plotting . Box ( ... data = [[ 1 , 2 ], [ 3 , 4 ], [ 2 , 1 ]], ... trace_names = [ 'a' , 'b' ] ... ) >>> box . fig Superclasses Configured Documented Pickleable TraceUpdater Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() TraceUpdater.fig TraceUpdater.traces TraceUpdater.update() from_quantile property \u00b6 Filter out data points before this quantile. horizontal property \u00b6 Whether to plot horizontally. remove_nan property \u00b6 Whether to remove NaN values. to_quantile property \u00b6 Filter out data points after this quantile. Gauge class \u00b6 Class with an initialization config. All subclasses of Configured are initialized using Config , which makes it easier to pickle. Settings are defined under configured in settings . Warning If any attribute has been overwritten that isn't listed in Configured.writeable_attrs , or if any Configured.__init__ argument depends upon global defaults, their values won't be copied over. Make sure to pass them explicitly to make the saved & loaded / copied instance resilient to changes in globals. Create a gauge plot. Args value :\u2002 float The value to be displayed. label :\u2002 str The label to be displayed. value_range :\u2002 tuple of float The value range of the gauge. cmap_name :\u2002 str A matplotlib-compatible colormap name. See the list of available colormaps . trace_kwargs :\u2002 dict Keyword arguments passed to the plotly.graph_objects.Indicator . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import vectorbt as vbt >>> gauge = vbt . plotting . Gauge ( ... value = 2 , ... value_range = ( 1 , 3 ), ... label = 'My Gauge' ... ) >>> gauge . fig Superclasses Configured Documented Pickleable TraceUpdater Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() TraceUpdater.fig TraceUpdater.traces TraceUpdater.update() cmap_name property \u00b6 A matplotlib-compatible colormap name. value_range property \u00b6 The value range of the gauge. Heatmap class \u00b6 Class with an initialization config. All subclasses of Configured are initialized using Config , which makes it easier to pickle. Settings are defined under configured in settings . Warning If any attribute has been overwritten that isn't listed in Configured.writeable_attrs , or if any Configured.__init__ argument depends upon global defaults, their values won't be copied over. Make sure to pass them explicitly to make the saved & loaded / copied instance resilient to changes in globals. Create a heatmap plot. Args data :\u2002 array_like Data in any format that can be converted to NumPy. Must be of shape ( y_labels , x_labels ). x_labels :\u2002 array_like X-axis labels, corresponding to columns in pandas. y_labels :\u2002 array_like Y-axis labels, corresponding to index in pandas. is_x_category :\u2002 bool Whether X-axis is a categorical axis. is_y_category :\u2002 bool Whether Y-axis is a categorical axis. trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Heatmap . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import vectorbt as vbt >>> heatmap = vbt . plotting . Heatmap ( ... data = [[ 1 , 2 ], [ 3 , 4 ]], ... x_labels = [ 'a' , 'b' ], ... y_labels = [ 'x' , 'y' ] ... ) >>> heatmap . fig Superclasses Configured Documented Pickleable TraceUpdater Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() TraceUpdater.fig TraceUpdater.traces TraceUpdater.update() Histogram class \u00b6 Class with an initialization config. All subclasses of Configured are initialized using Config , which makes it easier to pickle. Settings are defined under configured in settings . Warning If any attribute has been overwritten that isn't listed in Configured.writeable_attrs , or if any Configured.__init__ argument depends upon global defaults, their values won't be copied over. Make sure to pass them explicitly to make the saved & loaded / copied instance resilient to changes in globals. Create a histogram plot. Args data :\u2002 array_like Data in any format that can be converted to NumPy. Must be of shape (any, trace_names ). trace_names :\u2002 str or list of str Trace names, corresponding to columns in pandas. horizontal :\u2002 bool Whether to plot horizontally. remove_nan :\u2002 bool Whether to remove NaN values. from_quantile :\u2002 float Filter out data points before this quantile. Should be in range [0, 1] . to_quantile :\u2002 float Filter out data points after this quantile. Should be in range [0, 1] . trace_kwargs :\u2002 dict or list of dict Keyword arguments passed to plotly.graph_objects.Histogram . Can be specified per trace as a sequence of dicts. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import vectorbt as vbt >>> hist = vbt . plotting . Histogram ( ... data = [[ 1 , 2 ], [ 3 , 4 ], [ 2 , 1 ]], ... trace_names = [ 'a' , 'b' ] ... ) >>> hist . fig Superclasses Configured Documented Pickleable TraceUpdater Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() TraceUpdater.fig TraceUpdater.traces TraceUpdater.update() from_quantile property \u00b6 Filter out data points before this quantile. horizontal property \u00b6 Whether to plot horizontally. remove_nan property \u00b6 Whether to remove NaN values. to_quantile property \u00b6 Filter out data points after this quantile. Scatter class \u00b6 Class with an initialization config. All subclasses of Configured are initialized using Config , which makes it easier to pickle. Settings are defined under configured in settings . Warning If any attribute has been overwritten that isn't listed in Configured.writeable_attrs , or if any Configured.__init__ argument depends upon global defaults, their values won't be copied over. Make sure to pass them explicitly to make the saved & loaded / copied instance resilient to changes in globals. Create a scatter plot. Args data :\u2002 array_like Data in any format that can be converted to NumPy. Must be of shape ( x_labels , trace_names ). trace_names :\u2002 str or list of str Trace names, corresponding to columns in pandas. x_labels :\u2002 array_like X-axis labels, corresponding to index in pandas. trace_kwargs :\u2002 dict or list of dict Keyword arguments passed to plotly.graph_objects.Scatter . Can be specified per trace as a sequence of dicts. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import vectorbt as vbt >>> scatter = vbt . plotting . Scatter ( ... data = [[ 1 , 2 ], [ 3 , 4 ]], ... trace_names = [ 'a' , 'b' ], ... x_labels = [ 'x' , 'y' ] ... ) >>> scatter . fig Superclasses Configured Documented Pickleable TraceUpdater Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() TraceUpdater.fig TraceUpdater.traces TraceUpdater.update() TraceUpdater class \u00b6 Base trace updating class. Subclasses Bar Box Gauge Heatmap Histogram Scatter Volume fig property \u00b6 Figure. traces property \u00b6 Traces to update. update method \u00b6 TraceUpdater . update ( * args , ** kwargs ) Update the trace data. Volume class \u00b6 Class with an initialization config. All subclasses of Configured are initialized using Config , which makes it easier to pickle. Settings are defined under configured in settings . Warning If any attribute has been overwritten that isn't listed in Configured.writeable_attrs , or if any Configured.__init__ argument depends upon global defaults, their values won't be copied over. Make sure to pass them explicitly to make the saved & loaded / copied instance resilient to changes in globals. Create a volume plot. Args data :\u2002 array_like Data in any format that can be converted to NumPy. Must be a 3-dim array. x_labels :\u2002 array_like X-axis labels. y_labels :\u2002 array_like Y-axis labels. z_labels :\u2002 array_like Z-axis labels. trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Volume . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . scene_name :\u2002 str Reference to the 3D scene. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Note Figure widgets have currently problems displaying NaNs. Use .show() method for rendering. Usage >>> import vectorbt as vbt >>> import numpy as np >>> volume = vbt . plotting . Volume ( ... data = np . random . randint ( 1 , 10 , size = ( 3 , 3 , 3 )), ... x_labels = [ 'a' , 'b' , 'c' ], ... y_labels = [ 'd' , 'e' , 'f' ], ... z_labels = [ 'g' , 'h' , 'i' ] ... ) >>> volume . fig Superclasses Configured Documented Pickleable TraceUpdater Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() TraceUpdater.fig TraceUpdater.traces TraceUpdater.update()","title":"plotting"},{"location":"api/generic/plotting/#vectorbt.generic.plotting","text":"Base plotting functions. Provides functions for visualizing data in an efficient and convenient way. Each creates a figure widget that is compatible with ipywidgets and enables interactive data visualization in Jupyter Notebook and JupyterLab environments. For more details on using Plotly, see Getting Started with Plotly in Python . The module can be accessed directly via vbt.plotting . Warning In case of errors, it won't be visible in the notebook cell, but in the logs.","title":"vectorbt.generic.plotting"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.clean_labels","text":"clean_labels ( labels ) Clean labels. Plotly doesn't support multi-indexes.","title":"clean_labels()"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Bar","text":"Class with an initialization config. All subclasses of Configured are initialized using Config , which makes it easier to pickle. Settings are defined under configured in settings . Warning If any attribute has been overwritten that isn't listed in Configured.writeable_attrs , or if any Configured.__init__ argument depends upon global defaults, their values won't be copied over. Make sure to pass them explicitly to make the saved & loaded / copied instance resilient to changes in globals. Create a bar plot. Args data :\u2002 array_like Data in any format that can be converted to NumPy. Must be of shape ( x_labels , trace_names ). trace_names :\u2002 str or list of str Trace names, corresponding to columns in pandas. x_labels :\u2002 array_like X-axis labels, corresponding to index in pandas. trace_kwargs :\u2002 dict or list of dict Keyword arguments passed to plotly.graph_objects.Bar . Can be specified per trace as a sequence of dicts. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import vectorbt as vbt >>> bar = vbt . plotting . Bar ( ... data = [[ 1 , 2 ], [ 3 , 4 ]], ... trace_names = [ 'a' , 'b' ], ... x_labels = [ 'x' , 'y' ] ... ) >>> bar . fig Superclasses Configured Documented Pickleable TraceUpdater Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() TraceUpdater.fig TraceUpdater.traces","title":"Bar"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Bar.update","text":"Bar . update ( data ) Update the trace data. Usage >>> bar . update ([[ 2 , 1 ], [ 4 , 3 ]]) >>> bar . fig","title":"update()"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Box","text":"Class with an initialization config. All subclasses of Configured are initialized using Config , which makes it easier to pickle. Settings are defined under configured in settings . Warning If any attribute has been overwritten that isn't listed in Configured.writeable_attrs , or if any Configured.__init__ argument depends upon global defaults, their values won't be copied over. Make sure to pass them explicitly to make the saved & loaded / copied instance resilient to changes in globals. Create a box plot. For keyword arguments, see Histogram . Usage >>> import vectorbt as vbt >>> box = vbt . plotting . Box ( ... data = [[ 1 , 2 ], [ 3 , 4 ], [ 2 , 1 ]], ... trace_names = [ 'a' , 'b' ] ... ) >>> box . fig Superclasses Configured Documented Pickleable TraceUpdater Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() TraceUpdater.fig TraceUpdater.traces TraceUpdater.update()","title":"Box"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Box.from_quantile","text":"Filter out data points before this quantile.","title":"from_quantile"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Box.horizontal","text":"Whether to plot horizontally.","title":"horizontal"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Box.remove_nan","text":"Whether to remove NaN values.","title":"remove_nan"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Box.to_quantile","text":"Filter out data points after this quantile.","title":"to_quantile"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Gauge","text":"Class with an initialization config. All subclasses of Configured are initialized using Config , which makes it easier to pickle. Settings are defined under configured in settings . Warning If any attribute has been overwritten that isn't listed in Configured.writeable_attrs , or if any Configured.__init__ argument depends upon global defaults, their values won't be copied over. Make sure to pass them explicitly to make the saved & loaded / copied instance resilient to changes in globals. Create a gauge plot. Args value :\u2002 float The value to be displayed. label :\u2002 str The label to be displayed. value_range :\u2002 tuple of float The value range of the gauge. cmap_name :\u2002 str A matplotlib-compatible colormap name. See the list of available colormaps . trace_kwargs :\u2002 dict Keyword arguments passed to the plotly.graph_objects.Indicator . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import vectorbt as vbt >>> gauge = vbt . plotting . Gauge ( ... value = 2 , ... value_range = ( 1 , 3 ), ... label = 'My Gauge' ... ) >>> gauge . fig Superclasses Configured Documented Pickleable TraceUpdater Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() TraceUpdater.fig TraceUpdater.traces TraceUpdater.update()","title":"Gauge"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Gauge.cmap_name","text":"A matplotlib-compatible colormap name.","title":"cmap_name"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Gauge.value_range","text":"The value range of the gauge.","title":"value_range"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Heatmap","text":"Class with an initialization config. All subclasses of Configured are initialized using Config , which makes it easier to pickle. Settings are defined under configured in settings . Warning If any attribute has been overwritten that isn't listed in Configured.writeable_attrs , or if any Configured.__init__ argument depends upon global defaults, their values won't be copied over. Make sure to pass them explicitly to make the saved & loaded / copied instance resilient to changes in globals. Create a heatmap plot. Args data :\u2002 array_like Data in any format that can be converted to NumPy. Must be of shape ( y_labels , x_labels ). x_labels :\u2002 array_like X-axis labels, corresponding to columns in pandas. y_labels :\u2002 array_like Y-axis labels, corresponding to index in pandas. is_x_category :\u2002 bool Whether X-axis is a categorical axis. is_y_category :\u2002 bool Whether Y-axis is a categorical axis. trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Heatmap . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import vectorbt as vbt >>> heatmap = vbt . plotting . Heatmap ( ... data = [[ 1 , 2 ], [ 3 , 4 ]], ... x_labels = [ 'a' , 'b' ], ... y_labels = [ 'x' , 'y' ] ... ) >>> heatmap . fig Superclasses Configured Documented Pickleable TraceUpdater Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() TraceUpdater.fig TraceUpdater.traces TraceUpdater.update()","title":"Heatmap"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Histogram","text":"Class with an initialization config. All subclasses of Configured are initialized using Config , which makes it easier to pickle. Settings are defined under configured in settings . Warning If any attribute has been overwritten that isn't listed in Configured.writeable_attrs , or if any Configured.__init__ argument depends upon global defaults, their values won't be copied over. Make sure to pass them explicitly to make the saved & loaded / copied instance resilient to changes in globals. Create a histogram plot. Args data :\u2002 array_like Data in any format that can be converted to NumPy. Must be of shape (any, trace_names ). trace_names :\u2002 str or list of str Trace names, corresponding to columns in pandas. horizontal :\u2002 bool Whether to plot horizontally. remove_nan :\u2002 bool Whether to remove NaN values. from_quantile :\u2002 float Filter out data points before this quantile. Should be in range [0, 1] . to_quantile :\u2002 float Filter out data points after this quantile. Should be in range [0, 1] . trace_kwargs :\u2002 dict or list of dict Keyword arguments passed to plotly.graph_objects.Histogram . Can be specified per trace as a sequence of dicts. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import vectorbt as vbt >>> hist = vbt . plotting . Histogram ( ... data = [[ 1 , 2 ], [ 3 , 4 ], [ 2 , 1 ]], ... trace_names = [ 'a' , 'b' ] ... ) >>> hist . fig Superclasses Configured Documented Pickleable TraceUpdater Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() TraceUpdater.fig TraceUpdater.traces TraceUpdater.update()","title":"Histogram"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Histogram.from_quantile","text":"Filter out data points before this quantile.","title":"from_quantile"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Histogram.horizontal","text":"Whether to plot horizontally.","title":"horizontal"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Histogram.remove_nan","text":"Whether to remove NaN values.","title":"remove_nan"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Histogram.to_quantile","text":"Filter out data points after this quantile.","title":"to_quantile"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Scatter","text":"Class with an initialization config. All subclasses of Configured are initialized using Config , which makes it easier to pickle. Settings are defined under configured in settings . Warning If any attribute has been overwritten that isn't listed in Configured.writeable_attrs , or if any Configured.__init__ argument depends upon global defaults, their values won't be copied over. Make sure to pass them explicitly to make the saved & loaded / copied instance resilient to changes in globals. Create a scatter plot. Args data :\u2002 array_like Data in any format that can be converted to NumPy. Must be of shape ( x_labels , trace_names ). trace_names :\u2002 str or list of str Trace names, corresponding to columns in pandas. x_labels :\u2002 array_like X-axis labels, corresponding to index in pandas. trace_kwargs :\u2002 dict or list of dict Keyword arguments passed to plotly.graph_objects.Scatter . Can be specified per trace as a sequence of dicts. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import vectorbt as vbt >>> scatter = vbt . plotting . Scatter ( ... data = [[ 1 , 2 ], [ 3 , 4 ]], ... trace_names = [ 'a' , 'b' ], ... x_labels = [ 'x' , 'y' ] ... ) >>> scatter . fig Superclasses Configured Documented Pickleable TraceUpdater Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() TraceUpdater.fig TraceUpdater.traces TraceUpdater.update()","title":"Scatter"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.TraceUpdater","text":"Base trace updating class. Subclasses Bar Box Gauge Heatmap Histogram Scatter Volume","title":"TraceUpdater"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.TraceUpdater.fig","text":"Figure.","title":"fig"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.TraceUpdater.traces","text":"Traces to update.","title":"traces"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.TraceUpdater.update","text":"TraceUpdater . update ( * args , ** kwargs ) Update the trace data.","title":"update()"},{"location":"api/generic/plotting/#vectorbt.generic.plotting.Volume","text":"Class with an initialization config. All subclasses of Configured are initialized using Config , which makes it easier to pickle. Settings are defined under configured in settings . Warning If any attribute has been overwritten that isn't listed in Configured.writeable_attrs , or if any Configured.__init__ argument depends upon global defaults, their values won't be copied over. Make sure to pass them explicitly to make the saved & loaded / copied instance resilient to changes in globals. Create a volume plot. Args data :\u2002 array_like Data in any format that can be converted to NumPy. Must be a 3-dim array. x_labels :\u2002 array_like X-axis labels. y_labels :\u2002 array_like Y-axis labels. z_labels :\u2002 array_like Z-axis labels. trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Volume . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . scene_name :\u2002 str Reference to the 3D scene. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Note Figure widgets have currently problems displaying NaNs. Use .show() method for rendering. Usage >>> import vectorbt as vbt >>> import numpy as np >>> volume = vbt . plotting . Volume ( ... data = np . random . randint ( 1 , 10 , size = ( 3 , 3 , 3 )), ... x_labels = [ 'a' , 'b' , 'c' ], ... y_labels = [ 'd' , 'e' , 'f' ], ... z_labels = [ 'g' , 'h' , 'i' ] ... ) >>> volume . fig Superclasses Configured Documented Pickleable TraceUpdater Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() TraceUpdater.fig TraceUpdater.traces TraceUpdater.update()","title":"Volume"},{"location":"api/generic/ranges/","text":"ranges module \u00b6 Base class for working with range records. Range records capture information on ranges. They are useful for analyzing duration of processes, such as drawdowns, trades, and positions. They also come in handy when analyzing distance between events, such as entry and exit signals. Each range has a starting point and an ending point. For example, the points for range(20) are 0 and 20 (not 19!) respectively. Note Be aware that if a range hasn't ended in a column, its end_idx will point at the latest index. Make sure to account for this when computing custom metrics involving duration. >>> import vectorbt as vbt >>> import numpy as np >>> import pandas as pd >>> start = '2019-01-01 UTC' # crypto is in UTC >>> end = '2020-01-01 UTC' >>> price = vbt . YFData . download ( 'BTC-USD' , start = start , end = end ) . get ( 'Close' ) >>> fast_ma = vbt . MA . run ( price , 10 ) >>> slow_ma = vbt . MA . run ( price , 50 ) >>> fast_below_slow = fast_ma . ma_above ( slow_ma ) >>> ranges = vbt . Ranges . from_ts ( fast_below_slow , wrapper_kwargs = dict ( freq = 'd' )) >>> ranges . records_readable Range Id Column Start Timestamp End Timestamp \\ 0 0 0 2019-02-19 00:00:00+00:00 2019-07-25 00:00:00+00:00 1 1 0 2019-08-08 00:00:00+00:00 2019-08-19 00:00:00+00:00 2 2 0 2019-11-01 00:00:00+00:00 2019-11-20 00:00:00+00:00 Status 0 Closed 1 Closed 2 Closed >>> ranges . duration . max ( wrap_kwargs = dict ( to_timedelta = True )) Timedelta('74 days 00:00:00') From accessors \u00b6 Moreover, all generic accessors have a property ranges and a method get_ranges : >>> # vectorbt.generic.accessors.GenericAccessor.ranges.coverage >>> fast_below_slow . vbt . ranges . coverage () 0.35792349726775957 Stats \u00b6 Hint See StatsBuilderMixin.stats() and Ranges.metrics . >>> df = pd . DataFrame ({ ... 'a' : [ 1 , 2 , np . nan , np . nan , 5 , 6 ], ... 'b' : [ np . nan , 2 , np . nan , 4 , np . nan , 6 ] ... }) >>> ranges = df . vbt ( freq = 'd' ) . ranges >>> ranges [ 'a' ] . stats () Start 0 End 5 Period 6 days 00:00:00 Total Records 2 Coverage 4 days 00:00:00 Overlap Coverage 0 days 00:00:00 Duration: Min 2 days 00:00:00 Duration: Median 2 days 00:00:00 Duration: Max 2 days 00:00:00 Duration: Mean 2 days 00:00:00 Duration: Std 0 days 00:00:00 Name: a, dtype: object StatsBuilderMixin.stats() also supports (re-)grouping: >>> ranges . stats ( group_by = True ) Start 0 End 5 Period 6 days 00:00:00 Total Records 5 Coverage 5 days 00:00:00 Overlap Coverage 2 days 00:00:00 Duration: Min 1 days 00:00:00 Duration: Median 1 days 00:00:00 Duration: Max 2 days 00:00:00 Duration: Mean 1 days 09:36:00 Duration: Std 0 days 13:08:43.228968446 Name: group, dtype: object Plots \u00b6 Hint See PlotsBuilderMixin.plots() and Ranges.subplots . Ranges class has a single subplot based on Ranges.plot() : >>> ranges [ 'a' ] . plots () ranges_attach_field_config Config \u00b6 Config of fields to be attached to Ranges . Co nf ig( { \"status\" : { \"attach_filters\" : true } } ) ranges_field_config Config \u00b6 Field config for Ranges . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"col\" : \"int64\" , \"start_idx\" : \"int64\" , \"end_idx\" : \"int64\" , \"status\" : \"int64\" }, \"settings\" : { \"id\" : { \"title\" : \"Range Id\" }, \"idx\" : { \"name\" : \"end_idx\" }, \"start_idx\" : { \"title\" : \"Start Timestamp\" , \"mapping\" : \"index\" }, \"end_idx\" : { \"title\" : \"End Timestamp\" , \"mapping\" : \"index\" }, \"status\" : { \"title\" : \"Status\" , \"mapping\" : { \"Open\" : 0 , \"Closed\" : 1 } } } } ) Ranges class \u00b6 Extends Records for working with range records. Requires records_arr to have all fields defined in range_dt . Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin Records RecordsWithFields StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() Records.apply() Records.apply_mask() Records.build_field_config_doc() Records.col_arr Records.col_mapper Records.config Records.count() Records.get_apply_mapping_arr() Records.get_by_col_idxs() Records.get_field_arr() Records.get_field_mapping() Records.get_field_name() Records.get_field_setting() Records.get_field_title() Records.get_map_field() Records.get_map_field_to_index() Records.id_arr Records.idx_arr Records.iloc Records.indexing_func_meta() Records.indexing_kwargs Records.is_sorted() Records.loc Records.map() Records.map_array() Records.map_field() Records.override_field_config_doc() Records.records Records.records_arr Records.records_readable Records.replace() Records.self_aliases Records.sort() Records.values Records.wrapper Records.writeable_attrs StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses Drawdowns Trades avg_duration method \u00b6 Ranges . avg_duration ( group_by = None , wrap_kwargs = None , ** kwargs ) Average range duration (as timedelta). closed method \u00b6 Records filtered by status == 1 . col method \u00b6 Mapped array of the field col . coverage method \u00b6 Ranges . coverage ( overlapping = False , normalize = True , group_by = None , wrap_kwargs = None ) Coverage, that is, the number of steps that are covered by all ranges. See range_coverage_nb() . duration method \u00b6 Duration of each range (in raw format). end_idx method \u00b6 Mapped array of the field end_idx . field_config class variable \u00b6 Field config of Ranges . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"col\" : \"int64\" , \"start_idx\" : \"int64\" , \"end_idx\" : \"int64\" , \"status\" : \"int64\" }, \"settings\" : { \"id\" : { \"name\" : \"id\" , \"title\" : \"Range Id\" }, \"col\" : { \"name\" : \"col\" , \"title\" : \"Column\" , \"mapping\" : \"columns\" }, \"idx\" : { \"name\" : \"end_idx\" , \"title\" : \"Timestamp\" , \"mapping\" : \"index\" }, \"start_idx\" : { \"title\" : \"Start Timestamp\" , \"mapping\" : \"index\" }, \"end_idx\" : { \"title\" : \"End Timestamp\" , \"mapping\" : \"index\" }, \"status\" : { \"title\" : \"Status\" , \"mapping\" : { \"Open\" : 0 , \"Closed\" : 1 } } } } ) from_ts class method \u00b6 Ranges . from_ts ( ts , gap_value = None , attach_ts = True , wrapper_kwargs = None , ** kwargs ) Build Ranges from time series ts . Searches for sequences of True values in boolean data (False acts as a gap), positive values in integer data (-1 acts as a gap), and non-NaN values in any other data (NaN acts as a gap). **kwargs will be passed to Ranges . id method \u00b6 Mapped array of the field id . indexing_func method \u00b6 Ranges . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on Ranges . max_duration method \u00b6 Ranges . max_duration ( group_by = None , wrap_kwargs = None , ** kwargs ) Maximum range duration (as timedelta). metrics class variable \u00b6 Metrics supported by Ranges . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function Ranges.<lambda> at 0x7fac881e1c80>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function Ranges.<lambda> at 0x7fac881e1d08>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function Ranges.<lambda> at 0x7fac881e1d90>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"coverage\" : { \"title\" : \"Coverage\" , \"calc_func\" : \"coverage\" , \"overlapping\" : false , \"normalize\" : false , \"apply_to_timedelta\" : true , \"tags\" : [ \"ranges\" , \"coverage\" ] }, \"overlap_coverage\" : { \"title\" : \"Overlap Coverage\" , \"calc_func\" : \"coverage\" , \"overlapping\" : true , \"normalize\" : false , \"apply_to_timedelta\" : true , \"tags\" : [ \"ranges\" , \"coverage\" ] }, \"total_records\" : { \"title\" : \"Total Records\" , \"calc_func\" : \"count\" , \"tags\" : \"records\" }, \"duration\" : { \"title\" : \"Duration\" , \"calc_func\" : \"duration.describe\" , \"post_calc_func\" : \"<function Ranges.<lambda> at 0x7fac881e1e18>\" , \"apply_to_timedelta\" : true , \"tags\" : [ \"ranges\" , \"duration\" ] } } ) Returns Ranges._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable Ranges._metrics . open method \u00b6 Records filtered by status == 0 . plot method \u00b6 Ranges . plot ( column = None , top_n = 5 , plot_zones = True , ts_trace_kwargs = None , start_trace_kwargs = None , end_trace_kwargs = None , open_shape_kwargs = None , closed_shape_kwargs = None , add_trace_kwargs = None , xref = 'x' , yref = 'y' , fig = None , ** layout_kwargs ) Plot ranges. Args column :\u2002 str Name of the column to plot. top_n :\u2002 int Filter top N range records by maximum duration. plot_zones :\u2002 bool Whether to plot zones. ts_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for Ranges.ts . start_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for start values. end_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for end values. open_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for open zones. closed_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for closed zones. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import vectorbt as vbt >>> from datetime import datetime , timedelta >>> import pandas as pd >>> price = pd . Series ([ 1 , 2 , 1 , 2 , 3 , 2 , 1 , 2 ], name = 'Price' ) >>> price . index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( len ( price ))] >>> vbt . Ranges . from_ts ( price >= 2 , wrapper_kwargs = dict ( freq = '1 day' )) . plot () plots_defaults property \u00b6 Defaults for PlotsBuilderMixin.plots() . Merges Records.plots_defaults and ranges.plots from settings . start_idx method \u00b6 Mapped array of the field start_idx . stats_defaults property \u00b6 Defaults for StatsBuilderMixin.stats() . Merges Records.stats_defaults and ranges.stats from settings . status method \u00b6 Mapped array of the field status . subplots class variable \u00b6 Subplots supported by Ranges . Co nf ig( { \"plot\" : { \"title\" : \"Ranges\" , \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"tags\" : \"ranges\" } } ) Returns Ranges._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable Ranges._subplots . to_mask method \u00b6 Ranges . to_mask ( group_by = None , wrap_kwargs = None ) Convert ranges to a mask. See ranges_to_mask_nb() . ts property \u00b6 Original time series that records are built from (optional).","title":"ranges"},{"location":"api/generic/ranges/#vectorbt.generic.ranges","text":"Base class for working with range records. Range records capture information on ranges. They are useful for analyzing duration of processes, such as drawdowns, trades, and positions. They also come in handy when analyzing distance between events, such as entry and exit signals. Each range has a starting point and an ending point. For example, the points for range(20) are 0 and 20 (not 19!) respectively. Note Be aware that if a range hasn't ended in a column, its end_idx will point at the latest index. Make sure to account for this when computing custom metrics involving duration. >>> import vectorbt as vbt >>> import numpy as np >>> import pandas as pd >>> start = '2019-01-01 UTC' # crypto is in UTC >>> end = '2020-01-01 UTC' >>> price = vbt . YFData . download ( 'BTC-USD' , start = start , end = end ) . get ( 'Close' ) >>> fast_ma = vbt . MA . run ( price , 10 ) >>> slow_ma = vbt . MA . run ( price , 50 ) >>> fast_below_slow = fast_ma . ma_above ( slow_ma ) >>> ranges = vbt . Ranges . from_ts ( fast_below_slow , wrapper_kwargs = dict ( freq = 'd' )) >>> ranges . records_readable Range Id Column Start Timestamp End Timestamp \\ 0 0 0 2019-02-19 00:00:00+00:00 2019-07-25 00:00:00+00:00 1 1 0 2019-08-08 00:00:00+00:00 2019-08-19 00:00:00+00:00 2 2 0 2019-11-01 00:00:00+00:00 2019-11-20 00:00:00+00:00 Status 0 Closed 1 Closed 2 Closed >>> ranges . duration . max ( wrap_kwargs = dict ( to_timedelta = True )) Timedelta('74 days 00:00:00')","title":"vectorbt.generic.ranges"},{"location":"api/generic/ranges/#from-accessors","text":"Moreover, all generic accessors have a property ranges and a method get_ranges : >>> # vectorbt.generic.accessors.GenericAccessor.ranges.coverage >>> fast_below_slow . vbt . ranges . coverage () 0.35792349726775957","title":"From accessors"},{"location":"api/generic/ranges/#stats","text":"Hint See StatsBuilderMixin.stats() and Ranges.metrics . >>> df = pd . DataFrame ({ ... 'a' : [ 1 , 2 , np . nan , np . nan , 5 , 6 ], ... 'b' : [ np . nan , 2 , np . nan , 4 , np . nan , 6 ] ... }) >>> ranges = df . vbt ( freq = 'd' ) . ranges >>> ranges [ 'a' ] . stats () Start 0 End 5 Period 6 days 00:00:00 Total Records 2 Coverage 4 days 00:00:00 Overlap Coverage 0 days 00:00:00 Duration: Min 2 days 00:00:00 Duration: Median 2 days 00:00:00 Duration: Max 2 days 00:00:00 Duration: Mean 2 days 00:00:00 Duration: Std 0 days 00:00:00 Name: a, dtype: object StatsBuilderMixin.stats() also supports (re-)grouping: >>> ranges . stats ( group_by = True ) Start 0 End 5 Period 6 days 00:00:00 Total Records 5 Coverage 5 days 00:00:00 Overlap Coverage 2 days 00:00:00 Duration: Min 1 days 00:00:00 Duration: Median 1 days 00:00:00 Duration: Max 2 days 00:00:00 Duration: Mean 1 days 09:36:00 Duration: Std 0 days 13:08:43.228968446 Name: group, dtype: object","title":"Stats"},{"location":"api/generic/ranges/#plots","text":"Hint See PlotsBuilderMixin.plots() and Ranges.subplots . Ranges class has a single subplot based on Ranges.plot() : >>> ranges [ 'a' ] . plots ()","title":"Plots"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.ranges_attach_field_config","text":"Config of fields to be attached to Ranges . Co nf ig( { \"status\" : { \"attach_filters\" : true } } )","title":"ranges_attach_field_config"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.ranges_field_config","text":"Field config for Ranges . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"col\" : \"int64\" , \"start_idx\" : \"int64\" , \"end_idx\" : \"int64\" , \"status\" : \"int64\" }, \"settings\" : { \"id\" : { \"title\" : \"Range Id\" }, \"idx\" : { \"name\" : \"end_idx\" }, \"start_idx\" : { \"title\" : \"Start Timestamp\" , \"mapping\" : \"index\" }, \"end_idx\" : { \"title\" : \"End Timestamp\" , \"mapping\" : \"index\" }, \"status\" : { \"title\" : \"Status\" , \"mapping\" : { \"Open\" : 0 , \"Closed\" : 1 } } } } )","title":"ranges_field_config"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges","text":"Extends Records for working with range records. Requires records_arr to have all fields defined in range_dt . Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin Records RecordsWithFields StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() Records.apply() Records.apply_mask() Records.build_field_config_doc() Records.col_arr Records.col_mapper Records.config Records.count() Records.get_apply_mapping_arr() Records.get_by_col_idxs() Records.get_field_arr() Records.get_field_mapping() Records.get_field_name() Records.get_field_setting() Records.get_field_title() Records.get_map_field() Records.get_map_field_to_index() Records.id_arr Records.idx_arr Records.iloc Records.indexing_func_meta() Records.indexing_kwargs Records.is_sorted() Records.loc Records.map() Records.map_array() Records.map_field() Records.override_field_config_doc() Records.records Records.records_arr Records.records_readable Records.replace() Records.self_aliases Records.sort() Records.values Records.wrapper Records.writeable_attrs StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses Drawdowns Trades","title":"Ranges"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.avg_duration","text":"Ranges . avg_duration ( group_by = None , wrap_kwargs = None , ** kwargs ) Average range duration (as timedelta).","title":"avg_duration()"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.closed","text":"Records filtered by status == 1 .","title":"closed"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.col","text":"Mapped array of the field col .","title":"col"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.coverage","text":"Ranges . coverage ( overlapping = False , normalize = True , group_by = None , wrap_kwargs = None ) Coverage, that is, the number of steps that are covered by all ranges. See range_coverage_nb() .","title":"coverage()"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.duration","text":"Duration of each range (in raw format).","title":"duration"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.end_idx","text":"Mapped array of the field end_idx .","title":"end_idx"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.field_config","text":"Field config of Ranges . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"col\" : \"int64\" , \"start_idx\" : \"int64\" , \"end_idx\" : \"int64\" , \"status\" : \"int64\" }, \"settings\" : { \"id\" : { \"name\" : \"id\" , \"title\" : \"Range Id\" }, \"col\" : { \"name\" : \"col\" , \"title\" : \"Column\" , \"mapping\" : \"columns\" }, \"idx\" : { \"name\" : \"end_idx\" , \"title\" : \"Timestamp\" , \"mapping\" : \"index\" }, \"start_idx\" : { \"title\" : \"Start Timestamp\" , \"mapping\" : \"index\" }, \"end_idx\" : { \"title\" : \"End Timestamp\" , \"mapping\" : \"index\" }, \"status\" : { \"title\" : \"Status\" , \"mapping\" : { \"Open\" : 0 , \"Closed\" : 1 } } } } )","title":"field_config"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.from_ts","text":"Ranges . from_ts ( ts , gap_value = None , attach_ts = True , wrapper_kwargs = None , ** kwargs ) Build Ranges from time series ts . Searches for sequences of True values in boolean data (False acts as a gap), positive values in integer data (-1 acts as a gap), and non-NaN values in any other data (NaN acts as a gap). **kwargs will be passed to Ranges .","title":"from_ts()"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.id","text":"Mapped array of the field id .","title":"id"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.indexing_func","text":"Ranges . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on Ranges .","title":"indexing_func()"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.max_duration","text":"Ranges . max_duration ( group_by = None , wrap_kwargs = None , ** kwargs ) Maximum range duration (as timedelta).","title":"max_duration()"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.metrics","text":"Metrics supported by Ranges . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function Ranges.<lambda> at 0x7fac881e1c80>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function Ranges.<lambda> at 0x7fac881e1d08>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function Ranges.<lambda> at 0x7fac881e1d90>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"coverage\" : { \"title\" : \"Coverage\" , \"calc_func\" : \"coverage\" , \"overlapping\" : false , \"normalize\" : false , \"apply_to_timedelta\" : true , \"tags\" : [ \"ranges\" , \"coverage\" ] }, \"overlap_coverage\" : { \"title\" : \"Overlap Coverage\" , \"calc_func\" : \"coverage\" , \"overlapping\" : true , \"normalize\" : false , \"apply_to_timedelta\" : true , \"tags\" : [ \"ranges\" , \"coverage\" ] }, \"total_records\" : { \"title\" : \"Total Records\" , \"calc_func\" : \"count\" , \"tags\" : \"records\" }, \"duration\" : { \"title\" : \"Duration\" , \"calc_func\" : \"duration.describe\" , \"post_calc_func\" : \"<function Ranges.<lambda> at 0x7fac881e1e18>\" , \"apply_to_timedelta\" : true , \"tags\" : [ \"ranges\" , \"duration\" ] } } ) Returns Ranges._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable Ranges._metrics .","title":"metrics"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.open","text":"Records filtered by status == 0 .","title":"open"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.plot","text":"Ranges . plot ( column = None , top_n = 5 , plot_zones = True , ts_trace_kwargs = None , start_trace_kwargs = None , end_trace_kwargs = None , open_shape_kwargs = None , closed_shape_kwargs = None , add_trace_kwargs = None , xref = 'x' , yref = 'y' , fig = None , ** layout_kwargs ) Plot ranges. Args column :\u2002 str Name of the column to plot. top_n :\u2002 int Filter top N range records by maximum duration. plot_zones :\u2002 bool Whether to plot zones. ts_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for Ranges.ts . start_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for start values. end_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for end values. open_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for open zones. closed_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for closed zones. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import vectorbt as vbt >>> from datetime import datetime , timedelta >>> import pandas as pd >>> price = pd . Series ([ 1 , 2 , 1 , 2 , 3 , 2 , 1 , 2 ], name = 'Price' ) >>> price . index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( len ( price ))] >>> vbt . Ranges . from_ts ( price >= 2 , wrapper_kwargs = dict ( freq = '1 day' )) . plot ()","title":"plot()"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.plots_defaults","text":"Defaults for PlotsBuilderMixin.plots() . Merges Records.plots_defaults and ranges.plots from settings .","title":"plots_defaults"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.start_idx","text":"Mapped array of the field start_idx .","title":"start_idx"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.stats_defaults","text":"Defaults for StatsBuilderMixin.stats() . Merges Records.stats_defaults and ranges.stats from settings .","title":"stats_defaults"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.status","text":"Mapped array of the field status .","title":"status"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.subplots","text":"Subplots supported by Ranges . Co nf ig( { \"plot\" : { \"title\" : \"Ranges\" , \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"tags\" : \"ranges\" } } ) Returns Ranges._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable Ranges._subplots .","title":"subplots"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.to_mask","text":"Ranges . to_mask ( group_by = None , wrap_kwargs = None ) Convert ranges to a mask. See ranges_to_mask_nb() .","title":"to_mask()"},{"location":"api/generic/ranges/#vectorbt.generic.ranges.Ranges.ts","text":"Original time series that records are built from (optional).","title":"ts"},{"location":"api/generic/splitters/","text":"splitters module \u00b6 Splitters for cross-validation. Defines splitter classes similar (but may not compatible) to sklearn.model_selection.BaseCrossValidator . split_ranges_into_sets function \u00b6 split_ranges_into_sets ( start_idxs , end_idxs , set_lens = (), left_to_right = True ) Generate ranges between each in start_idxs and end_idxs and optionally split into one or more sets. Args start_idxs :\u2002 array_like Start indices. end_idxs :\u2002 array_like End indices. set_lens :\u2002 list of float Lengths of sets in each range. The number of returned sets is the length of set_lens plus one, which stores the remaining elements. Can be passed per range. left_to_right :\u2002 bool or list of bool Whether to resolve set_lens from left to right. Makes the last set variable, otherwise makes the first set variable. Can be passed per range. Usage set_lens=(0.5) : 50% in training set, the rest in test set set_lens=(0.5, 0.25) : 50% in training set, 25% in validation set, the rest in test set set_lens=(50, 30) : 50 in training set, 30 in validation set, the rest in test set set_lens=(50, 30) and left_to_right=False : 30 in test set, 50 in validation set, the rest in training set BaseSplitter class \u00b6 Abstract splitter class. Subclasses ExpandingSplitter RangeSplitter RollingSplitter split method \u00b6 BaseSplitter . split ( X , ** kwargs ) ExpandingSplitter class \u00b6 Expanding walk-forward splitter. Superclasses BaseSplitter split method \u00b6 ExpandingSplitter . split ( X , n = None , min_len = 1 , ** kwargs ) Similar to RollingSplitter.split() , but expanding. **kwargs are passed to split_ranges_into_sets() . RangeSplitter class \u00b6 Range splitter. Superclasses BaseSplitter split method \u00b6 RangeSplitter . split ( X , n = None , range_len = None , min_len = 1 , start_idxs = None , end_idxs = None , ** kwargs ) Either split into n ranges each range_len long, or split into ranges between start_idxs and end_idxs , and concatenate along the column axis. At least one of range_len , n , or start_idxs and end_idxs must be set: If range_len is None, are split evenly into n ranges. If n is None, returns the maximum number of ranges of length range_len (can be a percentage). If start_idxs and end_idxs , splits into ranges between both arrays. Both index arrays should be either NumPy arrays with absolute positions or pandas indexes with labels. The last index should be inclusive. The distance between each start and end index can be different, and smaller ranges are filled with NaNs. range_len can be a floating number between 0 and 1 to indicate a fraction of the total range. **kwargs are passed to split_ranges_into_sets() . RollingSplitter class \u00b6 Rolling walk-forward splitter. Superclasses BaseSplitter split method \u00b6 RollingSplitter . split ( X , n = None , window_len = None , min_len = 1 , ** kwargs ) Split by rolling a window. **kwargs are passed to split_ranges_into_sets() . SplitterT class \u00b6 Base class for protocol classes. Protocol classes are defined as:: class Proto(Protocol): def meth(self) -> int: ... Such classes are primarily used with static type checkers that recognize structural subtyping (static duck-typing), for example:: class C def meth(self) -> int: return 0 def func(x: Proto) -> int: return x.meth() func(C()) # Passes static type check See PEP 544 for details. Protocol classes decorated with @typing_extensions .runtime act as simple-minded runtime protocol that checks only the presence of given attributes, ignoring their type signatures. Protocol classes can be generic, they are defined as:: class GenProto(Protocol[T]): def meth(self) -> T: ... Superclasses typing_extensions.Protocol split method \u00b6 SplitterT . split ( X , ** kwargs )","title":"splitters"},{"location":"api/generic/splitters/#vectorbt.generic.splitters","text":"Splitters for cross-validation. Defines splitter classes similar (but may not compatible) to sklearn.model_selection.BaseCrossValidator .","title":"vectorbt.generic.splitters"},{"location":"api/generic/splitters/#vectorbt.generic.splitters.split_ranges_into_sets","text":"split_ranges_into_sets ( start_idxs , end_idxs , set_lens = (), left_to_right = True ) Generate ranges between each in start_idxs and end_idxs and optionally split into one or more sets. Args start_idxs :\u2002 array_like Start indices. end_idxs :\u2002 array_like End indices. set_lens :\u2002 list of float Lengths of sets in each range. The number of returned sets is the length of set_lens plus one, which stores the remaining elements. Can be passed per range. left_to_right :\u2002 bool or list of bool Whether to resolve set_lens from left to right. Makes the last set variable, otherwise makes the first set variable. Can be passed per range. Usage set_lens=(0.5) : 50% in training set, the rest in test set set_lens=(0.5, 0.25) : 50% in training set, 25% in validation set, the rest in test set set_lens=(50, 30) : 50 in training set, 30 in validation set, the rest in test set set_lens=(50, 30) and left_to_right=False : 30 in test set, 50 in validation set, the rest in training set","title":"split_ranges_into_sets()"},{"location":"api/generic/splitters/#vectorbt.generic.splitters.BaseSplitter","text":"Abstract splitter class. Subclasses ExpandingSplitter RangeSplitter RollingSplitter","title":"BaseSplitter"},{"location":"api/generic/splitters/#vectorbt.generic.splitters.BaseSplitter.split","text":"BaseSplitter . split ( X , ** kwargs )","title":"split()"},{"location":"api/generic/splitters/#vectorbt.generic.splitters.ExpandingSplitter","text":"Expanding walk-forward splitter. Superclasses BaseSplitter","title":"ExpandingSplitter"},{"location":"api/generic/splitters/#vectorbt.generic.splitters.ExpandingSplitter.split","text":"ExpandingSplitter . split ( X , n = None , min_len = 1 , ** kwargs ) Similar to RollingSplitter.split() , but expanding. **kwargs are passed to split_ranges_into_sets() .","title":"split()"},{"location":"api/generic/splitters/#vectorbt.generic.splitters.RangeSplitter","text":"Range splitter. Superclasses BaseSplitter","title":"RangeSplitter"},{"location":"api/generic/splitters/#vectorbt.generic.splitters.RangeSplitter.split","text":"RangeSplitter . split ( X , n = None , range_len = None , min_len = 1 , start_idxs = None , end_idxs = None , ** kwargs ) Either split into n ranges each range_len long, or split into ranges between start_idxs and end_idxs , and concatenate along the column axis. At least one of range_len , n , or start_idxs and end_idxs must be set: If range_len is None, are split evenly into n ranges. If n is None, returns the maximum number of ranges of length range_len (can be a percentage). If start_idxs and end_idxs , splits into ranges between both arrays. Both index arrays should be either NumPy arrays with absolute positions or pandas indexes with labels. The last index should be inclusive. The distance between each start and end index can be different, and smaller ranges are filled with NaNs. range_len can be a floating number between 0 and 1 to indicate a fraction of the total range. **kwargs are passed to split_ranges_into_sets() .","title":"split()"},{"location":"api/generic/splitters/#vectorbt.generic.splitters.RollingSplitter","text":"Rolling walk-forward splitter. Superclasses BaseSplitter","title":"RollingSplitter"},{"location":"api/generic/splitters/#vectorbt.generic.splitters.RollingSplitter.split","text":"RollingSplitter . split ( X , n = None , window_len = None , min_len = 1 , ** kwargs ) Split by rolling a window. **kwargs are passed to split_ranges_into_sets() .","title":"split()"},{"location":"api/generic/splitters/#vectorbt.generic.splitters.SplitterT","text":"Base class for protocol classes. Protocol classes are defined as:: class Proto(Protocol): def meth(self) -> int: ... Such classes are primarily used with static type checkers that recognize structural subtyping (static duck-typing), for example:: class C def meth(self) -> int: return 0 def func(x: Proto) -> int: return x.meth() func(C()) # Passes static type check See PEP 544 for details. Protocol classes decorated with @typing_extensions .runtime act as simple-minded runtime protocol that checks only the presence of given attributes, ignoring their type signatures. Protocol classes can be generic, they are defined as:: class GenProto(Protocol[T]): def meth(self) -> T: ... Superclasses typing_extensions.Protocol","title":"SplitterT"},{"location":"api/generic/splitters/#vectorbt.generic.splitters.SplitterT.split","text":"SplitterT . split ( X , ** kwargs )","title":"split()"},{"location":"api/generic/stats_builder/","text":"stats_builder module \u00b6 Mixin for building statistics out of performance metrics. MetaStatsBuilderMixin class \u00b6 Meta class that exposes a read-only class property StatsBuilderMixin.metrics . Superclasses builtins.type Subclasses MetaData MetaGenericAccessor MetaIndicatorBase MetaMappedArray MetaPortfolio MetaRecords metrics property \u00b6 Metrics supported by StatsBuilderMixin.stats() . StatsBuilderMixin class \u00b6 Mixin that implements StatsBuilderMixin.stats() . Required to be a subclass of Wrapping . Subclasses Data GenericAccessor IndicatorBase MappedArray Portfolio Records build_metrics_doc class method \u00b6 StatsBuilderMixin . build_metrics_doc ( source_cls = None ) Build metrics documentation. metrics class variable \u00b6 Metrics supported by StatsBuilderMixin . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function StatsBuilderMixin.<lambda> at 0x7fac882187b8>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function StatsBuilderMixin.<lambda> at 0x7fac88218840>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function StatsBuilderMixin.<lambda> at 0x7fac882188c8>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" } } ) Returns StatsBuilderMixin._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable StatsBuilderMixin._metrics . override_metrics_doc class method \u00b6 StatsBuilderMixin . override_metrics_doc ( __pdoc__ , source_cls = None ) Call this method on each subclass that overrides metrics . stats method \u00b6 StatsBuilderMixin . stats ( metrics = None , tags = None , column = None , group_by = None , agg_func =< function mean > , silence_warnings = None , template_mapping = None , settings = None , filters = None , metric_settings = None ) Compute various metrics on this object. Args metrics :\u2002 str , tuple , iterable , or dict Metrics to calculate. Each element can be either: a metric name (see keys in StatsBuilderMixin.metrics ) a tuple of a metric name and a settings dict as in StatsBuilderMixin.metrics . The settings dict can contain the following keys: title : Title of the metric. Defaults to the name. tags : Single or multiple tags to associate this metric with. If any of these tags is in tags , keeps this metric. check_{filter} and inv_check_{filter} : Whether to check this metric against a filter defined in filters . True (or False for inverse) means to keep this metric. calc_func (required): Calculation function for custom metrics. Should return either a scalar for one column/group, pd.Series for multiple columns/groups, or a dict of such for multiple sub-metrics. resolve_calc_func : whether to resolve calc_func . If the function can be accessed by traversing attributes of this object, you can specify the path to this function as a string (see deep_getattr() for the path format). If calc_func is a function, arguments from merged metric settings are matched with arguments in the signature (see below). If resolve_calc_func is False, calc_func should accept (resolved) self and dictionary of merged metric settings. Defaults to True. post_calc_func : Function to post-process the result of calc_func . Should accept (resolved) self, output of calc_func , and dictionary of merged metric settings, and return whatever is acceptable to be returned by calc_func . Defaults to None. fill_wrap_kwargs : Whether to fill wrap_kwargs with to_timedelta and silence_warnings . Defaults to False. apply_to_timedelta : Whether to apply ArrayWrapper.to_timedelta() on the result. To disable this globally, pass to_timedelta=False in settings . Defaults to False. pass_{arg} : Whether to pass any argument from the settings (see below). Defaults to True if this argument was found in the function's signature. Set to False to not pass. If argument to be passed was not found, pass_{arg} is removed. resolve_path_{arg} : Whether to resolve an argument that is meant to be an attribute of this object and is the first part of the path of calc_func . Passes only optional arguments. Defaults to True. See AttrResolver.resolve_attr() . resolve_{arg} : Whether to resolve an argument that is meant to be an attribute of this object and is present in the function's signature. Defaults to False. See AttrResolver.resolve_attr() . template_mapping : Mapping to replace templates in metric settings. Used across all settings. Any other keyword argument that overrides the settings or is passed directly to calc_func . If resolve_calc_func is True, the calculation function may \"request\" any of the following arguments by accepting them or if pass_{arg} was found in the settings dict: Each of AttrResolver.self_aliases : original object (ungrouped, with no column selected) group_by : won't be passed if it was used in resolving the first attribute of calc_func specified as a path, use pass_group_by=True to pass anyway column metric_name agg_func silence_warnings to_timedelta : replaced by True if None and frequency is set Any argument from settings Any attribute of this object if it meant to be resolved (see AttrResolver.resolve_attr() ) Pass metrics='all' to calculate all supported metrics. tags :\u2002 str or iterable Tags to select. See match_tags() . column :\u2002 str Name of the column/group. Hint There are two ways to select a column: obj['a'].stats() and obj.stats(column='a') . They both accomplish the same thing but in different ways: obj['a'].stats() computes statistics of the column 'a' only, while obj.stats(column='a') computes statistics of all columns first and only then selects the column 'a'. The first method is preferred when you have a lot of data or caching is disabled. The second method is preferred when most attributes have already been cached. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . agg_func :\u2002 callable Aggregation function to aggregate statistics across all columns. Defaults to mean. Should take pd.Series and return a const. Has only effect if column was specified or this object contains only one column of data. If agg_func has been overridden by a metric: it only takes effect if global agg_func is not None will raise a warning if it's None but the result of calculation has multiple values silence_warnings :\u2002 bool Whether to silence all warnings. template_mapping :\u2002 mapping Global mapping to replace templates. Gets merged over template_mapping from StatsBuilderMixin.stats_defaults . Applied on settings and then on each metric settings. filters :\u2002 dict Filters to apply. Each item consists of the filter name and settings dict. The settings dict can contain the following keys: filter_func : Filter function that should accept resolved self and merged settings for a metric, and return either True or False. warning_message : Warning message to be shown when skipping a metric. Can be a template that will be substituted using merged metric settings as mapping. Defaults to None. inv_warning_message : Same as warning_message but for inverse checks. Gets merged over filters from StatsBuilderMixin.stats_defaults . settings :\u2002 dict Global settings and resolution arguments. Extends/overrides settings from StatsBuilderMixin.stats_defaults . Gets extended/overridden by metric settings. metric_settings :\u2002 dict Keyword arguments for each metric. Extends/overrides all global and metric settings. For template logic, see vectorbt.utils.template . For defaults, see StatsBuilderMixin.stats_defaults . Hint There are two types of arguments: optional (or resolution) and mandatory arguments. Optional arguments are only passed if they are found in the function's signature. Mandatory arguments are passed regardless of this. Optional arguments can only be defined using settings (that is, globally), while mandatory arguments can be defined both using default metric settings and {metric_name}_kwargs . Overriding optional arguments using default metric settings or {metric_name}_kwargs won't turn them into mandatory. For this, pass pass_{arg}=True . Hint Make sure to resolve and then to re-use as many object attributes as possible to utilize built-in caching (even if global caching is disabled). Usage See vectorbt.portfolio.base for examples. stats_defaults property \u00b6 Defaults for StatsBuilderMixin.stats() . See stats_builder in settings . writeable_attrs property \u00b6 Set of writeable attributes that will be saved/copied along with the config.","title":"stats_builder"},{"location":"api/generic/stats_builder/#vectorbt.generic.stats_builder","text":"Mixin for building statistics out of performance metrics.","title":"vectorbt.generic.stats_builder"},{"location":"api/generic/stats_builder/#vectorbt.generic.stats_builder.MetaStatsBuilderMixin","text":"Meta class that exposes a read-only class property StatsBuilderMixin.metrics . Superclasses builtins.type Subclasses MetaData MetaGenericAccessor MetaIndicatorBase MetaMappedArray MetaPortfolio MetaRecords","title":"MetaStatsBuilderMixin"},{"location":"api/generic/stats_builder/#vectorbt.generic.stats_builder.MetaStatsBuilderMixin.metrics","text":"Metrics supported by StatsBuilderMixin.stats() .","title":"metrics"},{"location":"api/generic/stats_builder/#vectorbt.generic.stats_builder.StatsBuilderMixin","text":"Mixin that implements StatsBuilderMixin.stats() . Required to be a subclass of Wrapping . Subclasses Data GenericAccessor IndicatorBase MappedArray Portfolio Records","title":"StatsBuilderMixin"},{"location":"api/generic/stats_builder/#vectorbt.generic.stats_builder.StatsBuilderMixin.build_metrics_doc","text":"StatsBuilderMixin . build_metrics_doc ( source_cls = None ) Build metrics documentation.","title":"build_metrics_doc()"},{"location":"api/generic/stats_builder/#vectorbt.generic.stats_builder.StatsBuilderMixin.metrics","text":"Metrics supported by StatsBuilderMixin . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function StatsBuilderMixin.<lambda> at 0x7fac882187b8>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function StatsBuilderMixin.<lambda> at 0x7fac88218840>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function StatsBuilderMixin.<lambda> at 0x7fac882188c8>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" } } ) Returns StatsBuilderMixin._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable StatsBuilderMixin._metrics .","title":"metrics"},{"location":"api/generic/stats_builder/#vectorbt.generic.stats_builder.StatsBuilderMixin.override_metrics_doc","text":"StatsBuilderMixin . override_metrics_doc ( __pdoc__ , source_cls = None ) Call this method on each subclass that overrides metrics .","title":"override_metrics_doc()"},{"location":"api/generic/stats_builder/#vectorbt.generic.stats_builder.StatsBuilderMixin.stats","text":"StatsBuilderMixin . stats ( metrics = None , tags = None , column = None , group_by = None , agg_func =< function mean > , silence_warnings = None , template_mapping = None , settings = None , filters = None , metric_settings = None ) Compute various metrics on this object. Args metrics :\u2002 str , tuple , iterable , or dict Metrics to calculate. Each element can be either: a metric name (see keys in StatsBuilderMixin.metrics ) a tuple of a metric name and a settings dict as in StatsBuilderMixin.metrics . The settings dict can contain the following keys: title : Title of the metric. Defaults to the name. tags : Single or multiple tags to associate this metric with. If any of these tags is in tags , keeps this metric. check_{filter} and inv_check_{filter} : Whether to check this metric against a filter defined in filters . True (or False for inverse) means to keep this metric. calc_func (required): Calculation function for custom metrics. Should return either a scalar for one column/group, pd.Series for multiple columns/groups, or a dict of such for multiple sub-metrics. resolve_calc_func : whether to resolve calc_func . If the function can be accessed by traversing attributes of this object, you can specify the path to this function as a string (see deep_getattr() for the path format). If calc_func is a function, arguments from merged metric settings are matched with arguments in the signature (see below). If resolve_calc_func is False, calc_func should accept (resolved) self and dictionary of merged metric settings. Defaults to True. post_calc_func : Function to post-process the result of calc_func . Should accept (resolved) self, output of calc_func , and dictionary of merged metric settings, and return whatever is acceptable to be returned by calc_func . Defaults to None. fill_wrap_kwargs : Whether to fill wrap_kwargs with to_timedelta and silence_warnings . Defaults to False. apply_to_timedelta : Whether to apply ArrayWrapper.to_timedelta() on the result. To disable this globally, pass to_timedelta=False in settings . Defaults to False. pass_{arg} : Whether to pass any argument from the settings (see below). Defaults to True if this argument was found in the function's signature. Set to False to not pass. If argument to be passed was not found, pass_{arg} is removed. resolve_path_{arg} : Whether to resolve an argument that is meant to be an attribute of this object and is the first part of the path of calc_func . Passes only optional arguments. Defaults to True. See AttrResolver.resolve_attr() . resolve_{arg} : Whether to resolve an argument that is meant to be an attribute of this object and is present in the function's signature. Defaults to False. See AttrResolver.resolve_attr() . template_mapping : Mapping to replace templates in metric settings. Used across all settings. Any other keyword argument that overrides the settings or is passed directly to calc_func . If resolve_calc_func is True, the calculation function may \"request\" any of the following arguments by accepting them or if pass_{arg} was found in the settings dict: Each of AttrResolver.self_aliases : original object (ungrouped, with no column selected) group_by : won't be passed if it was used in resolving the first attribute of calc_func specified as a path, use pass_group_by=True to pass anyway column metric_name agg_func silence_warnings to_timedelta : replaced by True if None and frequency is set Any argument from settings Any attribute of this object if it meant to be resolved (see AttrResolver.resolve_attr() ) Pass metrics='all' to calculate all supported metrics. tags :\u2002 str or iterable Tags to select. See match_tags() . column :\u2002 str Name of the column/group. Hint There are two ways to select a column: obj['a'].stats() and obj.stats(column='a') . They both accomplish the same thing but in different ways: obj['a'].stats() computes statistics of the column 'a' only, while obj.stats(column='a') computes statistics of all columns first and only then selects the column 'a'. The first method is preferred when you have a lot of data or caching is disabled. The second method is preferred when most attributes have already been cached. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . agg_func :\u2002 callable Aggregation function to aggregate statistics across all columns. Defaults to mean. Should take pd.Series and return a const. Has only effect if column was specified or this object contains only one column of data. If agg_func has been overridden by a metric: it only takes effect if global agg_func is not None will raise a warning if it's None but the result of calculation has multiple values silence_warnings :\u2002 bool Whether to silence all warnings. template_mapping :\u2002 mapping Global mapping to replace templates. Gets merged over template_mapping from StatsBuilderMixin.stats_defaults . Applied on settings and then on each metric settings. filters :\u2002 dict Filters to apply. Each item consists of the filter name and settings dict. The settings dict can contain the following keys: filter_func : Filter function that should accept resolved self and merged settings for a metric, and return either True or False. warning_message : Warning message to be shown when skipping a metric. Can be a template that will be substituted using merged metric settings as mapping. Defaults to None. inv_warning_message : Same as warning_message but for inverse checks. Gets merged over filters from StatsBuilderMixin.stats_defaults . settings :\u2002 dict Global settings and resolution arguments. Extends/overrides settings from StatsBuilderMixin.stats_defaults . Gets extended/overridden by metric settings. metric_settings :\u2002 dict Keyword arguments for each metric. Extends/overrides all global and metric settings. For template logic, see vectorbt.utils.template . For defaults, see StatsBuilderMixin.stats_defaults . Hint There are two types of arguments: optional (or resolution) and mandatory arguments. Optional arguments are only passed if they are found in the function's signature. Mandatory arguments are passed regardless of this. Optional arguments can only be defined using settings (that is, globally), while mandatory arguments can be defined both using default metric settings and {metric_name}_kwargs . Overriding optional arguments using default metric settings or {metric_name}_kwargs won't turn them into mandatory. For this, pass pass_{arg}=True . Hint Make sure to resolve and then to re-use as many object attributes as possible to utilize built-in caching (even if global caching is disabled). Usage See vectorbt.portfolio.base for examples.","title":"stats()"},{"location":"api/generic/stats_builder/#vectorbt.generic.stats_builder.StatsBuilderMixin.stats_defaults","text":"Defaults for StatsBuilderMixin.stats() . See stats_builder in settings .","title":"stats_defaults"},{"location":"api/generic/stats_builder/#vectorbt.generic.stats_builder.StatsBuilderMixin.writeable_attrs","text":"Set of writeable attributes that will be saved/copied along with the config.","title":"writeable_attrs"},{"location":"api/indicators/","text":"indicators package \u00b6 Modules for building and running indicators. Technical indicators are used to see past trends and anticipate future moves. See Using Technical Indicators to Develop Trading Strategies . pandas_ta function \u00b6 pandas_ta ( * args , ** kwargs ) Shortcut for IndicatorFactory.from_pandas_ta() . ta function \u00b6 ta ( * args , ** kwargs ) Shortcut for IndicatorFactory.from_ta() . talib function \u00b6 talib ( * args , ** kwargs ) Shortcut for IndicatorFactory.from_talib() . Sub-modules \u00b6 vectorbt.indicators.basic vectorbt.indicators.configs vectorbt.indicators.factory vectorbt.indicators.nb","title":"indicators"},{"location":"api/indicators/#vectorbt.indicators","text":"Modules for building and running indicators. Technical indicators are used to see past trends and anticipate future moves. See Using Technical Indicators to Develop Trading Strategies .","title":"vectorbt.indicators"},{"location":"api/indicators/#vectorbt.indicators.pandas_ta","text":"pandas_ta ( * args , ** kwargs ) Shortcut for IndicatorFactory.from_pandas_ta() .","title":"pandas_ta()"},{"location":"api/indicators/#vectorbt.indicators.ta","text":"ta ( * args , ** kwargs ) Shortcut for IndicatorFactory.from_ta() .","title":"ta()"},{"location":"api/indicators/#vectorbt.indicators.talib","text":"talib ( * args , ** kwargs ) Shortcut for IndicatorFactory.from_talib() .","title":"talib()"},{"location":"api/indicators/#sub-modules","text":"vectorbt.indicators.basic vectorbt.indicators.configs vectorbt.indicators.factory vectorbt.indicators.nb","title":"Sub-modules"},{"location":"api/indicators/basic/","text":"basic module \u00b6 Indicators built with IndicatorFactory . You can access all the indicators either by vbt.* or vbt.indicators.* . >>> import pandas as pd >>> import vectorbt as vbt >>> # vectorbt.indicators.basic.MA >>> vbt . MA . run ( pd . Series ([ 1 , 2 , 3 ]), [ 2 , 3 ]) . ma ma_window 2 3 ma_ewm False False 0 NaN NaN 1 1.5 NaN 2 2.5 2.0 The advantage of these indicators over TA-Lib's is that they work primarily on 2-dimensional arrays and utilize caching, which makes them faster for matrices with huge number of columns. They also have plotting methods. Run for the examples below: >>> import vectorbt as vbt >>> from datetime import datetime >>> start = '2019-03-01 UTC' # crypto is in UTC >>> end = '2019-09-01 UTC' >>> cols = [ 'Open' , 'High' , 'Low' , 'Close' , 'Volume' ] >>> ohlcv = vbt . YFData . download ( \"BTC-USD\" , start = start , end = end ) . get ( cols ) >>> ohlcv Open High Low \\ Date 2019-03-01 00:00:00+00:00 3853.757080 3907.795410 3851.692383 2019-03-02 00:00:00+00:00 3855.318115 3874.607422 3832.127930 2019-03-03 00:00:00+00:00 3862.266113 3875.483643 3836.905762 ... ... ... ... 2019-08-30 00:00:00+00:00 9514.844727 9656.124023 9428.302734 2019-08-31 00:00:00+00:00 9597.539062 9673.220703 9531.799805 2019-09-01 00:00:00+00:00 9630.592773 9796.755859 9582.944336 Close Volume Date 2019-03-01 00:00:00+00:00 3859.583740 7661247975 2019-03-02 00:00:00+00:00 3864.415039 7578786076 2019-03-03 00:00:00+00:00 3847.175781 7253558152 ... ... ... 2019-08-30 00:00:00+00:00 9598.173828 13595263986 2019-08-31 00:00:00+00:00 9630.664062 11454806419 2019-09-01 00:00:00+00:00 9757.970703 11445355859 [185 rows x 5 columns] >>> ohlcv . vbt . ohlcv . plot () ATR class \u00b6 Average True Range (ATR). The indicator provide an indication of the degree of price volatility. Strong moves, in either direction, are often accompanied by large ranges, or large True Ranges. See Average True Range - ATR . Note Uses Simple MA and Exponential MA as compared to Wilder. Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.indicators.basic.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.indicators.basic._ATR apply_func method \u00b6 ATR . apply_func ( high , low , close , window , ewm , adjust , tr , cache_dict ) Apply function for ATR . atr property \u00b6 Output array. atr_above method \u00b6 ATR . atr_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where atr is above other . See combine_objs() . atr_below method \u00b6 ATR . atr_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where atr is below other . See combine_objs() . atr_crossed_above method \u00b6 ATR . atr_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where atr is crossed_above other . See combine_objs() . atr_crossed_below method \u00b6 ATR . atr_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where atr is crossed_below other . See combine_objs() . atr_equal method \u00b6 ATR . atr_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where atr is equal other . See combine_objs() . atr_stats method \u00b6 ATR . atr_stats ( * args , ** kwargs ) Stats of atr as generic. close method \u00b6 Input array. close_above method \u00b6 ATR . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 ATR . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 ATR . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 ATR . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 ATR . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 ATR . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func . ewm_list property \u00b6 List of ewm values. high method \u00b6 Input array. high_above method \u00b6 ATR . high_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is above other . See combine_objs() . high_below method \u00b6 ATR . high_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is below other . See combine_objs() . high_crossed_above method \u00b6 ATR . high_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is crossed_above other . See combine_objs() . high_crossed_below method \u00b6 ATR . high_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is crossed_below other . See combine_objs() . high_equal method \u00b6 ATR . high_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is equal other . See combine_objs() . high_stats method \u00b6 ATR . high_stats ( * args , ** kwargs ) Stats of high as generic. low method \u00b6 Input array. low_above method \u00b6 ATR . low_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is above other . See combine_objs() . low_below method \u00b6 ATR . low_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is below other . See combine_objs() . low_crossed_above method \u00b6 ATR . low_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is crossed_above other . See combine_objs() . low_crossed_below method \u00b6 ATR . low_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is crossed_below other . See combine_objs() . low_equal method \u00b6 ATR . low_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is equal other . See combine_objs() . low_stats method \u00b6 ATR . low_stats ( * args , ** kwargs ) Stats of low as generic. plot method \u00b6 _ATR . plot ( column = None , tr_trace_kwargs = None , atr_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot ATR.tr and ATR.atr . Args column :\u2002 str Name of the column to plot. tr_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for ATR.tr . atr_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for ATR.atr . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> vbt . ATR . run ( ohlcv [ 'High' ], ohlcv [ 'Low' ], ohlcv [ 'Close' ], 10 ) . plot () run class method \u00b6 ATR . run ( high , low , close , window = Default ( 14 ), ewm = Default ( True ), short_name = 'atr' , hide_params = None , hide_default = True , ** kwargs ) Run ATR indicator. Inputs: high , low , close Parameters: window , ewm Outputs: tr , atr Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 ATR . run_combs ( high , low , close , window = Default ( 14 ), ewm = Default ( True ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple ATR indicators using function comb_func . Inputs: high , low , close Parameters: window , ewm Outputs: tr , atr comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to ATR.run() . tr property \u00b6 Output array. tr_above method \u00b6 ATR . tr_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where tr is above other . See combine_objs() . tr_below method \u00b6 ATR . tr_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where tr is below other . See combine_objs() . tr_crossed_above method \u00b6 ATR . tr_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where tr is crossed_above other . See combine_objs() . tr_crossed_below method \u00b6 ATR . tr_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where tr is crossed_below other . See combine_objs() . tr_equal method \u00b6 ATR . tr_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where tr is equal other . See combine_objs() . tr_stats method \u00b6 ATR . tr_stats ( * args , ** kwargs ) Stats of tr as generic. window_list property \u00b6 List of window values. BBANDS class \u00b6 Bollinger Bands (BBANDS). A Bollinger Band\u00ae is a technical analysis tool defined by a set of lines plotted two standard deviations (positively and negatively) away from a simple moving average (SMA) of the security's price, but can be adjusted to user preferences. See Bollinger Band\u00ae . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.indicators.basic.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.indicators.basic._BBANDS alpha_list property \u00b6 List of alpha values. apply_func method \u00b6 BBANDS . apply_func ( close , window , ewm , alpha , adjust , ddof , ma_cache_dict , mstd_cache_dict ) Apply function for BBANDS . bandwidth method \u00b6 Custom property. bandwidth_above method \u00b6 BBANDS . bandwidth_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where bandwidth is above other . See combine_objs() . bandwidth_below method \u00b6 BBANDS . bandwidth_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where bandwidth is below other . See combine_objs() . bandwidth_crossed_above method \u00b6 BBANDS . bandwidth_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where bandwidth is crossed_above other . See combine_objs() . bandwidth_crossed_below method \u00b6 BBANDS . bandwidth_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where bandwidth is crossed_below other . See combine_objs() . bandwidth_equal method \u00b6 BBANDS . bandwidth_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where bandwidth is equal other . See combine_objs() . bandwidth_stats method \u00b6 BBANDS . bandwidth_stats ( * args , ** kwargs ) Stats of bandwidth as generic. close method \u00b6 Input array. close_above method \u00b6 BBANDS . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 BBANDS . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 BBANDS . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 BBANDS . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 BBANDS . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 BBANDS . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func . ewm_list property \u00b6 List of ewm values. lower property \u00b6 Output array. lower_above method \u00b6 BBANDS . lower_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where lower is above other . See combine_objs() . lower_below method \u00b6 BBANDS . lower_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where lower is below other . See combine_objs() . lower_crossed_above method \u00b6 BBANDS . lower_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where lower is crossed_above other . See combine_objs() . lower_crossed_below method \u00b6 BBANDS . lower_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where lower is crossed_below other . See combine_objs() . lower_equal method \u00b6 BBANDS . lower_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where lower is equal other . See combine_objs() . lower_stats method \u00b6 BBANDS . lower_stats ( * args , ** kwargs ) Stats of lower as generic. middle property \u00b6 Output array. middle_above method \u00b6 BBANDS . middle_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where middle is above other . See combine_objs() . middle_below method \u00b6 BBANDS . middle_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where middle is below other . See combine_objs() . middle_crossed_above method \u00b6 BBANDS . middle_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where middle is crossed_above other . See combine_objs() . middle_crossed_below method \u00b6 BBANDS . middle_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where middle is crossed_below other . See combine_objs() . middle_equal method \u00b6 BBANDS . middle_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where middle is equal other . See combine_objs() . middle_stats method \u00b6 BBANDS . middle_stats ( * args , ** kwargs ) Stats of middle as generic. percent_b method \u00b6 Custom property. percent_b_above method \u00b6 BBANDS . percent_b_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_b is above other . See combine_objs() . percent_b_below method \u00b6 BBANDS . percent_b_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_b is below other . See combine_objs() . percent_b_crossed_above method \u00b6 BBANDS . percent_b_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_b is crossed_above other . See combine_objs() . percent_b_crossed_below method \u00b6 BBANDS . percent_b_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_b is crossed_below other . See combine_objs() . percent_b_equal method \u00b6 BBANDS . percent_b_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_b is equal other . See combine_objs() . percent_b_stats method \u00b6 BBANDS . percent_b_stats ( * args , ** kwargs ) Stats of percent_b as generic. plot method \u00b6 _BBANDS . plot ( column = None , plot_close = True , close_trace_kwargs = None , middle_trace_kwargs = None , upper_trace_kwargs = None , lower_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot BBANDS.middle , BBANDS.upper and BBANDS.lower against BBANDS.close . Args column :\u2002 str Name of the column to plot. plot_close :\u2002 bool Whether to plot MA.close . close_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for BBANDS.close . middle_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for BBANDS.middle . upper_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for BBANDS.upper . lower_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for BBANDS.lower . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> vbt . BBANDS . run ( ohlcv [ 'Close' ]) . plot () run class method \u00b6 BBANDS . run ( close , window = Default ( 20 ), ewm = Default ( False ), alpha = Default ( 2 ), short_name = 'bb' , hide_params = None , hide_default = True , ** kwargs ) Run BBANDS indicator. Inputs: close Parameters: window , ewm , alpha Outputs: middle , upper , lower Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 BBANDS . run_combs ( close , window = Default ( 20 ), ewm = Default ( False ), alpha = Default ( 2 ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple BBANDS indicators using function comb_func . Inputs: close Parameters: window , ewm , alpha Outputs: middle , upper , lower comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to BBANDS.run() . upper property \u00b6 Output array. upper_above method \u00b6 BBANDS . upper_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where upper is above other . See combine_objs() . upper_below method \u00b6 BBANDS . upper_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where upper is below other . See combine_objs() . upper_crossed_above method \u00b6 BBANDS . upper_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where upper is crossed_above other . See combine_objs() . upper_crossed_below method \u00b6 BBANDS . upper_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where upper is crossed_below other . See combine_objs() . upper_equal method \u00b6 BBANDS . upper_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where upper is equal other . See combine_objs() . upper_stats method \u00b6 BBANDS . upper_stats ( * args , ** kwargs ) Stats of upper as generic. window_list property \u00b6 List of window values. MA class \u00b6 Moving Average (MA). A moving average is a widely used indicator in technical analysis that helps smooth out price action by filtering out the \u201cnoise\u201d from random short-term price fluctuations. See Moving Average (MA) . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.indicators.basic.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.indicators.basic._MA apply_func method \u00b6 MA . apply_func ( close , window , ewm , adjust , cache_dict ) Apply function for MA . close method \u00b6 Input array. close_above method \u00b6 MA . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 MA . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 MA . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 MA . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 MA . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 MA . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func . ewm_list property \u00b6 List of ewm values. ma property \u00b6 Output array. ma_above method \u00b6 MA . ma_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ma is above other . See combine_objs() . ma_below method \u00b6 MA . ma_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ma is below other . See combine_objs() . ma_crossed_above method \u00b6 MA . ma_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ma is crossed_above other . See combine_objs() . ma_crossed_below method \u00b6 MA . ma_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ma is crossed_below other . See combine_objs() . ma_equal method \u00b6 MA . ma_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ma is equal other . See combine_objs() . ma_stats method \u00b6 MA . ma_stats ( * args , ** kwargs ) Stats of ma as generic. plot method \u00b6 _MA . plot ( column = None , plot_close = True , close_trace_kwargs = None , ma_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot MA.ma against MA.close . Args column :\u2002 str Name of the column to plot. plot_close :\u2002 bool Whether to plot MA.close . close_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for MA.close . ma_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for MA.ma . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> vbt . MA . run ( ohlcv [ 'Close' ], 10 ) . plot () run class method \u00b6 MA . run ( close , window , ewm = Default ( False ), short_name = 'ma' , hide_params = None , hide_default = True , ** kwargs ) Run MA indicator. Inputs: close Parameters: window , ewm Outputs: ma Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 MA . run_combs ( close , window , ewm = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple MA indicators using function comb_func . Inputs: close Parameters: window , ewm Outputs: ma comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to MA.run() . window_list property \u00b6 List of window values. MACD class \u00b6 Moving Average Convergence Divergence (MACD). Is a trend-following momentum indicator that shows the relationship between two moving averages of prices. See Moving Average Convergence Divergence \u2013 MACD . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.indicators.basic.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.indicators.basic._MACD apply_func method \u00b6 MACD . apply_func ( close , fast_window , slow_window , signal_window , macd_ewm , signal_ewm , adjust , cache_dict ) Apply function for MACD . close method \u00b6 Input array. close_above method \u00b6 MACD . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 MACD . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 MACD . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 MACD . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 MACD . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 MACD . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func . fast_window_list property \u00b6 List of fast_window values. hist method \u00b6 Custom property. hist_above method \u00b6 MACD . hist_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where hist is above other . See combine_objs() . hist_below method \u00b6 MACD . hist_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where hist is below other . See combine_objs() . hist_crossed_above method \u00b6 MACD . hist_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where hist is crossed_above other . See combine_objs() . hist_crossed_below method \u00b6 MACD . hist_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where hist is crossed_below other . See combine_objs() . hist_equal method \u00b6 MACD . hist_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where hist is equal other . See combine_objs() . hist_stats method \u00b6 MACD . hist_stats ( * args , ** kwargs ) Stats of hist as generic. macd property \u00b6 Output array. macd_above method \u00b6 MACD . macd_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where macd is above other . See combine_objs() . macd_below method \u00b6 MACD . macd_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where macd is below other . See combine_objs() . macd_crossed_above method \u00b6 MACD . macd_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where macd is crossed_above other . See combine_objs() . macd_crossed_below method \u00b6 MACD . macd_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where macd is crossed_below other . See combine_objs() . macd_equal method \u00b6 MACD . macd_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where macd is equal other . See combine_objs() . macd_ewm_list property \u00b6 List of macd_ewm values. macd_stats method \u00b6 MACD . macd_stats ( * args , ** kwargs ) Stats of macd as generic. plot method \u00b6 _MACD . plot ( column = None , macd_trace_kwargs = None , signal_trace_kwargs = None , hist_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot MACD.macd , MACD.signal and MACD.hist . Args column :\u2002 str Name of the column to plot. macd_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for MACD.macd . signal_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for MACD.signal . hist_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Bar for MACD.hist . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> vbt . MACD . run ( ohlcv [ 'Close' ]) . plot () run class method \u00b6 MACD . run ( close , fast_window = Default ( 12 ), slow_window = Default ( 26 ), signal_window = Default ( 9 ), macd_ewm = Default ( False ), signal_ewm = Default ( False ), short_name = 'macd' , hide_params = None , hide_default = True , ** kwargs ) Run MACD indicator. Inputs: close Parameters: fast_window , slow_window , signal_window , macd_ewm , signal_ewm Outputs: macd , signal Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 MACD . run_combs ( close , fast_window = Default ( 12 ), slow_window = Default ( 26 ), signal_window = Default ( 9 ), macd_ewm = Default ( False ), signal_ewm = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple MACD indicators using function comb_func . Inputs: close Parameters: fast_window , slow_window , signal_window , macd_ewm , signal_ewm Outputs: macd , signal comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to MACD.run() . signal property \u00b6 Output array. signal_above method \u00b6 MACD . signal_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where signal is above other . See combine_objs() . signal_below method \u00b6 MACD . signal_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where signal is below other . See combine_objs() . signal_crossed_above method \u00b6 MACD . signal_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where signal is crossed_above other . See combine_objs() . signal_crossed_below method \u00b6 MACD . signal_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where signal is crossed_below other . See combine_objs() . signal_equal method \u00b6 MACD . signal_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where signal is equal other . See combine_objs() . signal_ewm_list property \u00b6 List of signal_ewm values. signal_stats method \u00b6 MACD . signal_stats ( * args , ** kwargs ) Stats of signal as generic. signal_window_list property \u00b6 List of signal_window values. slow_window_list property \u00b6 List of slow_window values. MSTD class \u00b6 Moving Standard Deviation (MSTD). Standard deviation is an indicator that measures the size of an assets recent price moves in order to predict how volatile the price may be in the future. Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.indicators.basic.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.indicators.basic._MSTD apply_func method \u00b6 MSTD . apply_func ( close , window , ewm , adjust , ddof , cache_dict ) Apply function for MSTD . close method \u00b6 Input array. close_above method \u00b6 MSTD . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 MSTD . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 MSTD . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 MSTD . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 MSTD . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 MSTD . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func . ewm_list property \u00b6 List of ewm values. mstd property \u00b6 Output array. mstd_above method \u00b6 MSTD . mstd_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where mstd is above other . See combine_objs() . mstd_below method \u00b6 MSTD . mstd_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where mstd is below other . See combine_objs() . mstd_crossed_above method \u00b6 MSTD . mstd_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where mstd is crossed_above other . See combine_objs() . mstd_crossed_below method \u00b6 MSTD . mstd_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where mstd is crossed_below other . See combine_objs() . mstd_equal method \u00b6 MSTD . mstd_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where mstd is equal other . See combine_objs() . mstd_stats method \u00b6 MSTD . mstd_stats ( * args , ** kwargs ) Stats of mstd as generic. plot method \u00b6 _MSTD . plot ( column = None , mstd_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot MSTD.mstd . Args column :\u2002 str Name of the column to plot. mstd_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for MSTD.mstd . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> vbt . MSTD . run ( ohlcv [ 'Close' ], 10 ) . plot () run class method \u00b6 MSTD . run ( close , window , ewm = Default ( False ), short_name = 'mstd' , hide_params = None , hide_default = True , ** kwargs ) Run MSTD indicator. Inputs: close Parameters: window , ewm Outputs: mstd Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 MSTD . run_combs ( close , window , ewm = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple MSTD indicators using function comb_func . Inputs: close Parameters: window , ewm Outputs: mstd comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to MSTD.run() . window_list property \u00b6 List of window values. OBV class \u00b6 On-balance volume (OBV). It relates price and volume in the stock market. OBV is based on a cumulative total volume. See On-Balance Volume (OBV) . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.indicators.basic.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.run_combs() IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.indicators.basic._OBV close method \u00b6 Input array. close_above method \u00b6 OBV . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 OBV . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 OBV . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 OBV . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 OBV . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 OBV . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 OBV . custom_func ( close , volume_ts ) Custom calculation function for OBV . obv property \u00b6 Output array. obv_above method \u00b6 OBV . obv_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where obv is above other . See combine_objs() . obv_below method \u00b6 OBV . obv_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where obv is below other . See combine_objs() . obv_crossed_above method \u00b6 OBV . obv_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where obv is crossed_above other . See combine_objs() . obv_crossed_below method \u00b6 OBV . obv_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where obv is crossed_below other . See combine_objs() . obv_equal method \u00b6 OBV . obv_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where obv is equal other . See combine_objs() . obv_stats method \u00b6 OBV . obv_stats ( * args , ** kwargs ) Stats of obv as generic. plot method \u00b6 _OBV . plot ( column = None , obv_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot OBV.obv . Args column :\u2002 str Name of the column to plot. obv_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for OBV.obv . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> vbt . OBV . run ( ohlcv [ 'Close' ], ohlcv [ 'Volume' ]) . plot () run class method \u00b6 OBV . run ( close , volume , short_name = 'obv' , hide_params = None , hide_default = True , ** kwargs ) Run OBV indicator. Inputs: close , volume Outputs: obv Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . volume method \u00b6 Input array. volume_above method \u00b6 OBV . volume_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where volume is above other . See combine_objs() . volume_below method \u00b6 OBV . volume_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where volume is below other . See combine_objs() . volume_crossed_above method \u00b6 OBV . volume_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where volume is crossed_above other . See combine_objs() . volume_crossed_below method \u00b6 OBV . volume_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where volume is crossed_below other . See combine_objs() . volume_equal method \u00b6 OBV . volume_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where volume is equal other . See combine_objs() . volume_stats method \u00b6 OBV . volume_stats ( * args , ** kwargs ) Stats of volume as generic. RSI class \u00b6 Relative Strength Index (RSI). Compares the magnitude of recent gains and losses over a specified time period to measure speed and change of price movements of a security. It is primarily used to attempt to identify overbought or oversold conditions in the trading of an asset. See Relative Strength Index (RSI) . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.indicators.basic.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.indicators.basic._RSI apply_func method \u00b6 RSI . apply_func ( close , window , ewm , adjust , cache_dict ) Apply function for RSI . close method \u00b6 Input array. close_above method \u00b6 RSI . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 RSI . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 RSI . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 RSI . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 RSI . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 RSI . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func . ewm_list property \u00b6 List of ewm values. plot method \u00b6 _RSI . plot ( column = None , levels = ( 30 , 70 ), rsi_trace_kwargs = None , add_trace_kwargs = None , xref = 'x' , yref = 'y' , fig = None , ** layout_kwargs ) Plot RSI.rsi . Args column :\u2002 str Name of the column to plot. levels :\u2002 tuple Two extremes: bottom and top. rsi_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for RSI.rsi . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> vbt . RSI . run ( ohlcv [ 'Close' ]) . plot () rsi property \u00b6 Output array. rsi_above method \u00b6 RSI . rsi_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where rsi is above other . See combine_objs() . rsi_below method \u00b6 RSI . rsi_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where rsi is below other . See combine_objs() . rsi_crossed_above method \u00b6 RSI . rsi_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where rsi is crossed_above other . See combine_objs() . rsi_crossed_below method \u00b6 RSI . rsi_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where rsi is crossed_below other . See combine_objs() . rsi_equal method \u00b6 RSI . rsi_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where rsi is equal other . See combine_objs() . rsi_stats method \u00b6 RSI . rsi_stats ( * args , ** kwargs ) Stats of rsi as generic. run class method \u00b6 RSI . run ( close , window = Default ( 14 ), ewm = Default ( False ), short_name = 'rsi' , hide_params = None , hide_default = True , ** kwargs ) Run RSI indicator. Inputs: close Parameters: window , ewm Outputs: rsi Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 RSI . run_combs ( close , window = Default ( 14 ), ewm = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple RSI indicators using function comb_func . Inputs: close Parameters: window , ewm Outputs: rsi comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to RSI.run() . window_list property \u00b6 List of window values. STOCH class \u00b6 Stochastic Oscillator (STOCH). A stochastic oscillator is a momentum indicator comparing a particular closing price of a security to a range of its prices over a certain period of time. It is used to generate overbought and oversold trading signals, utilizing a 0-100 bounded range of values. See Stochastic Oscillator . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.indicators.basic.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.indicators.basic._STOCH apply_func method \u00b6 STOCH . apply_func ( high , low , close , k_window , d_window , d_ewm , adjust , cache_dict ) Apply function for STOCH . close method \u00b6 Input array. close_above method \u00b6 STOCH . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 STOCH . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 STOCH . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 STOCH . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 STOCH . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 STOCH . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func . d_ewm_list property \u00b6 List of d_ewm values. d_window_list property \u00b6 List of d_window values. high method \u00b6 Input array. high_above method \u00b6 STOCH . high_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is above other . See combine_objs() . high_below method \u00b6 STOCH . high_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is below other . See combine_objs() . high_crossed_above method \u00b6 STOCH . high_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is crossed_above other . See combine_objs() . high_crossed_below method \u00b6 STOCH . high_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is crossed_below other . See combine_objs() . high_equal method \u00b6 STOCH . high_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is equal other . See combine_objs() . high_stats method \u00b6 STOCH . high_stats ( * args , ** kwargs ) Stats of high as generic. k_window_list property \u00b6 List of k_window values. low method \u00b6 Input array. low_above method \u00b6 STOCH . low_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is above other . See combine_objs() . low_below method \u00b6 STOCH . low_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is below other . See combine_objs() . low_crossed_above method \u00b6 STOCH . low_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is crossed_above other . See combine_objs() . low_crossed_below method \u00b6 STOCH . low_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is crossed_below other . See combine_objs() . low_equal method \u00b6 STOCH . low_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is equal other . See combine_objs() . low_stats method \u00b6 STOCH . low_stats ( * args , ** kwargs ) Stats of low as generic. percent_d property \u00b6 Output array. percent_d_above method \u00b6 STOCH . percent_d_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_d is above other . See combine_objs() . percent_d_below method \u00b6 STOCH . percent_d_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_d is below other . See combine_objs() . percent_d_crossed_above method \u00b6 STOCH . percent_d_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_d is crossed_above other . See combine_objs() . percent_d_crossed_below method \u00b6 STOCH . percent_d_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_d is crossed_below other . See combine_objs() . percent_d_equal method \u00b6 STOCH . percent_d_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_d is equal other . See combine_objs() . percent_d_stats method \u00b6 STOCH . percent_d_stats ( * args , ** kwargs ) Stats of percent_d as generic. percent_k property \u00b6 Output array. percent_k_above method \u00b6 STOCH . percent_k_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_k is above other . See combine_objs() . percent_k_below method \u00b6 STOCH . percent_k_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_k is below other . See combine_objs() . percent_k_crossed_above method \u00b6 STOCH . percent_k_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_k is crossed_above other . See combine_objs() . percent_k_crossed_below method \u00b6 STOCH . percent_k_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_k is crossed_below other . See combine_objs() . percent_k_equal method \u00b6 STOCH . percent_k_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_k is equal other . See combine_objs() . percent_k_stats method \u00b6 STOCH . percent_k_stats ( * args , ** kwargs ) Stats of percent_k as generic. plot method \u00b6 _STOCH . plot ( column = None , levels = ( 30 , 70 ), percent_k_trace_kwargs = None , percent_d_trace_kwargs = None , shape_kwargs = None , add_trace_kwargs = None , xref = 'x' , yref = 'y' , fig = None , ** layout_kwargs ) Plot STOCH.percent_k and STOCH.percent_d . Args column :\u2002 str Name of the column to plot. levels :\u2002 tuple Two extremes: bottom and top. percent_k_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for STOCH.percent_k . percent_d_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for STOCH.percent_d . shape_kwargs :\u2002 dict Keyword arguments passed to Figure or FigureWidget.add_shape for zone between levels. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> vbt . STOCH . run ( ohlcv [ 'High' ], ohlcv [ 'Low' ], ohlcv [ 'Close' ]) . plot () run class method \u00b6 STOCH . run ( high , low , close , k_window = Default ( 14 ), d_window = Default ( 3 ), d_ewm = Default ( False ), short_name = 'stoch' , hide_params = None , hide_default = True , ** kwargs ) Run STOCH indicator. Inputs: high , low , close Parameters: k_window , d_window , d_ewm Outputs: percent_k , percent_d Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 STOCH . run_combs ( high , low , close , k_window = Default ( 14 ), d_window = Default ( 3 ), d_ewm = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple STOCH indicators using function comb_func . Inputs: high , low , close Parameters: k_window , d_window , d_ewm Outputs: percent_k , percent_d comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to STOCH.run() .","title":"basic"},{"location":"api/indicators/basic/#vectorbt.indicators.basic","text":"Indicators built with IndicatorFactory . You can access all the indicators either by vbt.* or vbt.indicators.* . >>> import pandas as pd >>> import vectorbt as vbt >>> # vectorbt.indicators.basic.MA >>> vbt . MA . run ( pd . Series ([ 1 , 2 , 3 ]), [ 2 , 3 ]) . ma ma_window 2 3 ma_ewm False False 0 NaN NaN 1 1.5 NaN 2 2.5 2.0 The advantage of these indicators over TA-Lib's is that they work primarily on 2-dimensional arrays and utilize caching, which makes them faster for matrices with huge number of columns. They also have plotting methods. Run for the examples below: >>> import vectorbt as vbt >>> from datetime import datetime >>> start = '2019-03-01 UTC' # crypto is in UTC >>> end = '2019-09-01 UTC' >>> cols = [ 'Open' , 'High' , 'Low' , 'Close' , 'Volume' ] >>> ohlcv = vbt . YFData . download ( \"BTC-USD\" , start = start , end = end ) . get ( cols ) >>> ohlcv Open High Low \\ Date 2019-03-01 00:00:00+00:00 3853.757080 3907.795410 3851.692383 2019-03-02 00:00:00+00:00 3855.318115 3874.607422 3832.127930 2019-03-03 00:00:00+00:00 3862.266113 3875.483643 3836.905762 ... ... ... ... 2019-08-30 00:00:00+00:00 9514.844727 9656.124023 9428.302734 2019-08-31 00:00:00+00:00 9597.539062 9673.220703 9531.799805 2019-09-01 00:00:00+00:00 9630.592773 9796.755859 9582.944336 Close Volume Date 2019-03-01 00:00:00+00:00 3859.583740 7661247975 2019-03-02 00:00:00+00:00 3864.415039 7578786076 2019-03-03 00:00:00+00:00 3847.175781 7253558152 ... ... ... 2019-08-30 00:00:00+00:00 9598.173828 13595263986 2019-08-31 00:00:00+00:00 9630.664062 11454806419 2019-09-01 00:00:00+00:00 9757.970703 11445355859 [185 rows x 5 columns] >>> ohlcv . vbt . ohlcv . plot ()","title":"vectorbt.indicators.basic"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR","text":"Average True Range (ATR). The indicator provide an indication of the degree of price volatility. Strong moves, in either direction, are often accompanied by large ranges, or large True Ranges. See Average True Range - ATR . Note Uses Simple MA and Exponential MA as compared to Wilder. Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.indicators.basic.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.indicators.basic._ATR","title":"ATR"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.apply_func","text":"ATR . apply_func ( high , low , close , window , ewm , adjust , tr , cache_dict ) Apply function for ATR .","title":"apply_func()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.atr","text":"Output array.","title":"atr"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.atr_above","text":"ATR . atr_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where atr is above other . See combine_objs() .","title":"atr_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.atr_below","text":"ATR . atr_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where atr is below other . See combine_objs() .","title":"atr_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.atr_crossed_above","text":"ATR . atr_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where atr is crossed_above other . See combine_objs() .","title":"atr_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.atr_crossed_below","text":"ATR . atr_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where atr is crossed_below other . See combine_objs() .","title":"atr_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.atr_equal","text":"ATR . atr_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where atr is equal other . See combine_objs() .","title":"atr_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.atr_stats","text":"ATR . atr_stats ( * args , ** kwargs ) Stats of atr as generic.","title":"atr_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.close","text":"Input array.","title":"close"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.close_above","text":"ATR . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.close_below","text":"ATR . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.close_crossed_above","text":"ATR . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.close_crossed_below","text":"ATR . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.close_equal","text":"ATR . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.close_stats","text":"ATR . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.custom_func","text":"IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func .","title":"custom_func()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.ewm_list","text":"List of ewm values.","title":"ewm_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.high","text":"Input array.","title":"high"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.high_above","text":"ATR . high_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is above other . See combine_objs() .","title":"high_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.high_below","text":"ATR . high_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is below other . See combine_objs() .","title":"high_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.high_crossed_above","text":"ATR . high_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is crossed_above other . See combine_objs() .","title":"high_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.high_crossed_below","text":"ATR . high_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is crossed_below other . See combine_objs() .","title":"high_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.high_equal","text":"ATR . high_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is equal other . See combine_objs() .","title":"high_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.high_stats","text":"ATR . high_stats ( * args , ** kwargs ) Stats of high as generic.","title":"high_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.low","text":"Input array.","title":"low"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.low_above","text":"ATR . low_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is above other . See combine_objs() .","title":"low_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.low_below","text":"ATR . low_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is below other . See combine_objs() .","title":"low_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.low_crossed_above","text":"ATR . low_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is crossed_above other . See combine_objs() .","title":"low_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.low_crossed_below","text":"ATR . low_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is crossed_below other . See combine_objs() .","title":"low_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.low_equal","text":"ATR . low_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is equal other . See combine_objs() .","title":"low_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.low_stats","text":"ATR . low_stats ( * args , ** kwargs ) Stats of low as generic.","title":"low_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.plot","text":"_ATR . plot ( column = None , tr_trace_kwargs = None , atr_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot ATR.tr and ATR.atr . Args column :\u2002 str Name of the column to plot. tr_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for ATR.tr . atr_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for ATR.atr . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> vbt . ATR . run ( ohlcv [ 'High' ], ohlcv [ 'Low' ], ohlcv [ 'Close' ], 10 ) . plot ()","title":"plot()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.run","text":"ATR . run ( high , low , close , window = Default ( 14 ), ewm = Default ( True ), short_name = 'atr' , hide_params = None , hide_default = True , ** kwargs ) Run ATR indicator. Inputs: high , low , close Parameters: window , ewm Outputs: tr , atr Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.run_combs","text":"ATR . run_combs ( high , low , close , window = Default ( 14 ), ewm = Default ( True ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple ATR indicators using function comb_func . Inputs: high , low , close Parameters: window , ewm Outputs: tr , atr comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to ATR.run() .","title":"run_combs()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.tr","text":"Output array.","title":"tr"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.tr_above","text":"ATR . tr_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where tr is above other . See combine_objs() .","title":"tr_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.tr_below","text":"ATR . tr_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where tr is below other . See combine_objs() .","title":"tr_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.tr_crossed_above","text":"ATR . tr_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where tr is crossed_above other . See combine_objs() .","title":"tr_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.tr_crossed_below","text":"ATR . tr_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where tr is crossed_below other . See combine_objs() .","title":"tr_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.tr_equal","text":"ATR . tr_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where tr is equal other . See combine_objs() .","title":"tr_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.tr_stats","text":"ATR . tr_stats ( * args , ** kwargs ) Stats of tr as generic.","title":"tr_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.ATR.window_list","text":"List of window values.","title":"window_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS","text":"Bollinger Bands (BBANDS). A Bollinger Band\u00ae is a technical analysis tool defined by a set of lines plotted two standard deviations (positively and negatively) away from a simple moving average (SMA) of the security's price, but can be adjusted to user preferences. See Bollinger Band\u00ae . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.indicators.basic.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.indicators.basic._BBANDS","title":"BBANDS"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.alpha_list","text":"List of alpha values.","title":"alpha_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.apply_func","text":"BBANDS . apply_func ( close , window , ewm , alpha , adjust , ddof , ma_cache_dict , mstd_cache_dict ) Apply function for BBANDS .","title":"apply_func()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.bandwidth","text":"Custom property.","title":"bandwidth"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.bandwidth_above","text":"BBANDS . bandwidth_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where bandwidth is above other . See combine_objs() .","title":"bandwidth_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.bandwidth_below","text":"BBANDS . bandwidth_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where bandwidth is below other . See combine_objs() .","title":"bandwidth_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.bandwidth_crossed_above","text":"BBANDS . bandwidth_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where bandwidth is crossed_above other . See combine_objs() .","title":"bandwidth_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.bandwidth_crossed_below","text":"BBANDS . bandwidth_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where bandwidth is crossed_below other . See combine_objs() .","title":"bandwidth_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.bandwidth_equal","text":"BBANDS . bandwidth_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where bandwidth is equal other . See combine_objs() .","title":"bandwidth_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.bandwidth_stats","text":"BBANDS . bandwidth_stats ( * args , ** kwargs ) Stats of bandwidth as generic.","title":"bandwidth_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.close","text":"Input array.","title":"close"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.close_above","text":"BBANDS . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.close_below","text":"BBANDS . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.close_crossed_above","text":"BBANDS . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.close_crossed_below","text":"BBANDS . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.close_equal","text":"BBANDS . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.close_stats","text":"BBANDS . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.custom_func","text":"IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func .","title":"custom_func()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.ewm_list","text":"List of ewm values.","title":"ewm_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.lower","text":"Output array.","title":"lower"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.lower_above","text":"BBANDS . lower_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where lower is above other . See combine_objs() .","title":"lower_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.lower_below","text":"BBANDS . lower_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where lower is below other . See combine_objs() .","title":"lower_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.lower_crossed_above","text":"BBANDS . lower_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where lower is crossed_above other . See combine_objs() .","title":"lower_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.lower_crossed_below","text":"BBANDS . lower_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where lower is crossed_below other . See combine_objs() .","title":"lower_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.lower_equal","text":"BBANDS . lower_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where lower is equal other . See combine_objs() .","title":"lower_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.lower_stats","text":"BBANDS . lower_stats ( * args , ** kwargs ) Stats of lower as generic.","title":"lower_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.middle","text":"Output array.","title":"middle"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.middle_above","text":"BBANDS . middle_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where middle is above other . See combine_objs() .","title":"middle_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.middle_below","text":"BBANDS . middle_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where middle is below other . See combine_objs() .","title":"middle_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.middle_crossed_above","text":"BBANDS . middle_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where middle is crossed_above other . See combine_objs() .","title":"middle_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.middle_crossed_below","text":"BBANDS . middle_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where middle is crossed_below other . See combine_objs() .","title":"middle_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.middle_equal","text":"BBANDS . middle_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where middle is equal other . See combine_objs() .","title":"middle_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.middle_stats","text":"BBANDS . middle_stats ( * args , ** kwargs ) Stats of middle as generic.","title":"middle_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.percent_b","text":"Custom property.","title":"percent_b"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.percent_b_above","text":"BBANDS . percent_b_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_b is above other . See combine_objs() .","title":"percent_b_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.percent_b_below","text":"BBANDS . percent_b_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_b is below other . See combine_objs() .","title":"percent_b_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.percent_b_crossed_above","text":"BBANDS . percent_b_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_b is crossed_above other . See combine_objs() .","title":"percent_b_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.percent_b_crossed_below","text":"BBANDS . percent_b_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_b is crossed_below other . See combine_objs() .","title":"percent_b_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.percent_b_equal","text":"BBANDS . percent_b_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_b is equal other . See combine_objs() .","title":"percent_b_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.percent_b_stats","text":"BBANDS . percent_b_stats ( * args , ** kwargs ) Stats of percent_b as generic.","title":"percent_b_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.plot","text":"_BBANDS . plot ( column = None , plot_close = True , close_trace_kwargs = None , middle_trace_kwargs = None , upper_trace_kwargs = None , lower_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot BBANDS.middle , BBANDS.upper and BBANDS.lower against BBANDS.close . Args column :\u2002 str Name of the column to plot. plot_close :\u2002 bool Whether to plot MA.close . close_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for BBANDS.close . middle_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for BBANDS.middle . upper_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for BBANDS.upper . lower_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for BBANDS.lower . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> vbt . BBANDS . run ( ohlcv [ 'Close' ]) . plot ()","title":"plot()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.run","text":"BBANDS . run ( close , window = Default ( 20 ), ewm = Default ( False ), alpha = Default ( 2 ), short_name = 'bb' , hide_params = None , hide_default = True , ** kwargs ) Run BBANDS indicator. Inputs: close Parameters: window , ewm , alpha Outputs: middle , upper , lower Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.run_combs","text":"BBANDS . run_combs ( close , window = Default ( 20 ), ewm = Default ( False ), alpha = Default ( 2 ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple BBANDS indicators using function comb_func . Inputs: close Parameters: window , ewm , alpha Outputs: middle , upper , lower comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to BBANDS.run() .","title":"run_combs()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.upper","text":"Output array.","title":"upper"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.upper_above","text":"BBANDS . upper_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where upper is above other . See combine_objs() .","title":"upper_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.upper_below","text":"BBANDS . upper_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where upper is below other . See combine_objs() .","title":"upper_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.upper_crossed_above","text":"BBANDS . upper_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where upper is crossed_above other . See combine_objs() .","title":"upper_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.upper_crossed_below","text":"BBANDS . upper_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where upper is crossed_below other . See combine_objs() .","title":"upper_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.upper_equal","text":"BBANDS . upper_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where upper is equal other . See combine_objs() .","title":"upper_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.upper_stats","text":"BBANDS . upper_stats ( * args , ** kwargs ) Stats of upper as generic.","title":"upper_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.BBANDS.window_list","text":"List of window values.","title":"window_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA","text":"Moving Average (MA). A moving average is a widely used indicator in technical analysis that helps smooth out price action by filtering out the \u201cnoise\u201d from random short-term price fluctuations. See Moving Average (MA) . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.indicators.basic.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.indicators.basic._MA","title":"MA"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.apply_func","text":"MA . apply_func ( close , window , ewm , adjust , cache_dict ) Apply function for MA .","title":"apply_func()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.close","text":"Input array.","title":"close"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.close_above","text":"MA . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.close_below","text":"MA . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.close_crossed_above","text":"MA . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.close_crossed_below","text":"MA . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.close_equal","text":"MA . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.close_stats","text":"MA . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.custom_func","text":"IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func .","title":"custom_func()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.ewm_list","text":"List of ewm values.","title":"ewm_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.ma","text":"Output array.","title":"ma"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.ma_above","text":"MA . ma_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ma is above other . See combine_objs() .","title":"ma_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.ma_below","text":"MA . ma_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ma is below other . See combine_objs() .","title":"ma_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.ma_crossed_above","text":"MA . ma_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ma is crossed_above other . See combine_objs() .","title":"ma_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.ma_crossed_below","text":"MA . ma_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ma is crossed_below other . See combine_objs() .","title":"ma_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.ma_equal","text":"MA . ma_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ma is equal other . See combine_objs() .","title":"ma_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.ma_stats","text":"MA . ma_stats ( * args , ** kwargs ) Stats of ma as generic.","title":"ma_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.plot","text":"_MA . plot ( column = None , plot_close = True , close_trace_kwargs = None , ma_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot MA.ma against MA.close . Args column :\u2002 str Name of the column to plot. plot_close :\u2002 bool Whether to plot MA.close . close_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for MA.close . ma_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for MA.ma . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> vbt . MA . run ( ohlcv [ 'Close' ], 10 ) . plot ()","title":"plot()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.run","text":"MA . run ( close , window , ewm = Default ( False ), short_name = 'ma' , hide_params = None , hide_default = True , ** kwargs ) Run MA indicator. Inputs: close Parameters: window , ewm Outputs: ma Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.run_combs","text":"MA . run_combs ( close , window , ewm = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple MA indicators using function comb_func . Inputs: close Parameters: window , ewm Outputs: ma comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to MA.run() .","title":"run_combs()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MA.window_list","text":"List of window values.","title":"window_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD","text":"Moving Average Convergence Divergence (MACD). Is a trend-following momentum indicator that shows the relationship between two moving averages of prices. See Moving Average Convergence Divergence \u2013 MACD . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.indicators.basic.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.indicators.basic._MACD","title":"MACD"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.apply_func","text":"MACD . apply_func ( close , fast_window , slow_window , signal_window , macd_ewm , signal_ewm , adjust , cache_dict ) Apply function for MACD .","title":"apply_func()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.close","text":"Input array.","title":"close"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.close_above","text":"MACD . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.close_below","text":"MACD . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.close_crossed_above","text":"MACD . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.close_crossed_below","text":"MACD . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.close_equal","text":"MACD . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.close_stats","text":"MACD . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.custom_func","text":"IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func .","title":"custom_func()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.fast_window_list","text":"List of fast_window values.","title":"fast_window_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.hist","text":"Custom property.","title":"hist"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.hist_above","text":"MACD . hist_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where hist is above other . See combine_objs() .","title":"hist_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.hist_below","text":"MACD . hist_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where hist is below other . See combine_objs() .","title":"hist_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.hist_crossed_above","text":"MACD . hist_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where hist is crossed_above other . See combine_objs() .","title":"hist_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.hist_crossed_below","text":"MACD . hist_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where hist is crossed_below other . See combine_objs() .","title":"hist_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.hist_equal","text":"MACD . hist_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where hist is equal other . See combine_objs() .","title":"hist_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.hist_stats","text":"MACD . hist_stats ( * args , ** kwargs ) Stats of hist as generic.","title":"hist_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.macd","text":"Output array.","title":"macd"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.macd_above","text":"MACD . macd_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where macd is above other . See combine_objs() .","title":"macd_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.macd_below","text":"MACD . macd_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where macd is below other . See combine_objs() .","title":"macd_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.macd_crossed_above","text":"MACD . macd_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where macd is crossed_above other . See combine_objs() .","title":"macd_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.macd_crossed_below","text":"MACD . macd_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where macd is crossed_below other . See combine_objs() .","title":"macd_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.macd_equal","text":"MACD . macd_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where macd is equal other . See combine_objs() .","title":"macd_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.macd_ewm_list","text":"List of macd_ewm values.","title":"macd_ewm_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.macd_stats","text":"MACD . macd_stats ( * args , ** kwargs ) Stats of macd as generic.","title":"macd_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.plot","text":"_MACD . plot ( column = None , macd_trace_kwargs = None , signal_trace_kwargs = None , hist_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot MACD.macd , MACD.signal and MACD.hist . Args column :\u2002 str Name of the column to plot. macd_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for MACD.macd . signal_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for MACD.signal . hist_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Bar for MACD.hist . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> vbt . MACD . run ( ohlcv [ 'Close' ]) . plot ()","title":"plot()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.run","text":"MACD . run ( close , fast_window = Default ( 12 ), slow_window = Default ( 26 ), signal_window = Default ( 9 ), macd_ewm = Default ( False ), signal_ewm = Default ( False ), short_name = 'macd' , hide_params = None , hide_default = True , ** kwargs ) Run MACD indicator. Inputs: close Parameters: fast_window , slow_window , signal_window , macd_ewm , signal_ewm Outputs: macd , signal Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.run_combs","text":"MACD . run_combs ( close , fast_window = Default ( 12 ), slow_window = Default ( 26 ), signal_window = Default ( 9 ), macd_ewm = Default ( False ), signal_ewm = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple MACD indicators using function comb_func . Inputs: close Parameters: fast_window , slow_window , signal_window , macd_ewm , signal_ewm Outputs: macd , signal comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to MACD.run() .","title":"run_combs()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.signal","text":"Output array.","title":"signal"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.signal_above","text":"MACD . signal_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where signal is above other . See combine_objs() .","title":"signal_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.signal_below","text":"MACD . signal_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where signal is below other . See combine_objs() .","title":"signal_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.signal_crossed_above","text":"MACD . signal_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where signal is crossed_above other . See combine_objs() .","title":"signal_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.signal_crossed_below","text":"MACD . signal_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where signal is crossed_below other . See combine_objs() .","title":"signal_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.signal_equal","text":"MACD . signal_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where signal is equal other . See combine_objs() .","title":"signal_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.signal_ewm_list","text":"List of signal_ewm values.","title":"signal_ewm_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.signal_stats","text":"MACD . signal_stats ( * args , ** kwargs ) Stats of signal as generic.","title":"signal_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.signal_window_list","text":"List of signal_window values.","title":"signal_window_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MACD.slow_window_list","text":"List of slow_window values.","title":"slow_window_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD","text":"Moving Standard Deviation (MSTD). Standard deviation is an indicator that measures the size of an assets recent price moves in order to predict how volatile the price may be in the future. Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.indicators.basic.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.indicators.basic._MSTD","title":"MSTD"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.apply_func","text":"MSTD . apply_func ( close , window , ewm , adjust , ddof , cache_dict ) Apply function for MSTD .","title":"apply_func()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.close","text":"Input array.","title":"close"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.close_above","text":"MSTD . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.close_below","text":"MSTD . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.close_crossed_above","text":"MSTD . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.close_crossed_below","text":"MSTD . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.close_equal","text":"MSTD . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.close_stats","text":"MSTD . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.custom_func","text":"IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func .","title":"custom_func()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.ewm_list","text":"List of ewm values.","title":"ewm_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.mstd","text":"Output array.","title":"mstd"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.mstd_above","text":"MSTD . mstd_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where mstd is above other . See combine_objs() .","title":"mstd_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.mstd_below","text":"MSTD . mstd_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where mstd is below other . See combine_objs() .","title":"mstd_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.mstd_crossed_above","text":"MSTD . mstd_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where mstd is crossed_above other . See combine_objs() .","title":"mstd_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.mstd_crossed_below","text":"MSTD . mstd_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where mstd is crossed_below other . See combine_objs() .","title":"mstd_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.mstd_equal","text":"MSTD . mstd_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where mstd is equal other . See combine_objs() .","title":"mstd_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.mstd_stats","text":"MSTD . mstd_stats ( * args , ** kwargs ) Stats of mstd as generic.","title":"mstd_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.plot","text":"_MSTD . plot ( column = None , mstd_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot MSTD.mstd . Args column :\u2002 str Name of the column to plot. mstd_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for MSTD.mstd . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> vbt . MSTD . run ( ohlcv [ 'Close' ], 10 ) . plot ()","title":"plot()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.run","text":"MSTD . run ( close , window , ewm = Default ( False ), short_name = 'mstd' , hide_params = None , hide_default = True , ** kwargs ) Run MSTD indicator. Inputs: close Parameters: window , ewm Outputs: mstd Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.run_combs","text":"MSTD . run_combs ( close , window , ewm = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple MSTD indicators using function comb_func . Inputs: close Parameters: window , ewm Outputs: mstd comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to MSTD.run() .","title":"run_combs()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.MSTD.window_list","text":"List of window values.","title":"window_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV","text":"On-balance volume (OBV). It relates price and volume in the stock market. OBV is based on a cumulative total volume. See On-Balance Volume (OBV) . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.indicators.basic.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.run_combs() IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.indicators.basic._OBV","title":"OBV"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.close","text":"Input array.","title":"close"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.close_above","text":"OBV . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.close_below","text":"OBV . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.close_crossed_above","text":"OBV . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.close_crossed_below","text":"OBV . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.close_equal","text":"OBV . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.close_stats","text":"OBV . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.custom_func","text":"OBV . custom_func ( close , volume_ts ) Custom calculation function for OBV .","title":"custom_func()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.obv","text":"Output array.","title":"obv"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.obv_above","text":"OBV . obv_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where obv is above other . See combine_objs() .","title":"obv_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.obv_below","text":"OBV . obv_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where obv is below other . See combine_objs() .","title":"obv_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.obv_crossed_above","text":"OBV . obv_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where obv is crossed_above other . See combine_objs() .","title":"obv_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.obv_crossed_below","text":"OBV . obv_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where obv is crossed_below other . See combine_objs() .","title":"obv_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.obv_equal","text":"OBV . obv_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where obv is equal other . See combine_objs() .","title":"obv_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.obv_stats","text":"OBV . obv_stats ( * args , ** kwargs ) Stats of obv as generic.","title":"obv_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.plot","text":"_OBV . plot ( column = None , obv_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot OBV.obv . Args column :\u2002 str Name of the column to plot. obv_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for OBV.obv . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> vbt . OBV . run ( ohlcv [ 'Close' ], ohlcv [ 'Volume' ]) . plot ()","title":"plot()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.run","text":"OBV . run ( close , volume , short_name = 'obv' , hide_params = None , hide_default = True , ** kwargs ) Run OBV indicator. Inputs: close , volume Outputs: obv Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.volume","text":"Input array.","title":"volume"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.volume_above","text":"OBV . volume_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where volume is above other . See combine_objs() .","title":"volume_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.volume_below","text":"OBV . volume_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where volume is below other . See combine_objs() .","title":"volume_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.volume_crossed_above","text":"OBV . volume_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where volume is crossed_above other . See combine_objs() .","title":"volume_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.volume_crossed_below","text":"OBV . volume_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where volume is crossed_below other . See combine_objs() .","title":"volume_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.volume_equal","text":"OBV . volume_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where volume is equal other . See combine_objs() .","title":"volume_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.OBV.volume_stats","text":"OBV . volume_stats ( * args , ** kwargs ) Stats of volume as generic.","title":"volume_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI","text":"Relative Strength Index (RSI). Compares the magnitude of recent gains and losses over a specified time period to measure speed and change of price movements of a security. It is primarily used to attempt to identify overbought or oversold conditions in the trading of an asset. See Relative Strength Index (RSI) . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.indicators.basic.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.indicators.basic._RSI","title":"RSI"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.apply_func","text":"RSI . apply_func ( close , window , ewm , adjust , cache_dict ) Apply function for RSI .","title":"apply_func()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.close","text":"Input array.","title":"close"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.close_above","text":"RSI . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.close_below","text":"RSI . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.close_crossed_above","text":"RSI . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.close_crossed_below","text":"RSI . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.close_equal","text":"RSI . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.close_stats","text":"RSI . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.custom_func","text":"IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func .","title":"custom_func()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.ewm_list","text":"List of ewm values.","title":"ewm_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.plot","text":"_RSI . plot ( column = None , levels = ( 30 , 70 ), rsi_trace_kwargs = None , add_trace_kwargs = None , xref = 'x' , yref = 'y' , fig = None , ** layout_kwargs ) Plot RSI.rsi . Args column :\u2002 str Name of the column to plot. levels :\u2002 tuple Two extremes: bottom and top. rsi_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for RSI.rsi . add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> vbt . RSI . run ( ohlcv [ 'Close' ]) . plot ()","title":"plot()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.rsi","text":"Output array.","title":"rsi"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.rsi_above","text":"RSI . rsi_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where rsi is above other . See combine_objs() .","title":"rsi_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.rsi_below","text":"RSI . rsi_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where rsi is below other . See combine_objs() .","title":"rsi_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.rsi_crossed_above","text":"RSI . rsi_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where rsi is crossed_above other . See combine_objs() .","title":"rsi_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.rsi_crossed_below","text":"RSI . rsi_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where rsi is crossed_below other . See combine_objs() .","title":"rsi_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.rsi_equal","text":"RSI . rsi_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where rsi is equal other . See combine_objs() .","title":"rsi_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.rsi_stats","text":"RSI . rsi_stats ( * args , ** kwargs ) Stats of rsi as generic.","title":"rsi_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.run","text":"RSI . run ( close , window = Default ( 14 ), ewm = Default ( False ), short_name = 'rsi' , hide_params = None , hide_default = True , ** kwargs ) Run RSI indicator. Inputs: close Parameters: window , ewm Outputs: rsi Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.run_combs","text":"RSI . run_combs ( close , window = Default ( 14 ), ewm = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple RSI indicators using function comb_func . Inputs: close Parameters: window , ewm Outputs: rsi comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to RSI.run() .","title":"run_combs()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.RSI.window_list","text":"List of window values.","title":"window_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH","text":"Stochastic Oscillator (STOCH). A stochastic oscillator is a momentum indicator comparing a particular closing price of a security to a range of its prices over a certain period of time. It is used to generate overbought and oversold trading signals, utilizing a 0-100 bounded range of values. See Stochastic Oscillator . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.indicators.basic.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.indicators.basic._STOCH","title":"STOCH"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.apply_func","text":"STOCH . apply_func ( high , low , close , k_window , d_window , d_ewm , adjust , cache_dict ) Apply function for STOCH .","title":"apply_func()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.close","text":"Input array.","title":"close"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.close_above","text":"STOCH . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.close_below","text":"STOCH . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.close_crossed_above","text":"STOCH . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.close_crossed_below","text":"STOCH . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.close_equal","text":"STOCH . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.close_stats","text":"STOCH . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.custom_func","text":"IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func .","title":"custom_func()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.d_ewm_list","text":"List of d_ewm values.","title":"d_ewm_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.d_window_list","text":"List of d_window values.","title":"d_window_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.high","text":"Input array.","title":"high"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.high_above","text":"STOCH . high_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is above other . See combine_objs() .","title":"high_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.high_below","text":"STOCH . high_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is below other . See combine_objs() .","title":"high_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.high_crossed_above","text":"STOCH . high_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is crossed_above other . See combine_objs() .","title":"high_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.high_crossed_below","text":"STOCH . high_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is crossed_below other . See combine_objs() .","title":"high_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.high_equal","text":"STOCH . high_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is equal other . See combine_objs() .","title":"high_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.high_stats","text":"STOCH . high_stats ( * args , ** kwargs ) Stats of high as generic.","title":"high_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.k_window_list","text":"List of k_window values.","title":"k_window_list"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.low","text":"Input array.","title":"low"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.low_above","text":"STOCH . low_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is above other . See combine_objs() .","title":"low_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.low_below","text":"STOCH . low_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is below other . See combine_objs() .","title":"low_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.low_crossed_above","text":"STOCH . low_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is crossed_above other . See combine_objs() .","title":"low_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.low_crossed_below","text":"STOCH . low_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is crossed_below other . See combine_objs() .","title":"low_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.low_equal","text":"STOCH . low_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is equal other . See combine_objs() .","title":"low_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.low_stats","text":"STOCH . low_stats ( * args , ** kwargs ) Stats of low as generic.","title":"low_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.percent_d","text":"Output array.","title":"percent_d"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.percent_d_above","text":"STOCH . percent_d_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_d is above other . See combine_objs() .","title":"percent_d_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.percent_d_below","text":"STOCH . percent_d_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_d is below other . See combine_objs() .","title":"percent_d_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.percent_d_crossed_above","text":"STOCH . percent_d_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_d is crossed_above other . See combine_objs() .","title":"percent_d_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.percent_d_crossed_below","text":"STOCH . percent_d_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_d is crossed_below other . See combine_objs() .","title":"percent_d_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.percent_d_equal","text":"STOCH . percent_d_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_d is equal other . See combine_objs() .","title":"percent_d_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.percent_d_stats","text":"STOCH . percent_d_stats ( * args , ** kwargs ) Stats of percent_d as generic.","title":"percent_d_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.percent_k","text":"Output array.","title":"percent_k"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.percent_k_above","text":"STOCH . percent_k_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_k is above other . See combine_objs() .","title":"percent_k_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.percent_k_below","text":"STOCH . percent_k_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_k is below other . See combine_objs() .","title":"percent_k_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.percent_k_crossed_above","text":"STOCH . percent_k_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_k is crossed_above other . See combine_objs() .","title":"percent_k_crossed_above()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.percent_k_crossed_below","text":"STOCH . percent_k_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_k is crossed_below other . See combine_objs() .","title":"percent_k_crossed_below()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.percent_k_equal","text":"STOCH . percent_k_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where percent_k is equal other . See combine_objs() .","title":"percent_k_equal()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.percent_k_stats","text":"STOCH . percent_k_stats ( * args , ** kwargs ) Stats of percent_k as generic.","title":"percent_k_stats()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.plot","text":"_STOCH . plot ( column = None , levels = ( 30 , 70 ), percent_k_trace_kwargs = None , percent_d_trace_kwargs = None , shape_kwargs = None , add_trace_kwargs = None , xref = 'x' , yref = 'y' , fig = None , ** layout_kwargs ) Plot STOCH.percent_k and STOCH.percent_d . Args column :\u2002 str Name of the column to plot. levels :\u2002 tuple Two extremes: bottom and top. percent_k_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for STOCH.percent_k . percent_d_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for STOCH.percent_d . shape_kwargs :\u2002 dict Keyword arguments passed to Figure or FigureWidget.add_shape for zone between levels. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> vbt . STOCH . run ( ohlcv [ 'High' ], ohlcv [ 'Low' ], ohlcv [ 'Close' ]) . plot ()","title":"plot()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.run","text":"STOCH . run ( high , low , close , k_window = Default ( 14 ), d_window = Default ( 3 ), d_ewm = Default ( False ), short_name = 'stoch' , hide_params = None , hide_default = True , ** kwargs ) Run STOCH indicator. Inputs: high , low , close Parameters: k_window , d_window , d_ewm Outputs: percent_k , percent_d Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/indicators/basic/#vectorbt.indicators.basic.STOCH.run_combs","text":"STOCH . run_combs ( high , low , close , k_window = Default ( 14 ), d_window = Default ( 3 ), d_ewm = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple STOCH indicators using function comb_func . Inputs: high , low , close Parameters: k_window , d_window , d_ewm Outputs: percent_k , percent_d comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to STOCH.run() .","title":"run_combs()"},{"location":"api/indicators/configs/","text":"configs module \u00b6 Common configurations for indicators. flex_col_param_config Config \u00b6 Config for flexible column-wise parameters. flex_elem_param_config Config \u00b6 Config for flexible element-wise parameters.","title":"configs"},{"location":"api/indicators/configs/#vectorbt.indicators.configs","text":"Common configurations for indicators.","title":"vectorbt.indicators.configs"},{"location":"api/indicators/configs/#vectorbt.indicators.configs.flex_col_param_config","text":"Config for flexible column-wise parameters.","title":"flex_col_param_config"},{"location":"api/indicators/configs/#vectorbt.indicators.configs.flex_elem_param_config","text":"Config for flexible element-wise parameters.","title":"flex_elem_param_config"},{"location":"api/indicators/factory/","text":"factory module \u00b6 A factory for building new indicators with ease. The indicator factory class IndicatorFactory offers a convenient way to create technical indicators of any complexity. By providing it with information such as calculation functions and the names of your inputs, parameters, and outputs, it will create a stand-alone indicator class capable of running the indicator for an arbitrary combination of your inputs and parameters. It also creates methods for signal generation and supports common pandas and parameter indexing operations. Each indicator is basically a pipeline that: Accepts a list of input arrays (for example, OHLCV data) Accepts a list of parameter arrays (for example, window size) Accepts other relevant arguments and keyword arguments For each parameter combination, performs calculation on the input arrays Concatenates results into new output arrays (for example, rolling average) This pipeline can be well standardized, which is done by run_pipeline() . IndicatorFactory simplifies the usage of run_pipeline() by generating and pre-configuring a new Python class with various class methods for running the indicator. Each generated class includes the following features: Accepts input arrays of any compatible shape thanks to broadcasting Accepts output arrays written in-place instead of returning Accepts arbitrary parameter grids Supports caching and other optimizations out of the box Supports pandas and parameter indexing Offers helper methods for all inputs, outputs, and properties Consider the following price DataFrame composed of two columns, one per asset: >>> import vectorbt as vbt >>> import numpy as np >>> import pandas as pd >>> from numba import njit >>> from datetime import datetime >>> price = pd . DataFrame ({ ... 'a' : [ 1 , 2 , 3 , 4 , 5 ], ... 'b' : [ 5 , 4 , 3 , 2 , 1 ] ... }, index = pd . Index ([ ... datetime ( 2020 , 1 , 1 ), ... datetime ( 2020 , 1 , 2 ), ... datetime ( 2020 , 1 , 3 ), ... datetime ( 2020 , 1 , 4 ), ... datetime ( 2020 , 1 , 5 ), ... ])) . astype ( float ) >>> price a b 2020-01-01 1.0 5.0 2020-01-02 2.0 4.0 2020-01-03 3.0 3.0 2020-01-04 4.0 2.0 2020-01-05 5.0 1.0 For each column in the DataFrame, let's calculate a simple moving average and get its crossover with price. In particular, we want to test two different window sizes: 2 and 3. Naive approach \u00b6 A naive way of doing this: >>> ma_df = pd . DataFrame . vbt . concat ( ... price . rolling ( window = 2 ) . mean (), ... price . rolling ( window = 3 ) . mean (), ... keys = pd . Index ([ 2 , 3 ], name = 'ma_window' )) >>> ma_df ma_window 2 3 a b a b 2020-01-01 NaN NaN NaN NaN 2020-01-02 1.5 4.5 NaN NaN 2020-01-03 2.5 3.5 2.0 4.0 2020-01-04 3.5 2.5 3.0 3.0 2020-01-05 4.5 1.5 4.0 2.0 >>> above_signals = ( price . vbt . tile ( 2 ) . vbt > ma_df ) >>> above_signals = above_signals . vbt . signals . first ( after_false = True ) >>> above_signals ma_window 2 3 a b a b 2020-01-01 False False False False 2020-01-02 True False False False 2020-01-03 False False True False 2020-01-04 False False False False 2020-01-05 False False False False >>> below_signals = ( price . vbt . tile ( 2 ) . vbt < ma_df ) >>> below_signals = below_signals . vbt . signals . first ( after_false = True ) >>> below_signals ma_window 2 3 a b a b 2020-01-01 False False False False 2020-01-02 False True False False 2020-01-03 False False False True 2020-01-04 False False False False 2020-01-05 False False False False Now the same using IndicatorFactory : >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... param_names = [ 'window' ], ... output_names = [ 'ma' ], ... ) . from_apply_func ( vbt . nb . rolling_mean_nb ) >>> myind = MyInd . run ( price , [ 2 , 3 ]) >>> above_signals = myind . price_crossed_above ( myind . ma ) >>> below_signals = myind . price_crossed_below ( myind . ma ) The IndicatorFactory class is used to construct indicator classes from UDFs. First, we provide all the necessary information (indicator conig) to build the facade of the indicator, such as the names of inputs, parameters, and outputs, and the actual calculation function. The factory then generates a self-contained indicator class capable of running arbitrary configurations of inputs and parameters. To run any configuration, we can either use the run method (as we did above) or the run_combs method. run and run_combs methods \u00b6 The main method to run an indicator is run , which accepts arguments based on the config provided to the IndicatorFactory (see the example above). These arguments include input arrays, in-place output arrays, parameters, and arguments for run_pipeline() . The run_combs method takes the same inputs as the method above, but computes all combinations of passed parameters based on a combinatorial function and returns multiple instances that can be compared with each other. For example, this is useful to generate crossover signals of multiple moving averages: >>> myind1 , myind2 = MyInd . run_combs ( price , [ 2 , 3 , 4 ]) >>> myind1 . ma myind_1_window 2 3 a b a b a b 2020-01-01 NaN NaN NaN NaN NaN NaN 2020-01-02 1.5 4.5 1.5 4.5 NaN NaN 2020-01-03 2.5 3.5 2.5 3.5 2.0 4.0 2020-01-04 3.5 2.5 3.5 2.5 3.0 3.0 2020-01-05 4.5 1.5 4.5 1.5 4.0 2.0 >>> myind2 . ma myind_2_window 3 4 a b a b a b 2020-01-01 NaN NaN NaN NaN NaN NaN 2020-01-02 NaN NaN NaN NaN NaN NaN 2020-01-03 2.0 4.0 NaN NaN NaN NaN 2020-01-04 3.0 3.0 2.5 3.5 2.5 3.5 2020-01-05 4.0 2.0 3.5 2.5 3.5 2.5 >>> myind1 . ma_crossed_above ( myind2 . ma ) myind_1_window 2 3 myind_2_window 3 4 4 a b a b a b 2020-01-01 False False False False False False 2020-01-02 False False False False False False 2020-01-03 True False False False False False 2020-01-04 False False True False True False 2020-01-05 False False False False False False Its main advantage is that it doesn't need to re-compute each combination thanks to smart caching. To get details on what arguments are accepted by any of the class methods, use help : >>> help ( MyInd . run ) Help on method run: run(price, window, short_name='custom', hide_params=None, hide_default=True, **kwargs) method of builtins.type instance Run `Indicator` indicator. * Inputs: `price` * Parameters: `window` * Outputs: `ma` Pass a list of parameter names as `hide_params` to hide their column levels. Set `hide_default` to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to `vectorbt.indicators.factory.run_pipeline`. Parameters \u00b6 IndicatorFactory allows definition of arbitrary parameter grids. Parameters are variables that can hold one or more values. A single value can be passed as a scalar, an array, or any other object. Multiple values are passed as a list or an array (if the flag is_array_like is set to False for that parameter). If there are multiple parameters and each is having multiple values, their values will broadcast to a single shape: p1 p2 result 0 0 1 [(0, 1)] 1 [0, 1] [2] [(0, 2), (1, 2)] 2 [0, 1] [2, 3] [(0, 2), (1, 3)] 3 [0, 1] [2, 3, 4] error To illustrate the usage of parameters in indicators, let's build a basic indicator that returns 1 if the rolling mean is within upper and lower bounds, and -1 if it's outside: >>> @njit ... def apply_func_nb ( price , window , lower , upper ): ... output = np . full ( price . shape , np . nan , dtype = np . float_ ) ... for col in range ( price . shape [ 1 ]): ... for i in range ( window , price . shape [ 0 ]): ... mean = np . mean ( price [ i - window : i , col ]) ... output [ i , col ] = lower < mean < upper ... return output >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... param_names = [ 'window' , 'lower' , 'upper' ], ... output_names = [ 'output' ] ... ) . from_apply_func ( apply_func_nb ) By default, when per_column is set to False, each parameter is applied to the entire input. One parameter combination: >>> MyInd . run ( ... price , ... window = 2 , ... lower = 3 , ... upper = 5 ... ) . output custom_window 2 custom_lower 3 custom_upper 5 a b 2020-01-01 NaN NaN 2020-01-02 NaN NaN 2020-01-03 0.0 1.0 2020-01-04 0.0 1.0 2020-01-05 1.0 0.0 Multiple parameter combinations: >>> MyInd . run ( ... price , ... window = [ 2 , 3 ], ... lower = 3 , ... upper = 5 ... ) . output custom_window 2 3 custom_lower 3 3 custom_upper 5 5 a b a b 2020-01-01 NaN NaN NaN NaN 2020-01-02 NaN NaN NaN NaN 2020-01-03 0.0 1.0 NaN NaN 2020-01-04 0.0 1.0 0.0 1.0 2020-01-05 1.0 0.0 0.0 0.0 Product of parameter combinations: >>> MyInd . run ( ... price , ... window = [ 2 , 3 ], ... lower = [ 3 , 4 ], ... upper = 5 , ... param_product = True ... ) . output custom_window 2 3 custom_lower 3 4 3 4 custom_upper 5 5 5 5 a b a b a b a b 2020-01-01 NaN NaN NaN NaN NaN NaN NaN NaN 2020-01-02 NaN NaN NaN NaN NaN NaN NaN NaN 2020-01-03 0.0 1.0 0.0 1.0 NaN NaN NaN NaN 2020-01-04 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 2020-01-05 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Multiple parameter combinations, one per column: >>> MyInd . run ( ... price , ... window = [ 2 , 3 ], ... lower = [ 3 , 4 ], ... upper = 5 , ... per_column = True ... ) . output custom_window 2 3 custom_lower 3 4 custom_upper 5 5 a b 2020-01-01 NaN NaN 2020-01-02 NaN NaN 2020-01-03 0.0 NaN 2020-01-04 0.0 0.0 2020-01-05 1.0 0.0 Parameter defaults can be passed directly to the IndicatorFactory.from_custom_func() and IndicatorFactory.from_apply_func() , and overriden in the run method: >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... param_names = [ 'window' , 'lower' , 'upper' ], ... output_names = [ 'output' ] ... ) . from_apply_func ( apply_func_nb , window = 2 , lower = 3 , upper = 4 ) >>> MyInd . run ( price , upper = 5 ) . output custom_window 2 custom_lower 3 custom_upper 5 a b 2020-01-01 NaN NaN 2020-01-02 NaN NaN 2020-01-03 0.0 1.0 2020-01-04 0.0 1.0 2020-01-05 1.0 0.0 Some parameters are meant to be defined per row, column, or element of the input. By default, if we pass the parameter value as an array, the indicator will treat this array as a list of multiple values - one per input. To make the indicator view this array as a single value, set the flag is_array_like to True in param_settings . Also, to automatically broadcast the passed scalar/array to the input shape, set bc_to_input to True, 0 (index axis), or 1 (column axis). In our example, the parameter window can broadcast per column, and both parameters lower and upper can broadcast per element: >>> @njit ... def apply_func_nb ( price , window , lower , upper ): ... output = np . full ( price . shape , np . nan , dtype = np . float_ ) ... for col in range ( price . shape [ 1 ]): ... for i in range ( window [ col ], price . shape [ 0 ]): ... mean = np . mean ( price [ i - window [ col ]: i , col ]) ... output [ i , col ] = lower [ i , col ] < mean < upper [ i , col ] ... return output >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... param_names = [ 'window' , 'lower' , 'upper' ], ... output_names = [ 'output' ] ... ) . from_apply_func ( ... apply_func_nb , ... param_settings = dict ( ... window = dict ( is_array_like = True , bc_to_input = 1 , per_column = True ), ... lower = dict ( is_array_like = True , bc_to_input = True ), ... upper = dict ( is_array_like = True , bc_to_input = True ) ... ) ... ) >>> MyInd . run ( ... price , ... window = [ np . array ([ 2 , 3 ]), np . array ([ 3 , 4 ])], ... lower = np . array ([ 1 , 2 ]), ... upper = np . array ([ 3 , 4 ]), ... ) . output custom_window 2 3 4 custom_lower array_0 array_0 array_1 array_1 custom_upper array_0 array_0 array_1 array_1 a b a b 2020-01-01 NaN NaN NaN NaN 2020-01-02 NaN NaN NaN NaN 2020-01-03 1.0 NaN NaN NaN 2020-01-04 1.0 0.0 1.0 NaN 2020-01-05 0.0 1.0 0.0 1.0 Broadcasting a huge number of parameters to the input shape can consume lots of memory, especially when the array materializes. Luckily, vectorbt implements flexible broadcasting, which preserves the original dimensions of the parameter. This requires two changes: setting keep_raw to True in broadcast_kwargs and passing flex_2d to the apply function. There are two configs in vectorbt.indicators.configs exactly for this purpose: one for column-wise broadcasting and one for element-wise broadcasting: >>> from vectorbt.base.reshape_fns import flex_select_auto_nb >>> from vectorbt.indicators.configs import flex_col_param_config , flex_elem_param_config >>> @njit ... def apply_func_nb ( price , window , lower , upper , flex_2d ): ... output = np . full ( price . shape , np . nan , dtype = np . float_ ) ... for col in range ( price . shape [ 1 ]): ... _window = flex_select_auto_nb ( window , 0 , col , flex_2d ) ... for i in range ( _window , price . shape [ 0 ]): ... _lower = flex_select_auto_nb ( lower , i , col , flex_2d ) ... _upper = flex_select_auto_nb ( upper , i , col , flex_2d ) ... mean = np . mean ( price [ i - _window : i , col ]) ... output [ i , col ] = _lower < mean < _upper ... return output >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... param_names = [ 'window' , 'lower' , 'upper' ], ... output_names = [ 'output' ] ... ) . from_apply_func ( ... apply_func_nb , ... param_settings = dict ( ... window = flex_col_param_config , ... lower = flex_elem_param_config , ... upper = flex_elem_param_config ... ), ... pass_flex_2d = True ... ) Both bound parameters can now be passed as a scalar (value per whole input), a 1-dimensional array (value per row or column, depending upon whether input is a Series or a DataFrame), a 2-dimensional array (value per element), or a list of any of those. This allows for the highest parameter flexibility at the lowest memory cost. For example, let's build a grid of two parameter combinations, each being one window size per column and both bounds per element: >>> MyInd . run ( ... price , ... window = [ np . array ([ 2 , 3 ]), np . array ([ 3 , 4 ])], ... lower = price . values - 3 , ... upper = price . values + 3 , ... ) . output custom_window 2 3 4 custom_lower array_0 array_0 array_1 array_1 custom_upper array_0 array_0 array_1 array_1 a b a b 2020-01-01 NaN NaN NaN NaN 2020-01-02 NaN NaN NaN NaN 2020-01-03 1.0 NaN NaN NaN 2020-01-04 1.0 1.0 1.0 NaN 2020-01-05 1.0 1.0 1.0 1.0 Indicators can also be parameterless. See OBV . Inputs \u00b6 IndicatorFactory supports passing none, one, or multiple inputs. If multiple inputs are passed, it tries to broadcast them into a single shape. Remember that in vectorbt each column means a separate backtest instance. That's why in order to use multiple pieces of information, such as open, high, low, close, and volume, we need to provide them as separate pandas objects rather than a single DataFrame. Let's create a parameterless indicator that measures the position of the close price within each bar: >>> @njit ... def apply_func_nb ( high , low , close ): ... return ( close - low ) / ( high - low ) >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'high' , 'low' , 'close' ], ... output_names = [ 'output' ] ... ) . from_apply_func ( apply_func_nb ) >>> MyInd . run ( price + 1 , price - 1 , price ) . output a b 2020-01-01 0.5 0.5 2020-01-02 0.5 0.5 2020-01-03 0.5 0.5 2020-01-04 0.5 0.5 2020-01-05 0.5 0.5 To demonstrate broadcasting, let's pass high as a DataFrame, low as a Series, and close as a scalar: >>> df = pd . DataFrame ( np . random . uniform ( 1 , 2 , size = ( 5 , 2 ))) >>> sr = pd . Series ( np . random . uniform ( 0 , 1 , size = 5 )) >>> MyInd . run ( df , sr , 1 ) . output 0 1 0 0.960680 0.666820 1 0.400646 0.528456 2 0.093467 0.134777 3 0.037210 0.102411 4 0.529012 0.652602 By default, if a Series was passed, it's automatically expanded into a 2-dimensional array. To keep it as 1-dimensional, set to_2d to False. Similar to parameters, we can also define defaults for inputs. In addition to using scalars and arrays as default values, we can reference other inputs: >>> @njit ... def apply_func_nb ( ts1 , ts2 , ts3 ): ... return ts1 + ts2 + ts3 >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'ts1' , 'ts2' , 'ts3' ], ... output_names = [ 'output' ] ... ) . from_apply_func ( apply_func_nb , ts2 = 'ts1' , ts3 = 'ts1' ) >>> MyInd . run ( price ) . output a b 2020-01-01 3.0 15.0 2020-01-02 6.0 12.0 2020-01-03 9.0 9.0 2020-01-04 12.0 6.0 2020-01-05 15.0 3.0 >>> MyInd . run ( price , ts2 = price * 2 ) . output a b 2020-01-01 4.0 20.0 2020-01-02 8.0 16.0 2020-01-03 12.0 12.0 2020-01-04 16.0 8.0 2020-01-05 20.0 4.0 What if an indicator doesn't take any input arrays? In that case, we can force the user to at least provide the input shape. Let's define a generator that emulates random returns and generates synthetic price: >>> @njit ... def apply_func_nb ( input_shape , start , mu , sigma ): ... rand_returns = np . random . normal ( mu , sigma , input_shape ) ... return start * vbt . nb . nancumprod_nb ( rand_returns + 1 ) >>> MyInd = vbt . IndicatorFactory ( ... param_names = [ 'start' , 'mu' , 'sigma' ], ... output_names = [ 'output' ] ... ) . from_apply_func ( ... apply_func_nb , ... require_input_shape = True , ... seed = 42 ... ) >>> MyInd . run ( price . shape , 100 , 0 , 0.01 ) . output custom_start 100 custom_mu 0 custom_sigma 0.01 0.01 0 100.496714 99.861736 1 101.147620 101.382660 2 100.910779 101.145285 3 102.504375 101.921510 4 102.023143 102.474495 We can also supply pandas meta such as input_index and input_columns to the run method: >>> MyInd . run ( ... price . shape , 100 , 0 , 0.01 , ... input_index = price . index , input_columns = price . columns ... ) . output custom_start 100 custom_mu 0 custom_sigma 0.01 0.01 a b 2020-01-01 100.496714 99.861736 2020-01-02 101.147620 101.382660 2020-01-03 100.910779 101.145285 2020-01-04 102.504375 101.921510 2020-01-05 102.023143 102.474495 One can even build input-less indicator that decides on the output shape dynamically: >>> from vectorbt.base.combine_fns import apply_and_concat_one >>> def apply_func ( i , ps , input_shape ): ... out = np . full ( input_shape , 0 ) ... out [: ps [ i ]] = 1 ... return out >>> def custom_func ( ps ): ... input_shape = ( np . max ( ps ),) ... return apply_and_concat_one ( len ( ps ), apply_func , ps , input_shape ) >>> MyInd = vbt . IndicatorFactory ( ... param_names = [ 'p' ], ... output_names = [ 'output' ] ... ) . from_custom_func ( custom_func ) >>> MyInd . run ([ 1 , 2 , 3 , 4 , 5 ]) . output custom_p 1 2 3 4 5 0 1 1 1 1 1 1 0 1 1 1 1 2 0 0 1 1 1 3 0 0 0 1 1 4 0 0 0 0 1 Outputs \u00b6 There are two types of outputs: regular and in-place outputs: Regular outputs are one or more arrays returned by the function. Each should have an exact same shape and match the number of columns in the input multiplied by the number of parameter values. In-place outputs are not returned but modified in-place. They broadcast together with inputs and are passed to the calculation function as a list, one per parameter. Two regular outputs: >>> @njit ... def apply_func_nb ( price ): ... return price - 1 , price + 1 >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... output_names = [ 'out1' , 'out2' ] ... ) . from_apply_func ( apply_func_nb ) >>> myind = MyInd . run ( price ) >>> pd . testing . assert_frame_equal ( myind . out1 , myind . price - 1 ) >>> pd . testing . assert_frame_equal ( myind . out2 , myind . price + 1 ) One regular output and one in-place output: >>> @njit ... def apply_func_nb ( price , in_out2 ): ... in_out2 [:] = price + 1 ... return price - 1 >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... output_names = [ 'out1' ], ... in_output_names = [ 'in_out2' ] ... ) . from_apply_func ( apply_func_nb ) >>> myind = MyInd . run ( price ) >>> pd . testing . assert_frame_equal ( myind . out1 , myind . price - 1 ) >>> pd . testing . assert_frame_equal ( myind . in_out2 , myind . price + 1 ) Two in-place outputs: >>> @njit ... def apply_func_nb ( price , in_out1 , in_out2 ): ... in_out1 [:] = price - 1 ... in_out2 [:] = price + 1 >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... in_output_names = [ 'in_out1' , 'in_out2' ] ... ) . from_apply_func ( apply_func_nb ) >>> myind = MyInd . run ( price ) >>> pd . testing . assert_frame_equal ( myind . in_out1 , myind . price - 1 ) >>> pd . testing . assert_frame_equal ( myind . in_out2 , myind . price + 1 ) By default, in-place outputs are created as empty arrays with uninitialized values. This allows creation of optional outputs that, if not written, do not occupy much memory. Since not all outputs are meant to be of data type float , we can pass dtype in the in_output_settings . >>> @njit ... def apply_func_nb ( price , in_out ): ... in_out [:] = price > np . mean ( price ) >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... in_output_names = [ 'in_out' ] ... ) . from_apply_func ( ... apply_func_nb , ... in_output_settings = dict ( in_out = dict ( dtype = bool )) ... ) >>> MyInd . run ( price ) . in_out a b 2020-01-01 False True 2020-01-02 False True 2020-01-03 False False 2020-01-04 True False 2020-01-05 True False Another advantage of in-place outputs is that we can provide their initial state: >>> @njit ... def apply_func_nb ( price , in_out1 , in_out2 ): ... in_out1 [:] = in_out1 + price ... in_out2 [:] = in_out2 + price >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... in_output_names = [ 'in_out1' , 'in_out2' ] ... ) . from_apply_func ( ... apply_func_nb , ... in_out1 = 100 , ... in_out2 = 'price' ... ) >>> myind = MyInd . run ( price ) >>> myind . in_out1 a b 2020-01-01 101 105 2020-01-02 102 104 2020-01-03 103 103 2020-01-04 104 102 2020-01-05 105 101 >>> myind . in_out2 a b 2020-01-01 2.0 10.0 2020-01-02 4.0 8.0 2020-01-03 6.0 6.0 2020-01-04 8.0 4.0 2020-01-05 10.0 2.0 Without Numba \u00b6 It's also possible to supply a function that is not Numba-compiled. This is handy when working with third-party libraries (see the implementation of IndicatorFactory.from_talib() ). Additionally, we can set keep_pd to True to pass all inputs as pandas objects instead of raw NumPy arrays. Note Already broadcasted pandas meta will be provided; that is, each input array will have the same index and columns. Let's demonstrate this by wrapping a basic composed pandas_ta strategy: >>> import pandas_ta >>> def apply_func ( open , high , low , close , volume , ema_len , linreg_len ): ... df = pd . DataFrame ( dict ( open = open , high = high , low = low , close = close , volume = volume )) ... df . ta . strategy ( pandas_ta . Strategy ( \"MyStrategy\" , [ ... dict ( kind = 'ema' , length = ema_len ), ... dict ( kind = 'linreg' , close = 'EMA_' + str ( ema_len ), length = linreg_len ) ... ])) ... return tuple ([ df . iloc [:, i ] for i in range ( 5 , len ( df . columns ))]) >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'open' , 'high' , 'low' , 'close' , 'volume' ], ... param_names = [ 'ema_len' , 'linreg_len' ], ... output_names = [ 'ema' , 'ema_linreg' ] ... ) . from_apply_func ( ... apply_func , ... keep_pd = True , ... to_2d = False ... ) >>> my_ind = MyInd . run ( ... ohlcv [ 'Open' ], ... ohlcv [ 'High' ], ... ohlcv [ 'Low' ], ... ohlcv [ 'Close' ], ... ohlcv [ 'Volume' ], ... ema_len = 5 , ... linreg_len = [ 8 , 9 , 10 ] ... ) >>> my_ind . ema_linreg custom_ema_len 5 custom_linreg_len 8 9 10 date 2021-02-02 NaN NaN NaN 2021-02-03 NaN NaN NaN 2021-02-04 NaN NaN NaN 2021-02-05 NaN NaN NaN 2021-02-06 NaN NaN NaN ... ... ... ... 2021-02-25 52309.302811 52602.005326 52899.576568 2021-02-26 50797.264793 51224.188381 51590.825690 2021-02-28 49217.904905 49589.546052 50066.206828 2021-03-01 48316.305403 48553.540713 48911.701664 2021-03-02 47984.395969 47956.885953 48150.929668 In the example above, only one Series per open, high, low, close, and volume can be passed. To enable the indicator to process two-dimensional data, set to_2d to True and create a loop over each column in the apply_func . Hint Writing a native Numba-compiled code may provide a performance that is magnitudes higher than that offered by libraries that work on pandas. Raw outputs and caching \u00b6 IndicatorFactory re-uses calculation artifacts whenever possible. Since it was originally designed for hyperparameter optimization and there are times when parameter values gets repeated, prevention of processing the same parameter over and over again is inevitable for good performance. For instance, when the run_combs method is being used and run_unique is set to True, it first calculates the raw outputs of all unique parameter combinations and then uses them to build outputs for the whole parameter grid. Let's first take a look at a typical raw output by setting return_raw to True: >>> raw = vbt . MA . run ( price , 2 , [ False , True ], return_raw = True ) >>> raw ([array([[ nan, nan, nan, nan], [1.5 , 4.5 , 1.66666667, 4.33333333], [2.5 , 3.5 , 2.55555556, 3.44444444], [3.5 , 2.5 , 3.51851852, 2.48148148], [4.5 , 1.5 , 4.50617284, 1.49382716]])], [(2, False), (2, True)], 2, []) It consists of a list of the returned output arrays, a list of the zipped parameter combinations, the number of input columns, and other objects returned along with output arrays but not listed in output_names . The next time we decide to run the indicator on a subset of the parameters above, we can simply pass this tuple as the use_raw argument. This won't call the calculation function and will throw an error if some of the requested parameter combinations cannot be found in raw . >>> vbt . MA . run ( price , 2 , True , use_raw = raw ) . ma ma_window 2 ma_ewm True a b 2020-01-01 NaN NaN 2020-01-02 1.666667 4.333333 2020-01-03 2.555556 3.444444 2020-01-04 3.518519 2.481481 2020-01-05 4.506173 1.493827 Here is how the performance compares when repeatedly running the same parameter combination with and without run_unique : >>> a = np . random . uniform ( size = ( 1000 ,)) >>> % timeit vbt . MA . run ( a , np . full ( 1000 , 2 ), run_unique = False ) 73.4 ms \u00b1 4.76 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) >>> % timeit vbt . MA . run ( a , np . full ( 1000 , 2 ), run_unique = True ) 8.99 ms \u00b1 114 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) Note run_unique is disabled by default. Enable run_unique if input arrays have few columns and there are tons of repeated parameter combinations. Disable run_unique if input arrays are very wide, if two identical parameter combinations can lead to different results, or when requesting raw output, cache, or additional outputs outside of output_names . Another performance enhancement can be introduced by caching, which has to be implemented by the user. The class method IndicatorFactory.from_apply_func() has an argument cache_func , which is called prior to the main calculation. Consider the following scenario: we want to compute the relative distance between two expensive rolling windows. We have already decided on the value for the first window, and want to test thousands of values for the second window. Without caching, and even with run_unique enabled, the first rolling window will be re-calculated over and over again and waste our resources: >>> @njit ... def roll_mean_expensive_nb ( price , w ): ... for i in range ( 100 ): ... out = vbt . nb . rolling_mean_nb ( price , w ) ... return out >>> @njit ... def apply_func_nb ( price , w1 , w2 ): ... roll_mean1 = roll_mean_expensive_nb ( price , w1 ) ... roll_mean2 = roll_mean_expensive_nb ( price , w2 ) ... return ( roll_mean2 - roll_mean1 ) / roll_mean1 >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... param_names = [ 'w1' , 'w2' ], ... output_names = [ 'output' ], ... ) . from_apply_func ( apply_func_nb ) >>> MyInd . run ( price , 2 , 3 ) . output custom_w1 2 custom_w2 3 a b 2020-01-01 NaN NaN 2020-01-02 NaN NaN 2020-01-03 -0.200000 0.142857 2020-01-04 -0.142857 0.200000 2020-01-05 -0.111111 0.333333 >>> % timeit MyInd . run ( price , 2 , np . arange ( 2 , 1000 )) 264 ms \u00b1 3.22 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) To avoid this, let's cache all unique rolling windows: >>> @njit ... def cache_func_nb ( price , ws1 , ws2 ): ... cache_dict = dict () ... ws = ws1 . copy () ... ws . extend ( ws2 ) ... for i in range ( len ( ws )): ... h = hash (( ws [ i ])) ... if h not in cache_dict : ... cache_dict [ h ] = roll_mean_expensive_nb ( price , ws [ i ]) ... return cache_dict >>> @njit ... def apply_func_nb ( price , w1 , w2 , cache_dict ): ... return ( cache_dict [ hash ( w2 )] - cache_dict [ hash ( w1 )]) / cache_dict [ hash ( w1 )] >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... param_names = [ 'w1' , 'w2' ], ... output_names = [ 'output' ], ... ) . from_apply_func ( apply_func_nb , cache_func = cache_func_nb ) >>> MyInd . run ( price , 2 , 3 ) . output custom_w1 2 custom_w2 3 a b 2020-01-01 NaN NaN 2020-01-02 NaN NaN 2020-01-03 -0.200000 0.142857 2020-01-04 -0.142857 0.200000 2020-01-05 -0.111111 0.333333 >>> % timeit MyInd . run ( price , 2 , np . arange ( 2 , 1000 )) 145 ms \u00b1 4.55 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) We have cut down the processing time almost in half. Similar to raw outputs, we can force IndicatorFactory to return the cache, so it can be used in other calculations or even indicators. The clear advantage of this approach is that we don't rely on some fixed set of parameter combinations any more, but on the values of each parameter, which gives us more granularity in managing performance. >>> cache = MyInd . run ( price , 2 , np . arange ( 2 , 1000 ), return_cache = True ) >>> % timeit MyInd . run ( price , np . arange ( 2 , 1000 ), np . arange ( 2 , 1000 ), use_cache = cache ) 30.1 ms \u00b1 2 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) Custom properties and methods \u00b6 Use custom_output_props argument when constructing an indicator to define lazy outputs - outputs that are processed only when explicitly called. They will become cached properties and, in contrast to regular outputs, they can have an arbitrary shape. For example, let's attach a property that will calculate the distance between the moving average and the price. >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... param_names = [ 'window' ], ... output_names = [ 'ma' ], ... custom_output_props = dict ( distance = lambda self : ( self . price - self . ma ) / self . ma ) ... ) . from_apply_func ( vbt . nb . rolling_mean_nb ) >>> MyInd . run ( price , [ 2 , 3 ]) . distance custom_window 2 3 a b a b 2020-01-01 NaN NaN NaN NaN 2020-01-02 0.333333 -0.111111 NaN NaN 2020-01-03 0.200000 -0.142857 0.500000 -0.250000 2020-01-04 0.142857 -0.200000 0.333333 -0.333333 2020-01-05 0.111111 -0.333333 0.250000 -0.500000 Another way of defining own properties and methods is subclassing: >>> class MyIndExtended ( MyInd ): ... def plot ( self , column = None , ** kwargs ): ... self_col = self . select_one ( column = column , group_by = False ) ... return self . ma . vbt . plot ( ** kwargs ) >>> MyIndExtended . run ( price , [ 2 , 3 ])[( 2 , 'a' )] . plot () Helper properties and methods \u00b6 For all in input_names , in_output_names , output_names , and custom_output_props , IndicatorFactory will create a bunch of comparison and combination methods, such as for generating signals. What kind of methods are created can be regulated using dtype in the attr_settings dictionary. >>> from collections import namedtuple >>> MyEnum = namedtuple ( 'MyEnum' , [ 'one' , 'two' ])( 0 , 1 ) >>> def apply_func_nb ( price ): ... out_float = np . empty ( price . shape , dtype = np . float_ ) ... out_bool = np . empty ( price . shape , dtype = np . bool_ ) ... out_enum = np . empty ( price . shape , dtype = np . int_ ) ... return out_float , out_bool , out_enum >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... output_names = [ 'out_float' , 'out_bool' , 'out_enum' ], ... attr_settings = dict ( ... out_float = dict ( dtype = np . float_ ), ... out_bool = dict ( dtype = np . bool_ ), ... out_enum = dict ( dtype = MyEnum ) ... )) . from_apply_func ( apply_func_nb ) >>> myind = MyInd . run ( price ) >>> dir ( myind ) [ ... 'out_bool', 'out_bool_and', 'out_bool_or', 'out_bool_stats', 'out_bool_xor', 'out_enum', 'out_enum_readable', 'out_enum_stats', 'out_float', 'out_float_above', 'out_float_below', 'out_float_equal', 'out_float_stats', ... 'price', 'price_above', 'price_below', 'price_equal', 'price_stats', ... ] Each of these methods and properties are created for sheer convenience: to easily combine boolean arrays using logical rules and to compare numeric arrays. All operations are done strictly using NumPy. Another advantage is utilization of vectorbt's own broadcasting, such that one can combine inputs and outputs with an arbitrary array-like object, given their shapes can broadcast together. We can also do comparison with multiple objects at once by passing them as a tuple/list: >>> myind . price_above ([ 1.5 , 2.5 ]) custom_price_above 1.5 2.5 a b a b 2020-01-01 False True False True 2020-01-02 True True False True 2020-01-03 True True True True 2020-01-04 True True True False 2020-01-05 True False True False Indexing \u00b6 IndicatorFactory attaches pandas indexing to the indicator class thanks to ArrayWrapper . Supported are iloc , loc , *param_name*_loc , xs , and __getitem__ . This makes possible accessing rows and columns by labels, integer positions, and parameters. >>> ma = vbt . MA . run ( price , [ 2 , 3 ]) >>> ma [( 2 , 'b' )] <vectorbt.indicators.basic.MA at 0x7fe4d10ddcc0> >>> ma [( 2 , 'b' )] . ma 2020-01-01 NaN 2020-01-02 4.5 2020-01-03 3.5 2020-01-04 2.5 2020-01-05 1.5 Name: (2, b), dtype: float64 >>> ma . window_loc [ 2 ] . ma a b 2020-01-01 NaN NaN 2020-01-02 1.5 4.5 2020-01-03 2.5 3.5 2020-01-04 3.5 2.5 2020-01-05 4.5 1.5 TA-Lib \u00b6 Indicator factory also provides a class method IndicatorFactory.from_talib() that can be used to wrap any function from TA-Lib. It automatically fills all the neccessary information, such as input, parameter and output names. Stats \u00b6 Hint See StatsBuilderMixin.stats() . We can attach metrics to any new indicator class: >>> @njit ... def apply_func_nb ( price ): ... return price ** 2 , price ** 3 >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... output_names = [ 'out1' , 'out2' ], ... metrics = dict ( ... sum_diff = dict ( ... calc_func = lambda self : self . out2 . sum () - self . out1 . sum () ... ) ... ) ... ) . from_apply_func ( ... apply_func_nb ... ) >>> myind = MyInd . run ( price ) >>> myind . stats ( column = 'a' ) sum_diff 170.0 Name: a, dtype: float64 Plots \u00b6 Hint See PlotsBuilderMixin.plots() . Similarly to stats, we can attach subplots to any new indicator class: >>> @njit ... def apply_func_nb ( price ): ... return price ** 2 , price ** 3 >>> def plot_outputs ( out1 , out2 , column = None , fig = None ): ... fig = out1 [ column ] . rename ( 'out1' ) . vbt . plot ( fig = fig ) ... fig = out2 [ column ] . rename ( 'out2' ) . vbt . plot ( fig = fig ) >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... output_names = [ 'out1' , 'out2' ], ... subplots = dict ( ... plot_outputs = dict ( ... plot_func = plot_outputs , ... resolve_out1 = True , ... resolve_out2 = True ... ) ... ) ... ) . from_apply_func ( ... apply_func_nb ... ) >>> myind = MyInd . run ( price ) >>> myind . plots ( column = 'a' ) build_columns function \u00b6 build_columns ( param_list , input_columns , level_names = None , hide_levels = None , param_settings = None , per_column = False , ignore_default = False , ** kwargs ) For each parameter in param_list , create a new column level with parameter values and stack it on top of input_columns . Returns a list of parameter indexes and new columns. combine_objs function \u00b6 combine_objs ( obj , other , * args , level_name = None , keys = None , allow_multiple = True , ** kwargs ) Combines/compares obj to other , for example, to generate signals. Both will broadcast together. Pass other as a tuple or a list to compare with multiple arguments. In this case, a new column level will be created with the name level_name . See BaseAccessor.combine() . params_to_list function \u00b6 params_to_list ( params , is_tuple , is_array_like ) Cast parameters to a list. prepare_params function \u00b6 prepare_params ( param_list , param_settings = None , input_shape = None , to_2d = False ) Prepare parameters. run_pipeline function \u00b6 run_pipeline ( num_ret_outputs , custom_func , * args , require_input_shape = False , input_shape = None , input_index = None , input_columns = None , input_list = None , in_output_list = None , in_output_settings = None , broadcast_kwargs = None , param_list = None , param_product = False , param_settings = None , run_unique = False , silence_warnings = False , per_column = False , pass_col = False , keep_pd = False , to_2d = True , as_lists = False , pass_input_shape = False , pass_flex_2d = False , level_names = None , hide_levels = None , stacking_kwargs = None , return_raw = False , use_raw = None , wrapper_kwargs = None , seed = None , ** kwargs ) A pipeline for running an indicator, used by IndicatorFactory . Args num_ret_outputs :\u2002 int The number of output arrays returned by custom_func . custom_func :\u2002 callable A custom calculation function. See IndicatorFactory.from_custom_func() . *args Arguments passed to the custom_func . require_input_shape :\u2002 bool Whether to input shape is required. Will set pass_input_shape to True and raise an error if input_shape is None. input_shape :\u2002 tuple Shape to broadcast each input to. Can be passed to custom_func . See pass_input_shape . input_index :\u2002 index_like Sets index of each input. Can be used to label index if no inputs passed. input_columns :\u2002 index_like Sets columns of each input. Can be used to label columns if no inputs passed. input_list :\u2002 list of array_like A list of input arrays. in_output_list :\u2002 list of array_like A list of in-place output arrays. If an array should be generated, pass None. in_output_settings :\u2002 dict or list of dict Settings corresponding to each in-place output. Following keys are accepted: dtype : Create this array using this data type and np.empty . Default is None. broadcast_kwargs :\u2002 dict Keyword arguments passed to broadcast() to broadcast inputs. param_list :\u2002 list of any A list of parameters. Each element is either an array-like object or a single value of any type. param_product :\u2002 bool Whether to build a Cartesian product out of all parameters. param_settings :\u2002 dict or list of dict Settings corresponding to each parameter. Following keys are accepted: dtype : If data type is an enumerated type or other mapping, and a string as parameter value was passed, will convert it first. is_tuple : If tuple was passed, it will be considered as a single value. To treat it as multiple values, pack it into a list. is_array_like : If array-like object was passed, it will be considered as a single value. To treat it as multiple values, pack it into a list. bc_to_input : Whether to broadcast parameter to input size. You can also broadcast parameter to an axis by passing an integer. broadcast_kwargs : Keyword arguments passed to broadcast() . per_column : Whether each parameter value can be split per column such that it can be better reflected in a multi-index. Does not affect broadcasting. run_unique :\u2002 bool Whether to run only on unique parameter combinations. Disable if two identical parameter combinations can lead to different results (e.g., due to randomness) or if inputs are large and custom_func is fast. Note Cache, raw output, and output objects outside of num_ret_outputs will be returned for unique parameter combinations only. silence_warnings :\u2002 bool Whether to hide warnings such as coming from run_unique . per_column :\u2002 bool Whether to split the DataFrame into Series, one per column, and run custom_func on each Series. Each list of parameter values will broadcast to the number of columns and each parameter value will be applied per Series rather than per DataFrame. Input shape must be known beforehand. pass_col :\u2002 bool Whether to pass column index as keyword argument if per_column is set to True. keep_pd :\u2002 bool Whether to keep inputs as pandas objects, otherwise convert to NumPy arrays. to_2d :\u2002 bool Whether to reshape inputs to 2-dim arrays, otherwise keep as-is. as_lists :\u2002 bool Whether to pass inputs and parameters to custom_func as lists. If custom_func is Numba-compiled, passes tuples. pass_input_shape :\u2002 bool Whether to pass input_shape to custom_func as keyword argument. pass_flex_2d :\u2002 bool Whether to pass flex_2d to custom_func as keyword argument. level_names :\u2002 list of str A list of column level names corresponding to each parameter. Should have the same length as param_list . hide_levels :\u2002 list of int A list of indices of parameter levels to hide. stacking_kwargs :\u2002 dict Keyword arguments passed to repeat_index() , tile_index() , and stack_indexes() when stacking parameter and input column levels. return_raw :\u2002 bool Whether to return raw output without post-processing and hashed parameter tuples. use_raw :\u2002 bool Takes the raw results and uses them instead of running custom_func . wrapper_kwargs :\u2002 dict Keyword arguments passed to ArrayWrapper . seed :\u2002 int Set seed to make output deterministic. **kwargs Keyword arguments passed to the custom_func . Some common arguments include return_cache to return cache and use_cache to use cache. Those are only applicable to custom_func that supports it ( custom_func created using IndicatorFactory.from_apply_func() are supported by default). Returns Array wrapper, list of inputs ( np.ndarray ), input mapper ( np.ndarray ), list of outputs ( np.ndarray ), list of parameter arrays ( np.ndarray ), list of parameter mappers ( np.ndarray ), list of outputs that are outside of num_ret_outputs . Explanation Here is a subset of tasks that the function run_pipeline() does: Takes one or multiple array objects in input_list and broadcasts them. >>> sr = pd . Series ([ 1 , 2 ], index = [ 'x' , 'y' ]) >>> df = pd . DataFrame ([[ 3 , 4 ], [ 5 , 6 ]], index = [ 'x' , 'y' ], columns = [ 'a' , 'b' ]) >>> input_list = vbt . base . reshape_fns . broadcast ( sr , df ) >>> input_list [ 0 ] a b x 1 1 y 2 2 >>> input_list [ 1 ] a b x 3 4 y 5 6 Takes one or multiple parameters in param_list , converts them to NumPy arrays and broadcasts them. >>> p1 , p2 , p3 = 1 , [ 2 , 3 , 4 ], [ False ] >>> param_list = vbt . base . reshape_fns . broadcast ( p1 , p2 , p3 ) >>> param_list [ 0 ] array([1, 1, 1]) >>> param_list [ 1 ] array([2, 3, 4]) >>> param_list [ 2 ] array([False, False, False]) Performs calculation using custom_func to build output arrays ( output_list ) and other objects ( other_list , optionally). >>> def custom_func ( ts1 , ts2 , p1 , p2 , p3 , * args , ** kwargs ): ... return np . hstack (( ... ts1 + ts2 + p1 [ 0 ] * p2 [ 0 ], ... ts1 + ts2 + p1 [ 1 ] * p2 [ 1 ], ... ts1 + ts2 + p1 [ 2 ] * p2 [ 2 ], ... )) >>> output = custom_func ( * input_list , * param_list ) >>> output array([[ 6, 7, 7, 8, 8, 9], [ 9, 10, 10, 11, 11, 12]]) Creates new column hierarchy based on parameters and level names. >>> p1_columns = pd . Index ( param_list [ 0 ], name = 'p1' ) >>> p2_columns = pd . Index ( param_list [ 1 ], name = 'p2' ) >>> p3_columns = pd . Index ( param_list [ 2 ], name = 'p3' ) >>> p_columns = vbt . base . index_fns . stack_indexes ([ p1_columns , p2_columns , p3_columns ]) >>> new_columns = vbt . base . index_fns . combine_indexes ([ p_columns , input_list [ 0 ] . columns ]) >>> output_df = pd . DataFrame ( output , columns = new_columns ) >>> output_df p1 1 p2 2 3 4 p3 False False False False False False a b a b a b 0 6 7 7 8 8 9 1 9 10 10 11 11 12 Broadcasts objects in input_list to match the shape of objects in output_list through tiling. This is done to be able to compare them and generate signals, since we cannot compare NumPy arrays that have totally different shapes, such as (2, 2) and (2, 6). >>> new_input_list = [ ... input_list [ 0 ] . vbt . tile ( len ( param_list [ 0 ]), keys = p_columns ), ... input_list [ 1 ] . vbt . tile ( len ( param_list [ 0 ]), keys = p_columns ) ... ] >>> new_input_list [ 0 ] p1 1 p2 2 3 4 p3 False False False False False False a b a b a b 0 1 1 1 1 1 1 1 2 2 2 2 2 2 Builds parameter mappers that will link parameters from param_list to columns in input_list and output_list . This is done to enable column indexing using parameter values. IndicatorBase class \u00b6 Indicator base class. Properties should be set before instantiation. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() PlotsBuilderMixin.plots_defaults StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() StatsBuilderMixin.stats_defaults Wrapping.config Wrapping.iloc Wrapping.indexing_kwargs Wrapping.loc Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Wrapping.self_aliases Wrapping.wrapper Wrapping.writeable_attrs Subclasses ATR BBANDS BOLB FIXLB FMAX FMEAN FMIN FSTD LEXLB MA MACD MEANLB MSTD OBV OHLCSTCX OHLCSTX RAND RANDNX RANDX RPROB RPROBCX RPROBNX RPROBX RSI STCX STOCH STX TRENDLB in_output_names method \u00b6 Names of the in-place output arrays. indexing_func method \u00b6 IndicatorBase . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on IndicatorBase . input_names method \u00b6 Names of the input arrays. level_names property \u00b6 Column level names corresponding to each parameter. output_flags method \u00b6 Dictionary of output flags. output_names method \u00b6 Names of the regular output arrays. param_names method \u00b6 Names of the parameters. run class method \u00b6 IndicatorBase . run ( * args , ** kwargs ) Public run method. run_combs class method \u00b6 IndicatorBase . run_combs ( * args , ** kwargs ) Public run combinations method. short_name property \u00b6 Name of the indicator. IndicatorFactory class \u00b6 A factory for creating new indicators. Initialize IndicatorFactory to create a skeleton and then use a class method such as IndicatorFactory.from_custom_func() to bind a calculation function to the skeleton. Args class_name :\u2002 str Name for the created indicator class. class_docstring :\u2002 str Docstring for the created indicator class. module_name :\u2002 str Specify the module the class originates from. short_name :\u2002 str A short name of the indicator. Defaults to lower-case class_name . prepend_name :\u2002 bool Whether to prepend short_name to each parameter level. input_names :\u2002 list of str A list of names of input arrays. param_names :\u2002 list of str A list of names of parameters. in_output_names :\u2002 list of str A list of names of in-place output arrays. An in-place output is an output that is not returned but modified in-place. Some advantages of such outputs include: 1) they don't need to be returned, 2) they can be passed between functions as easily as inputs, 3) they can be provided with already allocated data to safe memory, 4) if data or default value are not provided, they are created empty to not occupy memory. output_names :\u2002 list of str A list of names of output arrays. output_flags :\u2002 dict A dictionary of in-place and regular output flags. custom_output_props :\u2002 dict A dictionary with user-defined functions that will be bound to the indicator class and wrapped with @cached_property . attr_settings :\u2002 dict A dictionary of settings by attribute name. Attributes can be input_names , in_output_names , output_names and custom_output_props . Following keys are accepted: dtype : Data type used to determine which methods to generate around this attribute. Set to None to disable. Default is np.float_ . Can be set to instance of collections.namedtuple acting as enumerated type, or any other mapping; It will then create a property with suffix readable that contains data in a string format. metrics :\u2002 dict Metrics supported by StatsBuilderMixin.stats() . If dict, will be converted to Config . stats_defaults :\u2002 callable or dict Defaults for StatsBuilderMixin.stats() . If dict, will be converted into a property. subplots :\u2002 dict Subplots supported by PlotsBuilderMixin.plots() . If dict, will be converted to Config . plots_defaults :\u2002 callable or dict Defaults for PlotsBuilderMixin.plots() . If dict, will be converted into a property. Note The __init__ method is not used for running the indicator, for this use run . The reason for this is indexing, which requires a clean __init__ method for creating a new indicator object with newly indexed attributes. Subclasses SignalFactory find_ta_indicator class method \u00b6 IndicatorFactory . find_ta_indicator ( cls_name ) Get ta indicator class by its name. from_apply_func method \u00b6 IndicatorFactory . from_apply_func ( apply_func , cache_func = None , pass_packed = False , kwargs_to_args = None , numba_loop = False , ** kwargs ) Build indicator class around a custom apply function. In contrast to IndicatorFactory.from_custom_func() , this method handles a lot of things for you, such as caching, parameter selection, and concatenation. Your part is writing a function apply_func that accepts a selection of parameters (single values as opposed to multiple values in IndicatorFactory.from_custom_func() ) and does the calculation. It then automatically concatenates the resulting arrays into a single array per output. While this approach is simpler, it's also less flexible, since we can only work with one parameter selection at a time and can't view all parameters. The UDF apply_func also can't take keyword arguments, nor it can return anything other than outputs listed in output_names . Note If apply_func is a Numba-compiled function: All inputs are automatically converted to NumPy arrays Each argument in *args must be of a Numba-compatible type You cannot pass keyword arguments Your outputs must be arrays of the same shape, data type and data order Args apply_func :\u2002 callable A function that takes inputs, selection of parameters, and other arguments, and does calculations to produce outputs. Arguments are passed to apply_func in the following order: input_shape if pass_input_shape is set to True and input_shape not in kwargs_to_args col if per_column and pass_col are set to True and col not in kwargs_to_args broadcast time-series arrays corresponding to input_names broadcast in-place output arrays corresponding to in_output_names single parameter selection corresponding to param_names variable arguments if var_args is set to True arguments listed in kwargs_to_args flex_2d if pass_flex_2d is set to True and flex_2d not in kwargs_to_args keyword arguments if apply_func is not Numba-compiled Can be Numba-compiled. Note Shape of each output should be the same and match the shape of each input. cache_func :\u2002 callable A caching function to preprocess data beforehand. Takes the same arguments as apply_func . Should return a single object or a tuple of objects. All returned objects will be passed unpacked as last arguments to apply_func . Can be Numba-compiled. pass_packed :\u2002 bool Whether to pass packed tuples for inputs, in-place outputs, and parameters. kwargs_to_args :\u2002 list of str Keyword arguments from kwargs dict to pass as positional arguments to the apply function. Should be used together with numba_loop set to True since Numba doesn't support variable keyword arguments. Defaults to []. Order matters. numba_loop :\u2002 bool Whether to loop using Numba. Set to True when iterating large number of times over small input, but note that Numba doesn't support variable keyword arguments. **kwargs Keyword arguments passed to IndicatorFactory.from_custom_func() . Returns Indicator Additionally, each run method now supports use_ray argument, which indicates whether to use Ray to execute apply_func in parallel. Only works with numba_loop set to False. See ray_apply() for related keyword arguments. Usage The following example produces the same indicator as the IndicatorFactory.from_custom_func() example. >>> @njit ... def apply_func_nb ( ts1 , ts2 , p1 , p2 , arg1 , arg2 ): ... return ts1 * p1 + arg1 , ts2 * p2 + arg2 >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'ts1' , 'ts2' ], ... param_names = [ 'p1' , 'p2' ], ... output_names = [ 'o1' , 'o2' ] ... ) . from_apply_func ( ... apply_func_nb , var_args = True , ... kwargs_to_args = [ 'arg2' ], arg2 = 200 ) >>> myInd = MyInd . run ( price , price * 2 , [ 1 , 2 ], [ 3 , 4 ], 100 ) >>> myInd . o1 custom_p1 1 2 custom_p2 3 4 a b a b 2020-01-01 101.0 105.0 102.0 110.0 2020-01-02 102.0 104.0 104.0 108.0 2020-01-03 103.0 103.0 106.0 106.0 2020-01-04 104.0 102.0 108.0 104.0 2020-01-05 105.0 101.0 110.0 102.0 >>> myInd . o2 custom_p1 1 2 custom_p2 3 4 a b a b 2020-01-01 206.0 230.0 208.0 240.0 2020-01-02 212.0 224.0 216.0 232.0 2020-01-03 218.0 218.0 224.0 224.0 2020-01-04 224.0 212.0 232.0 216.0 2020-01-05 230.0 206.0 240.0 208.0 from_custom_func method \u00b6 IndicatorFactory . from_custom_func ( custom_func , require_input_shape = False , param_settings = None , in_output_settings = None , hide_params = None , hide_default = True , var_args = False , keyword_only_args = False , ** pipeline_kwargs ) Build indicator class around a custom calculation function. In contrast to IndicatorFactory.from_apply_func() , this method offers full flexbility. It's up to we to handle caching and concatenate columns for each parameter (for example, by using apply_and_concat_one() ). Also, you should ensure that each output array has an appropriate number of columns, which is the number of columns in input arrays multiplied by the number of parameter combinations. Args custom_func :\u2002 callable A function that takes broadcast arrays corresponding to input_names , broadcast in-place output arrays corresponding to in_output_names , broadcast parameter arrays corresponding to param_names , and other arguments and keyword arguments, and returns outputs corresponding to output_names and other objects that are then returned with the indicator instance. Can be Numba-compiled. Note Shape of each output should be the same and match the shape of each input stacked n times (= the number of parameter values) along the column axis. require_input_shape :\u2002 bool Whether to input shape is required. param_settings :\u2002 dict A dictionary of parameter settings keyed by name. See run_pipeline() for keys. Can be overwritten by any run method. in_output_settings :\u2002 dict A dictionary of in-place output settings keyed by name. See run_pipeline() for keys. Can be overwritten by any run method. hide_params :\u2002 list of str Parameter names to hide column levels for. Can be overwritten by any run method. hide_default :\u2002 bool Whether to hide column levels of parameters with default value. Can be overwritten by any run method. var_args :\u2002 bool Whether run methods should accept variable arguments ( *args ). Set to True if custom_func accepts positional agruments that are not listed in the config. keyword_only_args :\u2002 bool Whether run methods should accept keyword-only arguments ( * ). Set to True to force the user to use keyword arguments (e.g., to avoid misplacing arguments). **pipeline_kwargs Keyword arguments passed to run_pipeline() . Can be overwritten by any run method. Can contain default values for param_names and in_output_names , but also custom positional and keyword arguments passed to the custom_func . Returns Indicator , and optionally other objects that are returned by custom_func and exceed output_names . Usage The following example produces the same indicator as the IndicatorFactory.from_apply_func() example. >>> @njit >>> def apply_func_nb ( i , ts1 , ts2 , p1 , p2 , arg1 , arg2 ): ... return ts1 * p1 [ i ] + arg1 , ts2 * p2 [ i ] + arg2 >>> @njit ... def custom_func ( ts1 , ts2 , p1 , p2 , arg1 , arg2 ): ... return vbt . base . combine_fns . apply_and_concat_multiple_nb ( ... len ( p1 ), apply_func_nb , ts1 , ts2 , p1 , p2 , arg1 , arg2 ) >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'ts1' , 'ts2' ], ... param_names = [ 'p1' , 'p2' ], ... output_names = [ 'o1' , 'o2' ] ... ) . from_custom_func ( custom_func , var_args = True , arg2 = 200 ) >>> myInd = MyInd . run ( price , price * 2 , [ 1 , 2 ], [ 3 , 4 ], 100 ) >>> myInd . o1 custom_p1 1 2 custom_p2 3 4 a b a b 2020-01-01 101.0 105.0 102.0 110.0 2020-01-02 102.0 104.0 104.0 108.0 2020-01-03 103.0 103.0 106.0 106.0 2020-01-04 104.0 102.0 108.0 104.0 2020-01-05 105.0 101.0 110.0 102.0 >>> myInd . o2 custom_p1 1 2 custom_p2 3 4 a b a b 2020-01-01 206.0 230.0 208.0 240.0 2020-01-02 212.0 224.0 216.0 232.0 2020-01-03 218.0 218.0 224.0 224.0 2020-01-04 224.0 212.0 232.0 216.0 2020-01-05 230.0 206.0 240.0 208.0 The difference between apply_func_nb here and in IndicatorFactory.from_apply_func() is that here it takes the index of the current parameter combination that can be used for parameter selection. You can also remove the entire apply_func_nb and define your logic in custom_func (which shouldn't necessarily be Numba-compiled): >>> @njit ... def custom_func ( ts1 , ts2 , p1 , p2 , arg1 , arg2 ): ... input_shape = ts1 . shape ... n_params = len ( p1 ) ... out1 = np . empty (( input_shape [ 0 ], input_shape [ 1 ] * n_params ), dtype = np . float_ ) ... out2 = np . empty (( input_shape [ 0 ], input_shape [ 1 ] * n_params ), dtype = np . float_ ) ... for k in range ( n_params ): ... for col in range ( input_shape [ 1 ]): ... for i in range ( input_shape [ 0 ]): ... out1 [ i , input_shape [ 1 ] * k + col ] = ts1 [ i , col ] * p1 [ k ] + arg1 ... out2 [ i , input_shape [ 1 ] * k + col ] = ts2 [ i , col ] * p2 [ k ] + arg2 ... return out1 , out2 from_pandas_ta class method \u00b6 IndicatorFactory . from_pandas_ta ( func_name , parse_kwargs = None , init_kwargs = None , ** kwargs ) Build an indicator class around a pandas-ta function. Requires pandas-ta installed. Args func_name :\u2002 str Function name. parse_kwargs :\u2002 dict Keyword arguments passed to IndicatorFactory.parse_pandas_ta_config() . init_kwargs :\u2002 dict Keyword arguments passed to IndicatorFactory . **kwargs Keyword arguments passed to IndicatorFactory.from_custom_func() . Returns Indicator Usage >>> SMA = vbt . IndicatorFactory . from_pandas_ta ( 'SMA' ) >>> sma = SMA . run ( price , length = [ 2 , 3 ]) >>> sma . sma sma_length 2 3 a b a b 2020-01-01 NaN NaN NaN NaN 2020-01-02 1.5 4.5 NaN NaN 2020-01-03 2.5 3.5 2.0 4.0 2020-01-04 3.5 2.5 3.0 3.0 2020-01-05 4.5 1.5 4.0 2.0 To get help on running the indicator, use the help command: >>> help ( SMA . run ) Help on method run: run(close, length=None, offset=None, short_name='sma', hide_params=None, hide_default=True, **kwargs) method of builtins.type instance Run `SMA` indicator. * Inputs: `close` * Parameters: `length`, `offset` * Outputs: `sma` Pass a list of parameter names as `hide_params` to hide their column levels. Set `hide_default` to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to [run_pipeline()](/api/indicators/factory/#vectorbt.indicators.factory.run_pipeline \"vectorbt.indicators.factory.run_pipeline\"). To get the indicator docstring, use the help command or print the __doc__ attribute: >>> print ( SMA . __doc__ ) Simple Moving Average (SMA) The Simple Moving Average is the classic moving average that is the equally weighted average over n periods. Sources: <https://www.tradingtechnologies.com/help/x-study/technical-indicator-definitions/simple-moving-average-sma/> Calculation: Default Inputs: length=10 SMA = SUM(close, length) / length Args: close (pd.Series): Series of 'close's length (int): It's period. Default: 10 offset (int): How many periods to offset the result. Default: 0 Kwargs: adjust (bool): Default: True presma (bool, optional): If True, uses SMA for initial value. fillna (value, optional): pd.DataFrame.fillna(value) fill_method (value, optional): Type of fill method Returns: pd.Series: New feature generated. from_ta class method \u00b6 IndicatorFactory . from_ta ( cls_name , init_kwargs = None , ** kwargs ) Build an indicator class around a ta class. Requires ta installed. Args cls_name :\u2002 str Class name. init_kwargs :\u2002 dict Keyword arguments passed to IndicatorFactory . **kwargs Keyword arguments passed to IndicatorFactory.from_custom_func() . Returns Indicator Usage >>> SMAIndicator = vbt . IndicatorFactory . from_ta ( 'SMAIndicator' ) >>> sma = SMAIndicator . run ( price , window = [ 2 , 3 ]) >>> sma . sma_indicator smaindicator_window 2 3 a b a b 2020-01-01 NaN NaN NaN NaN 2020-01-02 1.5 4.5 NaN NaN 2020-01-03 2.5 3.5 2.0 4.0 2020-01-04 3.5 2.5 3.0 3.0 2020-01-05 4.5 1.5 4.0 2.0 To get help on running the indicator, use the help command: >>> help ( SMAIndicator . run ) Help on method run: run(close, window, fillna=False, short_name='smaindicator', hide_params=None, hide_default=True, **kwargs) method of builtins.type instance Run `SMAIndicator` indicator. * Inputs: `close` * Parameters: `window`, `fillna` * Outputs: `sma_indicator` Pass a list of parameter names as `hide_params` to hide their column levels. Set `hide_default` to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to [run_pipeline()](/api/indicators/factory/#vectorbt.indicators.factory.run_pipeline \"vectorbt.indicators.factory.run_pipeline\"). To get the indicator docstring, use the help command or print the __doc__ attribute: >>> print ( SMAIndicator . __doc__ ) SMA - Simple Moving Average Args: close(pandas.Series): dataset 'Close' column. window(int): n period. fillna(bool): if True, fill nan values. from_talib class method \u00b6 IndicatorFactory . from_talib ( func_name , init_kwargs = None , ** kwargs ) Build an indicator class around a TA-Lib function. Requires TA-Lib installed. For input, parameter and output names, see docs . Args func_name :\u2002 str Function name. init_kwargs :\u2002 dict Keyword arguments passed to IndicatorFactory . **kwargs Keyword arguments passed to IndicatorFactory.from_custom_func() . Returns Indicator Usage >>> SMA = vbt . IndicatorFactory . from_talib ( 'SMA' ) >>> sma = SMA . run ( price , timeperiod = [ 2 , 3 ]) >>> sma . real sma_timeperiod 2 3 a b a b 2020-01-01 NaN NaN NaN NaN 2020-01-02 1.5 4.5 NaN NaN 2020-01-03 2.5 3.5 2.0 4.0 2020-01-04 3.5 2.5 3.0 3.0 2020-01-05 4.5 1.5 4.0 2.0 To get help on running the indicator, use the help command: >>> help ( SMA . run ) Help on method run: run(close, timeperiod=30, short_name='sma', hide_params=None, hide_default=True, **kwargs) method of builtins.type instance Run `SMA` indicator. * Inputs: `close` * Parameters: `timeperiod` * Outputs: `real` Pass a list of parameter names as `hide_params` to hide their column levels. Set `hide_default` to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to [run_pipeline()](/api/indicators/factory/#vectorbt.indicators.factory.run_pipeline \"vectorbt.indicators.factory.run_pipeline\"). get_pandas_ta_indicators class method \u00b6 IndicatorFactory . get_pandas_ta_indicators ( silence_warnings = True ) Get all pandas-ta indicators. Note Returns only the indicators that have been successfully parsed. get_ta_indicators class method \u00b6 IndicatorFactory . get_ta_indicators () Get all ta indicators. get_talib_indicators class method \u00b6 IndicatorFactory . get_talib_indicators () Get all TA-Lib indicators. parse_pandas_ta_config class method \u00b6 IndicatorFactory . parse_pandas_ta_config ( func , test_input_names = None , test_index_len = 100 ) Get the config of a pandas-ta indicator. parse_ta_config class method \u00b6 IndicatorFactory . parse_ta_config ( ind_cls ) Get the config of a ta indicator. MetaIndicatorBase class \u00b6 Meta class that exposes a read-only class property StatsBuilderMixin.metrics . Superclasses MetaPlotsBuilderMixin MetaStatsBuilderMixin builtins.type Inherited members MetaPlotsBuilderMixin.subplots MetaStatsBuilderMixin.metrics","title":"factory"},{"location":"api/indicators/factory/#vectorbt.indicators.factory","text":"A factory for building new indicators with ease. The indicator factory class IndicatorFactory offers a convenient way to create technical indicators of any complexity. By providing it with information such as calculation functions and the names of your inputs, parameters, and outputs, it will create a stand-alone indicator class capable of running the indicator for an arbitrary combination of your inputs and parameters. It also creates methods for signal generation and supports common pandas and parameter indexing operations. Each indicator is basically a pipeline that: Accepts a list of input arrays (for example, OHLCV data) Accepts a list of parameter arrays (for example, window size) Accepts other relevant arguments and keyword arguments For each parameter combination, performs calculation on the input arrays Concatenates results into new output arrays (for example, rolling average) This pipeline can be well standardized, which is done by run_pipeline() . IndicatorFactory simplifies the usage of run_pipeline() by generating and pre-configuring a new Python class with various class methods for running the indicator. Each generated class includes the following features: Accepts input arrays of any compatible shape thanks to broadcasting Accepts output arrays written in-place instead of returning Accepts arbitrary parameter grids Supports caching and other optimizations out of the box Supports pandas and parameter indexing Offers helper methods for all inputs, outputs, and properties Consider the following price DataFrame composed of two columns, one per asset: >>> import vectorbt as vbt >>> import numpy as np >>> import pandas as pd >>> from numba import njit >>> from datetime import datetime >>> price = pd . DataFrame ({ ... 'a' : [ 1 , 2 , 3 , 4 , 5 ], ... 'b' : [ 5 , 4 , 3 , 2 , 1 ] ... }, index = pd . Index ([ ... datetime ( 2020 , 1 , 1 ), ... datetime ( 2020 , 1 , 2 ), ... datetime ( 2020 , 1 , 3 ), ... datetime ( 2020 , 1 , 4 ), ... datetime ( 2020 , 1 , 5 ), ... ])) . astype ( float ) >>> price a b 2020-01-01 1.0 5.0 2020-01-02 2.0 4.0 2020-01-03 3.0 3.0 2020-01-04 4.0 2.0 2020-01-05 5.0 1.0 For each column in the DataFrame, let's calculate a simple moving average and get its crossover with price. In particular, we want to test two different window sizes: 2 and 3.","title":"vectorbt.indicators.factory"},{"location":"api/indicators/factory/#naive-approach","text":"A naive way of doing this: >>> ma_df = pd . DataFrame . vbt . concat ( ... price . rolling ( window = 2 ) . mean (), ... price . rolling ( window = 3 ) . mean (), ... keys = pd . Index ([ 2 , 3 ], name = 'ma_window' )) >>> ma_df ma_window 2 3 a b a b 2020-01-01 NaN NaN NaN NaN 2020-01-02 1.5 4.5 NaN NaN 2020-01-03 2.5 3.5 2.0 4.0 2020-01-04 3.5 2.5 3.0 3.0 2020-01-05 4.5 1.5 4.0 2.0 >>> above_signals = ( price . vbt . tile ( 2 ) . vbt > ma_df ) >>> above_signals = above_signals . vbt . signals . first ( after_false = True ) >>> above_signals ma_window 2 3 a b a b 2020-01-01 False False False False 2020-01-02 True False False False 2020-01-03 False False True False 2020-01-04 False False False False 2020-01-05 False False False False >>> below_signals = ( price . vbt . tile ( 2 ) . vbt < ma_df ) >>> below_signals = below_signals . vbt . signals . first ( after_false = True ) >>> below_signals ma_window 2 3 a b a b 2020-01-01 False False False False 2020-01-02 False True False False 2020-01-03 False False False True 2020-01-04 False False False False 2020-01-05 False False False False Now the same using IndicatorFactory : >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... param_names = [ 'window' ], ... output_names = [ 'ma' ], ... ) . from_apply_func ( vbt . nb . rolling_mean_nb ) >>> myind = MyInd . run ( price , [ 2 , 3 ]) >>> above_signals = myind . price_crossed_above ( myind . ma ) >>> below_signals = myind . price_crossed_below ( myind . ma ) The IndicatorFactory class is used to construct indicator classes from UDFs. First, we provide all the necessary information (indicator conig) to build the facade of the indicator, such as the names of inputs, parameters, and outputs, and the actual calculation function. The factory then generates a self-contained indicator class capable of running arbitrary configurations of inputs and parameters. To run any configuration, we can either use the run method (as we did above) or the run_combs method.","title":"Naive approach"},{"location":"api/indicators/factory/#run-and-run_combs-methods","text":"The main method to run an indicator is run , which accepts arguments based on the config provided to the IndicatorFactory (see the example above). These arguments include input arrays, in-place output arrays, parameters, and arguments for run_pipeline() . The run_combs method takes the same inputs as the method above, but computes all combinations of passed parameters based on a combinatorial function and returns multiple instances that can be compared with each other. For example, this is useful to generate crossover signals of multiple moving averages: >>> myind1 , myind2 = MyInd . run_combs ( price , [ 2 , 3 , 4 ]) >>> myind1 . ma myind_1_window 2 3 a b a b a b 2020-01-01 NaN NaN NaN NaN NaN NaN 2020-01-02 1.5 4.5 1.5 4.5 NaN NaN 2020-01-03 2.5 3.5 2.5 3.5 2.0 4.0 2020-01-04 3.5 2.5 3.5 2.5 3.0 3.0 2020-01-05 4.5 1.5 4.5 1.5 4.0 2.0 >>> myind2 . ma myind_2_window 3 4 a b a b a b 2020-01-01 NaN NaN NaN NaN NaN NaN 2020-01-02 NaN NaN NaN NaN NaN NaN 2020-01-03 2.0 4.0 NaN NaN NaN NaN 2020-01-04 3.0 3.0 2.5 3.5 2.5 3.5 2020-01-05 4.0 2.0 3.5 2.5 3.5 2.5 >>> myind1 . ma_crossed_above ( myind2 . ma ) myind_1_window 2 3 myind_2_window 3 4 4 a b a b a b 2020-01-01 False False False False False False 2020-01-02 False False False False False False 2020-01-03 True False False False False False 2020-01-04 False False True False True False 2020-01-05 False False False False False False Its main advantage is that it doesn't need to re-compute each combination thanks to smart caching. To get details on what arguments are accepted by any of the class methods, use help : >>> help ( MyInd . run ) Help on method run: run(price, window, short_name='custom', hide_params=None, hide_default=True, **kwargs) method of builtins.type instance Run `Indicator` indicator. * Inputs: `price` * Parameters: `window` * Outputs: `ma` Pass a list of parameter names as `hide_params` to hide their column levels. Set `hide_default` to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to `vectorbt.indicators.factory.run_pipeline`.","title":"run and run_combs methods"},{"location":"api/indicators/factory/#parameters","text":"IndicatorFactory allows definition of arbitrary parameter grids. Parameters are variables that can hold one or more values. A single value can be passed as a scalar, an array, or any other object. Multiple values are passed as a list or an array (if the flag is_array_like is set to False for that parameter). If there are multiple parameters and each is having multiple values, their values will broadcast to a single shape: p1 p2 result 0 0 1 [(0, 1)] 1 [0, 1] [2] [(0, 2), (1, 2)] 2 [0, 1] [2, 3] [(0, 2), (1, 3)] 3 [0, 1] [2, 3, 4] error To illustrate the usage of parameters in indicators, let's build a basic indicator that returns 1 if the rolling mean is within upper and lower bounds, and -1 if it's outside: >>> @njit ... def apply_func_nb ( price , window , lower , upper ): ... output = np . full ( price . shape , np . nan , dtype = np . float_ ) ... for col in range ( price . shape [ 1 ]): ... for i in range ( window , price . shape [ 0 ]): ... mean = np . mean ( price [ i - window : i , col ]) ... output [ i , col ] = lower < mean < upper ... return output >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... param_names = [ 'window' , 'lower' , 'upper' ], ... output_names = [ 'output' ] ... ) . from_apply_func ( apply_func_nb ) By default, when per_column is set to False, each parameter is applied to the entire input. One parameter combination: >>> MyInd . run ( ... price , ... window = 2 , ... lower = 3 , ... upper = 5 ... ) . output custom_window 2 custom_lower 3 custom_upper 5 a b 2020-01-01 NaN NaN 2020-01-02 NaN NaN 2020-01-03 0.0 1.0 2020-01-04 0.0 1.0 2020-01-05 1.0 0.0 Multiple parameter combinations: >>> MyInd . run ( ... price , ... window = [ 2 , 3 ], ... lower = 3 , ... upper = 5 ... ) . output custom_window 2 3 custom_lower 3 3 custom_upper 5 5 a b a b 2020-01-01 NaN NaN NaN NaN 2020-01-02 NaN NaN NaN NaN 2020-01-03 0.0 1.0 NaN NaN 2020-01-04 0.0 1.0 0.0 1.0 2020-01-05 1.0 0.0 0.0 0.0 Product of parameter combinations: >>> MyInd . run ( ... price , ... window = [ 2 , 3 ], ... lower = [ 3 , 4 ], ... upper = 5 , ... param_product = True ... ) . output custom_window 2 3 custom_lower 3 4 3 4 custom_upper 5 5 5 5 a b a b a b a b 2020-01-01 NaN NaN NaN NaN NaN NaN NaN NaN 2020-01-02 NaN NaN NaN NaN NaN NaN NaN NaN 2020-01-03 0.0 1.0 0.0 1.0 NaN NaN NaN NaN 2020-01-04 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 2020-01-05 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Multiple parameter combinations, one per column: >>> MyInd . run ( ... price , ... window = [ 2 , 3 ], ... lower = [ 3 , 4 ], ... upper = 5 , ... per_column = True ... ) . output custom_window 2 3 custom_lower 3 4 custom_upper 5 5 a b 2020-01-01 NaN NaN 2020-01-02 NaN NaN 2020-01-03 0.0 NaN 2020-01-04 0.0 0.0 2020-01-05 1.0 0.0 Parameter defaults can be passed directly to the IndicatorFactory.from_custom_func() and IndicatorFactory.from_apply_func() , and overriden in the run method: >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... param_names = [ 'window' , 'lower' , 'upper' ], ... output_names = [ 'output' ] ... ) . from_apply_func ( apply_func_nb , window = 2 , lower = 3 , upper = 4 ) >>> MyInd . run ( price , upper = 5 ) . output custom_window 2 custom_lower 3 custom_upper 5 a b 2020-01-01 NaN NaN 2020-01-02 NaN NaN 2020-01-03 0.0 1.0 2020-01-04 0.0 1.0 2020-01-05 1.0 0.0 Some parameters are meant to be defined per row, column, or element of the input. By default, if we pass the parameter value as an array, the indicator will treat this array as a list of multiple values - one per input. To make the indicator view this array as a single value, set the flag is_array_like to True in param_settings . Also, to automatically broadcast the passed scalar/array to the input shape, set bc_to_input to True, 0 (index axis), or 1 (column axis). In our example, the parameter window can broadcast per column, and both parameters lower and upper can broadcast per element: >>> @njit ... def apply_func_nb ( price , window , lower , upper ): ... output = np . full ( price . shape , np . nan , dtype = np . float_ ) ... for col in range ( price . shape [ 1 ]): ... for i in range ( window [ col ], price . shape [ 0 ]): ... mean = np . mean ( price [ i - window [ col ]: i , col ]) ... output [ i , col ] = lower [ i , col ] < mean < upper [ i , col ] ... return output >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... param_names = [ 'window' , 'lower' , 'upper' ], ... output_names = [ 'output' ] ... ) . from_apply_func ( ... apply_func_nb , ... param_settings = dict ( ... window = dict ( is_array_like = True , bc_to_input = 1 , per_column = True ), ... lower = dict ( is_array_like = True , bc_to_input = True ), ... upper = dict ( is_array_like = True , bc_to_input = True ) ... ) ... ) >>> MyInd . run ( ... price , ... window = [ np . array ([ 2 , 3 ]), np . array ([ 3 , 4 ])], ... lower = np . array ([ 1 , 2 ]), ... upper = np . array ([ 3 , 4 ]), ... ) . output custom_window 2 3 4 custom_lower array_0 array_0 array_1 array_1 custom_upper array_0 array_0 array_1 array_1 a b a b 2020-01-01 NaN NaN NaN NaN 2020-01-02 NaN NaN NaN NaN 2020-01-03 1.0 NaN NaN NaN 2020-01-04 1.0 0.0 1.0 NaN 2020-01-05 0.0 1.0 0.0 1.0 Broadcasting a huge number of parameters to the input shape can consume lots of memory, especially when the array materializes. Luckily, vectorbt implements flexible broadcasting, which preserves the original dimensions of the parameter. This requires two changes: setting keep_raw to True in broadcast_kwargs and passing flex_2d to the apply function. There are two configs in vectorbt.indicators.configs exactly for this purpose: one for column-wise broadcasting and one for element-wise broadcasting: >>> from vectorbt.base.reshape_fns import flex_select_auto_nb >>> from vectorbt.indicators.configs import flex_col_param_config , flex_elem_param_config >>> @njit ... def apply_func_nb ( price , window , lower , upper , flex_2d ): ... output = np . full ( price . shape , np . nan , dtype = np . float_ ) ... for col in range ( price . shape [ 1 ]): ... _window = flex_select_auto_nb ( window , 0 , col , flex_2d ) ... for i in range ( _window , price . shape [ 0 ]): ... _lower = flex_select_auto_nb ( lower , i , col , flex_2d ) ... _upper = flex_select_auto_nb ( upper , i , col , flex_2d ) ... mean = np . mean ( price [ i - _window : i , col ]) ... output [ i , col ] = _lower < mean < _upper ... return output >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... param_names = [ 'window' , 'lower' , 'upper' ], ... output_names = [ 'output' ] ... ) . from_apply_func ( ... apply_func_nb , ... param_settings = dict ( ... window = flex_col_param_config , ... lower = flex_elem_param_config , ... upper = flex_elem_param_config ... ), ... pass_flex_2d = True ... ) Both bound parameters can now be passed as a scalar (value per whole input), a 1-dimensional array (value per row or column, depending upon whether input is a Series or a DataFrame), a 2-dimensional array (value per element), or a list of any of those. This allows for the highest parameter flexibility at the lowest memory cost. For example, let's build a grid of two parameter combinations, each being one window size per column and both bounds per element: >>> MyInd . run ( ... price , ... window = [ np . array ([ 2 , 3 ]), np . array ([ 3 , 4 ])], ... lower = price . values - 3 , ... upper = price . values + 3 , ... ) . output custom_window 2 3 4 custom_lower array_0 array_0 array_1 array_1 custom_upper array_0 array_0 array_1 array_1 a b a b 2020-01-01 NaN NaN NaN NaN 2020-01-02 NaN NaN NaN NaN 2020-01-03 1.0 NaN NaN NaN 2020-01-04 1.0 1.0 1.0 NaN 2020-01-05 1.0 1.0 1.0 1.0 Indicators can also be parameterless. See OBV .","title":"Parameters"},{"location":"api/indicators/factory/#inputs","text":"IndicatorFactory supports passing none, one, or multiple inputs. If multiple inputs are passed, it tries to broadcast them into a single shape. Remember that in vectorbt each column means a separate backtest instance. That's why in order to use multiple pieces of information, such as open, high, low, close, and volume, we need to provide them as separate pandas objects rather than a single DataFrame. Let's create a parameterless indicator that measures the position of the close price within each bar: >>> @njit ... def apply_func_nb ( high , low , close ): ... return ( close - low ) / ( high - low ) >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'high' , 'low' , 'close' ], ... output_names = [ 'output' ] ... ) . from_apply_func ( apply_func_nb ) >>> MyInd . run ( price + 1 , price - 1 , price ) . output a b 2020-01-01 0.5 0.5 2020-01-02 0.5 0.5 2020-01-03 0.5 0.5 2020-01-04 0.5 0.5 2020-01-05 0.5 0.5 To demonstrate broadcasting, let's pass high as a DataFrame, low as a Series, and close as a scalar: >>> df = pd . DataFrame ( np . random . uniform ( 1 , 2 , size = ( 5 , 2 ))) >>> sr = pd . Series ( np . random . uniform ( 0 , 1 , size = 5 )) >>> MyInd . run ( df , sr , 1 ) . output 0 1 0 0.960680 0.666820 1 0.400646 0.528456 2 0.093467 0.134777 3 0.037210 0.102411 4 0.529012 0.652602 By default, if a Series was passed, it's automatically expanded into a 2-dimensional array. To keep it as 1-dimensional, set to_2d to False. Similar to parameters, we can also define defaults for inputs. In addition to using scalars and arrays as default values, we can reference other inputs: >>> @njit ... def apply_func_nb ( ts1 , ts2 , ts3 ): ... return ts1 + ts2 + ts3 >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'ts1' , 'ts2' , 'ts3' ], ... output_names = [ 'output' ] ... ) . from_apply_func ( apply_func_nb , ts2 = 'ts1' , ts3 = 'ts1' ) >>> MyInd . run ( price ) . output a b 2020-01-01 3.0 15.0 2020-01-02 6.0 12.0 2020-01-03 9.0 9.0 2020-01-04 12.0 6.0 2020-01-05 15.0 3.0 >>> MyInd . run ( price , ts2 = price * 2 ) . output a b 2020-01-01 4.0 20.0 2020-01-02 8.0 16.0 2020-01-03 12.0 12.0 2020-01-04 16.0 8.0 2020-01-05 20.0 4.0 What if an indicator doesn't take any input arrays? In that case, we can force the user to at least provide the input shape. Let's define a generator that emulates random returns and generates synthetic price: >>> @njit ... def apply_func_nb ( input_shape , start , mu , sigma ): ... rand_returns = np . random . normal ( mu , sigma , input_shape ) ... return start * vbt . nb . nancumprod_nb ( rand_returns + 1 ) >>> MyInd = vbt . IndicatorFactory ( ... param_names = [ 'start' , 'mu' , 'sigma' ], ... output_names = [ 'output' ] ... ) . from_apply_func ( ... apply_func_nb , ... require_input_shape = True , ... seed = 42 ... ) >>> MyInd . run ( price . shape , 100 , 0 , 0.01 ) . output custom_start 100 custom_mu 0 custom_sigma 0.01 0.01 0 100.496714 99.861736 1 101.147620 101.382660 2 100.910779 101.145285 3 102.504375 101.921510 4 102.023143 102.474495 We can also supply pandas meta such as input_index and input_columns to the run method: >>> MyInd . run ( ... price . shape , 100 , 0 , 0.01 , ... input_index = price . index , input_columns = price . columns ... ) . output custom_start 100 custom_mu 0 custom_sigma 0.01 0.01 a b 2020-01-01 100.496714 99.861736 2020-01-02 101.147620 101.382660 2020-01-03 100.910779 101.145285 2020-01-04 102.504375 101.921510 2020-01-05 102.023143 102.474495 One can even build input-less indicator that decides on the output shape dynamically: >>> from vectorbt.base.combine_fns import apply_and_concat_one >>> def apply_func ( i , ps , input_shape ): ... out = np . full ( input_shape , 0 ) ... out [: ps [ i ]] = 1 ... return out >>> def custom_func ( ps ): ... input_shape = ( np . max ( ps ),) ... return apply_and_concat_one ( len ( ps ), apply_func , ps , input_shape ) >>> MyInd = vbt . IndicatorFactory ( ... param_names = [ 'p' ], ... output_names = [ 'output' ] ... ) . from_custom_func ( custom_func ) >>> MyInd . run ([ 1 , 2 , 3 , 4 , 5 ]) . output custom_p 1 2 3 4 5 0 1 1 1 1 1 1 0 1 1 1 1 2 0 0 1 1 1 3 0 0 0 1 1 4 0 0 0 0 1","title":"Inputs"},{"location":"api/indicators/factory/#outputs","text":"There are two types of outputs: regular and in-place outputs: Regular outputs are one or more arrays returned by the function. Each should have an exact same shape and match the number of columns in the input multiplied by the number of parameter values. In-place outputs are not returned but modified in-place. They broadcast together with inputs and are passed to the calculation function as a list, one per parameter. Two regular outputs: >>> @njit ... def apply_func_nb ( price ): ... return price - 1 , price + 1 >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... output_names = [ 'out1' , 'out2' ] ... ) . from_apply_func ( apply_func_nb ) >>> myind = MyInd . run ( price ) >>> pd . testing . assert_frame_equal ( myind . out1 , myind . price - 1 ) >>> pd . testing . assert_frame_equal ( myind . out2 , myind . price + 1 ) One regular output and one in-place output: >>> @njit ... def apply_func_nb ( price , in_out2 ): ... in_out2 [:] = price + 1 ... return price - 1 >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... output_names = [ 'out1' ], ... in_output_names = [ 'in_out2' ] ... ) . from_apply_func ( apply_func_nb ) >>> myind = MyInd . run ( price ) >>> pd . testing . assert_frame_equal ( myind . out1 , myind . price - 1 ) >>> pd . testing . assert_frame_equal ( myind . in_out2 , myind . price + 1 ) Two in-place outputs: >>> @njit ... def apply_func_nb ( price , in_out1 , in_out2 ): ... in_out1 [:] = price - 1 ... in_out2 [:] = price + 1 >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... in_output_names = [ 'in_out1' , 'in_out2' ] ... ) . from_apply_func ( apply_func_nb ) >>> myind = MyInd . run ( price ) >>> pd . testing . assert_frame_equal ( myind . in_out1 , myind . price - 1 ) >>> pd . testing . assert_frame_equal ( myind . in_out2 , myind . price + 1 ) By default, in-place outputs are created as empty arrays with uninitialized values. This allows creation of optional outputs that, if not written, do not occupy much memory. Since not all outputs are meant to be of data type float , we can pass dtype in the in_output_settings . >>> @njit ... def apply_func_nb ( price , in_out ): ... in_out [:] = price > np . mean ( price ) >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... in_output_names = [ 'in_out' ] ... ) . from_apply_func ( ... apply_func_nb , ... in_output_settings = dict ( in_out = dict ( dtype = bool )) ... ) >>> MyInd . run ( price ) . in_out a b 2020-01-01 False True 2020-01-02 False True 2020-01-03 False False 2020-01-04 True False 2020-01-05 True False Another advantage of in-place outputs is that we can provide their initial state: >>> @njit ... def apply_func_nb ( price , in_out1 , in_out2 ): ... in_out1 [:] = in_out1 + price ... in_out2 [:] = in_out2 + price >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... in_output_names = [ 'in_out1' , 'in_out2' ] ... ) . from_apply_func ( ... apply_func_nb , ... in_out1 = 100 , ... in_out2 = 'price' ... ) >>> myind = MyInd . run ( price ) >>> myind . in_out1 a b 2020-01-01 101 105 2020-01-02 102 104 2020-01-03 103 103 2020-01-04 104 102 2020-01-05 105 101 >>> myind . in_out2 a b 2020-01-01 2.0 10.0 2020-01-02 4.0 8.0 2020-01-03 6.0 6.0 2020-01-04 8.0 4.0 2020-01-05 10.0 2.0","title":"Outputs"},{"location":"api/indicators/factory/#without-numba","text":"It's also possible to supply a function that is not Numba-compiled. This is handy when working with third-party libraries (see the implementation of IndicatorFactory.from_talib() ). Additionally, we can set keep_pd to True to pass all inputs as pandas objects instead of raw NumPy arrays. Note Already broadcasted pandas meta will be provided; that is, each input array will have the same index and columns. Let's demonstrate this by wrapping a basic composed pandas_ta strategy: >>> import pandas_ta >>> def apply_func ( open , high , low , close , volume , ema_len , linreg_len ): ... df = pd . DataFrame ( dict ( open = open , high = high , low = low , close = close , volume = volume )) ... df . ta . strategy ( pandas_ta . Strategy ( \"MyStrategy\" , [ ... dict ( kind = 'ema' , length = ema_len ), ... dict ( kind = 'linreg' , close = 'EMA_' + str ( ema_len ), length = linreg_len ) ... ])) ... return tuple ([ df . iloc [:, i ] for i in range ( 5 , len ( df . columns ))]) >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'open' , 'high' , 'low' , 'close' , 'volume' ], ... param_names = [ 'ema_len' , 'linreg_len' ], ... output_names = [ 'ema' , 'ema_linreg' ] ... ) . from_apply_func ( ... apply_func , ... keep_pd = True , ... to_2d = False ... ) >>> my_ind = MyInd . run ( ... ohlcv [ 'Open' ], ... ohlcv [ 'High' ], ... ohlcv [ 'Low' ], ... ohlcv [ 'Close' ], ... ohlcv [ 'Volume' ], ... ema_len = 5 , ... linreg_len = [ 8 , 9 , 10 ] ... ) >>> my_ind . ema_linreg custom_ema_len 5 custom_linreg_len 8 9 10 date 2021-02-02 NaN NaN NaN 2021-02-03 NaN NaN NaN 2021-02-04 NaN NaN NaN 2021-02-05 NaN NaN NaN 2021-02-06 NaN NaN NaN ... ... ... ... 2021-02-25 52309.302811 52602.005326 52899.576568 2021-02-26 50797.264793 51224.188381 51590.825690 2021-02-28 49217.904905 49589.546052 50066.206828 2021-03-01 48316.305403 48553.540713 48911.701664 2021-03-02 47984.395969 47956.885953 48150.929668 In the example above, only one Series per open, high, low, close, and volume can be passed. To enable the indicator to process two-dimensional data, set to_2d to True and create a loop over each column in the apply_func . Hint Writing a native Numba-compiled code may provide a performance that is magnitudes higher than that offered by libraries that work on pandas.","title":"Without Numba"},{"location":"api/indicators/factory/#raw-outputs-and-caching","text":"IndicatorFactory re-uses calculation artifacts whenever possible. Since it was originally designed for hyperparameter optimization and there are times when parameter values gets repeated, prevention of processing the same parameter over and over again is inevitable for good performance. For instance, when the run_combs method is being used and run_unique is set to True, it first calculates the raw outputs of all unique parameter combinations and then uses them to build outputs for the whole parameter grid. Let's first take a look at a typical raw output by setting return_raw to True: >>> raw = vbt . MA . run ( price , 2 , [ False , True ], return_raw = True ) >>> raw ([array([[ nan, nan, nan, nan], [1.5 , 4.5 , 1.66666667, 4.33333333], [2.5 , 3.5 , 2.55555556, 3.44444444], [3.5 , 2.5 , 3.51851852, 2.48148148], [4.5 , 1.5 , 4.50617284, 1.49382716]])], [(2, False), (2, True)], 2, []) It consists of a list of the returned output arrays, a list of the zipped parameter combinations, the number of input columns, and other objects returned along with output arrays but not listed in output_names . The next time we decide to run the indicator on a subset of the parameters above, we can simply pass this tuple as the use_raw argument. This won't call the calculation function and will throw an error if some of the requested parameter combinations cannot be found in raw . >>> vbt . MA . run ( price , 2 , True , use_raw = raw ) . ma ma_window 2 ma_ewm True a b 2020-01-01 NaN NaN 2020-01-02 1.666667 4.333333 2020-01-03 2.555556 3.444444 2020-01-04 3.518519 2.481481 2020-01-05 4.506173 1.493827 Here is how the performance compares when repeatedly running the same parameter combination with and without run_unique : >>> a = np . random . uniform ( size = ( 1000 ,)) >>> % timeit vbt . MA . run ( a , np . full ( 1000 , 2 ), run_unique = False ) 73.4 ms \u00b1 4.76 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) >>> % timeit vbt . MA . run ( a , np . full ( 1000 , 2 ), run_unique = True ) 8.99 ms \u00b1 114 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) Note run_unique is disabled by default. Enable run_unique if input arrays have few columns and there are tons of repeated parameter combinations. Disable run_unique if input arrays are very wide, if two identical parameter combinations can lead to different results, or when requesting raw output, cache, or additional outputs outside of output_names . Another performance enhancement can be introduced by caching, which has to be implemented by the user. The class method IndicatorFactory.from_apply_func() has an argument cache_func , which is called prior to the main calculation. Consider the following scenario: we want to compute the relative distance between two expensive rolling windows. We have already decided on the value for the first window, and want to test thousands of values for the second window. Without caching, and even with run_unique enabled, the first rolling window will be re-calculated over and over again and waste our resources: >>> @njit ... def roll_mean_expensive_nb ( price , w ): ... for i in range ( 100 ): ... out = vbt . nb . rolling_mean_nb ( price , w ) ... return out >>> @njit ... def apply_func_nb ( price , w1 , w2 ): ... roll_mean1 = roll_mean_expensive_nb ( price , w1 ) ... roll_mean2 = roll_mean_expensive_nb ( price , w2 ) ... return ( roll_mean2 - roll_mean1 ) / roll_mean1 >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... param_names = [ 'w1' , 'w2' ], ... output_names = [ 'output' ], ... ) . from_apply_func ( apply_func_nb ) >>> MyInd . run ( price , 2 , 3 ) . output custom_w1 2 custom_w2 3 a b 2020-01-01 NaN NaN 2020-01-02 NaN NaN 2020-01-03 -0.200000 0.142857 2020-01-04 -0.142857 0.200000 2020-01-05 -0.111111 0.333333 >>> % timeit MyInd . run ( price , 2 , np . arange ( 2 , 1000 )) 264 ms \u00b1 3.22 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) To avoid this, let's cache all unique rolling windows: >>> @njit ... def cache_func_nb ( price , ws1 , ws2 ): ... cache_dict = dict () ... ws = ws1 . copy () ... ws . extend ( ws2 ) ... for i in range ( len ( ws )): ... h = hash (( ws [ i ])) ... if h not in cache_dict : ... cache_dict [ h ] = roll_mean_expensive_nb ( price , ws [ i ]) ... return cache_dict >>> @njit ... def apply_func_nb ( price , w1 , w2 , cache_dict ): ... return ( cache_dict [ hash ( w2 )] - cache_dict [ hash ( w1 )]) / cache_dict [ hash ( w1 )] >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... param_names = [ 'w1' , 'w2' ], ... output_names = [ 'output' ], ... ) . from_apply_func ( apply_func_nb , cache_func = cache_func_nb ) >>> MyInd . run ( price , 2 , 3 ) . output custom_w1 2 custom_w2 3 a b 2020-01-01 NaN NaN 2020-01-02 NaN NaN 2020-01-03 -0.200000 0.142857 2020-01-04 -0.142857 0.200000 2020-01-05 -0.111111 0.333333 >>> % timeit MyInd . run ( price , 2 , np . arange ( 2 , 1000 )) 145 ms \u00b1 4.55 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) We have cut down the processing time almost in half. Similar to raw outputs, we can force IndicatorFactory to return the cache, so it can be used in other calculations or even indicators. The clear advantage of this approach is that we don't rely on some fixed set of parameter combinations any more, but on the values of each parameter, which gives us more granularity in managing performance. >>> cache = MyInd . run ( price , 2 , np . arange ( 2 , 1000 ), return_cache = True ) >>> % timeit MyInd . run ( price , np . arange ( 2 , 1000 ), np . arange ( 2 , 1000 ), use_cache = cache ) 30.1 ms \u00b1 2 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)","title":"Raw outputs and caching"},{"location":"api/indicators/factory/#custom-properties-and-methods","text":"Use custom_output_props argument when constructing an indicator to define lazy outputs - outputs that are processed only when explicitly called. They will become cached properties and, in contrast to regular outputs, they can have an arbitrary shape. For example, let's attach a property that will calculate the distance between the moving average and the price. >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... param_names = [ 'window' ], ... output_names = [ 'ma' ], ... custom_output_props = dict ( distance = lambda self : ( self . price - self . ma ) / self . ma ) ... ) . from_apply_func ( vbt . nb . rolling_mean_nb ) >>> MyInd . run ( price , [ 2 , 3 ]) . distance custom_window 2 3 a b a b 2020-01-01 NaN NaN NaN NaN 2020-01-02 0.333333 -0.111111 NaN NaN 2020-01-03 0.200000 -0.142857 0.500000 -0.250000 2020-01-04 0.142857 -0.200000 0.333333 -0.333333 2020-01-05 0.111111 -0.333333 0.250000 -0.500000 Another way of defining own properties and methods is subclassing: >>> class MyIndExtended ( MyInd ): ... def plot ( self , column = None , ** kwargs ): ... self_col = self . select_one ( column = column , group_by = False ) ... return self . ma . vbt . plot ( ** kwargs ) >>> MyIndExtended . run ( price , [ 2 , 3 ])[( 2 , 'a' )] . plot ()","title":"Custom properties and methods"},{"location":"api/indicators/factory/#helper-properties-and-methods","text":"For all in input_names , in_output_names , output_names , and custom_output_props , IndicatorFactory will create a bunch of comparison and combination methods, such as for generating signals. What kind of methods are created can be regulated using dtype in the attr_settings dictionary. >>> from collections import namedtuple >>> MyEnum = namedtuple ( 'MyEnum' , [ 'one' , 'two' ])( 0 , 1 ) >>> def apply_func_nb ( price ): ... out_float = np . empty ( price . shape , dtype = np . float_ ) ... out_bool = np . empty ( price . shape , dtype = np . bool_ ) ... out_enum = np . empty ( price . shape , dtype = np . int_ ) ... return out_float , out_bool , out_enum >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... output_names = [ 'out_float' , 'out_bool' , 'out_enum' ], ... attr_settings = dict ( ... out_float = dict ( dtype = np . float_ ), ... out_bool = dict ( dtype = np . bool_ ), ... out_enum = dict ( dtype = MyEnum ) ... )) . from_apply_func ( apply_func_nb ) >>> myind = MyInd . run ( price ) >>> dir ( myind ) [ ... 'out_bool', 'out_bool_and', 'out_bool_or', 'out_bool_stats', 'out_bool_xor', 'out_enum', 'out_enum_readable', 'out_enum_stats', 'out_float', 'out_float_above', 'out_float_below', 'out_float_equal', 'out_float_stats', ... 'price', 'price_above', 'price_below', 'price_equal', 'price_stats', ... ] Each of these methods and properties are created for sheer convenience: to easily combine boolean arrays using logical rules and to compare numeric arrays. All operations are done strictly using NumPy. Another advantage is utilization of vectorbt's own broadcasting, such that one can combine inputs and outputs with an arbitrary array-like object, given their shapes can broadcast together. We can also do comparison with multiple objects at once by passing them as a tuple/list: >>> myind . price_above ([ 1.5 , 2.5 ]) custom_price_above 1.5 2.5 a b a b 2020-01-01 False True False True 2020-01-02 True True False True 2020-01-03 True True True True 2020-01-04 True True True False 2020-01-05 True False True False","title":"Helper properties and methods"},{"location":"api/indicators/factory/#indexing","text":"IndicatorFactory attaches pandas indexing to the indicator class thanks to ArrayWrapper . Supported are iloc , loc , *param_name*_loc , xs , and __getitem__ . This makes possible accessing rows and columns by labels, integer positions, and parameters. >>> ma = vbt . MA . run ( price , [ 2 , 3 ]) >>> ma [( 2 , 'b' )] <vectorbt.indicators.basic.MA at 0x7fe4d10ddcc0> >>> ma [( 2 , 'b' )] . ma 2020-01-01 NaN 2020-01-02 4.5 2020-01-03 3.5 2020-01-04 2.5 2020-01-05 1.5 Name: (2, b), dtype: float64 >>> ma . window_loc [ 2 ] . ma a b 2020-01-01 NaN NaN 2020-01-02 1.5 4.5 2020-01-03 2.5 3.5 2020-01-04 3.5 2.5 2020-01-05 4.5 1.5","title":"Indexing"},{"location":"api/indicators/factory/#ta-lib","text":"Indicator factory also provides a class method IndicatorFactory.from_talib() that can be used to wrap any function from TA-Lib. It automatically fills all the neccessary information, such as input, parameter and output names.","title":"TA-Lib"},{"location":"api/indicators/factory/#stats","text":"Hint See StatsBuilderMixin.stats() . We can attach metrics to any new indicator class: >>> @njit ... def apply_func_nb ( price ): ... return price ** 2 , price ** 3 >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... output_names = [ 'out1' , 'out2' ], ... metrics = dict ( ... sum_diff = dict ( ... calc_func = lambda self : self . out2 . sum () - self . out1 . sum () ... ) ... ) ... ) . from_apply_func ( ... apply_func_nb ... ) >>> myind = MyInd . run ( price ) >>> myind . stats ( column = 'a' ) sum_diff 170.0 Name: a, dtype: float64","title":"Stats"},{"location":"api/indicators/factory/#plots","text":"Hint See PlotsBuilderMixin.plots() . Similarly to stats, we can attach subplots to any new indicator class: >>> @njit ... def apply_func_nb ( price ): ... return price ** 2 , price ** 3 >>> def plot_outputs ( out1 , out2 , column = None , fig = None ): ... fig = out1 [ column ] . rename ( 'out1' ) . vbt . plot ( fig = fig ) ... fig = out2 [ column ] . rename ( 'out2' ) . vbt . plot ( fig = fig ) >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'price' ], ... output_names = [ 'out1' , 'out2' ], ... subplots = dict ( ... plot_outputs = dict ( ... plot_func = plot_outputs , ... resolve_out1 = True , ... resolve_out2 = True ... ) ... ) ... ) . from_apply_func ( ... apply_func_nb ... ) >>> myind = MyInd . run ( price ) >>> myind . plots ( column = 'a' )","title":"Plots"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.build_columns","text":"build_columns ( param_list , input_columns , level_names = None , hide_levels = None , param_settings = None , per_column = False , ignore_default = False , ** kwargs ) For each parameter in param_list , create a new column level with parameter values and stack it on top of input_columns . Returns a list of parameter indexes and new columns.","title":"build_columns()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.combine_objs","text":"combine_objs ( obj , other , * args , level_name = None , keys = None , allow_multiple = True , ** kwargs ) Combines/compares obj to other , for example, to generate signals. Both will broadcast together. Pass other as a tuple or a list to compare with multiple arguments. In this case, a new column level will be created with the name level_name . See BaseAccessor.combine() .","title":"combine_objs()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.params_to_list","text":"params_to_list ( params , is_tuple , is_array_like ) Cast parameters to a list.","title":"params_to_list()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.prepare_params","text":"prepare_params ( param_list , param_settings = None , input_shape = None , to_2d = False ) Prepare parameters.","title":"prepare_params()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.run_pipeline","text":"run_pipeline ( num_ret_outputs , custom_func , * args , require_input_shape = False , input_shape = None , input_index = None , input_columns = None , input_list = None , in_output_list = None , in_output_settings = None , broadcast_kwargs = None , param_list = None , param_product = False , param_settings = None , run_unique = False , silence_warnings = False , per_column = False , pass_col = False , keep_pd = False , to_2d = True , as_lists = False , pass_input_shape = False , pass_flex_2d = False , level_names = None , hide_levels = None , stacking_kwargs = None , return_raw = False , use_raw = None , wrapper_kwargs = None , seed = None , ** kwargs ) A pipeline for running an indicator, used by IndicatorFactory . Args num_ret_outputs :\u2002 int The number of output arrays returned by custom_func . custom_func :\u2002 callable A custom calculation function. See IndicatorFactory.from_custom_func() . *args Arguments passed to the custom_func . require_input_shape :\u2002 bool Whether to input shape is required. Will set pass_input_shape to True and raise an error if input_shape is None. input_shape :\u2002 tuple Shape to broadcast each input to. Can be passed to custom_func . See pass_input_shape . input_index :\u2002 index_like Sets index of each input. Can be used to label index if no inputs passed. input_columns :\u2002 index_like Sets columns of each input. Can be used to label columns if no inputs passed. input_list :\u2002 list of array_like A list of input arrays. in_output_list :\u2002 list of array_like A list of in-place output arrays. If an array should be generated, pass None. in_output_settings :\u2002 dict or list of dict Settings corresponding to each in-place output. Following keys are accepted: dtype : Create this array using this data type and np.empty . Default is None. broadcast_kwargs :\u2002 dict Keyword arguments passed to broadcast() to broadcast inputs. param_list :\u2002 list of any A list of parameters. Each element is either an array-like object or a single value of any type. param_product :\u2002 bool Whether to build a Cartesian product out of all parameters. param_settings :\u2002 dict or list of dict Settings corresponding to each parameter. Following keys are accepted: dtype : If data type is an enumerated type or other mapping, and a string as parameter value was passed, will convert it first. is_tuple : If tuple was passed, it will be considered as a single value. To treat it as multiple values, pack it into a list. is_array_like : If array-like object was passed, it will be considered as a single value. To treat it as multiple values, pack it into a list. bc_to_input : Whether to broadcast parameter to input size. You can also broadcast parameter to an axis by passing an integer. broadcast_kwargs : Keyword arguments passed to broadcast() . per_column : Whether each parameter value can be split per column such that it can be better reflected in a multi-index. Does not affect broadcasting. run_unique :\u2002 bool Whether to run only on unique parameter combinations. Disable if two identical parameter combinations can lead to different results (e.g., due to randomness) or if inputs are large and custom_func is fast. Note Cache, raw output, and output objects outside of num_ret_outputs will be returned for unique parameter combinations only. silence_warnings :\u2002 bool Whether to hide warnings such as coming from run_unique . per_column :\u2002 bool Whether to split the DataFrame into Series, one per column, and run custom_func on each Series. Each list of parameter values will broadcast to the number of columns and each parameter value will be applied per Series rather than per DataFrame. Input shape must be known beforehand. pass_col :\u2002 bool Whether to pass column index as keyword argument if per_column is set to True. keep_pd :\u2002 bool Whether to keep inputs as pandas objects, otherwise convert to NumPy arrays. to_2d :\u2002 bool Whether to reshape inputs to 2-dim arrays, otherwise keep as-is. as_lists :\u2002 bool Whether to pass inputs and parameters to custom_func as lists. If custom_func is Numba-compiled, passes tuples. pass_input_shape :\u2002 bool Whether to pass input_shape to custom_func as keyword argument. pass_flex_2d :\u2002 bool Whether to pass flex_2d to custom_func as keyword argument. level_names :\u2002 list of str A list of column level names corresponding to each parameter. Should have the same length as param_list . hide_levels :\u2002 list of int A list of indices of parameter levels to hide. stacking_kwargs :\u2002 dict Keyword arguments passed to repeat_index() , tile_index() , and stack_indexes() when stacking parameter and input column levels. return_raw :\u2002 bool Whether to return raw output without post-processing and hashed parameter tuples. use_raw :\u2002 bool Takes the raw results and uses them instead of running custom_func . wrapper_kwargs :\u2002 dict Keyword arguments passed to ArrayWrapper . seed :\u2002 int Set seed to make output deterministic. **kwargs Keyword arguments passed to the custom_func . Some common arguments include return_cache to return cache and use_cache to use cache. Those are only applicable to custom_func that supports it ( custom_func created using IndicatorFactory.from_apply_func() are supported by default). Returns Array wrapper, list of inputs ( np.ndarray ), input mapper ( np.ndarray ), list of outputs ( np.ndarray ), list of parameter arrays ( np.ndarray ), list of parameter mappers ( np.ndarray ), list of outputs that are outside of num_ret_outputs . Explanation Here is a subset of tasks that the function run_pipeline() does: Takes one or multiple array objects in input_list and broadcasts them. >>> sr = pd . Series ([ 1 , 2 ], index = [ 'x' , 'y' ]) >>> df = pd . DataFrame ([[ 3 , 4 ], [ 5 , 6 ]], index = [ 'x' , 'y' ], columns = [ 'a' , 'b' ]) >>> input_list = vbt . base . reshape_fns . broadcast ( sr , df ) >>> input_list [ 0 ] a b x 1 1 y 2 2 >>> input_list [ 1 ] a b x 3 4 y 5 6 Takes one or multiple parameters in param_list , converts them to NumPy arrays and broadcasts them. >>> p1 , p2 , p3 = 1 , [ 2 , 3 , 4 ], [ False ] >>> param_list = vbt . base . reshape_fns . broadcast ( p1 , p2 , p3 ) >>> param_list [ 0 ] array([1, 1, 1]) >>> param_list [ 1 ] array([2, 3, 4]) >>> param_list [ 2 ] array([False, False, False]) Performs calculation using custom_func to build output arrays ( output_list ) and other objects ( other_list , optionally). >>> def custom_func ( ts1 , ts2 , p1 , p2 , p3 , * args , ** kwargs ): ... return np . hstack (( ... ts1 + ts2 + p1 [ 0 ] * p2 [ 0 ], ... ts1 + ts2 + p1 [ 1 ] * p2 [ 1 ], ... ts1 + ts2 + p1 [ 2 ] * p2 [ 2 ], ... )) >>> output = custom_func ( * input_list , * param_list ) >>> output array([[ 6, 7, 7, 8, 8, 9], [ 9, 10, 10, 11, 11, 12]]) Creates new column hierarchy based on parameters and level names. >>> p1_columns = pd . Index ( param_list [ 0 ], name = 'p1' ) >>> p2_columns = pd . Index ( param_list [ 1 ], name = 'p2' ) >>> p3_columns = pd . Index ( param_list [ 2 ], name = 'p3' ) >>> p_columns = vbt . base . index_fns . stack_indexes ([ p1_columns , p2_columns , p3_columns ]) >>> new_columns = vbt . base . index_fns . combine_indexes ([ p_columns , input_list [ 0 ] . columns ]) >>> output_df = pd . DataFrame ( output , columns = new_columns ) >>> output_df p1 1 p2 2 3 4 p3 False False False False False False a b a b a b 0 6 7 7 8 8 9 1 9 10 10 11 11 12 Broadcasts objects in input_list to match the shape of objects in output_list through tiling. This is done to be able to compare them and generate signals, since we cannot compare NumPy arrays that have totally different shapes, such as (2, 2) and (2, 6). >>> new_input_list = [ ... input_list [ 0 ] . vbt . tile ( len ( param_list [ 0 ]), keys = p_columns ), ... input_list [ 1 ] . vbt . tile ( len ( param_list [ 0 ]), keys = p_columns ) ... ] >>> new_input_list [ 0 ] p1 1 p2 2 3 4 p3 False False False False False False a b a b a b 0 1 1 1 1 1 1 1 2 2 2 2 2 2 Builds parameter mappers that will link parameters from param_list to columns in input_list and output_list . This is done to enable column indexing using parameter values.","title":"run_pipeline()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorBase","text":"Indicator base class. Properties should be set before instantiation. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() PlotsBuilderMixin.plots_defaults StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() StatsBuilderMixin.stats_defaults Wrapping.config Wrapping.iloc Wrapping.indexing_kwargs Wrapping.loc Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Wrapping.self_aliases Wrapping.wrapper Wrapping.writeable_attrs Subclasses ATR BBANDS BOLB FIXLB FMAX FMEAN FMIN FSTD LEXLB MA MACD MEANLB MSTD OBV OHLCSTCX OHLCSTX RAND RANDNX RANDX RPROB RPROBCX RPROBNX RPROBX RSI STCX STOCH STX TRENDLB","title":"IndicatorBase"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorBase.in_output_names","text":"Names of the in-place output arrays.","title":"in_output_names"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorBase.indexing_func","text":"IndicatorBase . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on IndicatorBase .","title":"indexing_func()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorBase.input_names","text":"Names of the input arrays.","title":"input_names"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorBase.level_names","text":"Column level names corresponding to each parameter.","title":"level_names"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorBase.output_flags","text":"Dictionary of output flags.","title":"output_flags"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorBase.output_names","text":"Names of the regular output arrays.","title":"output_names"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorBase.param_names","text":"Names of the parameters.","title":"param_names"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorBase.run","text":"IndicatorBase . run ( * args , ** kwargs ) Public run method.","title":"run()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorBase.run_combs","text":"IndicatorBase . run_combs ( * args , ** kwargs ) Public run combinations method.","title":"run_combs()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorBase.short_name","text":"Name of the indicator.","title":"short_name"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory","text":"A factory for creating new indicators. Initialize IndicatorFactory to create a skeleton and then use a class method such as IndicatorFactory.from_custom_func() to bind a calculation function to the skeleton. Args class_name :\u2002 str Name for the created indicator class. class_docstring :\u2002 str Docstring for the created indicator class. module_name :\u2002 str Specify the module the class originates from. short_name :\u2002 str A short name of the indicator. Defaults to lower-case class_name . prepend_name :\u2002 bool Whether to prepend short_name to each parameter level. input_names :\u2002 list of str A list of names of input arrays. param_names :\u2002 list of str A list of names of parameters. in_output_names :\u2002 list of str A list of names of in-place output arrays. An in-place output is an output that is not returned but modified in-place. Some advantages of such outputs include: 1) they don't need to be returned, 2) they can be passed between functions as easily as inputs, 3) they can be provided with already allocated data to safe memory, 4) if data or default value are not provided, they are created empty to not occupy memory. output_names :\u2002 list of str A list of names of output arrays. output_flags :\u2002 dict A dictionary of in-place and regular output flags. custom_output_props :\u2002 dict A dictionary with user-defined functions that will be bound to the indicator class and wrapped with @cached_property . attr_settings :\u2002 dict A dictionary of settings by attribute name. Attributes can be input_names , in_output_names , output_names and custom_output_props . Following keys are accepted: dtype : Data type used to determine which methods to generate around this attribute. Set to None to disable. Default is np.float_ . Can be set to instance of collections.namedtuple acting as enumerated type, or any other mapping; It will then create a property with suffix readable that contains data in a string format. metrics :\u2002 dict Metrics supported by StatsBuilderMixin.stats() . If dict, will be converted to Config . stats_defaults :\u2002 callable or dict Defaults for StatsBuilderMixin.stats() . If dict, will be converted into a property. subplots :\u2002 dict Subplots supported by PlotsBuilderMixin.plots() . If dict, will be converted to Config . plots_defaults :\u2002 callable or dict Defaults for PlotsBuilderMixin.plots() . If dict, will be converted into a property. Note The __init__ method is not used for running the indicator, for this use run . The reason for this is indexing, which requires a clean __init__ method for creating a new indicator object with newly indexed attributes. Subclasses SignalFactory","title":"IndicatorFactory"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.find_ta_indicator","text":"IndicatorFactory . find_ta_indicator ( cls_name ) Get ta indicator class by its name.","title":"find_ta_indicator()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.from_apply_func","text":"IndicatorFactory . from_apply_func ( apply_func , cache_func = None , pass_packed = False , kwargs_to_args = None , numba_loop = False , ** kwargs ) Build indicator class around a custom apply function. In contrast to IndicatorFactory.from_custom_func() , this method handles a lot of things for you, such as caching, parameter selection, and concatenation. Your part is writing a function apply_func that accepts a selection of parameters (single values as opposed to multiple values in IndicatorFactory.from_custom_func() ) and does the calculation. It then automatically concatenates the resulting arrays into a single array per output. While this approach is simpler, it's also less flexible, since we can only work with one parameter selection at a time and can't view all parameters. The UDF apply_func also can't take keyword arguments, nor it can return anything other than outputs listed in output_names . Note If apply_func is a Numba-compiled function: All inputs are automatically converted to NumPy arrays Each argument in *args must be of a Numba-compatible type You cannot pass keyword arguments Your outputs must be arrays of the same shape, data type and data order Args apply_func :\u2002 callable A function that takes inputs, selection of parameters, and other arguments, and does calculations to produce outputs. Arguments are passed to apply_func in the following order: input_shape if pass_input_shape is set to True and input_shape not in kwargs_to_args col if per_column and pass_col are set to True and col not in kwargs_to_args broadcast time-series arrays corresponding to input_names broadcast in-place output arrays corresponding to in_output_names single parameter selection corresponding to param_names variable arguments if var_args is set to True arguments listed in kwargs_to_args flex_2d if pass_flex_2d is set to True and flex_2d not in kwargs_to_args keyword arguments if apply_func is not Numba-compiled Can be Numba-compiled. Note Shape of each output should be the same and match the shape of each input. cache_func :\u2002 callable A caching function to preprocess data beforehand. Takes the same arguments as apply_func . Should return a single object or a tuple of objects. All returned objects will be passed unpacked as last arguments to apply_func . Can be Numba-compiled. pass_packed :\u2002 bool Whether to pass packed tuples for inputs, in-place outputs, and parameters. kwargs_to_args :\u2002 list of str Keyword arguments from kwargs dict to pass as positional arguments to the apply function. Should be used together with numba_loop set to True since Numba doesn't support variable keyword arguments. Defaults to []. Order matters. numba_loop :\u2002 bool Whether to loop using Numba. Set to True when iterating large number of times over small input, but note that Numba doesn't support variable keyword arguments. **kwargs Keyword arguments passed to IndicatorFactory.from_custom_func() . Returns Indicator Additionally, each run method now supports use_ray argument, which indicates whether to use Ray to execute apply_func in parallel. Only works with numba_loop set to False. See ray_apply() for related keyword arguments. Usage The following example produces the same indicator as the IndicatorFactory.from_custom_func() example. >>> @njit ... def apply_func_nb ( ts1 , ts2 , p1 , p2 , arg1 , arg2 ): ... return ts1 * p1 + arg1 , ts2 * p2 + arg2 >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'ts1' , 'ts2' ], ... param_names = [ 'p1' , 'p2' ], ... output_names = [ 'o1' , 'o2' ] ... ) . from_apply_func ( ... apply_func_nb , var_args = True , ... kwargs_to_args = [ 'arg2' ], arg2 = 200 ) >>> myInd = MyInd . run ( price , price * 2 , [ 1 , 2 ], [ 3 , 4 ], 100 ) >>> myInd . o1 custom_p1 1 2 custom_p2 3 4 a b a b 2020-01-01 101.0 105.0 102.0 110.0 2020-01-02 102.0 104.0 104.0 108.0 2020-01-03 103.0 103.0 106.0 106.0 2020-01-04 104.0 102.0 108.0 104.0 2020-01-05 105.0 101.0 110.0 102.0 >>> myInd . o2 custom_p1 1 2 custom_p2 3 4 a b a b 2020-01-01 206.0 230.0 208.0 240.0 2020-01-02 212.0 224.0 216.0 232.0 2020-01-03 218.0 218.0 224.0 224.0 2020-01-04 224.0 212.0 232.0 216.0 2020-01-05 230.0 206.0 240.0 208.0","title":"from_apply_func()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.from_custom_func","text":"IndicatorFactory . from_custom_func ( custom_func , require_input_shape = False , param_settings = None , in_output_settings = None , hide_params = None , hide_default = True , var_args = False , keyword_only_args = False , ** pipeline_kwargs ) Build indicator class around a custom calculation function. In contrast to IndicatorFactory.from_apply_func() , this method offers full flexbility. It's up to we to handle caching and concatenate columns for each parameter (for example, by using apply_and_concat_one() ). Also, you should ensure that each output array has an appropriate number of columns, which is the number of columns in input arrays multiplied by the number of parameter combinations. Args custom_func :\u2002 callable A function that takes broadcast arrays corresponding to input_names , broadcast in-place output arrays corresponding to in_output_names , broadcast parameter arrays corresponding to param_names , and other arguments and keyword arguments, and returns outputs corresponding to output_names and other objects that are then returned with the indicator instance. Can be Numba-compiled. Note Shape of each output should be the same and match the shape of each input stacked n times (= the number of parameter values) along the column axis. require_input_shape :\u2002 bool Whether to input shape is required. param_settings :\u2002 dict A dictionary of parameter settings keyed by name. See run_pipeline() for keys. Can be overwritten by any run method. in_output_settings :\u2002 dict A dictionary of in-place output settings keyed by name. See run_pipeline() for keys. Can be overwritten by any run method. hide_params :\u2002 list of str Parameter names to hide column levels for. Can be overwritten by any run method. hide_default :\u2002 bool Whether to hide column levels of parameters with default value. Can be overwritten by any run method. var_args :\u2002 bool Whether run methods should accept variable arguments ( *args ). Set to True if custom_func accepts positional agruments that are not listed in the config. keyword_only_args :\u2002 bool Whether run methods should accept keyword-only arguments ( * ). Set to True to force the user to use keyword arguments (e.g., to avoid misplacing arguments). **pipeline_kwargs Keyword arguments passed to run_pipeline() . Can be overwritten by any run method. Can contain default values for param_names and in_output_names , but also custom positional and keyword arguments passed to the custom_func . Returns Indicator , and optionally other objects that are returned by custom_func and exceed output_names . Usage The following example produces the same indicator as the IndicatorFactory.from_apply_func() example. >>> @njit >>> def apply_func_nb ( i , ts1 , ts2 , p1 , p2 , arg1 , arg2 ): ... return ts1 * p1 [ i ] + arg1 , ts2 * p2 [ i ] + arg2 >>> @njit ... def custom_func ( ts1 , ts2 , p1 , p2 , arg1 , arg2 ): ... return vbt . base . combine_fns . apply_and_concat_multiple_nb ( ... len ( p1 ), apply_func_nb , ts1 , ts2 , p1 , p2 , arg1 , arg2 ) >>> MyInd = vbt . IndicatorFactory ( ... input_names = [ 'ts1' , 'ts2' ], ... param_names = [ 'p1' , 'p2' ], ... output_names = [ 'o1' , 'o2' ] ... ) . from_custom_func ( custom_func , var_args = True , arg2 = 200 ) >>> myInd = MyInd . run ( price , price * 2 , [ 1 , 2 ], [ 3 , 4 ], 100 ) >>> myInd . o1 custom_p1 1 2 custom_p2 3 4 a b a b 2020-01-01 101.0 105.0 102.0 110.0 2020-01-02 102.0 104.0 104.0 108.0 2020-01-03 103.0 103.0 106.0 106.0 2020-01-04 104.0 102.0 108.0 104.0 2020-01-05 105.0 101.0 110.0 102.0 >>> myInd . o2 custom_p1 1 2 custom_p2 3 4 a b a b 2020-01-01 206.0 230.0 208.0 240.0 2020-01-02 212.0 224.0 216.0 232.0 2020-01-03 218.0 218.0 224.0 224.0 2020-01-04 224.0 212.0 232.0 216.0 2020-01-05 230.0 206.0 240.0 208.0 The difference between apply_func_nb here and in IndicatorFactory.from_apply_func() is that here it takes the index of the current parameter combination that can be used for parameter selection. You can also remove the entire apply_func_nb and define your logic in custom_func (which shouldn't necessarily be Numba-compiled): >>> @njit ... def custom_func ( ts1 , ts2 , p1 , p2 , arg1 , arg2 ): ... input_shape = ts1 . shape ... n_params = len ( p1 ) ... out1 = np . empty (( input_shape [ 0 ], input_shape [ 1 ] * n_params ), dtype = np . float_ ) ... out2 = np . empty (( input_shape [ 0 ], input_shape [ 1 ] * n_params ), dtype = np . float_ ) ... for k in range ( n_params ): ... for col in range ( input_shape [ 1 ]): ... for i in range ( input_shape [ 0 ]): ... out1 [ i , input_shape [ 1 ] * k + col ] = ts1 [ i , col ] * p1 [ k ] + arg1 ... out2 [ i , input_shape [ 1 ] * k + col ] = ts2 [ i , col ] * p2 [ k ] + arg2 ... return out1 , out2","title":"from_custom_func()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.from_pandas_ta","text":"IndicatorFactory . from_pandas_ta ( func_name , parse_kwargs = None , init_kwargs = None , ** kwargs ) Build an indicator class around a pandas-ta function. Requires pandas-ta installed. Args func_name :\u2002 str Function name. parse_kwargs :\u2002 dict Keyword arguments passed to IndicatorFactory.parse_pandas_ta_config() . init_kwargs :\u2002 dict Keyword arguments passed to IndicatorFactory . **kwargs Keyword arguments passed to IndicatorFactory.from_custom_func() . Returns Indicator Usage >>> SMA = vbt . IndicatorFactory . from_pandas_ta ( 'SMA' ) >>> sma = SMA . run ( price , length = [ 2 , 3 ]) >>> sma . sma sma_length 2 3 a b a b 2020-01-01 NaN NaN NaN NaN 2020-01-02 1.5 4.5 NaN NaN 2020-01-03 2.5 3.5 2.0 4.0 2020-01-04 3.5 2.5 3.0 3.0 2020-01-05 4.5 1.5 4.0 2.0 To get help on running the indicator, use the help command: >>> help ( SMA . run ) Help on method run: run(close, length=None, offset=None, short_name='sma', hide_params=None, hide_default=True, **kwargs) method of builtins.type instance Run `SMA` indicator. * Inputs: `close` * Parameters: `length`, `offset` * Outputs: `sma` Pass a list of parameter names as `hide_params` to hide their column levels. Set `hide_default` to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to [run_pipeline()](/api/indicators/factory/#vectorbt.indicators.factory.run_pipeline \"vectorbt.indicators.factory.run_pipeline\"). To get the indicator docstring, use the help command or print the __doc__ attribute: >>> print ( SMA . __doc__ ) Simple Moving Average (SMA) The Simple Moving Average is the classic moving average that is the equally weighted average over n periods. Sources: <https://www.tradingtechnologies.com/help/x-study/technical-indicator-definitions/simple-moving-average-sma/> Calculation: Default Inputs: length=10 SMA = SUM(close, length) / length Args: close (pd.Series): Series of 'close's length (int): It's period. Default: 10 offset (int): How many periods to offset the result. Default: 0 Kwargs: adjust (bool): Default: True presma (bool, optional): If True, uses SMA for initial value. fillna (value, optional): pd.DataFrame.fillna(value) fill_method (value, optional): Type of fill method Returns: pd.Series: New feature generated.","title":"from_pandas_ta()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.from_ta","text":"IndicatorFactory . from_ta ( cls_name , init_kwargs = None , ** kwargs ) Build an indicator class around a ta class. Requires ta installed. Args cls_name :\u2002 str Class name. init_kwargs :\u2002 dict Keyword arguments passed to IndicatorFactory . **kwargs Keyword arguments passed to IndicatorFactory.from_custom_func() . Returns Indicator Usage >>> SMAIndicator = vbt . IndicatorFactory . from_ta ( 'SMAIndicator' ) >>> sma = SMAIndicator . run ( price , window = [ 2 , 3 ]) >>> sma . sma_indicator smaindicator_window 2 3 a b a b 2020-01-01 NaN NaN NaN NaN 2020-01-02 1.5 4.5 NaN NaN 2020-01-03 2.5 3.5 2.0 4.0 2020-01-04 3.5 2.5 3.0 3.0 2020-01-05 4.5 1.5 4.0 2.0 To get help on running the indicator, use the help command: >>> help ( SMAIndicator . run ) Help on method run: run(close, window, fillna=False, short_name='smaindicator', hide_params=None, hide_default=True, **kwargs) method of builtins.type instance Run `SMAIndicator` indicator. * Inputs: `close` * Parameters: `window`, `fillna` * Outputs: `sma_indicator` Pass a list of parameter names as `hide_params` to hide their column levels. Set `hide_default` to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to [run_pipeline()](/api/indicators/factory/#vectorbt.indicators.factory.run_pipeline \"vectorbt.indicators.factory.run_pipeline\"). To get the indicator docstring, use the help command or print the __doc__ attribute: >>> print ( SMAIndicator . __doc__ ) SMA - Simple Moving Average Args: close(pandas.Series): dataset 'Close' column. window(int): n period. fillna(bool): if True, fill nan values.","title":"from_ta()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.from_talib","text":"IndicatorFactory . from_talib ( func_name , init_kwargs = None , ** kwargs ) Build an indicator class around a TA-Lib function. Requires TA-Lib installed. For input, parameter and output names, see docs . Args func_name :\u2002 str Function name. init_kwargs :\u2002 dict Keyword arguments passed to IndicatorFactory . **kwargs Keyword arguments passed to IndicatorFactory.from_custom_func() . Returns Indicator Usage >>> SMA = vbt . IndicatorFactory . from_talib ( 'SMA' ) >>> sma = SMA . run ( price , timeperiod = [ 2 , 3 ]) >>> sma . real sma_timeperiod 2 3 a b a b 2020-01-01 NaN NaN NaN NaN 2020-01-02 1.5 4.5 NaN NaN 2020-01-03 2.5 3.5 2.0 4.0 2020-01-04 3.5 2.5 3.0 3.0 2020-01-05 4.5 1.5 4.0 2.0 To get help on running the indicator, use the help command: >>> help ( SMA . run ) Help on method run: run(close, timeperiod=30, short_name='sma', hide_params=None, hide_default=True, **kwargs) method of builtins.type instance Run `SMA` indicator. * Inputs: `close` * Parameters: `timeperiod` * Outputs: `real` Pass a list of parameter names as `hide_params` to hide their column levels. Set `hide_default` to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to [run_pipeline()](/api/indicators/factory/#vectorbt.indicators.factory.run_pipeline \"vectorbt.indicators.factory.run_pipeline\").","title":"from_talib()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.get_pandas_ta_indicators","text":"IndicatorFactory . get_pandas_ta_indicators ( silence_warnings = True ) Get all pandas-ta indicators. Note Returns only the indicators that have been successfully parsed.","title":"get_pandas_ta_indicators()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.get_ta_indicators","text":"IndicatorFactory . get_ta_indicators () Get all ta indicators.","title":"get_ta_indicators()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.get_talib_indicators","text":"IndicatorFactory . get_talib_indicators () Get all TA-Lib indicators.","title":"get_talib_indicators()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.parse_pandas_ta_config","text":"IndicatorFactory . parse_pandas_ta_config ( func , test_input_names = None , test_index_len = 100 ) Get the config of a pandas-ta indicator.","title":"parse_pandas_ta_config()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.parse_ta_config","text":"IndicatorFactory . parse_ta_config ( ind_cls ) Get the config of a ta indicator.","title":"parse_ta_config()"},{"location":"api/indicators/factory/#vectorbt.indicators.factory.MetaIndicatorBase","text":"Meta class that exposes a read-only class property StatsBuilderMixin.metrics . Superclasses MetaPlotsBuilderMixin MetaStatsBuilderMixin builtins.type Inherited members MetaPlotsBuilderMixin.subplots MetaStatsBuilderMixin.metrics","title":"MetaIndicatorBase"},{"location":"api/indicators/nb/","text":"nb module \u00b6 Numba-compiled functions. Provides an arsenal of Numba-compiled functions that are used by indicator classes. These only accept NumPy arrays and other Numba-compatible types. Note vectorbt treats matrices as first-class citizens and expects input arrays to be 2-dim, unless function has suffix _1d or is meant to be input to another function. Data is processed along index (axis 0). All functions passed as argument should be Numba-compiled. atr_apply_nb function \u00b6 atr_apply_nb ( high , low , close , window , ewm , adjust , tr , cache_dict ) Apply function for ATR . atr_cache_nb function \u00b6 atr_cache_nb ( high , low , close , windows , ewms , adjust ) Caching function for ATR . bb_apply_nb function \u00b6 bb_apply_nb ( close , window , ewm , alpha , adjust , ddof , ma_cache_dict , mstd_cache_dict ) Apply function for BBANDS . bb_cache_nb function \u00b6 bb_cache_nb ( close , windows , ewms , alphas , adjust , ddof ) Caching function for BBANDS . ma_apply_nb function \u00b6 ma_apply_nb ( close , window , ewm , adjust , cache_dict ) Apply function for MA . ma_cache_nb function \u00b6 ma_cache_nb ( close , windows , ewms , adjust ) Caching function for MA . ma_nb function \u00b6 ma_nb ( a , window , ewm , adjust = False ) Compute simple or exponential moving average ( ewm=True ). macd_apply_nb function \u00b6 macd_apply_nb ( close , fast_window , slow_window , signal_window , macd_ewm , signal_ewm , adjust , cache_dict ) Apply function for MACD . macd_cache_nb function \u00b6 macd_cache_nb ( close , fast_windows , slow_windows , signal_windows , macd_ewms , signal_ewms , adjust ) Caching function for MACD . mstd_apply_nb function \u00b6 mstd_apply_nb ( close , window , ewm , adjust , ddof , cache_dict ) Apply function for MSTD . mstd_cache_nb function \u00b6 mstd_cache_nb ( close , windows , ewms , adjust , ddof ) Caching function for MSTD . mstd_nb function \u00b6 mstd_nb ( a , window , ewm , adjust = False , ddof = 0 ) Compute simple or exponential moving STD ( ewm=True ). obv_custom_nb function \u00b6 obv_custom_nb ( close , volume_ts ) Custom calculation function for OBV . rsi_apply_nb function \u00b6 rsi_apply_nb ( close , window , ewm , adjust , cache_dict ) Apply function for RSI . rsi_cache_nb function \u00b6 rsi_cache_nb ( close , windows , ewms , adjust ) Caching function for RSI . stoch_apply_nb function \u00b6 stoch_apply_nb ( high , low , close , k_window , d_window , d_ewm , adjust , cache_dict ) Apply function for STOCH . stoch_cache_nb function \u00b6 stoch_cache_nb ( high , low , close , k_windows , d_windows , d_ewms , adjust ) Caching function for STOCH . true_range_nb function \u00b6 true_range_nb ( high , low , close ) Calculate true range.","title":"nb"},{"location":"api/indicators/nb/#vectorbt.indicators.nb","text":"Numba-compiled functions. Provides an arsenal of Numba-compiled functions that are used by indicator classes. These only accept NumPy arrays and other Numba-compatible types. Note vectorbt treats matrices as first-class citizens and expects input arrays to be 2-dim, unless function has suffix _1d or is meant to be input to another function. Data is processed along index (axis 0). All functions passed as argument should be Numba-compiled.","title":"vectorbt.indicators.nb"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.atr_apply_nb","text":"atr_apply_nb ( high , low , close , window , ewm , adjust , tr , cache_dict ) Apply function for ATR .","title":"atr_apply_nb()"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.atr_cache_nb","text":"atr_cache_nb ( high , low , close , windows , ewms , adjust ) Caching function for ATR .","title":"atr_cache_nb()"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.bb_apply_nb","text":"bb_apply_nb ( close , window , ewm , alpha , adjust , ddof , ma_cache_dict , mstd_cache_dict ) Apply function for BBANDS .","title":"bb_apply_nb()"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.bb_cache_nb","text":"bb_cache_nb ( close , windows , ewms , alphas , adjust , ddof ) Caching function for BBANDS .","title":"bb_cache_nb()"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.ma_apply_nb","text":"ma_apply_nb ( close , window , ewm , adjust , cache_dict ) Apply function for MA .","title":"ma_apply_nb()"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.ma_cache_nb","text":"ma_cache_nb ( close , windows , ewms , adjust ) Caching function for MA .","title":"ma_cache_nb()"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.ma_nb","text":"ma_nb ( a , window , ewm , adjust = False ) Compute simple or exponential moving average ( ewm=True ).","title":"ma_nb()"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.macd_apply_nb","text":"macd_apply_nb ( close , fast_window , slow_window , signal_window , macd_ewm , signal_ewm , adjust , cache_dict ) Apply function for MACD .","title":"macd_apply_nb()"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.macd_cache_nb","text":"macd_cache_nb ( close , fast_windows , slow_windows , signal_windows , macd_ewms , signal_ewms , adjust ) Caching function for MACD .","title":"macd_cache_nb()"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.mstd_apply_nb","text":"mstd_apply_nb ( close , window , ewm , adjust , ddof , cache_dict ) Apply function for MSTD .","title":"mstd_apply_nb()"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.mstd_cache_nb","text":"mstd_cache_nb ( close , windows , ewms , adjust , ddof ) Caching function for MSTD .","title":"mstd_cache_nb()"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.mstd_nb","text":"mstd_nb ( a , window , ewm , adjust = False , ddof = 0 ) Compute simple or exponential moving STD ( ewm=True ).","title":"mstd_nb()"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.obv_custom_nb","text":"obv_custom_nb ( close , volume_ts ) Custom calculation function for OBV .","title":"obv_custom_nb()"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.rsi_apply_nb","text":"rsi_apply_nb ( close , window , ewm , adjust , cache_dict ) Apply function for RSI .","title":"rsi_apply_nb()"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.rsi_cache_nb","text":"rsi_cache_nb ( close , windows , ewms , adjust ) Caching function for RSI .","title":"rsi_cache_nb()"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.stoch_apply_nb","text":"stoch_apply_nb ( high , low , close , k_window , d_window , d_ewm , adjust , cache_dict ) Apply function for STOCH .","title":"stoch_apply_nb()"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.stoch_cache_nb","text":"stoch_cache_nb ( high , low , close , k_windows , d_windows , d_ewms , adjust ) Caching function for STOCH .","title":"stoch_cache_nb()"},{"location":"api/indicators/nb/#vectorbt.indicators.nb.true_range_nb","text":"true_range_nb ( high , low , close ) Calculate true range.","title":"true_range_nb()"},{"location":"api/labels/","text":"labels package \u00b6 Modules for building and running look-ahead indicators and label generators. Sub-modules \u00b6 vectorbt.labels.enums vectorbt.labels.generators vectorbt.labels.nb","title":"labels"},{"location":"api/labels/#vectorbt.labels","text":"Modules for building and running look-ahead indicators and label generators.","title":"vectorbt.labels"},{"location":"api/labels/#sub-modules","text":"vectorbt.labels.enums vectorbt.labels.generators vectorbt.labels.nb","title":"Sub-modules"},{"location":"api/labels/enums/","text":"enums module \u00b6 Named tuples and enumerated types. Defines enums and other schemas for vectorbt.labels . TrendMode TrendModeT \u00b6 Trend mode. { \"Binary\" : 0 , \"BinaryCont\" : 1 , \"BinaryContSat\" : 2 , \"PctChange\" : 3 , \"PctChangeNorm\" : 4 } Attributes Binary See bn_trend_labels_nb() . BinaryCont See bn_cont_trend_labels_nb() . BinaryContSat See bn_cont_sat_trend_labels_nb() . PctChange See pct_trend_labels_nb() . PctChangeNorm See pct_trend_labels_nb() with normalize set to True.","title":"enums"},{"location":"api/labels/enums/#vectorbt.labels.enums","text":"Named tuples and enumerated types. Defines enums and other schemas for vectorbt.labels .","title":"vectorbt.labels.enums"},{"location":"api/labels/enums/#vectorbt.labels.enums.TrendMode","text":"Trend mode. { \"Binary\" : 0 , \"BinaryCont\" : 1 , \"BinaryContSat\" : 2 , \"PctChange\" : 3 , \"PctChangeNorm\" : 4 } Attributes Binary See bn_trend_labels_nb() . BinaryCont See bn_cont_trend_labels_nb() . BinaryContSat See bn_cont_sat_trend_labels_nb() . PctChange See pct_trend_labels_nb() . PctChangeNorm See pct_trend_labels_nb() with normalize set to True.","title":"TrendMode"},{"location":"api/labels/generators/","text":"generators module \u00b6 Basic look-ahead indicators and label generators. You can access all the indicators either by vbt.* or vbt.labels.* . BOLB class \u00b6 Label generator based on breakout_labels_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.labels.generators._BOLB apply_func method \u00b6 BOLB . apply_func ( close , window , pos_th , neg_th , wait = 1 , flex_2d = True ) For each value, return 1 if any value in the next period is greater than the positive threshold (in %), -1 if less than the negative threshold, and 0 otherwise. First hit wins. close method \u00b6 Input array. close_above method \u00b6 BOLB . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 BOLB . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 BOLB . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 BOLB . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 BOLB . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 BOLB . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func . labels property \u00b6 Output array. labels_above method \u00b6 BOLB . labels_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is above other . See combine_objs() . labels_below method \u00b6 BOLB . labels_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is below other . See combine_objs() . labels_crossed_above method \u00b6 BOLB . labels_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_above other . See combine_objs() . labels_crossed_below method \u00b6 BOLB . labels_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_below other . See combine_objs() . labels_equal method \u00b6 BOLB . labels_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is equal other . See combine_objs() . labels_stats method \u00b6 BOLB . labels_stats ( * args , ** kwargs ) Stats of labels as generic. neg_th_list property \u00b6 List of neg_th values. plot method \u00b6 BOLB . plot ( column = None , ** kwargs ) Plot close and overlay it with the heatmap of labels . **kwargs are passed to GenericSRAccessor.overlay_with_heatmap() . pos_th_list property \u00b6 List of pos_th values. run class method \u00b6 BOLB . run ( close , window , pos_th = Default ( 0.0 ), neg_th = Default ( 0.0 ), short_name = 'bolb' , hide_params = None , hide_default = True , ** kwargs ) Run BOLB indicator. Inputs: close Parameters: window , pos_th , neg_th Outputs: labels Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 BOLB . run_combs ( close , window , pos_th = Default ( 0.0 ), neg_th = Default ( 0.0 ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple BOLB indicators using function comb_func . Inputs: close Parameters: window , pos_th , neg_th Outputs: labels comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to BOLB.run() . window_list property \u00b6 List of window values. FIXLB class \u00b6 Label generator based on fixed_labels_apply_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.labels.generators._FIXLB apply_func method \u00b6 FIXLB . apply_func ( close , n ) Get percentage change from the current value to a future value. close method \u00b6 Input array. close_above method \u00b6 FIXLB . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 FIXLB . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 FIXLB . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 FIXLB . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 FIXLB . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 FIXLB . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func . labels property \u00b6 Output array. labels_above method \u00b6 FIXLB . labels_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is above other . See combine_objs() . labels_below method \u00b6 FIXLB . labels_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is below other . See combine_objs() . labels_crossed_above method \u00b6 FIXLB . labels_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_above other . See combine_objs() . labels_crossed_below method \u00b6 FIXLB . labels_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_below other . See combine_objs() . labels_equal method \u00b6 FIXLB . labels_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is equal other . See combine_objs() . labels_stats method \u00b6 FIXLB . labels_stats ( * args , ** kwargs ) Stats of labels as generic. n_list property \u00b6 List of n values. plot method \u00b6 FIXLB . plot ( column = None , ** kwargs ) Plot close and overlay it with the heatmap of labels . **kwargs are passed to GenericSRAccessor.overlay_with_heatmap() . run class method \u00b6 FIXLB . run ( close , n , short_name = 'fixlb' , hide_params = None , hide_default = True , ** kwargs ) Run FIXLB indicator. Inputs: close Parameters: n Outputs: labels Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 FIXLB . run_combs ( close , n , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple FIXLB indicators using function comb_func . Inputs: close Parameters: n Outputs: labels comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to FIXLB.run() . FMAX class \u00b6 Look-ahead indicator based on future_max_apply_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() apply_func method \u00b6 FMAX . apply_func ( close , window , wait = 1 ) Get the maximum of the next period. close method \u00b6 Input array. close_above method \u00b6 FMAX . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 FMAX . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 FMAX . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 FMAX . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 FMAX . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 FMAX . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func . fmax property \u00b6 Output array. fmax_above method \u00b6 FMAX . fmax_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmax is above other . See combine_objs() . fmax_below method \u00b6 FMAX . fmax_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmax is below other . See combine_objs() . fmax_crossed_above method \u00b6 FMAX . fmax_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmax is crossed_above other . See combine_objs() . fmax_crossed_below method \u00b6 FMAX . fmax_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmax is crossed_below other . See combine_objs() . fmax_equal method \u00b6 FMAX . fmax_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmax is equal other . See combine_objs() . fmax_stats method \u00b6 FMAX . fmax_stats ( * args , ** kwargs ) Stats of fmax as generic. run class method \u00b6 FMAX . run ( close , window , short_name = 'fmax' , hide_params = None , hide_default = True , ** kwargs ) Run FMAX indicator. Inputs: close Parameters: window Outputs: fmax Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 FMAX . run_combs ( close , window , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple FMAX indicators using function comb_func . Inputs: close Parameters: window Outputs: fmax comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to FMAX.run() . window_list property \u00b6 List of window values. FMEAN class \u00b6 Look-ahead indicator based on future_mean_apply_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() apply_func method \u00b6 FMEAN . apply_func ( close , window , ewm , wait = 1 , adjust = False ) Get the mean of the next period. close method \u00b6 Input array. close_above method \u00b6 FMEAN . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 FMEAN . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 FMEAN . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 FMEAN . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 FMEAN . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 FMEAN . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func . ewm_list property \u00b6 List of ewm values. fmean property \u00b6 Output array. fmean_above method \u00b6 FMEAN . fmean_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmean is above other . See combine_objs() . fmean_below method \u00b6 FMEAN . fmean_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmean is below other . See combine_objs() . fmean_crossed_above method \u00b6 FMEAN . fmean_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmean is crossed_above other . See combine_objs() . fmean_crossed_below method \u00b6 FMEAN . fmean_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmean is crossed_below other . See combine_objs() . fmean_equal method \u00b6 FMEAN . fmean_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmean is equal other . See combine_objs() . fmean_stats method \u00b6 FMEAN . fmean_stats ( * args , ** kwargs ) Stats of fmean as generic. run class method \u00b6 FMEAN . run ( close , window , ewm = Default ( False ), short_name = 'fmean' , hide_params = None , hide_default = True , ** kwargs ) Run FMEAN indicator. Inputs: close Parameters: window , ewm Outputs: fmean Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 FMEAN . run_combs ( close , window , ewm = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple FMEAN indicators using function comb_func . Inputs: close Parameters: window , ewm Outputs: fmean comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to FMEAN.run() . window_list property \u00b6 List of window values. FMIN class \u00b6 Look-ahead indicator based on future_min_apply_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() apply_func method \u00b6 FMIN . apply_func ( close , window , wait = 1 ) Get the minimum of the next period. close method \u00b6 Input array. close_above method \u00b6 FMIN . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 FMIN . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 FMIN . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 FMIN . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 FMIN . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 FMIN . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func . fmin property \u00b6 Output array. fmin_above method \u00b6 FMIN . fmin_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmin is above other . See combine_objs() . fmin_below method \u00b6 FMIN . fmin_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmin is below other . See combine_objs() . fmin_crossed_above method \u00b6 FMIN . fmin_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmin is crossed_above other . See combine_objs() . fmin_crossed_below method \u00b6 FMIN . fmin_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmin is crossed_below other . See combine_objs() . fmin_equal method \u00b6 FMIN . fmin_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmin is equal other . See combine_objs() . fmin_stats method \u00b6 FMIN . fmin_stats ( * args , ** kwargs ) Stats of fmin as generic. run class method \u00b6 FMIN . run ( close , window , short_name = 'fmin' , hide_params = None , hide_default = True , ** kwargs ) Run FMIN indicator. Inputs: close Parameters: window Outputs: fmin Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 FMIN . run_combs ( close , window , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple FMIN indicators using function comb_func . Inputs: close Parameters: window Outputs: fmin comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to FMIN.run() . window_list property \u00b6 List of window values. FSTD class \u00b6 Look-ahead indicator based on future_std_apply_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() apply_func method \u00b6 FSTD . apply_func ( close , window , ewm , wait = 1 , adjust = False , ddof = 0 ) Get the standard deviation of the next period. close method \u00b6 Input array. close_above method \u00b6 FSTD . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 FSTD . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 FSTD . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 FSTD . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 FSTD . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 FSTD . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func . ewm_list property \u00b6 List of ewm values. fstd property \u00b6 Output array. fstd_above method \u00b6 FSTD . fstd_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fstd is above other . See combine_objs() . fstd_below method \u00b6 FSTD . fstd_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fstd is below other . See combine_objs() . fstd_crossed_above method \u00b6 FSTD . fstd_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fstd is crossed_above other . See combine_objs() . fstd_crossed_below method \u00b6 FSTD . fstd_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fstd is crossed_below other . See combine_objs() . fstd_equal method \u00b6 FSTD . fstd_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fstd is equal other . See combine_objs() . fstd_stats method \u00b6 FSTD . fstd_stats ( * args , ** kwargs ) Stats of fstd as generic. run class method \u00b6 FSTD . run ( close , window , ewm = Default ( False ), short_name = 'fstd' , hide_params = None , hide_default = True , ** kwargs ) Run FSTD indicator. Inputs: close Parameters: window , ewm Outputs: fstd Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 FSTD . run_combs ( close , window , ewm = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple FSTD indicators using function comb_func . Inputs: close Parameters: window , ewm Outputs: fstd comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to FSTD.run() . window_list property \u00b6 List of window values. LEXLB class \u00b6 Label generator based on local_extrema_apply_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.labels.generators._LEXLB apply_func method \u00b6 LEXLB . apply_func ( close , pos_th , neg_th , flex_2d = True ) Get array of local extrema denoted by 1 (peak) or -1 (trough), otherwise 0. Two adjacent peak and trough points should exceed the given threshold parameters. If any threshold is given element-wise, it will be applied per new/updated extremum. Inspired by https://www.mdpi.com/1099-4300/22/10/1162/pdf close method \u00b6 Input array. close_above method \u00b6 LEXLB . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 LEXLB . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 LEXLB . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 LEXLB . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 LEXLB . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 LEXLB . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func . labels property \u00b6 Output array. labels_above method \u00b6 LEXLB . labels_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is above other . See combine_objs() . labels_below method \u00b6 LEXLB . labels_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is below other . See combine_objs() . labels_crossed_above method \u00b6 LEXLB . labels_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_above other . See combine_objs() . labels_crossed_below method \u00b6 LEXLB . labels_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_below other . See combine_objs() . labels_equal method \u00b6 LEXLB . labels_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is equal other . See combine_objs() . labels_stats method \u00b6 LEXLB . labels_stats ( * args , ** kwargs ) Stats of labels as generic. neg_th_list property \u00b6 List of neg_th values. plot method \u00b6 LEXLB . plot ( column = None , ** kwargs ) Plot close and overlay it with the heatmap of labels . **kwargs are passed to GenericSRAccessor.overlay_with_heatmap() . pos_th_list property \u00b6 List of pos_th values. run class method \u00b6 LEXLB . run ( close , pos_th , neg_th , short_name = 'lexlb' , hide_params = None , hide_default = True , ** kwargs ) Run LEXLB indicator. Inputs: close Parameters: pos_th , neg_th Outputs: labels Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 LEXLB . run_combs ( close , pos_th , neg_th , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple LEXLB indicators using function comb_func . Inputs: close Parameters: pos_th , neg_th Outputs: labels comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to LEXLB.run() . MEANLB class \u00b6 Label generator based on mean_labels_apply_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.labels.generators._MEANLB apply_func method \u00b6 MEANLB . apply_func ( close , window , ewm , wait = 1 , adjust = False ) Get the percentage change from the current value to the average of the next period. close method \u00b6 Input array. close_above method \u00b6 MEANLB . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 MEANLB . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 MEANLB . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 MEANLB . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 MEANLB . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 MEANLB . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func . ewm_list property \u00b6 List of ewm values. labels property \u00b6 Output array. labels_above method \u00b6 MEANLB . labels_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is above other . See combine_objs() . labels_below method \u00b6 MEANLB . labels_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is below other . See combine_objs() . labels_crossed_above method \u00b6 MEANLB . labels_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_above other . See combine_objs() . labels_crossed_below method \u00b6 MEANLB . labels_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_below other . See combine_objs() . labels_equal method \u00b6 MEANLB . labels_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is equal other . See combine_objs() . labels_stats method \u00b6 MEANLB . labels_stats ( * args , ** kwargs ) Stats of labels as generic. plot method \u00b6 MEANLB . plot ( column = None , ** kwargs ) Plot close and overlay it with the heatmap of labels . **kwargs are passed to GenericSRAccessor.overlay_with_heatmap() . run class method \u00b6 MEANLB . run ( close , window , ewm = Default ( False ), short_name = 'meanlb' , hide_params = None , hide_default = True , ** kwargs ) Run MEANLB indicator. Inputs: close Parameters: window , ewm Outputs: labels Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 MEANLB . run_combs ( close , window , ewm = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple MEANLB indicators using function comb_func . Inputs: close Parameters: window , ewm Outputs: labels comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to MEANLB.run() . window_list property \u00b6 List of window values. TRENDLB class \u00b6 Label generator based on trend_labels_apply_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.labels.generators._TRENDLB apply_func method \u00b6 TRENDLB . apply_func ( close , pos_th , neg_th , mode , flex_2d = True ) Apply a trend labeling function based on TrendMode . close method \u00b6 Input array. close_above method \u00b6 TRENDLB . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 TRENDLB . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 TRENDLB . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 TRENDLB . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 TRENDLB . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 TRENDLB . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func . labels property \u00b6 Output array. labels_above method \u00b6 TRENDLB . labels_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is above other . See combine_objs() . labels_below method \u00b6 TRENDLB . labels_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is below other . See combine_objs() . labels_crossed_above method \u00b6 TRENDLB . labels_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_above other . See combine_objs() . labels_crossed_below method \u00b6 TRENDLB . labels_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_below other . See combine_objs() . labels_equal method \u00b6 TRENDLB . labels_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is equal other . See combine_objs() . labels_stats method \u00b6 TRENDLB . labels_stats ( * args , ** kwargs ) Stats of labels as generic. mode_list property \u00b6 List of mode values. neg_th_list property \u00b6 List of neg_th values. plot method \u00b6 TRENDLB . plot ( column = None , ** kwargs ) Plot close and overlay it with the heatmap of labels . **kwargs are passed to GenericSRAccessor.overlay_with_heatmap() . pos_th_list property \u00b6 List of pos_th values. run class method \u00b6 TRENDLB . run ( close , pos_th , neg_th , mode = Default ( 0 ), short_name = 'trendlb' , hide_params = None , hide_default = True , ** kwargs ) Run TRENDLB indicator. Inputs: close Parameters: pos_th , neg_th , mode Outputs: labels Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 TRENDLB . run_combs ( close , pos_th , neg_th , mode = Default ( 0 ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple TRENDLB indicators using function comb_func . Inputs: close Parameters: pos_th , neg_th , mode Outputs: labels comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to TRENDLB.run() .","title":"generators"},{"location":"api/labels/generators/#vectorbt.labels.generators","text":"Basic look-ahead indicators and label generators. You can access all the indicators either by vbt.* or vbt.labels.* .","title":"vectorbt.labels.generators"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB","text":"Label generator based on breakout_labels_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.labels.generators._BOLB","title":"BOLB"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.apply_func","text":"BOLB . apply_func ( close , window , pos_th , neg_th , wait = 1 , flex_2d = True ) For each value, return 1 if any value in the next period is greater than the positive threshold (in %), -1 if less than the negative threshold, and 0 otherwise. First hit wins.","title":"apply_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.close","text":"Input array.","title":"close"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.close_above","text":"BOLB . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.close_below","text":"BOLB . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.close_crossed_above","text":"BOLB . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.close_crossed_below","text":"BOLB . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.close_equal","text":"BOLB . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.close_stats","text":"BOLB . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.custom_func","text":"IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func .","title":"custom_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.labels","text":"Output array.","title":"labels"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.labels_above","text":"BOLB . labels_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is above other . See combine_objs() .","title":"labels_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.labels_below","text":"BOLB . labels_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is below other . See combine_objs() .","title":"labels_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.labels_crossed_above","text":"BOLB . labels_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_above other . See combine_objs() .","title":"labels_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.labels_crossed_below","text":"BOLB . labels_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_below other . See combine_objs() .","title":"labels_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.labels_equal","text":"BOLB . labels_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is equal other . See combine_objs() .","title":"labels_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.labels_stats","text":"BOLB . labels_stats ( * args , ** kwargs ) Stats of labels as generic.","title":"labels_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.neg_th_list","text":"List of neg_th values.","title":"neg_th_list"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.plot","text":"BOLB . plot ( column = None , ** kwargs ) Plot close and overlay it with the heatmap of labels . **kwargs are passed to GenericSRAccessor.overlay_with_heatmap() .","title":"plot()"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.pos_th_list","text":"List of pos_th values.","title":"pos_th_list"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.run","text":"BOLB . run ( close , window , pos_th = Default ( 0.0 ), neg_th = Default ( 0.0 ), short_name = 'bolb' , hide_params = None , hide_default = True , ** kwargs ) Run BOLB indicator. Inputs: close Parameters: window , pos_th , neg_th Outputs: labels Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.run_combs","text":"BOLB . run_combs ( close , window , pos_th = Default ( 0.0 ), neg_th = Default ( 0.0 ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple BOLB indicators using function comb_func . Inputs: close Parameters: window , pos_th , neg_th Outputs: labels comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to BOLB.run() .","title":"run_combs()"},{"location":"api/labels/generators/#vectorbt.labels.generators.BOLB.window_list","text":"List of window values.","title":"window_list"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB","text":"Label generator based on fixed_labels_apply_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.labels.generators._FIXLB","title":"FIXLB"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.apply_func","text":"FIXLB . apply_func ( close , n ) Get percentage change from the current value to a future value.","title":"apply_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.close","text":"Input array.","title":"close"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.close_above","text":"FIXLB . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.close_below","text":"FIXLB . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.close_crossed_above","text":"FIXLB . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.close_crossed_below","text":"FIXLB . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.close_equal","text":"FIXLB . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.close_stats","text":"FIXLB . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.custom_func","text":"IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func .","title":"custom_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.labels","text":"Output array.","title":"labels"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.labels_above","text":"FIXLB . labels_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is above other . See combine_objs() .","title":"labels_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.labels_below","text":"FIXLB . labels_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is below other . See combine_objs() .","title":"labels_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.labels_crossed_above","text":"FIXLB . labels_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_above other . See combine_objs() .","title":"labels_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.labels_crossed_below","text":"FIXLB . labels_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_below other . See combine_objs() .","title":"labels_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.labels_equal","text":"FIXLB . labels_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is equal other . See combine_objs() .","title":"labels_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.labels_stats","text":"FIXLB . labels_stats ( * args , ** kwargs ) Stats of labels as generic.","title":"labels_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.n_list","text":"List of n values.","title":"n_list"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.plot","text":"FIXLB . plot ( column = None , ** kwargs ) Plot close and overlay it with the heatmap of labels . **kwargs are passed to GenericSRAccessor.overlay_with_heatmap() .","title":"plot()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.run","text":"FIXLB . run ( close , n , short_name = 'fixlb' , hide_params = None , hide_default = True , ** kwargs ) Run FIXLB indicator. Inputs: close Parameters: n Outputs: labels Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FIXLB.run_combs","text":"FIXLB . run_combs ( close , n , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple FIXLB indicators using function comb_func . Inputs: close Parameters: n Outputs: labels comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to FIXLB.run() .","title":"run_combs()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX","text":"Look-ahead indicator based on future_max_apply_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"FMAX"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.apply_func","text":"FMAX . apply_func ( close , window , wait = 1 ) Get the maximum of the next period.","title":"apply_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.close","text":"Input array.","title":"close"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.close_above","text":"FMAX . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.close_below","text":"FMAX . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.close_crossed_above","text":"FMAX . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.close_crossed_below","text":"FMAX . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.close_equal","text":"FMAX . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.close_stats","text":"FMAX . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.custom_func","text":"IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func .","title":"custom_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.fmax","text":"Output array.","title":"fmax"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.fmax_above","text":"FMAX . fmax_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmax is above other . See combine_objs() .","title":"fmax_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.fmax_below","text":"FMAX . fmax_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmax is below other . See combine_objs() .","title":"fmax_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.fmax_crossed_above","text":"FMAX . fmax_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmax is crossed_above other . See combine_objs() .","title":"fmax_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.fmax_crossed_below","text":"FMAX . fmax_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmax is crossed_below other . See combine_objs() .","title":"fmax_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.fmax_equal","text":"FMAX . fmax_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmax is equal other . See combine_objs() .","title":"fmax_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.fmax_stats","text":"FMAX . fmax_stats ( * args , ** kwargs ) Stats of fmax as generic.","title":"fmax_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.run","text":"FMAX . run ( close , window , short_name = 'fmax' , hide_params = None , hide_default = True , ** kwargs ) Run FMAX indicator. Inputs: close Parameters: window Outputs: fmax Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.run_combs","text":"FMAX . run_combs ( close , window , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple FMAX indicators using function comb_func . Inputs: close Parameters: window Outputs: fmax comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to FMAX.run() .","title":"run_combs()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMAX.window_list","text":"List of window values.","title":"window_list"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN","text":"Look-ahead indicator based on future_mean_apply_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"FMEAN"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.apply_func","text":"FMEAN . apply_func ( close , window , ewm , wait = 1 , adjust = False ) Get the mean of the next period.","title":"apply_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.close","text":"Input array.","title":"close"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.close_above","text":"FMEAN . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.close_below","text":"FMEAN . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.close_crossed_above","text":"FMEAN . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.close_crossed_below","text":"FMEAN . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.close_equal","text":"FMEAN . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.close_stats","text":"FMEAN . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.custom_func","text":"IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func .","title":"custom_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.ewm_list","text":"List of ewm values.","title":"ewm_list"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.fmean","text":"Output array.","title":"fmean"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.fmean_above","text":"FMEAN . fmean_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmean is above other . See combine_objs() .","title":"fmean_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.fmean_below","text":"FMEAN . fmean_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmean is below other . See combine_objs() .","title":"fmean_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.fmean_crossed_above","text":"FMEAN . fmean_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmean is crossed_above other . See combine_objs() .","title":"fmean_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.fmean_crossed_below","text":"FMEAN . fmean_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmean is crossed_below other . See combine_objs() .","title":"fmean_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.fmean_equal","text":"FMEAN . fmean_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmean is equal other . See combine_objs() .","title":"fmean_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.fmean_stats","text":"FMEAN . fmean_stats ( * args , ** kwargs ) Stats of fmean as generic.","title":"fmean_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.run","text":"FMEAN . run ( close , window , ewm = Default ( False ), short_name = 'fmean' , hide_params = None , hide_default = True , ** kwargs ) Run FMEAN indicator. Inputs: close Parameters: window , ewm Outputs: fmean Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.run_combs","text":"FMEAN . run_combs ( close , window , ewm = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple FMEAN indicators using function comb_func . Inputs: close Parameters: window , ewm Outputs: fmean comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to FMEAN.run() .","title":"run_combs()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMEAN.window_list","text":"List of window values.","title":"window_list"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN","text":"Look-ahead indicator based on future_min_apply_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"FMIN"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.apply_func","text":"FMIN . apply_func ( close , window , wait = 1 ) Get the minimum of the next period.","title":"apply_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.close","text":"Input array.","title":"close"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.close_above","text":"FMIN . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.close_below","text":"FMIN . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.close_crossed_above","text":"FMIN . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.close_crossed_below","text":"FMIN . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.close_equal","text":"FMIN . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.close_stats","text":"FMIN . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.custom_func","text":"IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func .","title":"custom_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.fmin","text":"Output array.","title":"fmin"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.fmin_above","text":"FMIN . fmin_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmin is above other . See combine_objs() .","title":"fmin_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.fmin_below","text":"FMIN . fmin_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmin is below other . See combine_objs() .","title":"fmin_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.fmin_crossed_above","text":"FMIN . fmin_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmin is crossed_above other . See combine_objs() .","title":"fmin_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.fmin_crossed_below","text":"FMIN . fmin_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmin is crossed_below other . See combine_objs() .","title":"fmin_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.fmin_equal","text":"FMIN . fmin_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fmin is equal other . See combine_objs() .","title":"fmin_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.fmin_stats","text":"FMIN . fmin_stats ( * args , ** kwargs ) Stats of fmin as generic.","title":"fmin_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.run","text":"FMIN . run ( close , window , short_name = 'fmin' , hide_params = None , hide_default = True , ** kwargs ) Run FMIN indicator. Inputs: close Parameters: window Outputs: fmin Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.run_combs","text":"FMIN . run_combs ( close , window , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple FMIN indicators using function comb_func . Inputs: close Parameters: window Outputs: fmin comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to FMIN.run() .","title":"run_combs()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FMIN.window_list","text":"List of window values.","title":"window_list"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD","text":"Look-ahead indicator based on future_std_apply_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"FSTD"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.apply_func","text":"FSTD . apply_func ( close , window , ewm , wait = 1 , adjust = False , ddof = 0 ) Get the standard deviation of the next period.","title":"apply_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.close","text":"Input array.","title":"close"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.close_above","text":"FSTD . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.close_below","text":"FSTD . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.close_crossed_above","text":"FSTD . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.close_crossed_below","text":"FSTD . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.close_equal","text":"FSTD . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.close_stats","text":"FSTD . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.custom_func","text":"IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func .","title":"custom_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.ewm_list","text":"List of ewm values.","title":"ewm_list"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.fstd","text":"Output array.","title":"fstd"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.fstd_above","text":"FSTD . fstd_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fstd is above other . See combine_objs() .","title":"fstd_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.fstd_below","text":"FSTD . fstd_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fstd is below other . See combine_objs() .","title":"fstd_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.fstd_crossed_above","text":"FSTD . fstd_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fstd is crossed_above other . See combine_objs() .","title":"fstd_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.fstd_crossed_below","text":"FSTD . fstd_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fstd is crossed_below other . See combine_objs() .","title":"fstd_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.fstd_equal","text":"FSTD . fstd_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where fstd is equal other . See combine_objs() .","title":"fstd_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.fstd_stats","text":"FSTD . fstd_stats ( * args , ** kwargs ) Stats of fstd as generic.","title":"fstd_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.run","text":"FSTD . run ( close , window , ewm = Default ( False ), short_name = 'fstd' , hide_params = None , hide_default = True , ** kwargs ) Run FSTD indicator. Inputs: close Parameters: window , ewm Outputs: fstd Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.run_combs","text":"FSTD . run_combs ( close , window , ewm = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple FSTD indicators using function comb_func . Inputs: close Parameters: window , ewm Outputs: fstd comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to FSTD.run() .","title":"run_combs()"},{"location":"api/labels/generators/#vectorbt.labels.generators.FSTD.window_list","text":"List of window values.","title":"window_list"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB","text":"Label generator based on local_extrema_apply_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.labels.generators._LEXLB","title":"LEXLB"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.apply_func","text":"LEXLB . apply_func ( close , pos_th , neg_th , flex_2d = True ) Get array of local extrema denoted by 1 (peak) or -1 (trough), otherwise 0. Two adjacent peak and trough points should exceed the given threshold parameters. If any threshold is given element-wise, it will be applied per new/updated extremum. Inspired by https://www.mdpi.com/1099-4300/22/10/1162/pdf","title":"apply_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.close","text":"Input array.","title":"close"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.close_above","text":"LEXLB . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.close_below","text":"LEXLB . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.close_crossed_above","text":"LEXLB . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.close_crossed_below","text":"LEXLB . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.close_equal","text":"LEXLB . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.close_stats","text":"LEXLB . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.custom_func","text":"IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func .","title":"custom_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.labels","text":"Output array.","title":"labels"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.labels_above","text":"LEXLB . labels_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is above other . See combine_objs() .","title":"labels_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.labels_below","text":"LEXLB . labels_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is below other . See combine_objs() .","title":"labels_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.labels_crossed_above","text":"LEXLB . labels_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_above other . See combine_objs() .","title":"labels_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.labels_crossed_below","text":"LEXLB . labels_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_below other . See combine_objs() .","title":"labels_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.labels_equal","text":"LEXLB . labels_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is equal other . See combine_objs() .","title":"labels_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.labels_stats","text":"LEXLB . labels_stats ( * args , ** kwargs ) Stats of labels as generic.","title":"labels_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.neg_th_list","text":"List of neg_th values.","title":"neg_th_list"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.plot","text":"LEXLB . plot ( column = None , ** kwargs ) Plot close and overlay it with the heatmap of labels . **kwargs are passed to GenericSRAccessor.overlay_with_heatmap() .","title":"plot()"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.pos_th_list","text":"List of pos_th values.","title":"pos_th_list"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.run","text":"LEXLB . run ( close , pos_th , neg_th , short_name = 'lexlb' , hide_params = None , hide_default = True , ** kwargs ) Run LEXLB indicator. Inputs: close Parameters: pos_th , neg_th Outputs: labels Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/labels/generators/#vectorbt.labels.generators.LEXLB.run_combs","text":"LEXLB . run_combs ( close , pos_th , neg_th , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple LEXLB indicators using function comb_func . Inputs: close Parameters: pos_th , neg_th Outputs: labels comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to LEXLB.run() .","title":"run_combs()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB","text":"Label generator based on mean_labels_apply_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.labels.generators._MEANLB","title":"MEANLB"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.apply_func","text":"MEANLB . apply_func ( close , window , ewm , wait = 1 , adjust = False ) Get the percentage change from the current value to the average of the next period.","title":"apply_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.close","text":"Input array.","title":"close"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.close_above","text":"MEANLB . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.close_below","text":"MEANLB . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.close_crossed_above","text":"MEANLB . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.close_crossed_below","text":"MEANLB . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.close_equal","text":"MEANLB . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.close_stats","text":"MEANLB . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.custom_func","text":"IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func .","title":"custom_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.ewm_list","text":"List of ewm values.","title":"ewm_list"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.labels","text":"Output array.","title":"labels"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.labels_above","text":"MEANLB . labels_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is above other . See combine_objs() .","title":"labels_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.labels_below","text":"MEANLB . labels_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is below other . See combine_objs() .","title":"labels_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.labels_crossed_above","text":"MEANLB . labels_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_above other . See combine_objs() .","title":"labels_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.labels_crossed_below","text":"MEANLB . labels_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_below other . See combine_objs() .","title":"labels_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.labels_equal","text":"MEANLB . labels_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is equal other . See combine_objs() .","title":"labels_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.labels_stats","text":"MEANLB . labels_stats ( * args , ** kwargs ) Stats of labels as generic.","title":"labels_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.plot","text":"MEANLB . plot ( column = None , ** kwargs ) Plot close and overlay it with the heatmap of labels . **kwargs are passed to GenericSRAccessor.overlay_with_heatmap() .","title":"plot()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.run","text":"MEANLB . run ( close , window , ewm = Default ( False ), short_name = 'meanlb' , hide_params = None , hide_default = True , ** kwargs ) Run MEANLB indicator. Inputs: close Parameters: window , ewm Outputs: labels Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.run_combs","text":"MEANLB . run_combs ( close , window , ewm = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple MEANLB indicators using function comb_func . Inputs: close Parameters: window , ewm Outputs: labels comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to MEANLB.run() .","title":"run_combs()"},{"location":"api/labels/generators/#vectorbt.labels.generators.MEANLB.window_list","text":"List of window values.","title":"window_list"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB","text":"Label generator based on trend_labels_apply_nb() . Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.labels.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.labels.generators._TRENDLB","title":"TRENDLB"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.apply_func","text":"TRENDLB . apply_func ( close , pos_th , neg_th , mode , flex_2d = True ) Apply a trend labeling function based on TrendMode .","title":"apply_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.close","text":"Input array.","title":"close"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.close_above","text":"TRENDLB . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.close_below","text":"TRENDLB . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.close_crossed_above","text":"TRENDLB . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.close_crossed_below","text":"TRENDLB . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.close_equal","text":"TRENDLB . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.close_stats","text":"TRENDLB . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.custom_func","text":"IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func .","title":"custom_func()"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.labels","text":"Output array.","title":"labels"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.labels_above","text":"TRENDLB . labels_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is above other . See combine_objs() .","title":"labels_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.labels_below","text":"TRENDLB . labels_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is below other . See combine_objs() .","title":"labels_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.labels_crossed_above","text":"TRENDLB . labels_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_above other . See combine_objs() .","title":"labels_crossed_above()"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.labels_crossed_below","text":"TRENDLB . labels_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is crossed_below other . See combine_objs() .","title":"labels_crossed_below()"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.labels_equal","text":"TRENDLB . labels_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where labels is equal other . See combine_objs() .","title":"labels_equal()"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.labels_stats","text":"TRENDLB . labels_stats ( * args , ** kwargs ) Stats of labels as generic.","title":"labels_stats()"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.mode_list","text":"List of mode values.","title":"mode_list"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.neg_th_list","text":"List of neg_th values.","title":"neg_th_list"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.plot","text":"TRENDLB . plot ( column = None , ** kwargs ) Plot close and overlay it with the heatmap of labels . **kwargs are passed to GenericSRAccessor.overlay_with_heatmap() .","title":"plot()"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.pos_th_list","text":"List of pos_th values.","title":"pos_th_list"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.run","text":"TRENDLB . run ( close , pos_th , neg_th , mode = Default ( 0 ), short_name = 'trendlb' , hide_params = None , hide_default = True , ** kwargs ) Run TRENDLB indicator. Inputs: close Parameters: pos_th , neg_th , mode Outputs: labels Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/labels/generators/#vectorbt.labels.generators.TRENDLB.run_combs","text":"TRENDLB . run_combs ( close , pos_th , neg_th , mode = Default ( 0 ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple TRENDLB indicators using function comb_func . Inputs: close Parameters: pos_th , neg_th , mode Outputs: labels comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to TRENDLB.run() .","title":"run_combs()"},{"location":"api/labels/nb/","text":"nb module \u00b6 Numba-compiled functions. Provides an arsenal of Numba-compiled functions that are used by indicator classes. These only accept NumPy arrays and other Numba-compatible types. Note Set wait to 1 to exclude the current value from calculation of future values. Warning Do not attempt to use these functions for building features as they may introduce look-ahead bias to your model. bn_cont_sat_trend_labels_nb function \u00b6 bn_cont_sat_trend_labels_nb ( close , local_extrema , pos_th , neg_th , flex_2d = True ) Similar to bn_cont_trend_labels_nb() but sets each close value to 0 or 1 if the percentage change to the next extremum exceeds the threshold set for this range. bn_cont_trend_labels_nb function \u00b6 bn_cont_trend_labels_nb ( close , local_extrema ) Normalize each range between two extrema between 0 (will go up) and 1 (will go down). bn_trend_labels_nb function \u00b6 bn_trend_labels_nb ( close , local_extrema ) Return 0 for H-L and 1 for L-H. breakout_labels_nb function \u00b6 breakout_labels_nb ( close , window , pos_th , neg_th , wait = 1 , flex_2d = True ) For each value, return 1 if any value in the next period is greater than the positive threshold (in %), -1 if less than the negative threshold, and 0 otherwise. First hit wins. fixed_labels_apply_nb function \u00b6 fixed_labels_apply_nb ( close , n ) Get percentage change from the current value to a future value. future_max_apply_nb function \u00b6 future_max_apply_nb ( close , window , wait = 1 ) Get the maximum of the next period. future_mean_apply_nb function \u00b6 future_mean_apply_nb ( close , window , ewm , wait = 1 , adjust = False ) Get the mean of the next period. future_min_apply_nb function \u00b6 future_min_apply_nb ( close , window , wait = 1 ) Get the minimum of the next period. future_std_apply_nb function \u00b6 future_std_apply_nb ( close , window , ewm , wait = 1 , adjust = False , ddof = 0 ) Get the standard deviation of the next period. get_symmetric_neg_th_nb function \u00b6 get_symmetric_neg_th_nb ( pos_th ) Compute the negative return that is symmetric to a positive one. get_symmetric_pos_th_nb function \u00b6 get_symmetric_pos_th_nb ( neg_th ) Compute the positive return that is symmetric to a negative one. For example, 50% down requires 100% to go up to the initial level. local_extrema_apply_nb function \u00b6 local_extrema_apply_nb ( close , pos_th , neg_th , flex_2d = True ) Get array of local extrema denoted by 1 (peak) or -1 (trough), otherwise 0. Two adjacent peak and trough points should exceed the given threshold parameters. If any threshold is given element-wise, it will be applied per new/updated extremum. Inspired by https://www.mdpi.com/1099-4300/22/10/1162/pdf mean_labels_apply_nb function \u00b6 mean_labels_apply_nb ( close , window , ewm , wait = 1 , adjust = False ) Get the percentage change from the current value to the average of the next period. pct_trend_labels_nb function \u00b6 pct_trend_labels_nb ( close , local_extrema , normalize ) Compute the percentage change of the current value to the next extremum. trend_labels_apply_nb function \u00b6 trend_labels_apply_nb ( close , pos_th , neg_th , mode , flex_2d = True ) Apply a trend labeling function based on TrendMode .","title":"nb"},{"location":"api/labels/nb/#vectorbt.labels.nb","text":"Numba-compiled functions. Provides an arsenal of Numba-compiled functions that are used by indicator classes. These only accept NumPy arrays and other Numba-compatible types. Note Set wait to 1 to exclude the current value from calculation of future values. Warning Do not attempt to use these functions for building features as they may introduce look-ahead bias to your model.","title":"vectorbt.labels.nb"},{"location":"api/labels/nb/#vectorbt.labels.nb.bn_cont_sat_trend_labels_nb","text":"bn_cont_sat_trend_labels_nb ( close , local_extrema , pos_th , neg_th , flex_2d = True ) Similar to bn_cont_trend_labels_nb() but sets each close value to 0 or 1 if the percentage change to the next extremum exceeds the threshold set for this range.","title":"bn_cont_sat_trend_labels_nb()"},{"location":"api/labels/nb/#vectorbt.labels.nb.bn_cont_trend_labels_nb","text":"bn_cont_trend_labels_nb ( close , local_extrema ) Normalize each range between two extrema between 0 (will go up) and 1 (will go down).","title":"bn_cont_trend_labels_nb()"},{"location":"api/labels/nb/#vectorbt.labels.nb.bn_trend_labels_nb","text":"bn_trend_labels_nb ( close , local_extrema ) Return 0 for H-L and 1 for L-H.","title":"bn_trend_labels_nb()"},{"location":"api/labels/nb/#vectorbt.labels.nb.breakout_labels_nb","text":"breakout_labels_nb ( close , window , pos_th , neg_th , wait = 1 , flex_2d = True ) For each value, return 1 if any value in the next period is greater than the positive threshold (in %), -1 if less than the negative threshold, and 0 otherwise. First hit wins.","title":"breakout_labels_nb()"},{"location":"api/labels/nb/#vectorbt.labels.nb.fixed_labels_apply_nb","text":"fixed_labels_apply_nb ( close , n ) Get percentage change from the current value to a future value.","title":"fixed_labels_apply_nb()"},{"location":"api/labels/nb/#vectorbt.labels.nb.future_max_apply_nb","text":"future_max_apply_nb ( close , window , wait = 1 ) Get the maximum of the next period.","title":"future_max_apply_nb()"},{"location":"api/labels/nb/#vectorbt.labels.nb.future_mean_apply_nb","text":"future_mean_apply_nb ( close , window , ewm , wait = 1 , adjust = False ) Get the mean of the next period.","title":"future_mean_apply_nb()"},{"location":"api/labels/nb/#vectorbt.labels.nb.future_min_apply_nb","text":"future_min_apply_nb ( close , window , wait = 1 ) Get the minimum of the next period.","title":"future_min_apply_nb()"},{"location":"api/labels/nb/#vectorbt.labels.nb.future_std_apply_nb","text":"future_std_apply_nb ( close , window , ewm , wait = 1 , adjust = False , ddof = 0 ) Get the standard deviation of the next period.","title":"future_std_apply_nb()"},{"location":"api/labels/nb/#vectorbt.labels.nb.get_symmetric_neg_th_nb","text":"get_symmetric_neg_th_nb ( pos_th ) Compute the negative return that is symmetric to a positive one.","title":"get_symmetric_neg_th_nb()"},{"location":"api/labels/nb/#vectorbt.labels.nb.get_symmetric_pos_th_nb","text":"get_symmetric_pos_th_nb ( neg_th ) Compute the positive return that is symmetric to a negative one. For example, 50% down requires 100% to go up to the initial level.","title":"get_symmetric_pos_th_nb()"},{"location":"api/labels/nb/#vectorbt.labels.nb.local_extrema_apply_nb","text":"local_extrema_apply_nb ( close , pos_th , neg_th , flex_2d = True ) Get array of local extrema denoted by 1 (peak) or -1 (trough), otherwise 0. Two adjacent peak and trough points should exceed the given threshold parameters. If any threshold is given element-wise, it will be applied per new/updated extremum. Inspired by https://www.mdpi.com/1099-4300/22/10/1162/pdf","title":"local_extrema_apply_nb()"},{"location":"api/labels/nb/#vectorbt.labels.nb.mean_labels_apply_nb","text":"mean_labels_apply_nb ( close , window , ewm , wait = 1 , adjust = False ) Get the percentage change from the current value to the average of the next period.","title":"mean_labels_apply_nb()"},{"location":"api/labels/nb/#vectorbt.labels.nb.pct_trend_labels_nb","text":"pct_trend_labels_nb ( close , local_extrema , normalize ) Compute the percentage change of the current value to the next extremum.","title":"pct_trend_labels_nb()"},{"location":"api/labels/nb/#vectorbt.labels.nb.trend_labels_apply_nb","text":"trend_labels_apply_nb ( close , pos_th , neg_th , mode , flex_2d = True ) Apply a trend labeling function based on TrendMode .","title":"trend_labels_apply_nb()"},{"location":"api/messaging/","text":"messaging package \u00b6 Modules for messaging. Sub-modules \u00b6 vectorbt.messaging.telegram","title":"messaging"},{"location":"api/messaging/#vectorbt.messaging","text":"Modules for messaging.","title":"vectorbt.messaging"},{"location":"api/messaging/#sub-modules","text":"vectorbt.messaging.telegram","title":"Sub-modules"},{"location":"api/messaging/telegram/","text":"telegram module \u00b6 Messaging using python-telegram-bot . self_decorator function \u00b6 self_decorator ( func ) Pass bot object to func command. send_action function \u00b6 send_action ( action ) Sends action while processing func command. Suitable only for bound callbacks taking arguments self , update , context and optionally other. LogHandler class \u00b6 Handler to log user updates. Superclasses abc.ABC telegram.ext.handler.Handler typing.Generic check_update method \u00b6 LogHandler . check_update ( update ) This method is called to determine if an update should be handled by this handler instance. It should always be overridden. Note Custom updates types can be handled by the dispatcher. Therefore, an implementation of this method should always check the type of :attr: update . Args update (:obj: str | :class: telegram.Update ): The update to be tested. Returns Either :obj: None or :obj: False if the update should not be handled. Otherwise an object that will be passed to :meth: handle_update and :meth: collect_additional_context when the update gets handled. TelegramBot class \u00b6 Telegram bot. See Extensions \u2013 Your first Bot . **kwargs are passed to telegram.ext.updater.Updater and override settings under messaging.telegram in settings . Usage Let's extend TelegramBot to track cryptocurrency prices: >>> from telegram.ext import CommandHandler >>> import ccxt >>> import logging >>> import vectorbt as vbt >>> logging . basicConfig ( level = logging . INFO ) # enable logging >>> class MyTelegramBot ( vbt . TelegramBot ): ... @property ... def custom_handlers ( self ): ... return ( CommandHandler ( 'get' , self . get ),) ... ... @property ... def help_message ( self ): ... return \"Type /get [symbol] [exchange id (optional)] to get the latest price.\" ... ... def get ( self , update , context ): ... chat_id = update . effective_chat . id ... ... if len ( context . args ) == 1 : ... symbol = context . args [ 0 ] ... exchange = 'binance' ... elif len ( context . args ) == 2 : ... symbol = context . args [ 0 ] ... exchange = context . args [ 1 ] ... else : ... self . send_message ( chat_id , \"This command requires symbol and optionally exchange id.\" ) ... return ... try : ... ticker = getattr ( ccxt , exchange )() . fetchTicker ( symbol ) ... except Exception as e : ... self . send_message ( chat_id , str ( e )) ... return ... self . send_message ( chat_id , str ( ticker [ 'last' ])) >>> bot = MyTelegramBot ( token = 'YOUR_TOKEN' ) >>> bot . start () INFO:vectorbt.utils.messaging:Initializing bot INFO:vectorbt.utils.messaging:Loaded chat ids [447924619] INFO:vectorbt.utils.messaging:Running bot vectorbt_bot INFO:apscheduler.scheduler:Scheduler started INFO:vectorbt.utils.messaging:447924619 - Bot: \"I'm back online!\" INFO:vectorbt.utils.messaging:447924619 - User: \"/start\" INFO:vectorbt.utils.messaging:447924619 - Bot: \"Hello!\" INFO:vectorbt.utils.messaging:447924619 - User: \"/help\" INFO:vectorbt.utils.messaging:447924619 - Bot: \"Type /get [symbol] [exchange id (optional)] to get the latest price.\" INFO:vectorbt.utils.messaging:447924619 - User: \"/get BTC/USDT\" INFO:vectorbt.utils.messaging:447924619 - Bot: \"55530.55\" INFO:vectorbt.utils.messaging:447924619 - User: \"/get BTC/USD bitmex\" INFO:vectorbt.utils.messaging:447924619 - Bot: \"55509.0\" INFO:telegram.ext.updater:Received signal 2 (SIGINT), stopping... INFO:apscheduler.scheduler:Scheduler has been shut down Superclasses Configured Documented Pickleable Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() chat_ids property \u00b6 Chat ids that ever interacted with this bot. A chat id is added upon receiving the \"/start\" command. chat_migration_callback method \u00b6 TelegramBot . chat_migration_callback ( update , context ) Chat migration callback. custom_handlers property \u00b6 Custom handlers to add. Override to add custom handlers. Order counts. dispatcher property \u00b6 Dispatcher. error_callback method \u00b6 TelegramBot . error_callback ( update , context , * args ) Error callback. help_callback method \u00b6 TelegramBot . help_callback ( update , context ) Help command callback. help_message property \u00b6 Message to be sent upon \"/help\" command. Override to define your own message. log_handler property \u00b6 Log handler. running property \u00b6 Whether the bot is running. send method \u00b6 TelegramBot . send ( kind , chat_id , * args , log_msg = None , ** kwargs ) Send message of any kind to chat_id . send_giphy method \u00b6 TelegramBot . send_giphy ( chat_id , text , * args , giphy_kwargs = None , ** kwargs ) Send GIPHY from text to chat_id . send_giphy_to_all method \u00b6 TelegramBot . send_giphy_to_all ( text , * args , giphy_kwargs = None , ** kwargs ) Send GIPHY from text to all in TelegramBot.chat_ids . send_message method \u00b6 TelegramBot . send_message ( chat_id , text , * args , ** kwargs ) Send text message to chat_id . send_message_to_all method \u00b6 TelegramBot . send_message_to_all ( text , * args , ** kwargs ) Send text message to all in TelegramBot.chat_ids . send_to_all method \u00b6 TelegramBot . send_to_all ( kind , * args , ** kwargs ) Send message of any kind to all in TelegramBot.chat_ids . start method \u00b6 TelegramBot . start ( in_background = False , ** kwargs ) Start the bot. **kwargs are passed to telegram.ext.updater.Updater.start_polling and override settings under messaging.telegram in settings . start_callback method \u00b6 TelegramBot . start_callback ( update , context ) Start command callback. start_message property \u00b6 Message to be sent upon \"/start\" command. Override to define your own message. started_callback method \u00b6 TelegramBot . started_callback () Callback once the bot has been started. Override to execute custom commands upon starting the bot. stop method \u00b6 TelegramBot . stop () Stop the bot. unknown_callback method \u00b6 TelegramBot . unknown_callback ( update , context ) Unknown command callback. updater property \u00b6 Updater.","title":"telegram"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram","text":"Messaging using python-telegram-bot .","title":"vectorbt.messaging.telegram"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.self_decorator","text":"self_decorator ( func ) Pass bot object to func command.","title":"self_decorator()"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.send_action","text":"send_action ( action ) Sends action while processing func command. Suitable only for bound callbacks taking arguments self , update , context and optionally other.","title":"send_action()"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.LogHandler","text":"Handler to log user updates. Superclasses abc.ABC telegram.ext.handler.Handler typing.Generic","title":"LogHandler"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.LogHandler.check_update","text":"LogHandler . check_update ( update ) This method is called to determine if an update should be handled by this handler instance. It should always be overridden. Note Custom updates types can be handled by the dispatcher. Therefore, an implementation of this method should always check the type of :attr: update . Args update (:obj: str | :class: telegram.Update ): The update to be tested. Returns Either :obj: None or :obj: False if the update should not be handled. Otherwise an object that will be passed to :meth: handle_update and :meth: collect_additional_context when the update gets handled.","title":"check_update()"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot","text":"Telegram bot. See Extensions \u2013 Your first Bot . **kwargs are passed to telegram.ext.updater.Updater and override settings under messaging.telegram in settings . Usage Let's extend TelegramBot to track cryptocurrency prices: >>> from telegram.ext import CommandHandler >>> import ccxt >>> import logging >>> import vectorbt as vbt >>> logging . basicConfig ( level = logging . INFO ) # enable logging >>> class MyTelegramBot ( vbt . TelegramBot ): ... @property ... def custom_handlers ( self ): ... return ( CommandHandler ( 'get' , self . get ),) ... ... @property ... def help_message ( self ): ... return \"Type /get [symbol] [exchange id (optional)] to get the latest price.\" ... ... def get ( self , update , context ): ... chat_id = update . effective_chat . id ... ... if len ( context . args ) == 1 : ... symbol = context . args [ 0 ] ... exchange = 'binance' ... elif len ( context . args ) == 2 : ... symbol = context . args [ 0 ] ... exchange = context . args [ 1 ] ... else : ... self . send_message ( chat_id , \"This command requires symbol and optionally exchange id.\" ) ... return ... try : ... ticker = getattr ( ccxt , exchange )() . fetchTicker ( symbol ) ... except Exception as e : ... self . send_message ( chat_id , str ( e )) ... return ... self . send_message ( chat_id , str ( ticker [ 'last' ])) >>> bot = MyTelegramBot ( token = 'YOUR_TOKEN' ) >>> bot . start () INFO:vectorbt.utils.messaging:Initializing bot INFO:vectorbt.utils.messaging:Loaded chat ids [447924619] INFO:vectorbt.utils.messaging:Running bot vectorbt_bot INFO:apscheduler.scheduler:Scheduler started INFO:vectorbt.utils.messaging:447924619 - Bot: \"I'm back online!\" INFO:vectorbt.utils.messaging:447924619 - User: \"/start\" INFO:vectorbt.utils.messaging:447924619 - Bot: \"Hello!\" INFO:vectorbt.utils.messaging:447924619 - User: \"/help\" INFO:vectorbt.utils.messaging:447924619 - Bot: \"Type /get [symbol] [exchange id (optional)] to get the latest price.\" INFO:vectorbt.utils.messaging:447924619 - User: \"/get BTC/USDT\" INFO:vectorbt.utils.messaging:447924619 - Bot: \"55530.55\" INFO:vectorbt.utils.messaging:447924619 - User: \"/get BTC/USD bitmex\" INFO:vectorbt.utils.messaging:447924619 - Bot: \"55509.0\" INFO:telegram.ext.updater:Received signal 2 (SIGINT), stopping... INFO:apscheduler.scheduler:Scheduler has been shut down Superclasses Configured Documented Pickleable Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save()","title":"TelegramBot"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.chat_ids","text":"Chat ids that ever interacted with this bot. A chat id is added upon receiving the \"/start\" command.","title":"chat_ids"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.chat_migration_callback","text":"TelegramBot . chat_migration_callback ( update , context ) Chat migration callback.","title":"chat_migration_callback()"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.custom_handlers","text":"Custom handlers to add. Override to add custom handlers. Order counts.","title":"custom_handlers"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.dispatcher","text":"Dispatcher.","title":"dispatcher"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.error_callback","text":"TelegramBot . error_callback ( update , context , * args ) Error callback.","title":"error_callback()"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.help_callback","text":"TelegramBot . help_callback ( update , context ) Help command callback.","title":"help_callback()"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.help_message","text":"Message to be sent upon \"/help\" command. Override to define your own message.","title":"help_message"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.log_handler","text":"Log handler.","title":"log_handler"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.running","text":"Whether the bot is running.","title":"running"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.send","text":"TelegramBot . send ( kind , chat_id , * args , log_msg = None , ** kwargs ) Send message of any kind to chat_id .","title":"send()"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.send_giphy","text":"TelegramBot . send_giphy ( chat_id , text , * args , giphy_kwargs = None , ** kwargs ) Send GIPHY from text to chat_id .","title":"send_giphy()"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.send_giphy_to_all","text":"TelegramBot . send_giphy_to_all ( text , * args , giphy_kwargs = None , ** kwargs ) Send GIPHY from text to all in TelegramBot.chat_ids .","title":"send_giphy_to_all()"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.send_message","text":"TelegramBot . send_message ( chat_id , text , * args , ** kwargs ) Send text message to chat_id .","title":"send_message()"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.send_message_to_all","text":"TelegramBot . send_message_to_all ( text , * args , ** kwargs ) Send text message to all in TelegramBot.chat_ids .","title":"send_message_to_all()"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.send_to_all","text":"TelegramBot . send_to_all ( kind , * args , ** kwargs ) Send message of any kind to all in TelegramBot.chat_ids .","title":"send_to_all()"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.start","text":"TelegramBot . start ( in_background = False , ** kwargs ) Start the bot. **kwargs are passed to telegram.ext.updater.Updater.start_polling and override settings under messaging.telegram in settings .","title":"start()"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.start_callback","text":"TelegramBot . start_callback ( update , context ) Start command callback.","title":"start_callback()"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.start_message","text":"Message to be sent upon \"/start\" command. Override to define your own message.","title":"start_message"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.started_callback","text":"TelegramBot . started_callback () Callback once the bot has been started. Override to execute custom commands upon starting the bot.","title":"started_callback()"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.stop","text":"TelegramBot . stop () Stop the bot.","title":"stop()"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.unknown_callback","text":"TelegramBot . unknown_callback ( update , context ) Unknown command callback.","title":"unknown_callback()"},{"location":"api/messaging/telegram/#vectorbt.messaging.telegram.TelegramBot.updater","text":"Updater.","title":"updater"},{"location":"api/portfolio/","text":"portfolio package \u00b6 Modules for working with portfolios. Sub-modules \u00b6 vectorbt.portfolio.base vectorbt.portfolio.decorators vectorbt.portfolio.enums vectorbt.portfolio.logs vectorbt.portfolio.nb vectorbt.portfolio.orders vectorbt.portfolio.trades","title":"portfolio"},{"location":"api/portfolio/#vectorbt.portfolio","text":"Modules for working with portfolios.","title":"vectorbt.portfolio"},{"location":"api/portfolio/#sub-modules","text":"vectorbt.portfolio.base vectorbt.portfolio.decorators vectorbt.portfolio.enums vectorbt.portfolio.logs vectorbt.portfolio.nb vectorbt.portfolio.orders vectorbt.portfolio.trades","title":"Sub-modules"},{"location":"api/portfolio/base/","text":"base module \u00b6 Base class for modeling portfolio and measuring its performance. Provides the class Portfolio for modeling portfolio performance and calculating various risk and performance metrics. It uses Numba-compiled functions from vectorbt.portfolio.nb for most computations and record classes based on Records for evaluating events such as orders, logs, trades, positions, and drawdowns. The job of the Portfolio class is to create a series of positions allocated against a cash component, produce an equity curve, incorporate basic transaction costs and produce a set of statistics about its performance. In particular, it outputs position/profit metrics and drawdown information. Run for the examples below: >>> import numpy as np >>> import pandas as pd >>> from datetime import datetime >>> import talib >>> from numba import njit >>> import vectorbt as vbt >>> from vectorbt.utils.colors import adjust_opacity >>> from vectorbt.utils.enum_ import map_enum_fields >>> from vectorbt.base.reshape_fns import broadcast , flex_select_auto_nb , to_2d_array >>> from vectorbt.portfolio.enums import SizeType , Direction , NoOrder , OrderStatus , OrderSide >>> from vectorbt.portfolio import nb Workflow \u00b6 Portfolio class does quite a few things to simulate your strategy. Preparation phase (in the particular class method): Receives a set of inputs, such as signal arrays and other parameters Resolves parameter defaults by searching for them in the global settings Brings input arrays to a single shape Does some basic validation of inputs and converts Pandas objects to NumPy arrays Passes everything to a Numba-compiled simulation function Simulation phase (in the particular simulation function using Numba): The simulation function traverses the broadcasted shape element by element, row by row (time dimension), column by column (asset dimension) For each asset and timestamp (= element): Gets all available information related to this element and executes the logic Generates an order or skips the element altogether If an order has been issued, processes the order and fills/ignores/rejects it If the order has been filled, registers the result by appending it to the order records Updates the current state such as the cash and asset balances Construction phase (in the particular class method): Receives the returned order records and initializes a new Portfolio object Analysis phase (in the Portfolio object) Offers a broad range of risk & performance metrics based on order records Simulation modes \u00b6 There are three main simulation modes. From orders \u00b6 Portfolio.from_orders() is the most straightforward and the fastest out of all simulation modes. An order is a simple instruction that contains size, price, fees, and other information (see Order for details about what information a typical order requires). Instead of creating a Order tuple for each asset and timestamp (which may waste a lot of memory) and appending it to a (potentially huge) list for processing, Portfolio.from_orders() takes each of those information pieces as an array, broadcasts them against each other, and creates a Order tuple out of each element for us. Thanks to broadcasting, we can pass any of the information as a 2-dim array, as a 1-dim array per column or row, and as a constant. And we don't even need to provide every piece of information - vectorbt fills the missing data with default constants, without wasting memory. Here's an example: >>> size = pd . Series ([ 1 , - 1 , 1 , - 1 ]) # per row >>> price = pd . DataFrame ({ 'a' : [ 1 , 2 , 3 , 4 ], 'b' : [ 4 , 3 , 2 , 1 ]}) # per element >>> direction = [ 'longonly' , 'shortonly' ] # per column >>> fees = 0.01 # per frame >>> pf = vbt . Portfolio . from_orders ( price , size , direction = direction , fees = fees ) >>> pf . orders . records_readable Order Id Column Timestamp Size Price Fees Side 0 0 a 0 1.0 1.0 0.01 Buy 1 1 a 1 1.0 2.0 0.02 Sell 2 2 a 2 1.0 3.0 0.03 Buy 3 3 a 3 1.0 4.0 0.04 Sell 4 4 b 0 1.0 4.0 0.04 Sell 5 5 b 1 1.0 3.0 0.03 Buy 6 6 b 2 1.0 2.0 0.02 Sell 7 7 b 3 1.0 1.0 0.01 Buy This method is particularly useful in situations where you don't need any further logic apart from filling orders at predefined timestamps. If you want to issue orders depending upon the previous performance, the current state, or other custom conditions, head over to Portfolio.from_signals() or Portfolio.from_order_func() . From signals \u00b6 Portfolio.from_signals() is centered around signals. It adds an abstraction layer on top of Portfolio.from_orders() to automate some signaling processes. For example, by default, it won't let us execute another entry signal if we are already in the position. It also implements stop loss and take profit orders for exiting positions. Nevertheless, this method behaves similarly to Portfolio.from_orders() and accepts most of its arguments; in fact, by setting accumulate=True , it behaves quite similarly to Portfolio.from_orders() . Let's replicate the example above using signals: >>> entries = pd . Series ([ True , False , True , False ]) >>> exits = pd . Series ([ False , True , False , True ]) >>> pf = vbt . Portfolio . from_signals ( price , entries , exits , size = 1 , direction = direction , fees = fees ) >>> pf . orders . records_readable Order Id Column Timestamp Size Price Fees Side 0 0 a 0 1.0 1.0 0.01 Buy 1 1 a 1 1.0 2.0 0.02 Sell 2 2 a 2 1.0 3.0 0.03 Buy 3 3 a 3 1.0 4.0 0.04 Sell 4 4 b 0 1.0 4.0 0.04 Sell 5 5 b 1 1.0 3.0 0.03 Buy 6 6 b 2 1.0 2.0 0.02 Sell 7 7 b 3 1.0 1.0 0.01 Buy In a nutshell: this method automates some procedures that otherwise would be only possible by using Portfolio.from_order_func() while following the same broadcasting principles as Portfolio.from_orders() - the best of both worlds, given you can express your strategy as a sequence of signals. But as soon as your strategy requires any signal to depend upon more complex conditions or to generate multiple orders at once, it's best to run your custom signaling logic using Portfolio.from_order_func() . From order function \u00b6 Portfolio.from_order_func() is the most powerful form of simulation. Instead of pulling information from predefined arrays, it lets us define an arbitrary logic through callbacks. There are multiple kinds of callbacks, each called at some point while the simulation function traverses the shape. For example, apart from the main callback that returns an order ( order_func_nb ), there is a callback that does preprocessing on the entire group of columns at once. For more details on the general procedure and the callback zoo, see simulate_nb() . Let's replicate our example using an order function: >>> @njit >>> def order_func_nb ( c , size , direction , fees ): ... return nb . order_nb ( ... price = c . close [ c . i , c . col ], ... size = size [ c . i ], ... direction = direction [ c . col ], ... fees = fees ... ) >>> direction_num = map_enum_fields ( direction , Direction ) >>> pf = vbt . Portfolio . from_order_func ( ... price , ... order_func_nb , ... np . asarray ( size ), np . asarray ( direction_num ), fees ... ) >>> pf . orders . records_readable Order Id Column Timestamp Size Price Fees Side 0 0 a 0 1.0 1.0 0.01 Buy 1 1 a 1 1.0 2.0 0.02 Sell 2 2 a 2 1.0 3.0 0.03 Buy 3 3 a 3 1.0 4.0 0.04 Sell 4 4 b 0 1.0 4.0 0.04 Sell 5 5 b 1 1.0 3.0 0.03 Buy 6 6 b 2 1.0 2.0 0.02 Sell 7 7 b 3 1.0 1.0 0.01 Buy There is an even more flexible version available - flex_simulate_nb() (activated by passing flexible=True to Portfolio.from_order_func() ) - that allows creating multiple orders per symbol and bar. This method has many advantages: Realistic simulation as it follows the event-driven approach - less risk of exposure to the look-ahead bias Provides a lot of useful information during the runtime, such as the current position's PnL Enables putting all logic including custom indicators into a single place, and running it as the data comes in, in a memory-friendly manner But there are drawbacks too: Doesn't broadcast arrays - needs to be done by the user prior to the execution Requires at least a basic knowledge of NumPy and Numba Requires at least an intermediate knowledge of both to optimize for efficiency Example \u00b6 To showcase the features of Portfolio , run the following example: it checks candlestick data of 6 major cryptocurrencies in 2020 against every single pattern found in TA-Lib, and translates them into orders. >>> # Fetch price history >>> symbols = [ 'BTC-USD' , 'ETH-USD' , 'XRP-USD' , 'BNB-USD' , 'BCH-USD' , 'LTC-USD' ] >>> start = '2020-01-01 UTC' # crypto is UTC >>> end = '2020-09-01 UTC' >>> # OHLCV by column >>> ohlcv = vbt . YFData . download ( symbols , start = start , end = end ) . concat () >>> ohlcv [ 'Open' ] symbol BTC-USD ETH-USD XRP-USD BNB-USD \\ Date 2020-01-01 00:00:00+00:00 7194.892090 129.630661 0.192912 13.730962 2020-01-02 00:00:00+00:00 7202.551270 130.820038 0.192708 13.698126 2020-01-03 00:00:00+00:00 6984.428711 127.411263 0.187948 13.035329 ... ... ... ... ... 2020-08-30 00:00:00+00:00 11508.713867 399.616699 0.274568 23.009060 2020-08-31 00:00:00+00:00 11713.306641 428.509003 0.283065 23.647858 2020-09-01 00:00:00+00:00 11679.316406 434.874451 0.281612 23.185047 symbol BCH-USD LTC-USD Date 2020-01-01 00:00:00+00:00 204.671295 41.326534 2020-01-02 00:00:00+00:00 204.354538 42.018085 2020-01-03 00:00:00+00:00 196.007690 39.863129 ... ... ... 2020-08-30 00:00:00+00:00 268.842865 57.207737 2020-08-31 00:00:00+00:00 279.280426 62.844059 2020-09-01 00:00:00+00:00 274.480865 61.105076 [244 rows x 6 columns] >>> # Run every single pattern recognition indicator and combine the results >>> result = pd . DataFrame . vbt . empty_like ( ohlcv [ 'Open' ], fill_value = 0. ) >>> for pattern in talib . get_function_groups ()[ 'Pattern Recognition' ]: ... PRecognizer = vbt . IndicatorFactory . from_talib ( pattern ) ... pr = PRecognizer . run ( ohlcv [ 'Open' ], ohlcv [ 'High' ], ohlcv [ 'Low' ], ohlcv [ 'Close' ]) ... result = result + pr . integer >>> # Don't look into the future >>> result = result . vbt . fshift ( 1 ) >>> # Treat each number as order value in USD >>> size = result / ohlcv [ 'Open' ] >>> # Simulate portfolio >>> pf = vbt . Portfolio . from_orders ( ... ohlcv [ 'Close' ], size , price = ohlcv [ 'Open' ], ... init_cash = 'autoalign' , fees = 0.001 , slippage = 0.001 ) >>> # Visualize portfolio value >>> pf . value () . vbt . plot () Broadcasting \u00b6 Portfolio is very flexible towards inputs: Accepts both Series and DataFrames as inputs Broadcasts inputs to the same shape using vectorbt's own broadcasting rules Many inputs (such as fees ) can be passed as a single value, value per column/row, or as a matrix Implements flexible indexing wherever possible to save memory Flexible indexing \u00b6 Instead of expensive broadcasting, most methods keep the original shape and do indexing in a smart way. A nice feature of this is that it has almost no memory footprint and can broadcast in any direction indefinitely. For example, let's broadcast three inputs and select the last element using both approaches: >>> # Classic way >>> a = np . array ([ 1 , 2 , 3 ]) >>> b = np . array ([[ 4 ], [ 5 ], [ 6 ]]) >>> c = np . array ( 10 ) >>> a_ , b_ , c_ = broadcast ( a , b , c ) >>> a_ array([[1, 2, 3], [1, 2, 3], [1, 2, 3]]) >>> a_ [ 2 , 2 ] 3 >>> b_ array([[4, 4, 4], [5, 5, 5], [6, 6, 6]]) >>> b_ [ 2 , 2 ] 6 >>> c_ array([[10, 10, 10], [10, 10, 10], [10, 10, 10]]) >>> c_ [ 2 , 2 ] 10 >>> # Flexible indexing being done during simulation >>> flex_select_auto_nb ( a , 2 , 2 ) 3 >>> flex_select_auto_nb ( b , 2 , 2 ) 6 >>> flex_select_auto_nb ( c , 2 , 2 ) 10 Defaults \u00b6 If you look at the arguments of each class method, you will notice that most of them default to None. None has a special meaning in vectorbt: it's a command to pull the default value from the global settings config - settings . The branch for the Portfolio can be found under the key 'portfolio'. For example, the default size used in Portfolio.from_signals() and Portfolio.from_orders() is np.inf : >>> vbt . settings . portfolio [ 'size' ] inf Grouping \u00b6 One of the key features of Portfolio is the ability to group columns. Groups can be specified by group_by , which can be anything from positions or names of column levels, to a NumPy array with actual groups. Groups can be formed to share capital between columns (make sure to pass cash_sharing=True ) or to compute metrics for a combined portfolio of multiple independent columns. For example, let's divide our portfolio into two groups sharing the same cash balance: >>> # Simulate combined portfolio >>> group_by = pd . Index ([ ... 'first' , 'first' , 'first' , ... 'second' , 'second' , 'second' ... ], name = 'group' ) >>> comb_pf = vbt . Portfolio . from_orders ( ... ohlcv [ 'Close' ], size , price = ohlcv [ 'Open' ], ... init_cash = 'autoalign' , fees = 0.001 , slippage = 0.001 , ... group_by = group_by , cash_sharing = True ) >>> # Get total profit per group >>> comb_pf . total_profit () group first 26221.571200 second 10141.952674 Name: total_profit, dtype: float64 Not only can we analyze each group, but also each column in the group: >>> # Get total profit per column >>> comb_pf . total_profit ( group_by = False ) symbol BTC-USD 5792.120252 ETH-USD 16380.039692 XRP-USD 4049.411256 BNB-USD 6081.253551 BCH-USD 400.573418 LTC-USD 3660.125705 Name: total_profit, dtype: float64 In the same way, we can introduce new grouping to the method itself: >>> # Get total profit per group >>> pf . total_profit ( group_by = group_by ) group first 26221.571200 second 10141.952674 Name: total_profit, dtype: float64 Note If cash sharing is enabled, grouping can be disabled but cannot be modified. Indexing \u00b6 Like any other class subclassing Wrapping , we can do pandas indexing on a Portfolio instance, which forwards indexing operation to each object with columns: >>> pf [ 'BTC-USD' ] <vectorbt.portfolio.base.Portfolio at 0x7fac7517ac88> >>> pf [ 'BTC-USD' ] . total_profit () 5792.120252189081 Combined portfolio is indexed by group: >>> comb_pf [ 'first' ] <vectorbt.portfolio.base.Portfolio at 0x7fac5756b828> >>> comb_pf [ 'first' ] . total_profit () 26221.57120014546 Note Changing index (time axis) is not supported. The object should be treated as a Series rather than a DataFrame; for example, use pf.iloc[0] instead of pf.iloc[:, 0] . Indexing behavior depends solely upon ArrayWrapper . For example, if group_select is enabled indexing will be performed on groups, otherwise on single columns. You can pass wrapper arguments with wrapper_kwargs . Logging \u00b6 To collect more information on how a specific order was processed or to be able to track the whole simulation from the beginning to the end, we can turn on logging: >>> # Simulate portfolio with logging >>> pf = vbt . Portfolio . from_orders ( ... ohlcv [ 'Close' ], size , price = ohlcv [ 'Open' ], ... init_cash = 'autoalign' , fees = 0.001 , slippage = 0.001 , log = True ) >>> pf . logs . records id group col idx cash position debt free_cash val_price \\ 0 0 0 0 0 inf 0.000000 0.0 inf 7194.892090 1 1 0 0 1 inf 0.000000 0.0 inf 7202.551270 2 2 0 0 2 inf 0.000000 0.0 inf 6984.428711 ... ... ... ... ... ... ... ... ... ... 1461 1461 5 5 241 inf 272.389644 0.0 inf 57.207737 1462 1462 5 5 242 inf 274.137659 0.0 inf 62.844059 1463 1463 5 5 243 inf 282.093860 0.0 inf 61.105076 value ... new_free_cash new_val_price new_value res_size \\ 0 inf ... inf 7194.892090 inf NaN 1 inf ... inf 7202.551270 inf NaN 2 inf ... inf 6984.428711 inf NaN ... ... ... ... ... ... ... 1461 inf ... inf 57.207737 inf 1.748015 1462 inf ... inf 62.844059 inf 7.956202 1463 inf ... inf 61.105076 inf 1.636525 res_price res_fees res_side res_status res_status_info order_id 0 NaN NaN -1 1 0 -1 1 NaN NaN -1 1 5 -1 2 NaN NaN -1 1 5 -1 ... ... ... ... ... ... ... 1461 57.264945 0.1001 0 0 -1 1070 1462 62.906903 0.5005 0 0 -1 1071 1463 61.043971 0.0999 1 0 -1 1072 [1464 rows x 37 columns] Just as orders, logs are also records and thus can be easily analyzed: >>> pf . logs . res_status . value_counts () symbol BTC-USD ETH-USD XRP-USD BNB-USD BCH-USD LTC-USD Filled 184 172 177 178 177 185 Ignored 60 72 67 66 67 59 Logging can also be turned on just for one order, row, or column, since as many other variables it's specified per order and can broadcast automatically. Note Logging can slow down simulation. Caching \u00b6 Portfolio heavily relies upon caching. If a method or a property requires heavy computation, it's wrapped with cached_method() and cached_property respectively. Caching can be disabled globally via caching in settings . Note Because of caching, class is meant to be immutable and all properties are read-only. To change any attribute, use the copy method and pass the attribute as keyword argument. Alternatively, we can precisely point at attributes and methods that should or shouldn't be cached. For example, we can blacklist the entire Portfolio class except a few most called methods such as Portfolio.cash_flow() and Portfolio.asset_flow() : >>> vbt . settings . caching [ 'blacklist' ] . append ( ... vbt . CacheCondition ( base_cls = 'Portfolio' ) ... ) >>> vbt . settings . caching [ 'whitelist' ] . extend ([ ... vbt . CacheCondition ( base_cls = 'Portfolio' , func = 'cash_flow' ), ... vbt . CacheCondition ( base_cls = 'Portfolio' , func = 'asset_flow' ) ... ]) Define rules for one instance of Portfolio : >>> vbt . settings . caching [ 'blacklist' ] . append ( ... vbt . CacheCondition ( instance = pf ) ... ) >>> vbt . settings . caching [ 'whitelist' ] . extend ([ ... vbt . CacheCondition ( instance = pf , func = 'cash_flow' ), ... vbt . CacheCondition ( instance = pf , func = 'asset_flow' ) ... ]) See should_cache() for caching rules. To reset caching: >>> vbt . settings . caching . reset () Performance and memory \u00b6 If you're running out of memory when working with large arrays, make sure to disable caching and then store most important time series manually. For example, if you're interested in Sharpe ratio or other metrics based on returns, run and save Portfolio.returns() in a variable and then use the ReturnsAccessor to analyze them. Do not use methods akin to Portfolio.sharpe_ratio() because they will re-calculate returns each time. Alternatively, you can track portfolio value and returns using Portfolio.from_order_func() and its callbacks (preferably in post_segment_func_nb ): >>> pf_baseline = vbt . Portfolio . from_orders ( ... ohlcv [ 'Close' ], size , price = ohlcv [ 'Open' ], ... init_cash = 'autoalign' , fees = 0.001 , slippage = 0.001 , freq = 'd' ) >>> pf_baseline . sharpe_ratio () symbol BTC-USD 1.743437 ETH-USD 2.800903 XRP-USD 1.607904 BNB-USD 1.805373 BCH-USD 0.269392 LTC-USD 1.040494 Name: sharpe_ratio, dtype: float64 >>> @njit ... def order_func_nb ( c , size , price , fees , slippage ): ... return nb . order_nb ( ... size = nb . get_elem_nb ( c , size ), ... price = nb . get_elem_nb ( c , price ), ... fees = nb . get_elem_nb ( c , fees ), ... slippage = nb . get_elem_nb ( c , slippage ), ... ) >>> @njit ... def post_segment_func_nb ( c , returns_out ): ... returns_out [ c . i , c . group ] = c . last_return [ c . group ] >>> returns_out = np . empty_like ( ohlcv [ 'Close' ], dtype = np . float_ ) >>> pf = vbt . Portfolio . from_order_func ( ... ohlcv [ 'Close' ], ... order_func_nb , ... np . asarray ( size ), ... np . asarray ( ohlcv [ 'Open' ]), ... np . asarray ( 0.001 ), ... np . asarray ( 0.001 ), ... post_segment_func_nb = post_segment_func_nb , ... post_segment_args = ( returns_out ,), ... init_cash = pf_baseline . init_cash ... ) >>> returns = pf . wrapper . wrap ( returns_out ) >>> del pf >>> returns . vbt . returns ( freq = 'd' ) . sharpe_ratio () symbol BTC-USD -2.261443 ETH-USD 0.059538 XRP-USD 2.159093 BNB-USD 1.555386 BCH-USD 0.784214 LTC-USD 1.460077 Name: sharpe_ratio, dtype: float64 The only drawback of this approach is that you cannot use init_cash='auto' or init_cash='autoalign' because then, during the simulation, the portfolio value is np.inf and the returns are np.nan . Saving and loading \u00b6 Like any other class subclassing Pickleable , we can save a Portfolio instance to the disk with Pickleable.save() and load it with Pickleable.load() : >>> pf = vbt . Portfolio . from_orders ( ... ohlcv [ 'Close' ], size , price = ohlcv [ 'Open' ], ... init_cash = 'autoalign' , fees = 0.001 , slippage = 0.001 , freq = 'd' ) >>> pf . sharpe_ratio () symbol BTC-USD 1.743437 ETH-USD 2.800903 XRP-USD 1.607904 BNB-USD 1.805373 BCH-USD 0.269392 LTC-USD 1.040494 Name: sharpe_ratio, dtype: float64 >>> pf . save ( 'my_pf' ) >>> pf = vbt . Portfolio . load ( 'my_pf' ) >>> pf . sharpe_ratio () symbol BTC-USD 1.743437 ETH-USD 2.800903 XRP-USD 1.607904 BNB-USD 1.805373 BCH-USD 0.269392 LTC-USD 1.040494 Name: sharpe_ratio, dtype: float64 Note Save files won't include neither cached results nor global defaults. For example, passing fillna_close as None will also use None when the portfolio is loaded from disk. Make sure to either pass all arguments explicitly or to also save the settings config. Stats \u00b6 Hint See StatsBuilderMixin.stats() and Portfolio.metrics . Let's simulate a portfolio with two columns: >>> close = vbt . YFData . download ( ... \"BTC-USD\" , ... start = '2020-01-01 UTC' , ... end = '2020-09-01 UTC' ... ) . get ( 'Close' ) >>> pf = vbt . Portfolio . from_random_signals ( close , n = [ 10 , 20 ], seed = 42 ) >>> pf . wrapper . columns Int64Index([10, 20], dtype='int64', name='rand_n') Column, group, and tag selection \u00b6 To return the statistics for a particular column/group, use the column argument: >>> pf . stats ( column = 10 ) UserWarning: Metric 'sharpe_ratio' requires frequency to be set UserWarning: Metric 'calmar_ratio' requires frequency to be set UserWarning: Metric 'omega_ratio' requires frequency to be set UserWarning: Metric 'sortino_ratio' requires frequency to be set Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 Start Value 100.0 End Value 106.721585 Total Return [%] 6.721585 Benchmark Return [%] 66.252621 Max Gross Exposure [%] 100.0 Total Fees Paid 0.0 Max Drawdown [%] 22.190944 Max Drawdown Duration 101.0 Total Trades 10 Total Closed Trades 10 Total Open Trades 0 Open Trade PnL 0.0 Win Rate [%] 60.0 Best Trade [%] 15.31962 Worst Trade [%] -9.904223 Avg Winning Trade [%] 4.671959 Avg Losing Trade [%] -4.851205 Avg Winning Trade Duration 11.333333 Avg Losing Trade Duration 14.25 Profit Factor 1.347457 Expectancy 0.672158 Name: 10, dtype: object If vectorbt couldn't parse the frequency of close : 1) it won't return any duration in time units, 2) it won't return any metric that requires annualization, and 3) it will throw a bunch of warnings (you can silence those by passing silence_warnings=True ) We can provide the frequency as part of the settings dict: >>> pf . stats ( column = 10 , settings = dict ( freq = 'd' )) UserWarning: Changing the frequency will create a copy of this object. Consider setting the frequency upon object creation to re-use existing cache. Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 days 00:00:00 Start Value 100.0 End Value 106.721585 Total Return [%] 6.721585 Benchmark Return [%] 66.252621 Max Gross Exposure [%] 100.0 Total Fees Paid 0.0 Max Drawdown [%] 22.190944 Max Drawdown Duration 101 days 00:00:00 Total Trades 10 Total Closed Trades 10 Total Open Trades 0 Open Trade PnL 0.0 Win Rate [%] 60.0 Best Trade [%] 15.31962 Worst Trade [%] -9.904223 Avg Winning Trade [%] 4.671959 Avg Losing Trade [%] -4.851205 Avg Winning Trade Duration 11 days 08:00:00 Avg Losing Trade Duration 14 days 06:00:00 Profit Factor 1.347457 Expectancy 0.672158 Sharpe Ratio 0.445231 Calmar Ratio 0.460573 Omega Ratio 1.099192 Sortino Ratio 0.706986 Name: 10, dtype: object But in this case, our portfolio will be copied to set the new frequency and we wouldn't be able to re-use its cached attributes. Let's define the frequency upon the simulation instead: >>> pf = vbt . Portfolio . from_random_signals ( close , n = [ 10 , 20 ], seed = 42 , freq = 'd' ) We can change the grouping of the portfolio on the fly. Let's form a single group: >>> pf . stats ( group_by = True ) Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 days 00:00:00 Start Value 200.0 End Value 277.49299 Total Return [%] 38.746495 Benchmark Return [%] 66.252621 Max Gross Exposure [%] 100.0 Total Fees Paid 0.0 Max Drawdown [%] 14.219327 Max Drawdown Duration 86 days 00:00:00 Total Trades 30 Total Closed Trades 30 Total Open Trades 0 Open Trade PnL 0.0 Win Rate [%] 66.666667 Best Trade [%] 18.332559 Worst Trade [%] -9.904223 Avg Winning Trade [%] 5.754788 Avg Losing Trade [%] -4.718907 Avg Winning Trade Duration 7 days 19:12:00 Avg Losing Trade Duration 8 days 07:12:00 Profit Factor 2.427948 Expectancy 2.5831 Sharpe Ratio 1.57907 Calmar Ratio 4.445448 Omega Ratio 1.334032 Sortino Ratio 2.59669 Name: group, dtype: object We can see how the initial cash has changed from $100 to $200, indicating that both columns now contribute to the performance. Aggregation \u00b6 If the portfolio consists of multiple columns/groups and no column/group has been selected, each metric is aggregated across all columns/groups based on agg_func , which is np.mean by default. >>> pf . stats () UserWarning: Object has multiple columns. Aggregating using <function mean at 0x7fc77152bb70>. Pass column to select a single column/group. Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 days 00:00:00 Start Value 100.0 End Value 138.746495 Total Return [%] 38.746495 Benchmark Return [%] 66.252621 Max Gross Exposure [%] 100.0 Total Fees Paid 0.0 Max Drawdown [%] 20.35869 Max Drawdown Duration 93 days 00:00:00 Total Trades 15.0 Total Closed Trades 15.0 Total Open Trades 0.0 Open Trade PnL 0.0 Win Rate [%] 65.0 Best Trade [%] 16.82609 Worst Trade [%] -9.701273 Avg Winning Trade [%] 5.445408 Avg Losing Trade [%] -4.740956 Avg Winning Trade Duration 8 days 19:25:42.857142857 Avg Losing Trade Duration 9 days 07:00:00 Profit Factor 2.186957 Expectancy 2.105364 Sharpe Ratio 1.165695 Calmar Ratio 3.541079 Omega Ratio 1.331624 Sortino Ratio 2.084565 Name: agg_func_mean, dtype: object Here, the Sharpe ratio of 0.445231 (column=10) and 1.88616 (column=20) lead to the avarage of 1.16569. We can also return a DataFrame with statistics per column/group by passing agg_func=None : >>> pf . stats ( agg_func = None ) Start End Period ... Sortino Ratio rand_n ... 10 2020-01-01 00:00:00+00:00 2020-09-01 00:00:00+00:00 244 days ... 0.706986 20 2020-01-01 00:00:00+00:00 2020-09-01 00:00:00+00:00 244 days ... 3.462144 [2 rows x 25 columns] Metric selection \u00b6 To select metrics, use the metrics argument (see Portfolio.metrics for supported metrics): >>> pf . stats ( metrics = [ 'sharpe_ratio' , 'sortino_ratio' ], column = 10 ) Sharpe Ratio 0.445231 Sortino Ratio 0.706986 Name: 10, dtype: float64 We can also select specific tags (see any metric from Portfolio.metrics that has the tag key): >>> pf . stats ( column = 10 , tags = [ 'trades' ]) Total Trades 10 Total Open Trades 0 Open Trade PnL 0 Long Trades [%] 100 Win Rate [%] 60 Best Trade [%] 15.3196 Worst Trade [%] -9.90422 Avg Winning Trade [%] 4.67196 Avg Winning Trade Duration 11 days 08:00:00 Avg Losing Trade [%] -4.8512 Avg Losing Trade Duration 14 days 06:00:00 Profit Factor 1.34746 Expectancy 0.672158 Name: 10, dtype: object Or provide a boolean expression: >>> pf . stats ( column = 10 , tags = 'trades and open and not closed' ) Total Open Trades 0.0 Open Trade PnL 0.0 Name: 10, dtype: float64 The reason why we included \"not closed\" along with \"open\" is because some metrics such as the win rate have both tags attached since they are based upon both open and closed trades/positions (to see this, pass settings=dict(incl_open=True) and tags='trades and open' ). Passing parameters \u00b6 We can use settings to pass parameters used across multiple metrics. For example, let's pass required and risk-free return to all return metrics: >>> pf . stats ( column = 10 , settings = dict ( required_return = 0.1 , risk_free = 0.01 )) Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 days 00:00:00 Start Value 100.0 End Value 106.721585 Total Return [%] 6.721585 Benchmark Return [%] 66.252621 Max Gross Exposure [%] 100.0 Total Fees Paid 0.0 Max Drawdown [%] 22.190944 Max Drawdown Duration 101 days 00:00:00 Total Trades 10 Total Closed Trades 10 Total Open Trades 0 Open Trade PnL 0.0 Win Rate [%] 60.0 Best Trade [%] 15.31962 Worst Trade [%] -9.904223 Avg Winning Trade [%] 4.671959 Avg Losing Trade [%] -4.851205 Avg Winning Trade Duration 11 days 08:00:00 Avg Losing Trade Duration 14 days 06:00:00 Profit Factor 1.347457 Expectancy 0.672158 Sharpe Ratio -9.504742 << here Calmar Ratio 0.460573 << here Omega Ratio 0.233279 << here Sortino Ratio -18.763407 << here Name: 10, dtype: object Passing any argument inside of settings either overrides an existing default, or acts as an optional argument that is passed to the calculation function upon resolution (see below). Both required_return and risk_free can be found in the signature of the 4 ratio methods, so vectorbt knows exactly it has to pass them. Let's imagine that the signature of ReturnsAccessor.sharpe_ratio() doesn't list those arguments: vectorbt would simply call this method without passing those two arguments. In such case, we have two options: 1) Set parameters globally using settings and set pass_{arg}=True individually using metric_settings : >>> pf . stats ( ... column = 10 , ... settings = dict ( required_return = 0.1 , risk_free = 0.01 ), ... metric_settings = dict ( ... sharpe_ratio = dict ( pass_risk_free = True ), ... omega_ratio = dict ( pass_required_return = True , pass_risk_free = True ), ... sortino_ratio = dict ( pass_required_return = True ) ... ) ... ) 2) Set parameters individually using metric_settings : >>> pf . stats ( ... column = 10 , ... metric_settings = dict ( ... sharpe_ratio = dict ( risk_free = 0.01 ), ... omega_ratio = dict ( required_return = 0.1 , risk_free = 0.01 ), ... sortino_ratio = dict ( required_return = 0.1 ) ... ) ... ) Custom metrics \u00b6 To calculate a custom metric, we need to provide at least two things: short name and a settings dict with the title and calculation function (see arguments in StatsBuilderMixin ): >>> max_winning_streak = ( ... 'max_winning_streak' , ... dict ( ... title = 'Max Winning Streak' , ... calc_func = lambda trades : trades . winning_streak . max (), ... resolve_trades = True ... ) ... ) >>> pf . stats ( metrics = max_winning_streak , column = 10 ) Max Winning Streak 3.0 Name: 10, dtype: float64 You might wonder how vectorbt knows which arguments to pass to calc_func ? In the example above, the calculation function expects two arguments: trades and group_by . To automatically pass any of the them, vectorbt searches for each in the current settings. As trades cannot be found, it either throws an error or tries to resolve this argument if resolve_{arg}=True was passed. Argument resolution is the process of searching for property/method with the same name (also with prefix get_ ) in the attributes of the current portfolio, automatically passing the current settings such as group_by if they are present in the method's signature (a similar resolution procedure), and calling the method/property. The result of the resolution process is then passed as arg (or trades in our example). Here's an example without resolution of arguments: >>> max_winning_streak = ( ... 'max_winning_streak' , ... dict ( ... title = 'Max Winning Streak' , ... calc_func = lambda self , group_by : ... self . get_trades ( group_by = group_by ) . winning_streak . max () ... ) ... ) >>> pf . stats ( metrics = max_winning_streak , column = 10 ) Max Winning Streak 3.0 Name: 10, dtype: float64 And here's an example without resolution of the calculation function: >>> max_winning_streak = ( ... 'max_winning_streak' , ... dict ( ... title = 'Max Winning Streak' , ... calc_func = lambda self , settings : ... self . get_trades ( group_by = settings [ 'group_by' ]) . winning_streak . max (), ... resolve_calc_func = False ... ) ... ) >>> pf . stats ( metrics = max_winning_streak , column = 10 ) Max Winning Streak 3.0 Name: 10, dtype: float64 Since max_winning_streak method can be expressed as a path from this portfolio, we can simply write: >>> max_winning_streak = ( ... 'max_winning_streak' , ... dict ( ... title = 'Max Winning Streak' , ... calc_func = 'trades.winning_streak.max' ... ) ... ) In this case, we don't have to pass resolve_trades=True any more as vectorbt does it automatically. Another advantage is that vectorbt can access the signature of the last method in the path ( MappedArray.max() in our case) and resolve its arguments. To switch between entry trades, exit trades, and positions, use the trades_type setting. Additionally, you can pass incl_open=True to also include open trades. >>> pf . stats ( column = 10 , settings = dict ( trades_type = 'positions' , incl_open = True )) Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 days 00:00:00 Start Value 100.0 End Value 106.721585 Total Return [%] 6.721585 Benchmark Return [%] 66.252621 Max Gross Exposure [%] 100.0 Total Fees Paid 0.0 Max Drawdown [%] 22.190944 Max Drawdown Duration 100 days 00:00:00 Total Trades 10 Total Closed Trades 10 Total Open Trades 0 Open Trade PnL 0.0 Win Rate [%] 60.0 Best Trade [%] 15.31962 Worst Trade [%] -9.904223 Avg Winning Trade [%] 4.671959 Avg Losing Trade [%] -4.851205 Avg Winning Trade Duration 11 days 08:00:00 Avg Losing Trade Duration 14 days 06:00:00 Profit Factor 1.347457 Expectancy 0.672158 Sharpe Ratio 0.445231 Calmar Ratio 0.460573 Omega Ratio 1.099192 Sortino Ratio 0.706986 Name: 10, dtype: object Any default metric setting or even global setting can be overridden by the user using metric-specific keyword arguments. Here, we override the global aggregation function for max_dd_duration : >>> pf . stats ( agg_func = lambda sr : sr . mean (), ... metric_settings = dict ( ... max_dd_duration = dict ( agg_func = lambda sr : sr . max ()) ... ) ... ) UserWarning: Object has multiple columns. Aggregating using <function <lambda> at 0x7fbf6e77b268>. Pass column to select a single column/group. Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 days 00:00:00 Start Value 100.0 End Value 138.746495 Total Return [%] 38.746495 Benchmark Return [%] 66.252621 Max Gross Exposure [%] 100.0 Total Fees Paid 0.0 Max Drawdown [%] 20.35869 Max Drawdown Duration 101 days 00:00:00 << here Total Trades 15.0 Total Closed Trades 15.0 Total Open Trades 0.0 Open Trade PnL 0.0 Win Rate [%] 65.0 Best Trade [%] 16.82609 Worst Trade [%] -9.701273 Avg Winning Trade [%] 5.445408 Avg Losing Trade [%] -4.740956 Avg Winning Trade Duration 8 days 19:25:42.857142857 Avg Losing Trade Duration 9 days 07:00:00 Profit Factor 2.186957 Expectancy 2.105364 Sharpe Ratio 1.165695 Calmar Ratio 3.541079 Omega Ratio 1.331624 Sortino Ratio 2.084565 Name: agg_func_<lambda>, dtype: object Let's create a simple metric that returns a passed value to demonstrate how vectorbt overrides settings, from least to most important: >>> # vbt.settings.portfolio.stats >>> vbt . settings . portfolio . stats [ 'settings' ][ 'my_arg' ] = 100 >>> my_arg_metric = ( 'my_arg_metric' , dict ( title = 'My Arg' , calc_func = lambda my_arg : my_arg )) >>> pf . stats ( my_arg_metric , column = 10 ) My Arg 100 Name: 10, dtype: int64 >>> # settings >>> vbt.settings.portfolio.stats >>> pf . stats ( my_arg_metric , column = 10 , settings = dict ( my_arg = 200 )) My Arg 200 Name: 10, dtype: int64 >>> # metric settings >>> settings >>> my_arg_metric = ( 'my_arg_metric' , dict ( title = 'My Arg' , my_arg = 300 , calc_func = lambda my_arg : my_arg )) >>> pf . stats ( my_arg_metric , column = 10 , settings = dict ( my_arg = 200 )) My Arg 300 Name: 10, dtype: int64 >>> # metric_settings >>> metric settings >>> pf . stats ( my_arg_metric , column = 10 , settings = dict ( my_arg = 200 ), ... metric_settings = dict ( my_arg_metric = dict ( my_arg = 400 ))) My Arg 400 Name: 10, dtype: int64 Here's an example of a parametrized metric. Let's get the number of trades with PnL over some amount: >>> trade_min_pnl_cnt = ( ... 'trade_min_pnl_cnt' , ... dict ( ... title = vbt . Sub ( 'Trades with PnL over $$$ {min_pnl} ' ), ... calc_func = lambda trades , min_pnl : trades . apply_mask ( ... trades . pnl . values >= min_pnl ) . count (), ... resolve_trades = True ... ) ... ) >>> pf . stats ( ... metrics = trade_min_pnl_cnt , column = 10 , ... metric_settings = dict ( trade_min_pnl_cnt = dict ( min_pnl = 0 ))) Trades with PnL over $0 6 Name: stats, dtype: int64 >>> pf . stats ( ... metrics = trade_min_pnl_cnt , column = 10 , ... metric_settings = dict ( trade_min_pnl_cnt = dict ( min_pnl = 10 ))) Trades with PnL over $10 1 Name: stats, dtype: int64 If the same metric name was encountered more than once, vectorbt automatically appends an underscore and its position, so we can pass keyword arguments to each metric separately: >>> pf . stats ( ... metrics = [ ... trade_min_pnl_cnt , ... trade_min_pnl_cnt , ... trade_min_pnl_cnt ... ], ... column = 10 , ... metric_settings = dict ( ... trade_min_pnl_cnt_0 = dict ( min_pnl = 0 ), ... trade_min_pnl_cnt_1 = dict ( min_pnl = 10 ), ... trade_min_pnl_cnt_2 = dict ( min_pnl = 20 )) ... ) Trades with PnL over $0 6 Trades with PnL over $10 1 Trades with PnL over $20 0 Name: stats, dtype: int64 To add a custom metric to the list of all metrics, we have three options. The first option is to change the Portfolio.metrics dict in-place (this will append to the end): >>> pf . metrics [ 'max_winning_streak' ] = max_winning_streak [ 1 ] >>> pf . stats ( column = 10 ) Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 days 00:00:00 Start Value 100.0 End Value 106.721585 Total Return [%] 6.721585 Benchmark Return [%] 66.252621 Max Gross Exposure [%] 100.0 Total Fees Paid 0.0 Max Drawdown [%] 22.190944 Max Drawdown Duration 101 days 00:00:00 Total Trades 10 Total Closed Trades 10 Total Open Trades 0 Open Trade PnL 0.0 Win Rate [%] 60.0 Best Trade [%] 15.31962 Worst Trade [%] -9.904223 Avg Winning Trade [%] 4.671959 Avg Losing Trade [%] -4.851205 Avg Winning Trade Duration 11 days 08:00:00 Avg Losing Trade Duration 14 days 06:00:00 Profit Factor 1.347457 Expectancy 0.672158 Sharpe Ratio 0.445231 Calmar Ratio 0.460573 Omega Ratio 1.099192 Sortino Ratio 0.706986 Max Winning Streak 3.0 << here Name: 10, dtype: object Since Portfolio.metrics is of type Config , we can reset it at any time to get default metrics: >>> pf . metrics . reset () The second option is to copy Portfolio.metrics , append our metric, and pass as metrics argument: >>> my_metrics = list ( pf . metrics . items ()) + [ max_winning_streak ] >>> pf . stats ( metrics = my_metrics , column = 10 ) The third option is to set metrics globally under portfolio.stats in settings . >>> vbt . settings . portfolio [ 'stats' ][ 'metrics' ] = my_metrics >>> pf . stats ( column = 10 ) Returns stats \u00b6 We can compute the stats solely based on the portfolio's returns using Portfolio.returns_stats() , which calls StatsBuilderMixin.stats() . >>> pf . returns_stats ( column = 10 ) Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 days 00:00:00 Total Return [%] 6.721585 Benchmark Return [%] 66.252621 Annualized Return [%] 10.22056 Annualized Volatility [%] 36.683518 Max Drawdown [%] 22.190944 Max Drawdown Duration 100 days 00:00:00 Sharpe Ratio 0.445231 Calmar Ratio 0.460573 Omega Ratio 1.099192 Sortino Ratio 0.706986 Skew 1.328259 Kurtosis 10.80246 Tail Ratio 1.057913 Common Sense Ratio 1.166037 Value at Risk -0.031011 Alpha -0.075109 Beta 0.220351 Name: 10, dtype: object Most metrics defined in ReturnsAccessor are also available as attributes of Portfolio : >>> pf . sharpe_ratio () randnx_n 10 0.445231 20 1.886158 Name: sharpe_ratio, dtype: float64 Moreover, we can access quantstats functions using QSAdapter : >>> pf . qs . sharpe () randnx_n 10 0.445231 20 1.886158 dtype: float64 >>> pf [ 10 ] . qs . plot_snapshot () Plots \u00b6 Hint See PlotsBuilderMixin.plots() . The features implemented in this method are very similar to StatsBuilderMixin.stats() . See also the examples under StatsBuilderMixin.stats() . Plot portfolio of a random strategy: >>> pf . plot ( column = 10 ) You can choose any of the subplots in Portfolio.subplots , in any order, and control their appearance using keyword arguments: >>> pf . plot ( ... subplots = [ 'drawdowns' , 'underwater' ], ... column = 10 , ... subplot_settings = dict ( ... drawdowns = dict ( top_n = 3 ), ... underwater = dict ( ... trace_kwargs = dict ( ... line = dict ( color = '#FF6F00' ), ... fillcolor = adjust_opacity ( '#FF6F00' , 0.3 ) ... ) ... ) ... ) ... ) To create a new subplot, a preferred way is to pass a plotting function: >>> def plot_order_size ( pf , size , column = None , add_trace_kwargs = None , fig = None ): ... size = pf . select_one_from_obj ( size , pf . wrapper . regroup ( False ), column = column ) ... size . rename ( 'Order Size' ) . vbt . barplot ( ... add_trace_kwargs = add_trace_kwargs , fig = fig ) >>> order_size = pf . orders . size . to_pd ( fill_value = 0. ) >>> pf . plot ( subplots = [ ... 'orders' , ... ( 'order_size' , dict ( ... title = 'Order Size' , ... yaxis_kwargs = dict ( title = 'Order size' ), ... check_is_not_grouped = True , ... plot_func = plot_order_size ... )) ... ], ... column = 10 , ... subplot_settings = dict ( ... order_size = dict ( ... size = order_size ... ) ... ) ... ) Alternatively, you can create a placeholder and overwrite it manually later: >>> fig = pf . plot ( subplots = [ ... 'orders' , ... ( 'order_size' , dict ( ... title = 'Order Size' , ... yaxis_kwargs = dict ( title = 'Order size' ), ... check_is_not_grouped = True ... )) # placeholder ... ], column = 10 ) >>> order_size [ 10 ] . rename ( 'Order Size' ) . vbt . barplot ( ... add_trace_kwargs = dict ( row = 2 , col = 1 ), ... fig = fig ... ) If a plotting function can in any way be accessed from the current portfolio, you can pass the path to this function (see deep_getattr() for the path format). You can additionally use templates to make some parameters to depend upon passed keyword arguments: >>> subplots = [ ... ( 'cumulative_returns' , dict ( ... title = 'Cumulative Returns' , ... yaxis_kwargs = dict ( title = 'Cumulative returns' ), ... plot_func = 'returns.vbt.returns.cumulative.vbt.plot' , ... pass_add_trace_kwargs = True ... )), ... ( 'rolling_drawdown' , dict ( ... title = 'Rolling Drawdown' , ... yaxis_kwargs = dict ( title = 'Rolling drawdown' ), ... plot_func = [ ... 'returns.vbt.returns' , # returns accessor ... ( ... 'rolling_max_drawdown' , # function name ... ( vbt . Rep ( 'window' ),)), # positional arguments ... 'vbt.plot' # plotting function ... ], ... pass_add_trace_kwargs = True , ... trace_names = [ vbt . Sub ( 'rolling_drawdown($ {window} )' )], # add window to the trace name ... )) ... ] >>> pf . plot ( ... subplots , ... column = 10 , ... subplot_settings = dict ( ... rolling_drawdown = dict ( ... template_mapping = dict ( ... window = 10 ... ) ... ) ... ) ... ) You can also replace templates across all subplots by using the global template mapping: >>> pf . plot ( subplots , column = 10 , template_mapping = dict ( window = 10 )) returns_acc_config Config \u00b6 Config of returns accessor methods to be added to Portfolio . Co nf ig( { \"daily_returns\" : { \"source_name\" : \"daily\" }, \"annual_returns\" : { \"source_name\" : \"annual\" }, \"cumulative_returns\" : { \"source_name\" : \"cumulative\" }, \"annualized_return\" : { \"source_name\" : \"annualized\" }, \"annualized_volatility\" : {}, \"calmar_ratio\" : {}, \"omega_ratio\" : {}, \"sharpe_ratio\" : {}, \"deflated_sharpe_ratio\" : {}, \"downside_risk\" : {}, \"sortino_ratio\" : {}, \"information_ratio\" : {}, \"beta\" : {}, \"alpha\" : {}, \"tail_ratio\" : {}, \"value_at_risk\" : {}, \"cond_value_at_risk\" : {}, \"capture\" : {}, \"up_capture\" : {}, \"down_capture\" : {}, \"drawdown\" : {}, \"max_drawdown\" : {} } ) MetaPortfolio class \u00b6 Meta class that exposes a read-only class property StatsBuilderMixin.metrics . Superclasses MetaPlotsBuilderMixin MetaStatsBuilderMixin builtins.type Inherited members MetaPlotsBuilderMixin.subplots MetaStatsBuilderMixin.metrics Portfolio class \u00b6 Class for modeling portfolio and measuring its performance. Args wrapper :\u2002 ArrayWrapper Array wrapper. See ArrayWrapper . close :\u2002 array_like Last asset price at each time step. order_records :\u2002 array_like A structured NumPy array of order records. log_records :\u2002 array_like A structured NumPy array of log records. init_cash :\u2002 InitCashMode , float or array_like of float Initial capital. cash_sharing :\u2002 bool Whether to share cash within the same group. call_seq :\u2002 array_like of int Sequence of calls per row and group. Defaults to None. fillna_close :\u2002 bool Whether to forward and backward fill NaN values in close . Applied after the simulation to avoid NaNs in asset value. See Portfolio.get_filled_close() . trades_type :\u2002 str or int Default Trades to use across Portfolio . See TradesType . For defaults, see portfolio in settings . Note Use class methods with from_ prefix to build a portfolio. The __init__ method is reserved for indexing purposes. Note This class is meant to be immutable. To change any attribute, use Configured.replace() . Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.config Wrapping.iloc Wrapping.indexing_kwargs Wrapping.loc Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Wrapping.self_aliases Wrapping.wrapper Wrapping.writeable_attrs alpha method \u00b6 Portfolio . alpha ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.alpha() . annual_returns method \u00b6 Portfolio . annual_returns ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.annual() . annualized_return method \u00b6 Portfolio . annualized_return ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.annualized() . annualized_volatility method \u00b6 Portfolio . annualized_volatility ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.annualized_volatility() . asset_flow method \u00b6 Portfolio . asset_flow ( direction = 'both' , wrap_kwargs = None ) Get asset flow series per column. Returns the total transacted amount of assets at each time step. asset_returns method \u00b6 Portfolio . asset_returns ( group_by = None , wrap_kwargs = None ) Get asset return series per column/group. This type of returns is based solely on cash flows and asset value rather than portfolio value. It ignores passive cash and thus it will return the same numbers irrespective of the amount of cash currently available, even np.inf . The scale of returns is comparable to that of going all in and keeping available cash at zero. asset_value method \u00b6 Portfolio . asset_value ( direction = 'both' , group_by = None , wrap_kwargs = None ) Get asset value series per column/group. assets method \u00b6 Portfolio . assets ( direction = 'both' , wrap_kwargs = None ) Get asset series per column. Returns the current position at each time step. benchmark_rets method \u00b6 Portfolio . benchmark_returns ( group_by = None , wrap_kwargs = None ) Get return series per column/group based on benchmark value. benchmark_returns method \u00b6 Portfolio . benchmark_returns ( group_by = None , wrap_kwargs = None ) Get return series per column/group based on benchmark value. benchmark_value method \u00b6 Portfolio . benchmark_value ( group_by = None , wrap_kwargs = None ) Get market benchmark value series per column/group. If grouped, evenly distributes the initial cash among assets in the group. Note Does not take into account fees and slippage. For this, create a separate portfolio. beta method \u00b6 Portfolio . beta ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.beta() . call_seq property \u00b6 Sequence of calls per row and group. calmar_ratio method \u00b6 Portfolio . calmar_ratio ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.calmar_ratio() . capture method \u00b6 Portfolio . capture ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.capture() . cash method \u00b6 Portfolio . cash ( group_by = None , in_sim_order = False , free = False , wrap_kwargs = None ) Get cash balance series per column/group. See the explanation on in_sim_order in Portfolio.value() . For free , see Portfolio.cash_flow() . cash_flow method \u00b6 Portfolio . cash_flow ( group_by = None , free = False , wrap_kwargs = None ) Get cash flow series per column/group. Use free to return the flow of the free cash, which never goes above the initial level, because an operation always costs money. cash_sharing property \u00b6 Whether to share cash within the same group. close property \u00b6 Price per unit series. cond_value_at_risk method \u00b6 Portfolio . cond_value_at_risk ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.cond_value_at_risk() . cumulative_returns method \u00b6 Portfolio . cumulative_returns ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.cumulative() . daily_returns method \u00b6 Portfolio . daily_returns ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.daily() . deflated_sharpe_ratio method \u00b6 Portfolio . deflated_sharpe_ratio ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.deflated_sharpe_ratio() . down_capture method \u00b6 Portfolio . down_capture ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.down_capture() . downside_risk method \u00b6 Portfolio . downside_risk ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.downside_risk() . drawdown method \u00b6 Portfolio . drawdown ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.drawdown() . drawdowns method \u00b6 Portfolio.get_drawdowns() with default arguments. entry_trades method \u00b6 Portfolio.get_entry_trades() with default arguments. exit_trades method \u00b6 Portfolio.get_exit_trades() with default arguments. fillna_close property \u00b6 Whether to forward-backward fill NaN values in Portfolio.close . final_value method \u00b6 Portfolio . final_value ( group_by = None , wrap_kwargs = None ) Get total profit per column/group. from_holding class method \u00b6 Portfolio . from_holding ( close , ** kwargs ) Simulate portfolio from holding. Based on Portfolio.from_signals() . >>> close = pd . Series ([ 1 , 2 , 3 , 4 , 5 ]) >>> pf = vbt . Portfolio . from_holding ( close ) >>> pf . final_value () 500.0 from_order_func class method \u00b6 Portfolio . from_order_func ( close , order_func_nb , * order_args , flexible = None , init_cash = None , cash_sharing = None , call_seq = None , segment_mask = None , call_pre_segment = None , call_post_segment = None , pre_sim_func_nb = no_pre_func_nb , pre_sim_args = (), post_sim_func_nb = no_post_func_nb , post_sim_args = (), pre_group_func_nb = no_pre_func_nb , pre_group_args = (), post_group_func_nb = no_post_func_nb , post_group_args = (), pre_row_func_nb = no_pre_func_nb , pre_row_args = (), post_row_func_nb = no_post_func_nb , post_row_args = (), pre_segment_func_nb = no_pre_func_nb , pre_segment_args = (), post_segment_func_nb = no_post_func_nb , post_segment_args = (), post_order_func_nb = no_post_func_nb , post_order_args = (), ffill_val_price = None , update_value = None , fill_pos_record = None , row_wise = None , use_numba = None , max_orders = None , max_logs = None , seed = None , group_by = None , broadcast_named_args = None , broadcast_kwargs = None , template_mapping = None , wrapper_kwargs = None , freq = None , attach_call_seq = None , ** kwargs ) Build portfolio from a custom order function. Hint See simulate_nb() for illustrations and argument definitions. For more details on individual simulation functions: not row_wise and not flexible : See simulate_nb() not row_wise and flexible : See flex_simulate_nb() row_wise and not flexible : See simulate_row_wise_nb() row_wise and flexible : See flex_simulate_row_wise_nb() Args close :\u2002 array_like Last asset price at each time step. Will broadcast to target_shape . Used for calculating unrealized PnL and portfolio value. order_func_nb :\u2002 callable Order generation function. *order_args Arguments passed to order_func_nb . flexible :\u2002 bool Whether to simulate using a flexible order function. This lifts the limit of one order per tick and symbol. init_cash :\u2002 InitCashMode , float or array_like of float Initial capital. See init_cash in Portfolio.from_orders() . cash_sharing :\u2002 bool Whether to share cash within the same group. If group_by is None, group_by becomes True to form a single group with cash sharing. call_seq :\u2002 CallSeqType or array_like Default sequence of calls per row and group. Use CallSeqType to select a sequence type. Set to array to specify custom sequence. Will not broadcast. Note CallSeqType.Auto should be implemented manually. Use sort_call_seq_nb() or sort_call_seq_out_nb() in pre_segment_func_nb . segment_mask :\u2002 int or array_like of bool Mask of whether a particular segment should be executed. Supplying an integer will activate every n-th row. Supplying a boolean or an array of boolean will broadcast to the number of rows and groups. Does not broadcast together with close and broadcast_named_args , only against the final shape. call_pre_segment :\u2002 bool Whether to call pre_segment_func_nb regardless of segment_mask . call_post_segment :\u2002 bool Whether to call post_segment_func_nb regardless of segment_mask . pre_sim_func_nb :\u2002 callable Function called before simulation. Defaults to no_pre_func_nb() . pre_sim_args :\u2002 tuple Packed arguments passed to pre_sim_func_nb . Defaults to () . post_sim_func_nb :\u2002 callable Function called after simulation. Defaults to no_post_func_nb() . post_sim_args :\u2002 tuple Packed arguments passed to post_sim_func_nb . Defaults to () . pre_group_func_nb :\u2002 callable Function called before each group. Defaults to no_pre_func_nb() . Called only if row_wise is False. pre_group_args :\u2002 tuple Packed arguments passed to pre_group_func_nb . Defaults to () . post_group_func_nb :\u2002 callable Function called after each group. Defaults to no_post_func_nb() . Called only if row_wise is False. post_group_args :\u2002 tuple Packed arguments passed to post_group_func_nb . Defaults to () . pre_row_func_nb :\u2002 callable Function called before each row. Defaults to no_pre_func_nb() . Called only if row_wise is True. pre_row_args :\u2002 tuple Packed arguments passed to pre_row_func_nb . Defaults to () . post_row_func_nb :\u2002 callable Function called after each row. Defaults to no_post_func_nb() . Called only if row_wise is True. post_row_args :\u2002 tuple Packed arguments passed to post_row_func_nb . Defaults to () . pre_segment_func_nb :\u2002 callable Function called before each segment. Defaults to no_pre_func_nb() . pre_segment_args :\u2002 tuple Packed arguments passed to pre_segment_func_nb . Defaults to () . post_segment_func_nb :\u2002 callable Function called after each segment. Defaults to no_post_func_nb() . post_segment_args :\u2002 tuple Packed arguments passed to post_segment_func_nb . Defaults to () . post_order_func_nb :\u2002 callable Callback that is called after the order has been processed. post_order_args :\u2002 tuple Packed arguments passed to post_order_func_nb . Defaults to () . ffill_val_price :\u2002 bool Whether to track valuation price only if it's known. Otherwise, unknown close will lead to NaN in valuation price at the next timestamp. update_value :\u2002 bool Whether to update group value after each filled order. fill_pos_record :\u2002 bool Whether to fill position record. Disable this to make simulation a bit faster for simple use cases. row_wise :\u2002 bool Whether to iterate over rows rather than columns/groups. use_numba :\u2002 bool Whether to run the main simulation function using Numba. Note Disabling it does not disable Numba for other functions. If neccessary, you should ensure that every other function does not uses Numba as well. You can do this by using the py_func attribute of that function. Or, you could disable Numba globally by doing os.environ['NUMBA_DISABLE_JIT'] = '1' . max_orders :\u2002 int Size of the order records array. Defaults to the number of elements in the broadcasted shape. Set to a lower number if you run out of memory. max_logs :\u2002 int Size of the log records array. Defaults to the number of elements in the broadcasted shape. Set to a lower number if you run out of memory. seed :\u2002 int See Portfolio.from_orders() . group_by :\u2002 any See Portfolio.from_orders() . broadcast_named_args :\u2002 dict See Portfolio.from_signals() . broadcast_kwargs :\u2002 dict See Portfolio.from_orders() . template_mapping :\u2002 mapping See Portfolio.from_signals() . wrapper_kwargs :\u2002 dict See Portfolio.from_orders() . freq :\u2002 any See Portfolio.from_orders() . attach_call_seq :\u2002 bool See Portfolio.from_orders() . **kwargs Keyword arguments passed to the __init__ method. For defaults, see portfolio in settings . Note All passed functions should be Numba-compiled if Numba is enabled. Also see notes on Portfolio.from_orders() . Note In contrast to other methods, the valuation price is previous close instead of the order price since the price of an order is unknown before the call (which is more realistic by the way). You can still override the valuation price in pre_segment_func_nb . Usage Buy 10 units each tick using closing price: >>> @njit ... def order_func_nb ( c , size ): ... return nb . order_nb ( size = size ) >>> close = pd . Series ([ 1 , 2 , 3 , 4 , 5 ]) >>> pf = vbt . Portfolio . from_order_func ( close , order_func_nb , 10 ) >>> pf . assets () 0 10.0 1 20.0 2 30.0 3 40.0 4 40.0 dtype: float64 >>> pf . cash () 0 90.0 1 70.0 2 40.0 3 0.0 4 0.0 dtype: float64 Reverse each position by first closing it. Keep state of last position to determine which position to open next (just as an example, there are easier ways to do this): >>> @njit ... def pre_group_func_nb ( c ): ... last_pos_state = np . array ([ - 1 ]) ... return ( last_pos_state ,) >>> @njit ... def order_func_nb ( c , last_pos_state ): ... if c . position_now != 0 : ... return nb . close_position_nb () ... ... if last_pos_state [ 0 ] == 1 : ... size = - np . inf # open short ... last_pos_state [ 0 ] = - 1 ... else : ... size = np . inf # open long ... last_pos_state [ 0 ] = 1 ... return nb . order_nb ( size = size ) >>> pf = vbt . Portfolio . from_order_func ( ... close , ... order_func_nb , ... pre_group_func_nb = pre_group_func_nb ... ) >>> pf . assets () 0 100.000000 1 0.000000 2 -66.666667 3 0.000000 4 26.666667 dtype: float64 >>> pf . cash () 0 0.000000 1 200.000000 2 400.000000 3 133.333333 4 0.000000 dtype: float64 Equal-weighted portfolio as in the example under simulate_nb() : >>> @njit ... def pre_group_func_nb ( c ): ... order_value_out = np . empty ( c . group_len , dtype = np . float_ ) ... return ( order_value_out ,) >>> @njit ... def pre_segment_func_nb ( c , order_value_out , size , price , size_type , direction ): ... for col in range ( c . from_col , c . to_col ): ... c . last_val_price [ col ] = nb . get_col_elem_nb ( c , col , price ) ... nb . sort_call_seq_nb ( c , size , size_type , direction , order_value_out ) ... return () >>> @njit ... def order_func_nb ( c , size , price , size_type , direction , fees , fixed_fees , slippage ): ... return nb . order_nb ( ... size = nb . get_elem_nb ( c , size ), ... price = nb . get_elem_nb ( c , price ), ... size_type = nb . get_elem_nb ( c , size_type ), ... direction = nb . get_elem_nb ( c , direction ), ... fees = nb . get_elem_nb ( c , fees ), ... fixed_fees = nb . get_elem_nb ( c , fixed_fees ), ... slippage = nb . get_elem_nb ( c , slippage ) ... ) >>> np . random . seed ( 42 ) >>> close = np . random . uniform ( 1 , 10 , size = ( 5 , 3 )) >>> size_template = vbt . RepEval ( 'np.asarray(1 / group_lens[0])' ) >>> pf = vbt . Portfolio . from_order_func ( ... close , ... order_func_nb , ... size_template , # order_args as *args ... vbt . Rep ( 'price' ), ... vbt . Rep ( 'size_type' ), ... vbt . Rep ( 'direction' ), ... vbt . Rep ( 'fees' ), ... vbt . Rep ( 'fixed_fees' ), ... vbt . Rep ( 'slippage' ), ... segment_mask = 2 , # rebalance every second tick ... pre_group_func_nb = pre_group_func_nb , ... pre_segment_func_nb = pre_segment_func_nb , ... pre_segment_args = ( ... size_template , ... vbt . Rep ( 'price' ), ... vbt . Rep ( 'size_type' ), ... vbt . Rep ( 'direction' ) ... ), ... broadcast_named_args = dict ( # broadcast against each other ... price = close , ... size_type = SizeType . TargetPercent , ... direction = Direction . LongOnly , ... fees = 0.001 , ... fixed_fees = 1. , ... slippage = 0.001 ... ), ... template_mapping = dict ( np = np ), # required by size_template ... cash_sharing = True , group_by = True , # one group with cash sharing ... ) >>> pf . asset_value ( group_by = False ) . vbt . plot () Templates are a very powerful tool to prepare any custom arguments after they are broadcast and before they are passed to the simulation function. In the example above, we use broadcast_named_args to broadcast some arguments against each other and templates to pass those objects to callbacks. Additionally, we used an evaluation template to compute the size based on the number of assets in each group. You may ask: why should we bother using broadcasting and templates if we could just pass size=1/3 ? Because of flexibility those features provide: we can now pass whatever parameter combinations we want and it will work flawlessly. For example, to create two groups of equally-allocated positions, we need to change only two parameters: >>> close = np . random . uniform ( 1 , 10 , size = ( 5 , 6 )) # 6 columns instead of 3 >>> group_by = [ 'g1' , 'g1' , 'g1' , 'g2' , 'g2' , 'g2' ] # 2 groups instead of 1 >>> pf [ 'g1' ] . asset_value ( group_by = False ) . vbt . plot () >>> pf [ 'g2' ] . asset_value ( group_by = False ) . vbt . plot () Combine multiple exit conditions. Exit early if the price hits some threshold before an actual exit: >>> @njit ... def pre_sim_func_nb ( c ): ... # We need to define stop price per column once ... stop_price = np . full ( c . target_shape [ 1 ], np . nan , dtype = np . float_ ) ... return ( stop_price ,) >>> @njit ... def order_func_nb ( c , stop_price , entries , exits , size ): ... # Select info related to this order ... entry_now = nb . get_elem_nb ( c , entries ) ... exit_now = nb . get_elem_nb ( c , exits ) ... size_now = nb . get_elem_nb ( c , size ) ... price_now = nb . get_elem_nb ( c , c . close ) ... stop_price_now = stop_price [ c . col ] ... ... # Our logic ... if entry_now : ... if c . position_now == 0 : ... return nb . order_nb ( ... size = size_now , ... price = price_now , ... direction = Direction . LongOnly ) ... elif exit_now or price_now >= stop_price_now : ... if c . position_now > 0 : ... return nb . order_nb ( ... size =- size_now , ... price = price_now , ... direction = Direction . LongOnly ) ... return NoOrder >>> @njit ... def post_order_func_nb ( c , stop_price , stop ): ... # Same broadcasting as for size ... stop_now = nb . get_elem_nb ( c , stop ) ... ... if c . order_result . status == OrderStatus . Filled : ... if c . order_result . side == OrderSide . Buy : ... # Position entered: Set stop condition ... stop_price [ c . col ] = ( 1 + stop_now ) * c . order_result . price ... else : ... # Position exited: Remove stop condition ... stop_price [ c . col ] = np . nan >>> def simulate ( close , entries , exits , size , threshold ): ... return vbt . Portfolio . from_order_func ( ... close , ... order_func_nb , ... vbt . Rep ( 'entries' ), vbt . Rep ( 'exits' ), vbt . Rep ( 'size' ), # order_args ... pre_sim_func_nb = pre_sim_func_nb , ... post_order_func_nb = post_order_func_nb , ... post_order_args = ( vbt . Rep ( 'threshold' ),), ... broadcast_named_args = dict ( # broadcast against each other ... entries = entries , ... exits = exits , ... size = size , ... threshold = threshold ... ) ... ) >>> close = pd . Series ([ 10 , 11 , 12 , 13 , 14 ]) >>> entries = pd . Series ([ True , True , False , False , False ]) >>> exits = pd . Series ([ False , False , False , True , True ]) >>> simulate ( close , entries , exits , np . inf , 0.1 ) . asset_flow () 0 10.0 1 0.0 2 -10.0 3 0.0 4 0.0 dtype: float64 >>> simulate ( close , entries , exits , np . inf , 0.2 ) . asset_flow () 0 10.0 1 0.0 2 -10.0 3 0.0 4 0.0 dtype: float64 >>> simulate ( close , entries , exits , np . nan ) . asset_flow () 0 10.0 1 0.0 2 0.0 3 -10.0 4 0.0 dtype: float64 The reason why stop of 10% does not result in an order at the second time step is because it comes at the same time as entry, so it must wait until no entry is present. This can be changed by replacing the statement \"elif\" with \"if\", which would execute an exit regardless if an entry is present (similar to using ConflictMode.Opposite in Portfolio.from_signals() ). We can also test the parameter combinations above all at once (thanks to broadcasting): >>> size = pd . DataFrame ( ... [[ 0.1 , 0.2 , np . nan ]], ... columns = pd . Index ([ '0.1' , '0.2' , 'nan' ], name = 'size' ) ... ) >>> simulate ( close , entries , exits , np . inf , size ) . asset_flow () size 0.1 0.2 nan 0 10.0 10.0 10.0 1 0.0 0.0 0.0 2 -10.0 -10.0 0.0 3 0.0 0.0 -10.0 4 0.0 0.0 0.0 Let's illustrate how to generate multiple orders per symbol and bar. For each bar, buy at open and sell at close: >>> @njit ... def flex_order_func_nb ( c , open , size ): ... if c . call_idx == 0 : ... return c . from_col , nb . order_nb ( size = size , price = open [ c . i , c . from_col ]) ... if c . call_idx == 1 : ... return c . from_col , nb . close_position_nb ( price = c . close [ c . i , c . from_col ]) ... return - 1 , NoOrder >>> open = pd . DataFrame ({ 'a' : [ 1 , 2 , 3 ], 'b' : [ 4 , 5 , 6 ]}) >>> close = pd . DataFrame ({ 'a' : [ 2 , 3 , 4 ], 'b' : [ 3 , 4 , 5 ]}) >>> size = 1 >>> pf = vbt . Portfolio . from_order_func ( ... close , ... flex_order_func_nb , ... to_2d_array ( open ), size , ... flexible = True , max_orders = close . shape [ 0 ] * close . shape [ 1 ] * 2 ) >>> pf . orders . records_readable Order Id Timestamp Column Size Price Fees Side 0 0 0 a 1.0 1.0 0.0 Buy 1 1 0 a 1.0 2.0 0.0 Sell 2 2 1 a 1.0 2.0 0.0 Buy 3 3 1 a 1.0 3.0 0.0 Sell 4 4 2 a 1.0 3.0 0.0 Buy 5 5 2 a 1.0 4.0 0.0 Sell 6 6 0 b 1.0 4.0 0.0 Buy 7 7 0 b 1.0 3.0 0.0 Sell 8 8 1 b 1.0 5.0 0.0 Buy 9 9 1 b 1.0 4.0 0.0 Sell 10 10 2 b 1.0 6.0 0.0 Buy 11 11 2 b 1.0 5.0 0.0 Sell Warning Each bar is effectively a black box - we don't know how the price moves inside. Since trades must come in an order that replicates that of the real world, the only reliable pieces of information are the opening and the closing price. from_orders class method \u00b6 Portfolio . from_orders ( close , size = None , size_type = None , direction = None , price = None , fees = None , fixed_fees = None , slippage = None , min_size = None , max_size = None , size_granularity = None , reject_prob = None , lock_cash = None , allow_partial = None , raise_reject = None , log = None , val_price = None , init_cash = None , cash_sharing = None , call_seq = None , ffill_val_price = None , update_value = None , max_orders = None , max_logs = None , seed = None , group_by = None , broadcast_kwargs = None , wrapper_kwargs = None , freq = None , attach_call_seq = None , ** kwargs ) Simulate portfolio from orders - size, price, fees, and other information. Args close :\u2002 array_like Last asset price at each time step. Will broadcast. Used for calculating unrealized PnL and portfolio value. size :\u2002 float or array_like Size to order. See Order.size . Will broadcast. size_type :\u2002 SizeType or array_like See SizeType . See Order.size_type . Will broadcast. Note SizeType.Percent does not support position reversal. Switch to a single direction. Warning Be cautious using SizeType.Percent with call_seq set to 'auto'. To execute sell orders before buy orders, the value of each order in the group needs to be approximated in advance. But since SizeType.Percent depends upon the cash balance, which cannot be calculated in advance since it may change after each order, this can yield a non-optimal call sequence. direction :\u2002 Direction or array_like See Direction . See Order.direction . Will broadcast. price :\u2002 array_like of float Order price. See Order.price . Defaults to np.inf . Will broadcast. Note Make sure to use the same timestamp for all order prices in the group with cash sharing and call_seq set to CallSeqType.Auto . fees :\u2002 float or array_like Fees in percentage of the order value. See Order.fees . Will broadcast. fixed_fees :\u2002 float or array_like Fixed amount of fees to pay per order. See Order.fixed_fees . Will broadcast. slippage :\u2002 float or array_like Slippage in percentage of price. See Order.slippage . Will broadcast. min_size :\u2002 float or array_like Minimum size for an order to be accepted. See Order.min_size . Will broadcast. max_size :\u2002 float or array_like Maximum size for an order. See Order.max_size . Will broadcast. Will be partially filled if exceeded. size_granularity :\u2002 float or array_like Granularity of the size. See Order.size_granularity . Will broadcast. reject_prob :\u2002 float or array_like Order rejection probability. See Order.reject_prob . Will broadcast. lock_cash :\u2002 bool or array_like Whether to lock cash when shorting. See Order.lock_cash . Will broadcast. allow_partial :\u2002 bool or array_like Whether to allow partial fills. See Order.allow_partial . Will broadcast. Does not apply when size is np.inf . raise_reject :\u2002 bool or array_like Whether to raise an exception if order gets rejected. See Order.raise_reject . Will broadcast. log :\u2002 bool or array_like Whether to log orders. See Order.log . Will broadcast. val_price :\u2002 array_like of float Asset valuation price. Will broadcast. Any -np.inf element is replaced by the latest valuation price (the previous close or the latest known valuation price if ffill_val_price ). Any np.inf element is replaced by the current order price. Used at the time of decision making to calculate value of each asset in the group, for example, to convert target value into target amount. Note In contrast to Portfolio.from_order_func() , order price is known beforehand (kind of), thus val_price is set to the current order price (using np.inf ) by default. To valuate using previous close, set it in the settings to -np.inf . Note Make sure to use timestamp for val_price that comes before timestamps of all orders in the group with cash sharing (previous close for example), otherwise you're cheating yourself. init_cash :\u2002 InitCashMode , float or array_like of float Initial capital. By default, will broadcast to the number of columns. If cash sharing is enabled, will broadcast to the number of groups. See InitCashMode to find optimal initial cash. Note Mode InitCashMode.AutoAlign is applied after the portfolio is initialized to set the same initial cash for all columns/groups. Changing grouping will change the initial cash, so be aware when indexing. cash_sharing :\u2002 bool Whether to share cash within the same group. If group_by is None, group_by becomes True to form a single group with cash sharing. Warning Introduces cross-asset dependencies. This method presumes that in a group of assets that share the same capital all orders will be executed within the same tick and retain their price regardless of their position in the queue, even though they depend upon each other and thus cannot be executed in parallel. call_seq :\u2002 CallSeqType or array_like Default sequence of calls per row and group. Each value in this sequence should indicate the position of column in the group to call next. Processing of call_seq goes always from left to right. For example, [2, 0, 1] would first call column 'c', then 'a', and finally 'b'. Use CallSeqType to select a sequence type. Set to array to specify custom sequence. Will not broadcast. If CallSeqType.Auto selected, rearranges calls dynamically based on order value. Calculates value of all orders per row and group, and sorts them by this value. Sell orders will be executed first to release funds for buy orders. Warning CallSeqType.Auto should be used with caution: It not only presumes that order prices are known beforehand, but also that orders can be executed in arbitrary order and still retain their price. In reality, this is hardly the case: after processing one asset, some time has passed and the price for other assets might have already changed. Even if you're able to specify a slippage large enough to compensate for this behavior, slippage itself should depend upon execution order. This method doesn't let you do that. If one order is rejected, it still may execute next orders and possibly leave them without required funds. For more control, use Portfolio.from_order_func() . ffill_val_price :\u2002 bool Whether to track valuation price only if it's known. Otherwise, unknown close will lead to NaN in valuation price at the next timestamp. update_value :\u2002 bool Whether to update group value after each filled order. max_orders :\u2002 int Size of the order records array. Defaults to the number of elements in the broadcasted shape. Set to a lower number if you run out of memory. max_logs :\u2002 int Size of the log records array. Defaults to the number of elements in the broadcasted shape if any of the log is True, otherwise to 1. Set to a lower number if you run out of memory. seed :\u2002 int Seed to be set for both call_seq and at the beginning of the simulation. group_by :\u2002 any Group columns. See ColumnGrouper . broadcast_kwargs :\u2002 dict Keyword arguments passed to broadcast() . wrapper_kwargs :\u2002 dict Keyword arguments passed to ArrayWrapper . freq :\u2002 any Index frequency in case it cannot be parsed from close . attach_call_seq :\u2002 bool Whether to pass call_seq to the constructor. Makes sense if you want to analyze some metrics in the simulation order. Otherwise, just takes memory. **kwargs Keyword arguments passed to the __init__ method. All broadcastable arguments will broadcast using broadcast() but keep original shape to utilize flexible indexing and to save memory. For defaults, see portfolio in settings . Note When call_seq is not CallSeqType.Auto , at each timestamp, processing of the assets in a group goes strictly in order defined in call_seq . This order can't be changed dynamically. This has one big implication for this particular method: the last asset in the call stack cannot be processed until other assets are processed. This is the reason why rebalancing cannot work properly in this setting: one has to specify percentages for all assets beforehand and then tweak the processing order to sell to-be-sold assets first in order to release funds for to-be-bought assets. This can be automatically done by using CallSeqType.Auto . Hint All broadcastable arguments can be set per frame, series, row, column, or element. Usage Buy 10 units each tick: >>> close = pd . Series ([ 1 , 2 , 3 , 4 , 5 ]) >>> pf = vbt . Portfolio . from_orders ( close , 10 ) >>> pf . assets () 0 10.0 1 20.0 2 30.0 3 40.0 4 40.0 dtype: float64 >>> pf . cash () 0 90.0 1 70.0 2 40.0 3 0.0 4 0.0 dtype: float64 Reverse each position by first closing it: >>> size = [ 1 , 0 , - 1 , 0 , 1 ] >>> pf = vbt . Portfolio . from_orders ( close , size , size_type = 'targetpercent' ) >>> pf . assets () 0 100.000000 1 0.000000 2 -66.666667 3 0.000000 4 26.666667 dtype: float64 >>> pf . cash () 0 0.000000 1 200.000000 2 400.000000 3 133.333333 4 0.000000 dtype: float64 Equal-weighted portfolio as in simulate_nb() example (it's more compact but has less control over execution): >>> np . random . seed ( 42 ) >>> close = pd . DataFrame ( np . random . uniform ( 1 , 10 , size = ( 5 , 3 ))) >>> size = pd . Series ( np . full ( 5 , 1 / 3 )) # each column 33.3% >>> size [ 1 :: 2 ] = np . nan # skip every second tick >>> pf = vbt . Portfolio . from_orders ( ... close , # acts both as reference and order price here ... size , ... size_type = 'targetpercent' , ... call_seq = 'auto' , # first sell then buy ... group_by = True , # one group ... cash_sharing = True , # assets share the same cash ... fees = 0.001 , fixed_fees = 1. , slippage = 0.001 # costs ... ) >>> pf . asset_value ( group_by = False ) . vbt . plot () from_random_signals class method \u00b6 Portfolio . from_random_signals ( close , n = None , prob = None , entry_prob = None , exit_prob = None , param_product = False , seed = None , run_kwargs = None , ** kwargs ) Simulate portfolio from random entry and exit signals. Generates signals based either on the number of signals n or the probability of encountering a signal prob . If n is set, see RANDNX . If prob is set, see RPROBNX . Based on Portfolio.from_signals() . Note To generate random signals, the shape of close is used. Broadcasting with other arrays happens after the generation. Usage Test multiple combinations of random entries and exits: >>> close = pd . Series ([ 1 , 2 , 3 , 4 , 5 ]) >>> pf = vbt . Portfolio . from_random_signals ( close , n = [ 2 , 1 , 0 ], seed = 42 ) >>> pf . orders . count () randnx_n 2 4 1 2 0 0 Name: count, dtype: int64 Test the Cartesian product of entry and exit encounter probabilities: >>> pf = vbt . Portfolio . from_random_signals ( ... close , ... entry_prob = [ 0 , 0.5 , 1 ], ... exit_prob = [ 0 , 0.5 , 1 ], ... param_product = True , ... seed = 42 ) >>> pf . orders . count () rprobnx_entry_prob rprobnx_exit_prob 0.0 0.0 0 0.5 0 1.0 0 0.5 0.0 1 0.5 4 1.0 3 1.0 0.0 1 0.5 4 1.0 5 Name: count, dtype: int64 from_signals class method \u00b6 Portfolio . from_signals ( close , entries = None , exits = None , short_entries = None , short_exits = None , signal_func_nb = no_signal_func_nb , signal_args = (), size = None , size_type = None , price = None , fees = None , fixed_fees = None , slippage = None , min_size = None , max_size = None , size_granularity = None , reject_prob = None , lock_cash = None , allow_partial = None , raise_reject = None , log = None , accumulate = None , upon_long_conflict = None , upon_short_conflict = None , upon_dir_conflict = None , upon_opposite_entry = None , direction = None , val_price = None , open = None , high = None , low = None , sl_stop = None , sl_trail = None , tp_stop = None , stop_entry_price = None , stop_exit_price = None , upon_stop_exit = None , upon_stop_update = None , adjust_sl_func_nb = no_adjust_sl_func_nb , adjust_sl_args = (), adjust_tp_func_nb = no_adjust_tp_func_nb , adjust_tp_args = (), use_stops = None , init_cash = None , cash_sharing = None , call_seq = None , ffill_val_price = None , update_value = None , max_orders = None , max_logs = None , seed = None , group_by = None , broadcast_named_args = None , broadcast_kwargs = None , template_mapping = None , wrapper_kwargs = None , freq = None , attach_call_seq = None , ** kwargs ) Simulate portfolio from entry and exit signals. See simulate_from_signal_func_nb() . You have three options to provide signals: entries and exits : The direction of each pair of signals is taken from direction argument. Best to use when the direction doesn't change throughout time. Uses dir_enex_signal_func_nb() as signal_func_nb . Hint entries and exits can be easily translated to direction-aware signals: (True, True, 'longonly') -> True, True, False, False (True, True, 'shortonly') -> False, False, True, True (True, True, 'both') -> True, False, True, False entries (acting as long), exits (acting as long), short_entries , and short_exits : The direction is already built into the arrays. Best to use when the direction changes frequently (for example, if you have one indicator providing long signals and one providing short signals). Uses ls_enex_signal_func_nb() as signal_func_nb . signal_func_nb and signal_args : Custom signal function that returns direction-aware signals. Best to use when signals should be placed dynamically based on custom conditions. Args close :\u2002 array_like See Portfolio.from_orders() . entries :\u2002 array_like of bool Boolean array of entry signals. Defaults to True if all other signal arrays are not set, otherwise False. Will broadcast. If short_entries and short_exits are not set: Acts as a long signal if direction is all or longonly , otherwise short. If short_entries or short_exits are set: Acts as long_entries . exits :\u2002 array_like of bool Boolean array of exit signals. Defaults to False. Will broadcast. If short_entries and short_exits are not set: Acts as a short signal if direction is all or longonly , otherwise long. If short_entries or short_exits are set: Acts as long_exits . short_entries :\u2002 array_like of bool Boolean array of short entry signals. Defaults to False. Will broadcast. short_exits :\u2002 array_like of bool Boolean array of short exit signals. Defaults to False. Will broadcast. signal_func_nb :\u2002 callable Function called to generate signals. Should accept SignalContext and *signal_args . Should return long entry signal, long exit signal, short entry signal, and short exit signal. Note Stop signal has priority: signal_func_nb is executed only if there is no stop signal. signal_args :\u2002 tuple Packed arguments passed to signal_func_nb . Defaults to () . size :\u2002 float or array_like See Portfolio.from_orders() . Note Negative size is not allowed. You should express direction using signals. size_type :\u2002 SizeType or array_like See Portfolio.from_orders() . Only SizeType.Amount , SizeType.Value , and SizeType.Percent are supported. Other modes such as target percentage are not compatible with signals since their logic may contradict the direction of the signal. Note SizeType.Percent does not support position reversal. Switch to a single direction or use vectorbt.portfolio.enums.OppositeEntryMode.Close to close the position first. See warning in Portfolio.from_orders() . price :\u2002 array_like of float See Portfolio.from_orders() . fees :\u2002 float or array_like See Portfolio.from_orders() . fixed_fees :\u2002 float or array_like See Portfolio.from_orders() . slippage :\u2002 float or array_like See Portfolio.from_orders() . min_size :\u2002 float or array_like See Portfolio.from_orders() . max_size :\u2002 float or array_like See Portfolio.from_orders() . Will be partially filled if exceeded. You might not be able to properly close the position if accumulation is enabled and max_size is too low. size_granularity :\u2002 float or array_like See Portfolio.from_orders() . reject_prob :\u2002 float or array_like See Portfolio.from_orders() . lock_cash :\u2002 bool or array_like See Portfolio.from_orders() . allow_partial :\u2002 bool or array_like See Portfolio.from_orders() . raise_reject :\u2002 bool or array_like See Portfolio.from_orders() . log :\u2002 bool or array_like See Portfolio.from_orders() . accumulate :\u2002 bool , AccumulationMode or array_like See AccumulationMode . If True, becomes 'both'. If False, becomes 'disabled'. Will broadcast. When enabled, Portfolio.from_signals() behaves similarly to Portfolio.from_orders() . upon_long_conflict :\u2002 ConflictMode or array_like Conflict mode for long signals. See ConflictMode . Will broadcast. upon_short_conflict :\u2002 ConflictMode or array_like Conflict mode for short signals. See ConflictMode . Will broadcast. upon_dir_conflict :\u2002 DirectionConflictMode or array_like See DirectionConflictMode . Will broadcast. upon_opposite_entry :\u2002 OppositeEntryMode or array_like See OppositeEntryMode . Will broadcast. direction :\u2002 Direction or array_like See Portfolio.from_orders() . Takes only effect if short_entries and short_exits are not set. val_price :\u2002 array_like of float See Portfolio.from_orders() . open :\u2002 array_like of float First asset price at each time step. Defaults to np.nan , which gets replaced by close . Will broadcast. Used solely for stop signals. high :\u2002 array_like of float Highest asset price at each time step. Defaults to np.nan , which gets replaced by the maximum out of open and close . Will broadcast. Used solely for stop signals. low :\u2002 array_like of float Lowest asset price at each time step. Defaults to np.nan , which gets replaced by the minimum out of open and close . Will broadcast. Used solely for stop signals. sl_stop :\u2002 array_like of float Stop loss. Will broadcast. A percentage below/above the acquisition price for long/short position. Note that 0.01 = 1%. sl_trail :\u2002 array_like of bool Whether sl_stop should be trailing. Will broadcast. tp_stop :\u2002 array_like of float Take profit. Will broadcast. A percentage above/below the acquisition price for long/short position. Note that 0.01 = 1%. stop_entry_price :\u2002 StopEntryPrice or array_like See StopEntryPrice . Will broadcast. If provided on per-element basis, gets applied upon entry. stop_exit_price :\u2002 StopExitPrice or array_like See StopExitPrice . Will broadcast. If provided on per-element basis, gets applied upon exit. upon_stop_exit :\u2002 StopExitMode or array_like See StopExitMode . Will broadcast. If provided on per-element basis, gets applied upon exit. upon_stop_update :\u2002 StopUpdateMode or array_like See StopUpdateMode . Will broadcast. Only has effect if accumulation is enabled. If provided on per-element basis, gets applied upon repeated entry. adjust_sl_func_nb :\u2002 callable Function to adjust stop loss. Defaults to no_adjust_sl_func_nb() . Called for each element before each row. Should accept AdjustSLContext and *adjust_sl_args . Should return a tuple of a new stop value and trailing flag. adjust_sl_args :\u2002 tuple Packed arguments passed to adjust_sl_func_nb . Defaults to () . adjust_tp_func_nb :\u2002 callable Function to adjust take profit. Defaults to no_adjust_tp_func_nb() . Called for each element before each row. Should accept AdjustTPContext and *adjust_tp_args . of the stop, and *adjust_tp_args . Should return a new stop value. adjust_tp_args :\u2002 tuple Packed arguments passed to adjust_tp_func_nb . Defaults to () . use_stops :\u2002 bool Whether to use stops. Defaults to None, which becomes True if any of the stops are not NaN or any of the adjustment functions are custom. Disable this to make simulation a bit faster for simple use cases. init_cash :\u2002 InitCashMode , float or array_like of float See Portfolio.from_orders() . cash_sharing :\u2002 bool See Portfolio.from_orders() . call_seq :\u2002 CallSeqType or array_like See Portfolio.from_orders() . ffill_val_price :\u2002 bool See Portfolio.from_orders() . update_value :\u2002 bool See Portfolio.from_orders() . max_orders :\u2002 int See Portfolio.from_orders() . max_logs :\u2002 int See Portfolio.from_orders() . seed :\u2002 int See Portfolio.from_orders() . group_by :\u2002 any See Portfolio.from_orders() . broadcast_named_args :\u2002 dict Dictionary with named arguments to broadcast. You can then pass argument names to the functions and this method will substitute them by their corresponding broadcasted objects. broadcast_kwargs :\u2002 dict See Portfolio.from_orders() . template_mapping :\u2002 mapping Mapping to replace templates in arguments. wrapper_kwargs :\u2002 dict See Portfolio.from_orders() . freq :\u2002 any See Portfolio.from_orders() . attach_call_seq :\u2002 bool See Portfolio.from_orders() . **kwargs Keyword arguments passed to the __init__ method. All broadcastable arguments will broadcast using broadcast() but keep original shape to utilize flexible indexing and to save memory. For defaults, see portfolio in settings . Note Stop signal has priority - it's executed before other signals within the same bar. That is, if a stop signal is present, no other signals are generated and executed since there is a limit of one order per symbol and bar. Hint If you generated signals using close price, don't forget to shift your signals by one tick forward, for example, with signals.vbt.fshift(1) . In general, make sure to use a price that comes after the signal. Also see notes and hints for Portfolio.from_orders() . Usage By default, if all signal arrays are None, entries becomes True, which opens a position at the very first tick and does nothing else: >>> close = pd . Series ([ 1 , 2 , 3 , 4 , 5 ]) >>> pf = vbt . Portfolio . from_signals ( close , size = 1 ) >>> pf . asset_flow () 0 1.0 1 0.0 2 0.0 3 0.0 4 0.0 dtype: float64 Entry opens long, exit closes long: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... size = 1 , ... direction = 'longonly' ... ) >>> pf . asset_flow () 0 1.0 1 0.0 2 0.0 3 -1.0 4 0.0 dtype: float64 >>> # Using direction-aware arrays instead of `direction` >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), # long_entries ... exits = pd . Series ([ False , False , True , True , True ]), # long_exits ... short_entries = False , ... short_exits = False , ... size = 1 ... ) >>> pf . asset_flow () 0 1.0 1 0.0 2 0.0 3 -1.0 4 0.0 dtype: float64 Notice how both short_entries and short_exits are provided as constants - as any other broadcastable argument, they are treated as arrays where each element is False. Entry opens short, exit closes short: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... size = 1 , ... direction = 'shortonly' ... ) >>> pf . asset_flow () 0 -1.0 1 0.0 2 0.0 3 1.0 4 0.0 dtype: float64 >>> # Using direction-aware arrays instead of `direction` >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = False , # long_entries ... exits = False , # long_exits ... short_entries = pd . Series ([ True , True , True , False , False ]), ... short_exits = pd . Series ([ False , False , True , True , True ]), ... size = 1 ... ) >>> pf . asset_flow () 0 -1.0 1 0.0 2 0.0 3 1.0 4 0.0 dtype: float64 Entry opens long and closes short, exit closes long and opens short: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... size = 1 , ... direction = 'both' ... ) >>> pf . asset_flow () 0 1.0 1 0.0 2 0.0 3 -2.0 4 0.0 dtype: float64 >>> # Using direction-aware arrays instead of `direction` >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), # long_entries ... exits = False , # long_exits ... short_entries = pd . Series ([ False , False , True , True , True ]), ... short_exits = False , ... size = 1 ... ) >>> pf . asset_flow () 0 1.0 1 0.0 2 0.0 3 -2.0 4 0.0 dtype: float64 More complex signal combinations are best expressed using direction-aware arrays. For example, ignore opposite signals as long as the current position is open: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , False , False , False , False ]), # long_entries ... exits = pd . Series ([ False , False , True , False , False ]), # long_exits ... short_entries = pd . Series ([ False , True , False , True , False ]), ... short_exits = pd . Series ([ False , False , False , False , True ]), ... size = 1 , ... upon_opposite_entry = 'ignore' ... ) >>> pf . asset_flow () 0 1.0 1 0.0 2 -1.0 3 -1.0 4 1.0 dtype: float64 First opposite signal closes the position, second one opens a new position: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... size = 1 , ... direction = 'both' , ... upon_opposite_entry = 'close' ... ) >>> pf . asset_flow () 0 1.0 1 0.0 2 0.0 3 -1.0 4 -1.0 dtype: float64 If both long entry and exit signals are True (a signal conflict), choose exit: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... size = 1. , ... direction = 'longonly' , ... upon_long_conflict = 'exit' ) >>> pf . asset_flow () 0 1.0 1 0.0 2 -1.0 3 0.0 4 0.0 dtype: float64 If both long entry and short entry signal are True (a direction conflict), choose short: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... size = 1. , ... direction = 'both' , ... upon_dir_conflict = 'short' ) >>> pf . asset_flow () 0 1.0 1 0.0 2 -2.0 3 0.0 4 0.0 dtype: float64 Note Remember that when direction is set to 'both', entries become long_entries and exits become short_entries , so this becomes a conflict of directions rather than signals. If there are both signal and direction conflicts: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = True , # long_entries ... exits = True , # long_exits ... short_entries = True , ... short_exits = True , ... size = 1 , ... upon_long_conflict = 'entry' , ... upon_short_conflict = 'entry' , ... upon_dir_conflict = 'short' ... ) >>> pf . asset_flow () 0 -1.0 1 0.0 2 0.0 3 0.0 4 0.0 dtype: float64 Turn on accumulation of signals. Entry means long order, exit means short order (acts similar to from_orders ): >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... size = 1. , ... direction = 'both' , ... accumulate = True ) >>> pf . asset_flow () 0 1.0 1 1.0 2 0.0 3 -1.0 4 -1.0 dtype: float64 Allow increasing a position (of any direction), deny decreasing a position: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... size = 1. , ... direction = 'both' , ... accumulate = 'addonly' ) >>> pf . asset_flow () 0 1.0 << open a long position 1 1.0 << add to the position 2 0.0 3 -3.0 << close and open a short position 4 -1.0 << add to the position dtype: float64 Testing multiple parameters (via broadcasting): >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... direction = [ list ( Direction )], ... broadcast_kwargs = dict ( columns_from = Direction . _fields )) >>> pf . asset_flow () Long Short All 0 100.0 -100.0 100.0 1 0.0 0.0 0.0 2 0.0 0.0 0.0 3 -100.0 50.0 -200.0 4 0.0 0.0 0.0 Set risk/reward ratio by passing trailing stop loss and take profit thresholds: >>> close = pd . Series ([ 10 , 11 , 12 , 11 , 10 , 9 ]) >>> entries = pd . Series ([ True , False , False , False , False , False ]) >>> exits = pd . Series ([ False , False , False , False , False , True ]) >>> pf = vbt . Portfolio . from_signals ( ... close , entries , exits , ... sl_stop = 0.1 , sl_trail = True , tp_stop = 0.2 ) # take profit hit >>> pf . asset_flow () 0 10.0 1 0.0 2 -10.0 3 0.0 4 0.0 5 0.0 dtype: float64 >>> pf = vbt . Portfolio . from_signals ( ... close , entries , exits , ... sl_stop = 0.1 , sl_trail = True , tp_stop = 0.3 ) # stop loss hit >>> pf . asset_flow () 0 10.0 1 0.0 2 0.0 3 0.0 4 -10.0 5 0.0 dtype: float64 >>> pf = vbt . Portfolio . from_signals ( ... close , entries , exits , ... sl_stop = np . inf , sl_trail = True , tp_stop = np . inf ) # nothing hit, exit as usual >>> pf . asset_flow () 0 10.0 1 0.0 2 0.0 3 0.0 4 0.0 5 -10.0 dtype: float64 Note When the stop price is hit, the stop signal invalidates any other signal defined for this bar. Thus, make sure that your signaling logic happens at the very end of the bar (for example, by using the closing price), otherwise you may expose yourself to a look-ahead bias. See StopExitPrice for more details. We can implement our own stop loss or take profit, or adjust the existing one at each time step. Let's implement stepped stop-loss : >>> @njit ... def adjust_sl_func_nb ( c ): ... current_profit = ( c . val_price_now - c . init_price ) / c . init_price ... if current_profit >= 0.40 : ... return 0.25 , True ... elif current_profit >= 0.25 : ... return 0.15 , True ... elif current_profit >= 0.20 : ... return 0.07 , True ... return c . curr_stop , c . curr_trail >>> close = pd . Series ([ 10 , 11 , 12 , 11 , 10 ]) >>> pf = vbt . Portfolio . from_signals ( close , adjust_sl_func_nb = adjust_sl_func_nb ) >>> pf . asset_flow () 0 10.0 1 0.0 2 0.0 3 -10.0 # 7% from 12 hit 4 11.0 dtype: float64 Sometimes there is a need to provide or transform signals dynamically. For this, we can implement a custom signal function signal_func_nb . For example, let's implement a signal function that takes two numerical arrays - long and short one - and transforms them into 4 direction-aware boolean arrays that vectorbt understands: >>> @njit ... def signal_func_nb ( c , long_num_arr , short_num_arr ): ... long_num = nb . get_elem_nb ( c , long_num_arr ) ... short_num = nb . get_elem_nb ( c , short_num_arr ) ... is_long_entry = long_num > 0 ... is_long_exit = long_num < 0 ... is_short_entry = short_num > 0 ... is_short_exit = short_num < 0 ... return is_long_entry , is_long_exit , is_short_entry , is_short_exit >>> pf = vbt . Portfolio . from_signals ( ... pd . Series ([ 1 , 2 , 3 , 4 , 5 ]), ... signal_func_nb = signal_func_nb , ... signal_args = ( vbt . Rep ( 'long_num_arr' ), vbt . Rep ( 'short_num_arr' )), ... broadcast_named_args = dict ( ... long_num_arr = pd . Series ([ 1 , 0 , - 1 , 0 , 0 ]), ... short_num_arr = pd . Series ([ 0 , 1 , 0 , 1 , - 1 ]) ... ), ... size = 1 , ... upon_opposite_entry = 'ignore' ... ) >>> pf . asset_flow () 0 1.0 1 0.0 2 -1.0 3 -1.0 4 1.0 dtype: float64 Passing both arrays as broadcast_named_args broadcasts them internally as any other array, so we don't have to worry about their dimensions every time we change our data. get_drawdowns method \u00b6 Portfolio . get_drawdowns ( group_by = None , wrap_kwargs = None , wrapper_kwargs = None , ** kwargs ) Get drawdown records from Portfolio.value() . See Drawdowns . get_entry_trades method \u00b6 Portfolio . get_entry_trades ( group_by = None , ** kwargs ) Get entry trade records. See EntryTrades . get_exit_trades method \u00b6 Portfolio . get_exit_trades ( group_by = None , ** kwargs ) Get exit trade records. See ExitTrades . get_filled_close method \u00b6 Portfolio . get_filled_close ( wrap_kwargs = None ) Forward-backward-fill NaN values in Portfolio.close get_init_cash method \u00b6 Portfolio . get_init_cash ( group_by = None , wrap_kwargs = None ) Initial amount of cash per column/group with default arguments. Note If the initial cash balance was found automatically and no own cash is used throughout the simulation (for example, when shorting), it will be set to 1 instead of 0 to enable smooth calculation of returns. get_logs method \u00b6 Portfolio . get_logs ( group_by = None , ** kwargs ) Get log records. See Logs . get_orders method \u00b6 Portfolio . get_orders ( group_by = None , ** kwargs ) Get order records. See Orders . get_positions method \u00b6 Portfolio . get_positions ( group_by = None , ** kwargs ) Get position records. See Positions . get_qs method \u00b6 Portfolio . get_qs ( group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) Get quantstats adapter of type QSAdapter . **kwargs are passed to the adapter constructor. get_returns_acc method \u00b6 Portfolio . get_returns_acc ( group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , defaults = None , ** kwargs ) Get returns accessor of type ReturnsAccessor . Hint You can find most methods of this accessor as (cacheable) attributes of this portfolio. get_trades method \u00b6 Portfolio . get_trades ( group_by = None , ** kwargs ) Get trade/position records depending upon Portfolio.trades_type . gross_exposure method \u00b6 Portfolio . gross_exposure ( direction = 'both' , group_by = None , wrap_kwargs = None ) Get gross exposure. indexing_func method \u00b6 Portfolio . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on Portfolio . information_ratio method \u00b6 Portfolio . information_ratio ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.information_ratio() . init_cash method \u00b6 Portfolio.get_init_cash() with default arguments. log_records property \u00b6 A structured NumPy array of log records. logs method \u00b6 Portfolio.get_logs() with default arguments. max_drawdown method \u00b6 Portfolio . max_drawdown ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.max_drawdown() . metrics class variable \u00b6 Metrics supported by Portfolio . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2c8c8>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2c950>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2c9d8>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"start_value\" : { \"title\" : \"Start Value\" , \"calc_func\" : \"get_init_cash\" , \"tags\" : \"portfolio\" }, \"end_value\" : { \"title\" : \"End Value\" , \"calc_func\" : \"final_value\" , \"tags\" : \"portfolio\" }, \"total_return\" : { \"title\" : \"Total Return [%]\" , \"calc_func\" : \"total_return\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2ca60>\" , \"tags\" : \"portfolio\" }, \"benchmark_return\" : { \"title\" : \"Benchmark Return [%]\" , \"calc_func\" : \"benchmark_rets.vbt.returns.total\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2cae8>\" , \"tags\" : \"portfolio\" }, \"max_gross_exposure\" : { \"title\" : \"Max Gross Exposure [%]\" , \"calc_func\" : \"gross_exposure.vbt.max\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2cb70>\" , \"tags\" : \"portfolio\" }, \"total_fees_paid\" : { \"title\" : \"Total Fees Paid\" , \"calc_func\" : \"orders.fees.sum\" , \"tags\" : [ \"portfolio\" , \"orders\" ] }, \"max_dd\" : { \"title\" : \"Max Drawdown [%]\" , \"calc_func\" : \"drawdowns.max_drawdown\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2cbf8>\" , \"tags\" : [ \"portfolio\" , \"drawdowns\" ] }, \"max_dd_duration\" : { \"title\" : \"Max Drawdown Duration\" , \"calc_func\" : \"drawdowns.max_duration\" , \"fill_wrap_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"drawdowns\" , \"duration\" ] }, \"total_trades\" : { \"title\" : \"Total Trades\" , \"calc_func\" : \"trades.count\" , \"incl_open\" : true , \"tags\" : [ \"portfolio\" , \"trades\" ] }, \"total_closed_trades\" : { \"title\" : \"Total Closed Trades\" , \"calc_func\" : \"trades.closed.count\" , \"tags\" : [ \"portfolio\" , \"trades\" , \"closed\" ] }, \"total_open_trades\" : { \"title\" : \"Total Open Trades\" , \"calc_func\" : \"trades.open.count\" , \"incl_open\" : true , \"tags\" : [ \"portfolio\" , \"trades\" , \"open\" ] }, \"open_trade_pnl\" : { \"title\" : \"Open Trade PnL\" , \"calc_func\" : \"trades.open.pnl.sum\" , \"incl_open\" : true , \"tags\" : [ \"portfolio\" , \"trades\" , \"open\" ] }, \"win_rate\" : { \"title\" : \"Win Rate [%]\" , \"calc_func\" : \"trades.win_rate\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2cc80>\" , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags]\\\", mapping={})\" }, \"best_trade\" : { \"title\" : \"Best Trade [%]\" , \"calc_func\" : \"trades.returns.max\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2cd08>\" , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags]\\\", mapping={})\" }, \"worst_trade\" : { \"title\" : \"Worst Trade [%]\" , \"calc_func\" : \"trades.returns.min\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2cd90>\" , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags]\\\", mapping={})\" }, \"avg_winning_trade\" : { \"title\" : \"Avg Winning Trade [%]\" , \"calc_func\" : \"trades.winning.returns.mean\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2ce18>\" , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags, 'winning']\\\", mapping={})\" }, \"avg_losing_trade\" : { \"title\" : \"Avg Losing Trade [%]\" , \"calc_func\" : \"trades.losing.returns.mean\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2cea0>\" , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags, 'losing']\\\", mapping={})\" }, \"avg_winning_trade_duration\" : { \"title\" : \"Avg Winning Trade Duration\" , \"calc_func\" : \"trades.winning.duration.mean\" , \"apply_to_timedelta\" : true , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags, 'winning', 'duration']\\\", mapping={})\" }, \"avg_losing_trade_duration\" : { \"title\" : \"Avg Losing Trade Duration\" , \"calc_func\" : \"trades.losing.duration.mean\" , \"apply_to_timedelta\" : true , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags, 'losing', 'duration']\\\", mapping={})\" }, \"profit_factor\" : { \"title\" : \"Profit Factor\" , \"calc_func\" : \"trades.profit_factor\" , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags]\\\", mapping={})\" }, \"expectancy\" : { \"title\" : \"Expectancy\" , \"calc_func\" : \"trades.expectancy\" , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags]\\\", mapping={})\" }, \"sharpe_ratio\" : { \"title\" : \"Sharpe Ratio\" , \"calc_func\" : \"returns_acc.sharpe_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : [ \"portfolio\" , \"returns\" ] }, \"calmar_ratio\" : { \"title\" : \"Calmar Ratio\" , \"calc_func\" : \"returns_acc.calmar_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : [ \"portfolio\" , \"returns\" ] }, \"omega_ratio\" : { \"title\" : \"Omega Ratio\" , \"calc_func\" : \"returns_acc.omega_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : [ \"portfolio\" , \"returns\" ] }, \"sortino_ratio\" : { \"title\" : \"Sortino Ratio\" , \"calc_func\" : \"returns_acc.sortino_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : [ \"portfolio\" , \"returns\" ] } } ) Returns Portfolio._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable Portfolio._metrics . net_exposure method \u00b6 Portfolio . net_exposure ( group_by = None , wrap_kwargs = None ) Get net exposure. omega_ratio method \u00b6 Portfolio . omega_ratio ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.omega_ratio() . order_records property \u00b6 A structured NumPy array of order records. orders method \u00b6 Portfolio.get_orders() with default arguments. plot method \u00b6 PlotsBuilderMixin . plots ( subplots = None , tags = None , column = None , group_by = None , silence_warnings = None , template_mapping = None , settings = None , filters = None , subplot_settings = None , show_titles = None , hide_id_labels = None , group_id_labels = None , make_subplots_kwargs = None , ** layout_kwargs ) See PlotsBuilderMixin.plots() . plot_asset_flow method \u00b6 Portfolio . plot_asset_flow ( column = None , direction = 'both' , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column of asset flow. Args column :\u2002 str Name of the column to plot. direction :\u2002 Direction See Direction . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericAccessor.plot() . plot_asset_value method \u00b6 Portfolio . plot_asset_value ( column = None , group_by = None , direction = 'both' , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column/group of asset value. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . direction :\u2002 Direction See Direction . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericSRAccessor.plot_against() . plot_assets method \u00b6 Portfolio . plot_assets ( column = None , direction = 'both' , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column of assets. Args column :\u2002 str Name of the column to plot. direction :\u2002 Direction See Direction . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericSRAccessor.plot_against() . plot_cash method \u00b6 Portfolio . plot_cash ( column = None , group_by = None , free = False , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column/group of cash balance. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . free :\u2002 bool Whether to plot the flow of the free cash. xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericSRAccessor.plot_against() . plot_cash_flow method \u00b6 Portfolio . plot_cash_flow ( column = None , group_by = None , free = False , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column/group of cash flow. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . free :\u2002 bool Whether to plot the flow of the free cash. xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericAccessor.plot() . plot_cum_returns method \u00b6 Portfolio . plot_cum_returns ( column = None , group_by = None , benchmark_rets = None , use_asset_returns = False , ** kwargs ) Plot one column/group of cumulative returns. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . benchmark_rets :\u2002 array_like Benchmark returns. If None, will use Portfolio.benchmark_returns() . use_asset_returns :\u2002 bool Whether to plot asset returns. **kwargs Keyword arguments passed to ReturnsSRAccessor.plot_cumulative() . plot_drawdowns method \u00b6 Portfolio . plot_drawdowns ( column = None , group_by = None , ** kwargs ) Plot one column/group of drawdowns. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . **kwargs Keyword arguments passed to Drawdowns.plot() . plot_gross_exposure method \u00b6 Portfolio . plot_gross_exposure ( column = None , group_by = None , direction = 'both' , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column/group of gross exposure. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . direction :\u2002 Direction See Direction . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericSRAccessor.plot_against() . plot_net_exposure method \u00b6 Portfolio . plot_net_exposure ( column = None , group_by = None , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column/group of net exposure. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericSRAccessor.plot_against() . plot_orders method \u00b6 Portfolio . plot_orders ( column = None , ** kwargs ) Plot one column/group of orders. plot_position_pnl method \u00b6 Portfolio . plot_position_pnl ( column = None , ** kwargs ) Plot one column/group of position PnL. plot_positions method \u00b6 Portfolio . plot_positions ( column = None , ** kwargs ) Plot one column/group of positions. plot_trade_pnl method \u00b6 Portfolio . plot_trade_pnl ( column = None , ** kwargs ) Plot one column/group of trade PnL. plot_trades method \u00b6 Portfolio . plot_trades ( column = None , ** kwargs ) Plot one column/group of trades. plot_underwater method \u00b6 Portfolio . plot_underwater ( column = None , group_by = None , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column/group of underwater. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericAccessor.plot() . plot_value method \u00b6 Portfolio . plot_value ( column = None , group_by = None , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column/group of value. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . free :\u2002 bool Whether to plot free cash flow. xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericSRAccessor.plot_against() . plots_defaults property \u00b6 Defaults for PlotsBuilderMixin.plots() . Merges PlotsBuilderMixin.plots_defaults and portfolio.plots from settings . position_coverage method \u00b6 Portfolio . position_coverage ( direction = 'both' , group_by = None , wrap_kwargs = None ) Get position coverage per column/group. position_mask method \u00b6 Portfolio . position_mask ( direction = 'both' , group_by = None , wrap_kwargs = None ) Get position mask per column/group. An element is True if the asset is in the market at this tick. positions method \u00b6 Portfolio.get_positions() with default arguments. post_resolve_attr method \u00b6 Portfolio . post_resolve_attr ( attr , out , final_kwargs = None ) Post-process an object after resolution. Uses the following keys: incl_open : Whether to include open trades/positions when resolving an argument that is an instance of Trades . pre_resolve_attr method \u00b6 Portfolio . pre_resolve_attr ( attr , final_kwargs = None ) Pre-process an attribute before resolution. Uses the following keys: use_asset_returns : Whether to use Portfolio.asset_returns() when resolving returns argument. trades_type : Which trade type to use when resolving trades argument. qs method \u00b6 Portfolio.get_qs() with default arguments. regroup method \u00b6 Portfolio . regroup ( group_by , ** kwargs ) Regroup this object. See Wrapping.regroup() . Note All cached objects will be lost. returns method \u00b6 Portfolio . returns ( group_by = None , in_sim_order = False , wrap_kwargs = None ) Get return series per column/group based on portfolio value. returns_acc property \u00b6 Portfolio.get_returns_acc() with default arguments. returns_stats method \u00b6 Portfolio . returns_stats ( group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , defaults = None , ** kwargs ) Compute various statistics on returns of this portfolio. See Portfolio.returns_acc and ReturnsAccessor.metrics . kwargs will be passed to StatsBuilderMixin.stats() method. If benchmark_rets is not set, uses Portfolio.benchmark_returns() . sharpe_ratio method \u00b6 Portfolio . sharpe_ratio ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.sharpe_ratio() . sortino_ratio method \u00b6 Portfolio . sortino_ratio ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.sortino_ratio() . stats_defaults property \u00b6 Defaults for StatsBuilderMixin.stats() . Merges StatsBuilderMixin.stats_defaults and portfolio.stats from settings . subplots class variable \u00b6 Subplots supported by Portfolio . Co nf ig( { \"orders\" : { \"title\" : \"Orders\" , \"yaxis_kwargs\" : { \"title\" : \"Price\" }, \"check_is_not_grouped\" : true , \"plot_func\" : \"orders.plot\" , \"tags\" : [ \"portfolio\" , \"orders\" ] }, \"trades\" : { \"title\" : \"Trades\" , \"yaxis_kwargs\" : { \"title\" : \"Price\" }, \"check_is_not_grouped\" : true , \"plot_func\" : \"trades.plot\" , \"tags\" : [ \"portfolio\" , \"trades\" ] }, \"trade_pnl\" : { \"title\" : \"Trade PnL\" , \"yaxis_kwargs\" : { \"title\" : \"Trade PnL\" }, \"check_is_not_grouped\" : true , \"plot_func\" : \"trades.plot_pnl\" , \"tags\" : [ \"portfolio\" , \"trades\" ] }, \"asset_flow\" : { \"title\" : \"Asset Flow\" , \"yaxis_kwargs\" : { \"title\" : \"Asset flow\" }, \"check_is_not_grouped\" : true , \"plot_func\" : \"plot_asset_flow\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"assets\" ] }, \"cash_flow\" : { \"title\" : \"Cash Flow\" , \"yaxis_kwargs\" : { \"title\" : \"Cash flow\" }, \"plot_func\" : \"plot_cash_flow\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"cash\" ] }, \"assets\" : { \"title\" : \"Assets\" , \"yaxis_kwargs\" : { \"title\" : \"Assets\" }, \"check_is_not_grouped\" : true , \"plot_func\" : \"plot_assets\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"assets\" ] }, \"cash\" : { \"title\" : \"Cash\" , \"yaxis_kwargs\" : { \"title\" : \"Cash\" }, \"plot_func\" : \"plot_cash\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"cash\" ] }, \"asset_value\" : { \"title\" : \"Asset Value\" , \"yaxis_kwargs\" : { \"title\" : \"Asset value\" }, \"plot_func\" : \"plot_asset_value\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"assets\" , \"value\" ] }, \"value\" : { \"title\" : \"Value\" , \"yaxis_kwargs\" : { \"title\" : \"Value\" }, \"plot_func\" : \"plot_value\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"value\" ] }, \"cum_returns\" : { \"title\" : \"Cumulative Returns\" , \"yaxis_kwargs\" : { \"title\" : \"Cumulative returns\" }, \"plot_func\" : \"plot_cum_returns\" , \"pass_hline_shape_kwargs\" : true , \"pass_add_trace_kwargs\" : true , \"pass_xref\" : true , \"pass_yref\" : true , \"tags\" : [ \"portfolio\" , \"returns\" ] }, \"drawdowns\" : { \"title\" : \"Drawdowns\" , \"yaxis_kwargs\" : { \"title\" : \"Value\" }, \"plot_func\" : \"plot_drawdowns\" , \"pass_add_trace_kwargs\" : true , \"pass_xref\" : true , \"pass_yref\" : true , \"tags\" : [ \"portfolio\" , \"value\" , \"drawdowns\" ] }, \"underwater\" : { \"title\" : \"Underwater\" , \"yaxis_kwargs\" : { \"title\" : \"Drawdown\" }, \"plot_func\" : \"plot_underwater\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"value\" , \"drawdowns\" ] }, \"gross_exposure\" : { \"title\" : \"Gross Exposure\" , \"yaxis_kwargs\" : { \"title\" : \"Gross exposure\" }, \"plot_func\" : \"plot_gross_exposure\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"exposure\" ] }, \"net_exposure\" : { \"title\" : \"Net Exposure\" , \"yaxis_kwargs\" : { \"title\" : \"Net exposure\" }, \"plot_func\" : \"plot_net_exposure\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"exposure\" ] } } ) Returns Portfolio._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable Portfolio._subplots . tail_ratio method \u00b6 Portfolio . tail_ratio ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.tail_ratio() . total_benchmark_return method \u00b6 Portfolio . total_benchmark_return ( group_by = None , wrap_kwargs = None ) Get total benchmark return. total_profit method \u00b6 Portfolio . total_profit ( group_by = None , wrap_kwargs = None ) Get total profit per column/group. Calculated directly from order records (fast). total_return method \u00b6 Portfolio . total_return ( group_by = None , wrap_kwargs = None ) Get total profit per column/group. trades method \u00b6 Portfolio.get_trades() with default arguments. trades_type property \u00b6 Default Trades to use across Portfolio . up_capture method \u00b6 Portfolio . up_capture ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.up_capture() . value method \u00b6 Portfolio . value ( group_by = None , in_sim_order = False , wrap_kwargs = None ) Get portfolio value series per column/group. By default, will generate portfolio value for each asset based on cash flows and thus independent from other assets, with the initial cash balance and position being that of the entire group. Useful for generating returns and comparing assets within the same group. When group_by is False and in_sim_order is True, returns value generated in simulation order (see row-major order . This value cannot be used for generating returns as-is. Useful to analyze how value evolved throughout simulation. value_at_risk method \u00b6 Portfolio . value_at_risk ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.value_at_risk() .","title":"base"},{"location":"api/portfolio/base/#vectorbt.portfolio.base","text":"Base class for modeling portfolio and measuring its performance. Provides the class Portfolio for modeling portfolio performance and calculating various risk and performance metrics. It uses Numba-compiled functions from vectorbt.portfolio.nb for most computations and record classes based on Records for evaluating events such as orders, logs, trades, positions, and drawdowns. The job of the Portfolio class is to create a series of positions allocated against a cash component, produce an equity curve, incorporate basic transaction costs and produce a set of statistics about its performance. In particular, it outputs position/profit metrics and drawdown information. Run for the examples below: >>> import numpy as np >>> import pandas as pd >>> from datetime import datetime >>> import talib >>> from numba import njit >>> import vectorbt as vbt >>> from vectorbt.utils.colors import adjust_opacity >>> from vectorbt.utils.enum_ import map_enum_fields >>> from vectorbt.base.reshape_fns import broadcast , flex_select_auto_nb , to_2d_array >>> from vectorbt.portfolio.enums import SizeType , Direction , NoOrder , OrderStatus , OrderSide >>> from vectorbt.portfolio import nb","title":"vectorbt.portfolio.base"},{"location":"api/portfolio/base/#workflow","text":"Portfolio class does quite a few things to simulate your strategy. Preparation phase (in the particular class method): Receives a set of inputs, such as signal arrays and other parameters Resolves parameter defaults by searching for them in the global settings Brings input arrays to a single shape Does some basic validation of inputs and converts Pandas objects to NumPy arrays Passes everything to a Numba-compiled simulation function Simulation phase (in the particular simulation function using Numba): The simulation function traverses the broadcasted shape element by element, row by row (time dimension), column by column (asset dimension) For each asset and timestamp (= element): Gets all available information related to this element and executes the logic Generates an order or skips the element altogether If an order has been issued, processes the order and fills/ignores/rejects it If the order has been filled, registers the result by appending it to the order records Updates the current state such as the cash and asset balances Construction phase (in the particular class method): Receives the returned order records and initializes a new Portfolio object Analysis phase (in the Portfolio object) Offers a broad range of risk & performance metrics based on order records","title":"Workflow"},{"location":"api/portfolio/base/#simulation-modes","text":"There are three main simulation modes.","title":"Simulation modes"},{"location":"api/portfolio/base/#from-orders","text":"Portfolio.from_orders() is the most straightforward and the fastest out of all simulation modes. An order is a simple instruction that contains size, price, fees, and other information (see Order for details about what information a typical order requires). Instead of creating a Order tuple for each asset and timestamp (which may waste a lot of memory) and appending it to a (potentially huge) list for processing, Portfolio.from_orders() takes each of those information pieces as an array, broadcasts them against each other, and creates a Order tuple out of each element for us. Thanks to broadcasting, we can pass any of the information as a 2-dim array, as a 1-dim array per column or row, and as a constant. And we don't even need to provide every piece of information - vectorbt fills the missing data with default constants, without wasting memory. Here's an example: >>> size = pd . Series ([ 1 , - 1 , 1 , - 1 ]) # per row >>> price = pd . DataFrame ({ 'a' : [ 1 , 2 , 3 , 4 ], 'b' : [ 4 , 3 , 2 , 1 ]}) # per element >>> direction = [ 'longonly' , 'shortonly' ] # per column >>> fees = 0.01 # per frame >>> pf = vbt . Portfolio . from_orders ( price , size , direction = direction , fees = fees ) >>> pf . orders . records_readable Order Id Column Timestamp Size Price Fees Side 0 0 a 0 1.0 1.0 0.01 Buy 1 1 a 1 1.0 2.0 0.02 Sell 2 2 a 2 1.0 3.0 0.03 Buy 3 3 a 3 1.0 4.0 0.04 Sell 4 4 b 0 1.0 4.0 0.04 Sell 5 5 b 1 1.0 3.0 0.03 Buy 6 6 b 2 1.0 2.0 0.02 Sell 7 7 b 3 1.0 1.0 0.01 Buy This method is particularly useful in situations where you don't need any further logic apart from filling orders at predefined timestamps. If you want to issue orders depending upon the previous performance, the current state, or other custom conditions, head over to Portfolio.from_signals() or Portfolio.from_order_func() .","title":"From orders"},{"location":"api/portfolio/base/#from-signals","text":"Portfolio.from_signals() is centered around signals. It adds an abstraction layer on top of Portfolio.from_orders() to automate some signaling processes. For example, by default, it won't let us execute another entry signal if we are already in the position. It also implements stop loss and take profit orders for exiting positions. Nevertheless, this method behaves similarly to Portfolio.from_orders() and accepts most of its arguments; in fact, by setting accumulate=True , it behaves quite similarly to Portfolio.from_orders() . Let's replicate the example above using signals: >>> entries = pd . Series ([ True , False , True , False ]) >>> exits = pd . Series ([ False , True , False , True ]) >>> pf = vbt . Portfolio . from_signals ( price , entries , exits , size = 1 , direction = direction , fees = fees ) >>> pf . orders . records_readable Order Id Column Timestamp Size Price Fees Side 0 0 a 0 1.0 1.0 0.01 Buy 1 1 a 1 1.0 2.0 0.02 Sell 2 2 a 2 1.0 3.0 0.03 Buy 3 3 a 3 1.0 4.0 0.04 Sell 4 4 b 0 1.0 4.0 0.04 Sell 5 5 b 1 1.0 3.0 0.03 Buy 6 6 b 2 1.0 2.0 0.02 Sell 7 7 b 3 1.0 1.0 0.01 Buy In a nutshell: this method automates some procedures that otherwise would be only possible by using Portfolio.from_order_func() while following the same broadcasting principles as Portfolio.from_orders() - the best of both worlds, given you can express your strategy as a sequence of signals. But as soon as your strategy requires any signal to depend upon more complex conditions or to generate multiple orders at once, it's best to run your custom signaling logic using Portfolio.from_order_func() .","title":"From signals"},{"location":"api/portfolio/base/#from-order-function","text":"Portfolio.from_order_func() is the most powerful form of simulation. Instead of pulling information from predefined arrays, it lets us define an arbitrary logic through callbacks. There are multiple kinds of callbacks, each called at some point while the simulation function traverses the shape. For example, apart from the main callback that returns an order ( order_func_nb ), there is a callback that does preprocessing on the entire group of columns at once. For more details on the general procedure and the callback zoo, see simulate_nb() . Let's replicate our example using an order function: >>> @njit >>> def order_func_nb ( c , size , direction , fees ): ... return nb . order_nb ( ... price = c . close [ c . i , c . col ], ... size = size [ c . i ], ... direction = direction [ c . col ], ... fees = fees ... ) >>> direction_num = map_enum_fields ( direction , Direction ) >>> pf = vbt . Portfolio . from_order_func ( ... price , ... order_func_nb , ... np . asarray ( size ), np . asarray ( direction_num ), fees ... ) >>> pf . orders . records_readable Order Id Column Timestamp Size Price Fees Side 0 0 a 0 1.0 1.0 0.01 Buy 1 1 a 1 1.0 2.0 0.02 Sell 2 2 a 2 1.0 3.0 0.03 Buy 3 3 a 3 1.0 4.0 0.04 Sell 4 4 b 0 1.0 4.0 0.04 Sell 5 5 b 1 1.0 3.0 0.03 Buy 6 6 b 2 1.0 2.0 0.02 Sell 7 7 b 3 1.0 1.0 0.01 Buy There is an even more flexible version available - flex_simulate_nb() (activated by passing flexible=True to Portfolio.from_order_func() ) - that allows creating multiple orders per symbol and bar. This method has many advantages: Realistic simulation as it follows the event-driven approach - less risk of exposure to the look-ahead bias Provides a lot of useful information during the runtime, such as the current position's PnL Enables putting all logic including custom indicators into a single place, and running it as the data comes in, in a memory-friendly manner But there are drawbacks too: Doesn't broadcast arrays - needs to be done by the user prior to the execution Requires at least a basic knowledge of NumPy and Numba Requires at least an intermediate knowledge of both to optimize for efficiency","title":"From order function"},{"location":"api/portfolio/base/#example","text":"To showcase the features of Portfolio , run the following example: it checks candlestick data of 6 major cryptocurrencies in 2020 against every single pattern found in TA-Lib, and translates them into orders. >>> # Fetch price history >>> symbols = [ 'BTC-USD' , 'ETH-USD' , 'XRP-USD' , 'BNB-USD' , 'BCH-USD' , 'LTC-USD' ] >>> start = '2020-01-01 UTC' # crypto is UTC >>> end = '2020-09-01 UTC' >>> # OHLCV by column >>> ohlcv = vbt . YFData . download ( symbols , start = start , end = end ) . concat () >>> ohlcv [ 'Open' ] symbol BTC-USD ETH-USD XRP-USD BNB-USD \\ Date 2020-01-01 00:00:00+00:00 7194.892090 129.630661 0.192912 13.730962 2020-01-02 00:00:00+00:00 7202.551270 130.820038 0.192708 13.698126 2020-01-03 00:00:00+00:00 6984.428711 127.411263 0.187948 13.035329 ... ... ... ... ... 2020-08-30 00:00:00+00:00 11508.713867 399.616699 0.274568 23.009060 2020-08-31 00:00:00+00:00 11713.306641 428.509003 0.283065 23.647858 2020-09-01 00:00:00+00:00 11679.316406 434.874451 0.281612 23.185047 symbol BCH-USD LTC-USD Date 2020-01-01 00:00:00+00:00 204.671295 41.326534 2020-01-02 00:00:00+00:00 204.354538 42.018085 2020-01-03 00:00:00+00:00 196.007690 39.863129 ... ... ... 2020-08-30 00:00:00+00:00 268.842865 57.207737 2020-08-31 00:00:00+00:00 279.280426 62.844059 2020-09-01 00:00:00+00:00 274.480865 61.105076 [244 rows x 6 columns] >>> # Run every single pattern recognition indicator and combine the results >>> result = pd . DataFrame . vbt . empty_like ( ohlcv [ 'Open' ], fill_value = 0. ) >>> for pattern in talib . get_function_groups ()[ 'Pattern Recognition' ]: ... PRecognizer = vbt . IndicatorFactory . from_talib ( pattern ) ... pr = PRecognizer . run ( ohlcv [ 'Open' ], ohlcv [ 'High' ], ohlcv [ 'Low' ], ohlcv [ 'Close' ]) ... result = result + pr . integer >>> # Don't look into the future >>> result = result . vbt . fshift ( 1 ) >>> # Treat each number as order value in USD >>> size = result / ohlcv [ 'Open' ] >>> # Simulate portfolio >>> pf = vbt . Portfolio . from_orders ( ... ohlcv [ 'Close' ], size , price = ohlcv [ 'Open' ], ... init_cash = 'autoalign' , fees = 0.001 , slippage = 0.001 ) >>> # Visualize portfolio value >>> pf . value () . vbt . plot ()","title":"Example"},{"location":"api/portfolio/base/#broadcasting","text":"Portfolio is very flexible towards inputs: Accepts both Series and DataFrames as inputs Broadcasts inputs to the same shape using vectorbt's own broadcasting rules Many inputs (such as fees ) can be passed as a single value, value per column/row, or as a matrix Implements flexible indexing wherever possible to save memory","title":"Broadcasting"},{"location":"api/portfolio/base/#flexible-indexing","text":"Instead of expensive broadcasting, most methods keep the original shape and do indexing in a smart way. A nice feature of this is that it has almost no memory footprint and can broadcast in any direction indefinitely. For example, let's broadcast three inputs and select the last element using both approaches: >>> # Classic way >>> a = np . array ([ 1 , 2 , 3 ]) >>> b = np . array ([[ 4 ], [ 5 ], [ 6 ]]) >>> c = np . array ( 10 ) >>> a_ , b_ , c_ = broadcast ( a , b , c ) >>> a_ array([[1, 2, 3], [1, 2, 3], [1, 2, 3]]) >>> a_ [ 2 , 2 ] 3 >>> b_ array([[4, 4, 4], [5, 5, 5], [6, 6, 6]]) >>> b_ [ 2 , 2 ] 6 >>> c_ array([[10, 10, 10], [10, 10, 10], [10, 10, 10]]) >>> c_ [ 2 , 2 ] 10 >>> # Flexible indexing being done during simulation >>> flex_select_auto_nb ( a , 2 , 2 ) 3 >>> flex_select_auto_nb ( b , 2 , 2 ) 6 >>> flex_select_auto_nb ( c , 2 , 2 ) 10","title":"Flexible indexing"},{"location":"api/portfolio/base/#defaults","text":"If you look at the arguments of each class method, you will notice that most of them default to None. None has a special meaning in vectorbt: it's a command to pull the default value from the global settings config - settings . The branch for the Portfolio can be found under the key 'portfolio'. For example, the default size used in Portfolio.from_signals() and Portfolio.from_orders() is np.inf : >>> vbt . settings . portfolio [ 'size' ] inf","title":"Defaults"},{"location":"api/portfolio/base/#grouping","text":"One of the key features of Portfolio is the ability to group columns. Groups can be specified by group_by , which can be anything from positions or names of column levels, to a NumPy array with actual groups. Groups can be formed to share capital between columns (make sure to pass cash_sharing=True ) or to compute metrics for a combined portfolio of multiple independent columns. For example, let's divide our portfolio into two groups sharing the same cash balance: >>> # Simulate combined portfolio >>> group_by = pd . Index ([ ... 'first' , 'first' , 'first' , ... 'second' , 'second' , 'second' ... ], name = 'group' ) >>> comb_pf = vbt . Portfolio . from_orders ( ... ohlcv [ 'Close' ], size , price = ohlcv [ 'Open' ], ... init_cash = 'autoalign' , fees = 0.001 , slippage = 0.001 , ... group_by = group_by , cash_sharing = True ) >>> # Get total profit per group >>> comb_pf . total_profit () group first 26221.571200 second 10141.952674 Name: total_profit, dtype: float64 Not only can we analyze each group, but also each column in the group: >>> # Get total profit per column >>> comb_pf . total_profit ( group_by = False ) symbol BTC-USD 5792.120252 ETH-USD 16380.039692 XRP-USD 4049.411256 BNB-USD 6081.253551 BCH-USD 400.573418 LTC-USD 3660.125705 Name: total_profit, dtype: float64 In the same way, we can introduce new grouping to the method itself: >>> # Get total profit per group >>> pf . total_profit ( group_by = group_by ) group first 26221.571200 second 10141.952674 Name: total_profit, dtype: float64 Note If cash sharing is enabled, grouping can be disabled but cannot be modified.","title":"Grouping"},{"location":"api/portfolio/base/#indexing","text":"Like any other class subclassing Wrapping , we can do pandas indexing on a Portfolio instance, which forwards indexing operation to each object with columns: >>> pf [ 'BTC-USD' ] <vectorbt.portfolio.base.Portfolio at 0x7fac7517ac88> >>> pf [ 'BTC-USD' ] . total_profit () 5792.120252189081 Combined portfolio is indexed by group: >>> comb_pf [ 'first' ] <vectorbt.portfolio.base.Portfolio at 0x7fac5756b828> >>> comb_pf [ 'first' ] . total_profit () 26221.57120014546 Note Changing index (time axis) is not supported. The object should be treated as a Series rather than a DataFrame; for example, use pf.iloc[0] instead of pf.iloc[:, 0] . Indexing behavior depends solely upon ArrayWrapper . For example, if group_select is enabled indexing will be performed on groups, otherwise on single columns. You can pass wrapper arguments with wrapper_kwargs .","title":"Indexing"},{"location":"api/portfolio/base/#logging","text":"To collect more information on how a specific order was processed or to be able to track the whole simulation from the beginning to the end, we can turn on logging: >>> # Simulate portfolio with logging >>> pf = vbt . Portfolio . from_orders ( ... ohlcv [ 'Close' ], size , price = ohlcv [ 'Open' ], ... init_cash = 'autoalign' , fees = 0.001 , slippage = 0.001 , log = True ) >>> pf . logs . records id group col idx cash position debt free_cash val_price \\ 0 0 0 0 0 inf 0.000000 0.0 inf 7194.892090 1 1 0 0 1 inf 0.000000 0.0 inf 7202.551270 2 2 0 0 2 inf 0.000000 0.0 inf 6984.428711 ... ... ... ... ... ... ... ... ... ... 1461 1461 5 5 241 inf 272.389644 0.0 inf 57.207737 1462 1462 5 5 242 inf 274.137659 0.0 inf 62.844059 1463 1463 5 5 243 inf 282.093860 0.0 inf 61.105076 value ... new_free_cash new_val_price new_value res_size \\ 0 inf ... inf 7194.892090 inf NaN 1 inf ... inf 7202.551270 inf NaN 2 inf ... inf 6984.428711 inf NaN ... ... ... ... ... ... ... 1461 inf ... inf 57.207737 inf 1.748015 1462 inf ... inf 62.844059 inf 7.956202 1463 inf ... inf 61.105076 inf 1.636525 res_price res_fees res_side res_status res_status_info order_id 0 NaN NaN -1 1 0 -1 1 NaN NaN -1 1 5 -1 2 NaN NaN -1 1 5 -1 ... ... ... ... ... ... ... 1461 57.264945 0.1001 0 0 -1 1070 1462 62.906903 0.5005 0 0 -1 1071 1463 61.043971 0.0999 1 0 -1 1072 [1464 rows x 37 columns] Just as orders, logs are also records and thus can be easily analyzed: >>> pf . logs . res_status . value_counts () symbol BTC-USD ETH-USD XRP-USD BNB-USD BCH-USD LTC-USD Filled 184 172 177 178 177 185 Ignored 60 72 67 66 67 59 Logging can also be turned on just for one order, row, or column, since as many other variables it's specified per order and can broadcast automatically. Note Logging can slow down simulation.","title":"Logging"},{"location":"api/portfolio/base/#caching","text":"Portfolio heavily relies upon caching. If a method or a property requires heavy computation, it's wrapped with cached_method() and cached_property respectively. Caching can be disabled globally via caching in settings . Note Because of caching, class is meant to be immutable and all properties are read-only. To change any attribute, use the copy method and pass the attribute as keyword argument. Alternatively, we can precisely point at attributes and methods that should or shouldn't be cached. For example, we can blacklist the entire Portfolio class except a few most called methods such as Portfolio.cash_flow() and Portfolio.asset_flow() : >>> vbt . settings . caching [ 'blacklist' ] . append ( ... vbt . CacheCondition ( base_cls = 'Portfolio' ) ... ) >>> vbt . settings . caching [ 'whitelist' ] . extend ([ ... vbt . CacheCondition ( base_cls = 'Portfolio' , func = 'cash_flow' ), ... vbt . CacheCondition ( base_cls = 'Portfolio' , func = 'asset_flow' ) ... ]) Define rules for one instance of Portfolio : >>> vbt . settings . caching [ 'blacklist' ] . append ( ... vbt . CacheCondition ( instance = pf ) ... ) >>> vbt . settings . caching [ 'whitelist' ] . extend ([ ... vbt . CacheCondition ( instance = pf , func = 'cash_flow' ), ... vbt . CacheCondition ( instance = pf , func = 'asset_flow' ) ... ]) See should_cache() for caching rules. To reset caching: >>> vbt . settings . caching . reset ()","title":"Caching"},{"location":"api/portfolio/base/#performance-and-memory","text":"If you're running out of memory when working with large arrays, make sure to disable caching and then store most important time series manually. For example, if you're interested in Sharpe ratio or other metrics based on returns, run and save Portfolio.returns() in a variable and then use the ReturnsAccessor to analyze them. Do not use methods akin to Portfolio.sharpe_ratio() because they will re-calculate returns each time. Alternatively, you can track portfolio value and returns using Portfolio.from_order_func() and its callbacks (preferably in post_segment_func_nb ): >>> pf_baseline = vbt . Portfolio . from_orders ( ... ohlcv [ 'Close' ], size , price = ohlcv [ 'Open' ], ... init_cash = 'autoalign' , fees = 0.001 , slippage = 0.001 , freq = 'd' ) >>> pf_baseline . sharpe_ratio () symbol BTC-USD 1.743437 ETH-USD 2.800903 XRP-USD 1.607904 BNB-USD 1.805373 BCH-USD 0.269392 LTC-USD 1.040494 Name: sharpe_ratio, dtype: float64 >>> @njit ... def order_func_nb ( c , size , price , fees , slippage ): ... return nb . order_nb ( ... size = nb . get_elem_nb ( c , size ), ... price = nb . get_elem_nb ( c , price ), ... fees = nb . get_elem_nb ( c , fees ), ... slippage = nb . get_elem_nb ( c , slippage ), ... ) >>> @njit ... def post_segment_func_nb ( c , returns_out ): ... returns_out [ c . i , c . group ] = c . last_return [ c . group ] >>> returns_out = np . empty_like ( ohlcv [ 'Close' ], dtype = np . float_ ) >>> pf = vbt . Portfolio . from_order_func ( ... ohlcv [ 'Close' ], ... order_func_nb , ... np . asarray ( size ), ... np . asarray ( ohlcv [ 'Open' ]), ... np . asarray ( 0.001 ), ... np . asarray ( 0.001 ), ... post_segment_func_nb = post_segment_func_nb , ... post_segment_args = ( returns_out ,), ... init_cash = pf_baseline . init_cash ... ) >>> returns = pf . wrapper . wrap ( returns_out ) >>> del pf >>> returns . vbt . returns ( freq = 'd' ) . sharpe_ratio () symbol BTC-USD -2.261443 ETH-USD 0.059538 XRP-USD 2.159093 BNB-USD 1.555386 BCH-USD 0.784214 LTC-USD 1.460077 Name: sharpe_ratio, dtype: float64 The only drawback of this approach is that you cannot use init_cash='auto' or init_cash='autoalign' because then, during the simulation, the portfolio value is np.inf and the returns are np.nan .","title":"Performance and memory"},{"location":"api/portfolio/base/#saving-and-loading","text":"Like any other class subclassing Pickleable , we can save a Portfolio instance to the disk with Pickleable.save() and load it with Pickleable.load() : >>> pf = vbt . Portfolio . from_orders ( ... ohlcv [ 'Close' ], size , price = ohlcv [ 'Open' ], ... init_cash = 'autoalign' , fees = 0.001 , slippage = 0.001 , freq = 'd' ) >>> pf . sharpe_ratio () symbol BTC-USD 1.743437 ETH-USD 2.800903 XRP-USD 1.607904 BNB-USD 1.805373 BCH-USD 0.269392 LTC-USD 1.040494 Name: sharpe_ratio, dtype: float64 >>> pf . save ( 'my_pf' ) >>> pf = vbt . Portfolio . load ( 'my_pf' ) >>> pf . sharpe_ratio () symbol BTC-USD 1.743437 ETH-USD 2.800903 XRP-USD 1.607904 BNB-USD 1.805373 BCH-USD 0.269392 LTC-USD 1.040494 Name: sharpe_ratio, dtype: float64 Note Save files won't include neither cached results nor global defaults. For example, passing fillna_close as None will also use None when the portfolio is loaded from disk. Make sure to either pass all arguments explicitly or to also save the settings config.","title":"Saving and loading"},{"location":"api/portfolio/base/#stats","text":"Hint See StatsBuilderMixin.stats() and Portfolio.metrics . Let's simulate a portfolio with two columns: >>> close = vbt . YFData . download ( ... \"BTC-USD\" , ... start = '2020-01-01 UTC' , ... end = '2020-09-01 UTC' ... ) . get ( 'Close' ) >>> pf = vbt . Portfolio . from_random_signals ( close , n = [ 10 , 20 ], seed = 42 ) >>> pf . wrapper . columns Int64Index([10, 20], dtype='int64', name='rand_n')","title":"Stats"},{"location":"api/portfolio/base/#column-group-and-tag-selection","text":"To return the statistics for a particular column/group, use the column argument: >>> pf . stats ( column = 10 ) UserWarning: Metric 'sharpe_ratio' requires frequency to be set UserWarning: Metric 'calmar_ratio' requires frequency to be set UserWarning: Metric 'omega_ratio' requires frequency to be set UserWarning: Metric 'sortino_ratio' requires frequency to be set Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 Start Value 100.0 End Value 106.721585 Total Return [%] 6.721585 Benchmark Return [%] 66.252621 Max Gross Exposure [%] 100.0 Total Fees Paid 0.0 Max Drawdown [%] 22.190944 Max Drawdown Duration 101.0 Total Trades 10 Total Closed Trades 10 Total Open Trades 0 Open Trade PnL 0.0 Win Rate [%] 60.0 Best Trade [%] 15.31962 Worst Trade [%] -9.904223 Avg Winning Trade [%] 4.671959 Avg Losing Trade [%] -4.851205 Avg Winning Trade Duration 11.333333 Avg Losing Trade Duration 14.25 Profit Factor 1.347457 Expectancy 0.672158 Name: 10, dtype: object If vectorbt couldn't parse the frequency of close : 1) it won't return any duration in time units, 2) it won't return any metric that requires annualization, and 3) it will throw a bunch of warnings (you can silence those by passing silence_warnings=True ) We can provide the frequency as part of the settings dict: >>> pf . stats ( column = 10 , settings = dict ( freq = 'd' )) UserWarning: Changing the frequency will create a copy of this object. Consider setting the frequency upon object creation to re-use existing cache. Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 days 00:00:00 Start Value 100.0 End Value 106.721585 Total Return [%] 6.721585 Benchmark Return [%] 66.252621 Max Gross Exposure [%] 100.0 Total Fees Paid 0.0 Max Drawdown [%] 22.190944 Max Drawdown Duration 101 days 00:00:00 Total Trades 10 Total Closed Trades 10 Total Open Trades 0 Open Trade PnL 0.0 Win Rate [%] 60.0 Best Trade [%] 15.31962 Worst Trade [%] -9.904223 Avg Winning Trade [%] 4.671959 Avg Losing Trade [%] -4.851205 Avg Winning Trade Duration 11 days 08:00:00 Avg Losing Trade Duration 14 days 06:00:00 Profit Factor 1.347457 Expectancy 0.672158 Sharpe Ratio 0.445231 Calmar Ratio 0.460573 Omega Ratio 1.099192 Sortino Ratio 0.706986 Name: 10, dtype: object But in this case, our portfolio will be copied to set the new frequency and we wouldn't be able to re-use its cached attributes. Let's define the frequency upon the simulation instead: >>> pf = vbt . Portfolio . from_random_signals ( close , n = [ 10 , 20 ], seed = 42 , freq = 'd' ) We can change the grouping of the portfolio on the fly. Let's form a single group: >>> pf . stats ( group_by = True ) Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 days 00:00:00 Start Value 200.0 End Value 277.49299 Total Return [%] 38.746495 Benchmark Return [%] 66.252621 Max Gross Exposure [%] 100.0 Total Fees Paid 0.0 Max Drawdown [%] 14.219327 Max Drawdown Duration 86 days 00:00:00 Total Trades 30 Total Closed Trades 30 Total Open Trades 0 Open Trade PnL 0.0 Win Rate [%] 66.666667 Best Trade [%] 18.332559 Worst Trade [%] -9.904223 Avg Winning Trade [%] 5.754788 Avg Losing Trade [%] -4.718907 Avg Winning Trade Duration 7 days 19:12:00 Avg Losing Trade Duration 8 days 07:12:00 Profit Factor 2.427948 Expectancy 2.5831 Sharpe Ratio 1.57907 Calmar Ratio 4.445448 Omega Ratio 1.334032 Sortino Ratio 2.59669 Name: group, dtype: object We can see how the initial cash has changed from $100 to $200, indicating that both columns now contribute to the performance.","title":"Column, group, and tag selection"},{"location":"api/portfolio/base/#aggregation","text":"If the portfolio consists of multiple columns/groups and no column/group has been selected, each metric is aggregated across all columns/groups based on agg_func , which is np.mean by default. >>> pf . stats () UserWarning: Object has multiple columns. Aggregating using <function mean at 0x7fc77152bb70>. Pass column to select a single column/group. Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 days 00:00:00 Start Value 100.0 End Value 138.746495 Total Return [%] 38.746495 Benchmark Return [%] 66.252621 Max Gross Exposure [%] 100.0 Total Fees Paid 0.0 Max Drawdown [%] 20.35869 Max Drawdown Duration 93 days 00:00:00 Total Trades 15.0 Total Closed Trades 15.0 Total Open Trades 0.0 Open Trade PnL 0.0 Win Rate [%] 65.0 Best Trade [%] 16.82609 Worst Trade [%] -9.701273 Avg Winning Trade [%] 5.445408 Avg Losing Trade [%] -4.740956 Avg Winning Trade Duration 8 days 19:25:42.857142857 Avg Losing Trade Duration 9 days 07:00:00 Profit Factor 2.186957 Expectancy 2.105364 Sharpe Ratio 1.165695 Calmar Ratio 3.541079 Omega Ratio 1.331624 Sortino Ratio 2.084565 Name: agg_func_mean, dtype: object Here, the Sharpe ratio of 0.445231 (column=10) and 1.88616 (column=20) lead to the avarage of 1.16569. We can also return a DataFrame with statistics per column/group by passing agg_func=None : >>> pf . stats ( agg_func = None ) Start End Period ... Sortino Ratio rand_n ... 10 2020-01-01 00:00:00+00:00 2020-09-01 00:00:00+00:00 244 days ... 0.706986 20 2020-01-01 00:00:00+00:00 2020-09-01 00:00:00+00:00 244 days ... 3.462144 [2 rows x 25 columns]","title":"Aggregation"},{"location":"api/portfolio/base/#metric-selection","text":"To select metrics, use the metrics argument (see Portfolio.metrics for supported metrics): >>> pf . stats ( metrics = [ 'sharpe_ratio' , 'sortino_ratio' ], column = 10 ) Sharpe Ratio 0.445231 Sortino Ratio 0.706986 Name: 10, dtype: float64 We can also select specific tags (see any metric from Portfolio.metrics that has the tag key): >>> pf . stats ( column = 10 , tags = [ 'trades' ]) Total Trades 10 Total Open Trades 0 Open Trade PnL 0 Long Trades [%] 100 Win Rate [%] 60 Best Trade [%] 15.3196 Worst Trade [%] -9.90422 Avg Winning Trade [%] 4.67196 Avg Winning Trade Duration 11 days 08:00:00 Avg Losing Trade [%] -4.8512 Avg Losing Trade Duration 14 days 06:00:00 Profit Factor 1.34746 Expectancy 0.672158 Name: 10, dtype: object Or provide a boolean expression: >>> pf . stats ( column = 10 , tags = 'trades and open and not closed' ) Total Open Trades 0.0 Open Trade PnL 0.0 Name: 10, dtype: float64 The reason why we included \"not closed\" along with \"open\" is because some metrics such as the win rate have both tags attached since they are based upon both open and closed trades/positions (to see this, pass settings=dict(incl_open=True) and tags='trades and open' ).","title":"Metric selection"},{"location":"api/portfolio/base/#passing-parameters","text":"We can use settings to pass parameters used across multiple metrics. For example, let's pass required and risk-free return to all return metrics: >>> pf . stats ( column = 10 , settings = dict ( required_return = 0.1 , risk_free = 0.01 )) Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 days 00:00:00 Start Value 100.0 End Value 106.721585 Total Return [%] 6.721585 Benchmark Return [%] 66.252621 Max Gross Exposure [%] 100.0 Total Fees Paid 0.0 Max Drawdown [%] 22.190944 Max Drawdown Duration 101 days 00:00:00 Total Trades 10 Total Closed Trades 10 Total Open Trades 0 Open Trade PnL 0.0 Win Rate [%] 60.0 Best Trade [%] 15.31962 Worst Trade [%] -9.904223 Avg Winning Trade [%] 4.671959 Avg Losing Trade [%] -4.851205 Avg Winning Trade Duration 11 days 08:00:00 Avg Losing Trade Duration 14 days 06:00:00 Profit Factor 1.347457 Expectancy 0.672158 Sharpe Ratio -9.504742 << here Calmar Ratio 0.460573 << here Omega Ratio 0.233279 << here Sortino Ratio -18.763407 << here Name: 10, dtype: object Passing any argument inside of settings either overrides an existing default, or acts as an optional argument that is passed to the calculation function upon resolution (see below). Both required_return and risk_free can be found in the signature of the 4 ratio methods, so vectorbt knows exactly it has to pass them. Let's imagine that the signature of ReturnsAccessor.sharpe_ratio() doesn't list those arguments: vectorbt would simply call this method without passing those two arguments. In such case, we have two options: 1) Set parameters globally using settings and set pass_{arg}=True individually using metric_settings : >>> pf . stats ( ... column = 10 , ... settings = dict ( required_return = 0.1 , risk_free = 0.01 ), ... metric_settings = dict ( ... sharpe_ratio = dict ( pass_risk_free = True ), ... omega_ratio = dict ( pass_required_return = True , pass_risk_free = True ), ... sortino_ratio = dict ( pass_required_return = True ) ... ) ... ) 2) Set parameters individually using metric_settings : >>> pf . stats ( ... column = 10 , ... metric_settings = dict ( ... sharpe_ratio = dict ( risk_free = 0.01 ), ... omega_ratio = dict ( required_return = 0.1 , risk_free = 0.01 ), ... sortino_ratio = dict ( required_return = 0.1 ) ... ) ... )","title":"Passing parameters"},{"location":"api/portfolio/base/#custom-metrics","text":"To calculate a custom metric, we need to provide at least two things: short name and a settings dict with the title and calculation function (see arguments in StatsBuilderMixin ): >>> max_winning_streak = ( ... 'max_winning_streak' , ... dict ( ... title = 'Max Winning Streak' , ... calc_func = lambda trades : trades . winning_streak . max (), ... resolve_trades = True ... ) ... ) >>> pf . stats ( metrics = max_winning_streak , column = 10 ) Max Winning Streak 3.0 Name: 10, dtype: float64 You might wonder how vectorbt knows which arguments to pass to calc_func ? In the example above, the calculation function expects two arguments: trades and group_by . To automatically pass any of the them, vectorbt searches for each in the current settings. As trades cannot be found, it either throws an error or tries to resolve this argument if resolve_{arg}=True was passed. Argument resolution is the process of searching for property/method with the same name (also with prefix get_ ) in the attributes of the current portfolio, automatically passing the current settings such as group_by if they are present in the method's signature (a similar resolution procedure), and calling the method/property. The result of the resolution process is then passed as arg (or trades in our example). Here's an example without resolution of arguments: >>> max_winning_streak = ( ... 'max_winning_streak' , ... dict ( ... title = 'Max Winning Streak' , ... calc_func = lambda self , group_by : ... self . get_trades ( group_by = group_by ) . winning_streak . max () ... ) ... ) >>> pf . stats ( metrics = max_winning_streak , column = 10 ) Max Winning Streak 3.0 Name: 10, dtype: float64 And here's an example without resolution of the calculation function: >>> max_winning_streak = ( ... 'max_winning_streak' , ... dict ( ... title = 'Max Winning Streak' , ... calc_func = lambda self , settings : ... self . get_trades ( group_by = settings [ 'group_by' ]) . winning_streak . max (), ... resolve_calc_func = False ... ) ... ) >>> pf . stats ( metrics = max_winning_streak , column = 10 ) Max Winning Streak 3.0 Name: 10, dtype: float64 Since max_winning_streak method can be expressed as a path from this portfolio, we can simply write: >>> max_winning_streak = ( ... 'max_winning_streak' , ... dict ( ... title = 'Max Winning Streak' , ... calc_func = 'trades.winning_streak.max' ... ) ... ) In this case, we don't have to pass resolve_trades=True any more as vectorbt does it automatically. Another advantage is that vectorbt can access the signature of the last method in the path ( MappedArray.max() in our case) and resolve its arguments. To switch between entry trades, exit trades, and positions, use the trades_type setting. Additionally, you can pass incl_open=True to also include open trades. >>> pf . stats ( column = 10 , settings = dict ( trades_type = 'positions' , incl_open = True )) Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 days 00:00:00 Start Value 100.0 End Value 106.721585 Total Return [%] 6.721585 Benchmark Return [%] 66.252621 Max Gross Exposure [%] 100.0 Total Fees Paid 0.0 Max Drawdown [%] 22.190944 Max Drawdown Duration 100 days 00:00:00 Total Trades 10 Total Closed Trades 10 Total Open Trades 0 Open Trade PnL 0.0 Win Rate [%] 60.0 Best Trade [%] 15.31962 Worst Trade [%] -9.904223 Avg Winning Trade [%] 4.671959 Avg Losing Trade [%] -4.851205 Avg Winning Trade Duration 11 days 08:00:00 Avg Losing Trade Duration 14 days 06:00:00 Profit Factor 1.347457 Expectancy 0.672158 Sharpe Ratio 0.445231 Calmar Ratio 0.460573 Omega Ratio 1.099192 Sortino Ratio 0.706986 Name: 10, dtype: object Any default metric setting or even global setting can be overridden by the user using metric-specific keyword arguments. Here, we override the global aggregation function for max_dd_duration : >>> pf . stats ( agg_func = lambda sr : sr . mean (), ... metric_settings = dict ( ... max_dd_duration = dict ( agg_func = lambda sr : sr . max ()) ... ) ... ) UserWarning: Object has multiple columns. Aggregating using <function <lambda> at 0x7fbf6e77b268>. Pass column to select a single column/group. Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 days 00:00:00 Start Value 100.0 End Value 138.746495 Total Return [%] 38.746495 Benchmark Return [%] 66.252621 Max Gross Exposure [%] 100.0 Total Fees Paid 0.0 Max Drawdown [%] 20.35869 Max Drawdown Duration 101 days 00:00:00 << here Total Trades 15.0 Total Closed Trades 15.0 Total Open Trades 0.0 Open Trade PnL 0.0 Win Rate [%] 65.0 Best Trade [%] 16.82609 Worst Trade [%] -9.701273 Avg Winning Trade [%] 5.445408 Avg Losing Trade [%] -4.740956 Avg Winning Trade Duration 8 days 19:25:42.857142857 Avg Losing Trade Duration 9 days 07:00:00 Profit Factor 2.186957 Expectancy 2.105364 Sharpe Ratio 1.165695 Calmar Ratio 3.541079 Omega Ratio 1.331624 Sortino Ratio 2.084565 Name: agg_func_<lambda>, dtype: object Let's create a simple metric that returns a passed value to demonstrate how vectorbt overrides settings, from least to most important: >>> # vbt.settings.portfolio.stats >>> vbt . settings . portfolio . stats [ 'settings' ][ 'my_arg' ] = 100 >>> my_arg_metric = ( 'my_arg_metric' , dict ( title = 'My Arg' , calc_func = lambda my_arg : my_arg )) >>> pf . stats ( my_arg_metric , column = 10 ) My Arg 100 Name: 10, dtype: int64 >>> # settings >>> vbt.settings.portfolio.stats >>> pf . stats ( my_arg_metric , column = 10 , settings = dict ( my_arg = 200 )) My Arg 200 Name: 10, dtype: int64 >>> # metric settings >>> settings >>> my_arg_metric = ( 'my_arg_metric' , dict ( title = 'My Arg' , my_arg = 300 , calc_func = lambda my_arg : my_arg )) >>> pf . stats ( my_arg_metric , column = 10 , settings = dict ( my_arg = 200 )) My Arg 300 Name: 10, dtype: int64 >>> # metric_settings >>> metric settings >>> pf . stats ( my_arg_metric , column = 10 , settings = dict ( my_arg = 200 ), ... metric_settings = dict ( my_arg_metric = dict ( my_arg = 400 ))) My Arg 400 Name: 10, dtype: int64 Here's an example of a parametrized metric. Let's get the number of trades with PnL over some amount: >>> trade_min_pnl_cnt = ( ... 'trade_min_pnl_cnt' , ... dict ( ... title = vbt . Sub ( 'Trades with PnL over $$$ {min_pnl} ' ), ... calc_func = lambda trades , min_pnl : trades . apply_mask ( ... trades . pnl . values >= min_pnl ) . count (), ... resolve_trades = True ... ) ... ) >>> pf . stats ( ... metrics = trade_min_pnl_cnt , column = 10 , ... metric_settings = dict ( trade_min_pnl_cnt = dict ( min_pnl = 0 ))) Trades with PnL over $0 6 Name: stats, dtype: int64 >>> pf . stats ( ... metrics = trade_min_pnl_cnt , column = 10 , ... metric_settings = dict ( trade_min_pnl_cnt = dict ( min_pnl = 10 ))) Trades with PnL over $10 1 Name: stats, dtype: int64 If the same metric name was encountered more than once, vectorbt automatically appends an underscore and its position, so we can pass keyword arguments to each metric separately: >>> pf . stats ( ... metrics = [ ... trade_min_pnl_cnt , ... trade_min_pnl_cnt , ... trade_min_pnl_cnt ... ], ... column = 10 , ... metric_settings = dict ( ... trade_min_pnl_cnt_0 = dict ( min_pnl = 0 ), ... trade_min_pnl_cnt_1 = dict ( min_pnl = 10 ), ... trade_min_pnl_cnt_2 = dict ( min_pnl = 20 )) ... ) Trades with PnL over $0 6 Trades with PnL over $10 1 Trades with PnL over $20 0 Name: stats, dtype: int64 To add a custom metric to the list of all metrics, we have three options. The first option is to change the Portfolio.metrics dict in-place (this will append to the end): >>> pf . metrics [ 'max_winning_streak' ] = max_winning_streak [ 1 ] >>> pf . stats ( column = 10 ) Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 days 00:00:00 Start Value 100.0 End Value 106.721585 Total Return [%] 6.721585 Benchmark Return [%] 66.252621 Max Gross Exposure [%] 100.0 Total Fees Paid 0.0 Max Drawdown [%] 22.190944 Max Drawdown Duration 101 days 00:00:00 Total Trades 10 Total Closed Trades 10 Total Open Trades 0 Open Trade PnL 0.0 Win Rate [%] 60.0 Best Trade [%] 15.31962 Worst Trade [%] -9.904223 Avg Winning Trade [%] 4.671959 Avg Losing Trade [%] -4.851205 Avg Winning Trade Duration 11 days 08:00:00 Avg Losing Trade Duration 14 days 06:00:00 Profit Factor 1.347457 Expectancy 0.672158 Sharpe Ratio 0.445231 Calmar Ratio 0.460573 Omega Ratio 1.099192 Sortino Ratio 0.706986 Max Winning Streak 3.0 << here Name: 10, dtype: object Since Portfolio.metrics is of type Config , we can reset it at any time to get default metrics: >>> pf . metrics . reset () The second option is to copy Portfolio.metrics , append our metric, and pass as metrics argument: >>> my_metrics = list ( pf . metrics . items ()) + [ max_winning_streak ] >>> pf . stats ( metrics = my_metrics , column = 10 ) The third option is to set metrics globally under portfolio.stats in settings . >>> vbt . settings . portfolio [ 'stats' ][ 'metrics' ] = my_metrics >>> pf . stats ( column = 10 )","title":"Custom metrics"},{"location":"api/portfolio/base/#returns-stats","text":"We can compute the stats solely based on the portfolio's returns using Portfolio.returns_stats() , which calls StatsBuilderMixin.stats() . >>> pf . returns_stats ( column = 10 ) Start 2020-01-01 00:00:00+00:00 End 2020-09-01 00:00:00+00:00 Period 244 days 00:00:00 Total Return [%] 6.721585 Benchmark Return [%] 66.252621 Annualized Return [%] 10.22056 Annualized Volatility [%] 36.683518 Max Drawdown [%] 22.190944 Max Drawdown Duration 100 days 00:00:00 Sharpe Ratio 0.445231 Calmar Ratio 0.460573 Omega Ratio 1.099192 Sortino Ratio 0.706986 Skew 1.328259 Kurtosis 10.80246 Tail Ratio 1.057913 Common Sense Ratio 1.166037 Value at Risk -0.031011 Alpha -0.075109 Beta 0.220351 Name: 10, dtype: object Most metrics defined in ReturnsAccessor are also available as attributes of Portfolio : >>> pf . sharpe_ratio () randnx_n 10 0.445231 20 1.886158 Name: sharpe_ratio, dtype: float64 Moreover, we can access quantstats functions using QSAdapter : >>> pf . qs . sharpe () randnx_n 10 0.445231 20 1.886158 dtype: float64 >>> pf [ 10 ] . qs . plot_snapshot ()","title":"Returns stats"},{"location":"api/portfolio/base/#plots","text":"Hint See PlotsBuilderMixin.plots() . The features implemented in this method are very similar to StatsBuilderMixin.stats() . See also the examples under StatsBuilderMixin.stats() . Plot portfolio of a random strategy: >>> pf . plot ( column = 10 ) You can choose any of the subplots in Portfolio.subplots , in any order, and control their appearance using keyword arguments: >>> pf . plot ( ... subplots = [ 'drawdowns' , 'underwater' ], ... column = 10 , ... subplot_settings = dict ( ... drawdowns = dict ( top_n = 3 ), ... underwater = dict ( ... trace_kwargs = dict ( ... line = dict ( color = '#FF6F00' ), ... fillcolor = adjust_opacity ( '#FF6F00' , 0.3 ) ... ) ... ) ... ) ... ) To create a new subplot, a preferred way is to pass a plotting function: >>> def plot_order_size ( pf , size , column = None , add_trace_kwargs = None , fig = None ): ... size = pf . select_one_from_obj ( size , pf . wrapper . regroup ( False ), column = column ) ... size . rename ( 'Order Size' ) . vbt . barplot ( ... add_trace_kwargs = add_trace_kwargs , fig = fig ) >>> order_size = pf . orders . size . to_pd ( fill_value = 0. ) >>> pf . plot ( subplots = [ ... 'orders' , ... ( 'order_size' , dict ( ... title = 'Order Size' , ... yaxis_kwargs = dict ( title = 'Order size' ), ... check_is_not_grouped = True , ... plot_func = plot_order_size ... )) ... ], ... column = 10 , ... subplot_settings = dict ( ... order_size = dict ( ... size = order_size ... ) ... ) ... ) Alternatively, you can create a placeholder and overwrite it manually later: >>> fig = pf . plot ( subplots = [ ... 'orders' , ... ( 'order_size' , dict ( ... title = 'Order Size' , ... yaxis_kwargs = dict ( title = 'Order size' ), ... check_is_not_grouped = True ... )) # placeholder ... ], column = 10 ) >>> order_size [ 10 ] . rename ( 'Order Size' ) . vbt . barplot ( ... add_trace_kwargs = dict ( row = 2 , col = 1 ), ... fig = fig ... ) If a plotting function can in any way be accessed from the current portfolio, you can pass the path to this function (see deep_getattr() for the path format). You can additionally use templates to make some parameters to depend upon passed keyword arguments: >>> subplots = [ ... ( 'cumulative_returns' , dict ( ... title = 'Cumulative Returns' , ... yaxis_kwargs = dict ( title = 'Cumulative returns' ), ... plot_func = 'returns.vbt.returns.cumulative.vbt.plot' , ... pass_add_trace_kwargs = True ... )), ... ( 'rolling_drawdown' , dict ( ... title = 'Rolling Drawdown' , ... yaxis_kwargs = dict ( title = 'Rolling drawdown' ), ... plot_func = [ ... 'returns.vbt.returns' , # returns accessor ... ( ... 'rolling_max_drawdown' , # function name ... ( vbt . Rep ( 'window' ),)), # positional arguments ... 'vbt.plot' # plotting function ... ], ... pass_add_trace_kwargs = True , ... trace_names = [ vbt . Sub ( 'rolling_drawdown($ {window} )' )], # add window to the trace name ... )) ... ] >>> pf . plot ( ... subplots , ... column = 10 , ... subplot_settings = dict ( ... rolling_drawdown = dict ( ... template_mapping = dict ( ... window = 10 ... ) ... ) ... ) ... ) You can also replace templates across all subplots by using the global template mapping: >>> pf . plot ( subplots , column = 10 , template_mapping = dict ( window = 10 ))","title":"Plots"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.returns_acc_config","text":"Config of returns accessor methods to be added to Portfolio . Co nf ig( { \"daily_returns\" : { \"source_name\" : \"daily\" }, \"annual_returns\" : { \"source_name\" : \"annual\" }, \"cumulative_returns\" : { \"source_name\" : \"cumulative\" }, \"annualized_return\" : { \"source_name\" : \"annualized\" }, \"annualized_volatility\" : {}, \"calmar_ratio\" : {}, \"omega_ratio\" : {}, \"sharpe_ratio\" : {}, \"deflated_sharpe_ratio\" : {}, \"downside_risk\" : {}, \"sortino_ratio\" : {}, \"information_ratio\" : {}, \"beta\" : {}, \"alpha\" : {}, \"tail_ratio\" : {}, \"value_at_risk\" : {}, \"cond_value_at_risk\" : {}, \"capture\" : {}, \"up_capture\" : {}, \"down_capture\" : {}, \"drawdown\" : {}, \"max_drawdown\" : {} } )","title":"returns_acc_config"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.MetaPortfolio","text":"Meta class that exposes a read-only class property StatsBuilderMixin.metrics . Superclasses MetaPlotsBuilderMixin MetaStatsBuilderMixin builtins.type Inherited members MetaPlotsBuilderMixin.subplots MetaStatsBuilderMixin.metrics","title":"MetaPortfolio"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio","text":"Class for modeling portfolio and measuring its performance. Args wrapper :\u2002 ArrayWrapper Array wrapper. See ArrayWrapper . close :\u2002 array_like Last asset price at each time step. order_records :\u2002 array_like A structured NumPy array of order records. log_records :\u2002 array_like A structured NumPy array of log records. init_cash :\u2002 InitCashMode , float or array_like of float Initial capital. cash_sharing :\u2002 bool Whether to share cash within the same group. call_seq :\u2002 array_like of int Sequence of calls per row and group. Defaults to None. fillna_close :\u2002 bool Whether to forward and backward fill NaN values in close . Applied after the simulation to avoid NaNs in asset value. See Portfolio.get_filled_close() . trades_type :\u2002 str or int Default Trades to use across Portfolio . See TradesType . For defaults, see portfolio in settings . Note Use class methods with from_ prefix to build a portfolio. The __init__ method is reserved for indexing purposes. Note This class is meant to be immutable. To change any attribute, use Configured.replace() . Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.config Wrapping.iloc Wrapping.indexing_kwargs Wrapping.loc Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Wrapping.self_aliases Wrapping.wrapper Wrapping.writeable_attrs","title":"Portfolio"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.alpha","text":"Portfolio . alpha ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.alpha() .","title":"alpha()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.annual_returns","text":"Portfolio . annual_returns ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.annual() .","title":"annual_returns()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.annualized_return","text":"Portfolio . annualized_return ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.annualized() .","title":"annualized_return()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.annualized_volatility","text":"Portfolio . annualized_volatility ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.annualized_volatility() .","title":"annualized_volatility()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.asset_flow","text":"Portfolio . asset_flow ( direction = 'both' , wrap_kwargs = None ) Get asset flow series per column. Returns the total transacted amount of assets at each time step.","title":"asset_flow()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.asset_returns","text":"Portfolio . asset_returns ( group_by = None , wrap_kwargs = None ) Get asset return series per column/group. This type of returns is based solely on cash flows and asset value rather than portfolio value. It ignores passive cash and thus it will return the same numbers irrespective of the amount of cash currently available, even np.inf . The scale of returns is comparable to that of going all in and keeping available cash at zero.","title":"asset_returns()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.asset_value","text":"Portfolio . asset_value ( direction = 'both' , group_by = None , wrap_kwargs = None ) Get asset value series per column/group.","title":"asset_value()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.assets","text":"Portfolio . assets ( direction = 'both' , wrap_kwargs = None ) Get asset series per column. Returns the current position at each time step.","title":"assets()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.benchmark_rets","text":"Portfolio . benchmark_returns ( group_by = None , wrap_kwargs = None ) Get return series per column/group based on benchmark value.","title":"benchmark_rets()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.benchmark_returns","text":"Portfolio . benchmark_returns ( group_by = None , wrap_kwargs = None ) Get return series per column/group based on benchmark value.","title":"benchmark_returns()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.benchmark_value","text":"Portfolio . benchmark_value ( group_by = None , wrap_kwargs = None ) Get market benchmark value series per column/group. If grouped, evenly distributes the initial cash among assets in the group. Note Does not take into account fees and slippage. For this, create a separate portfolio.","title":"benchmark_value()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.beta","text":"Portfolio . beta ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.beta() .","title":"beta()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.call_seq","text":"Sequence of calls per row and group.","title":"call_seq"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.calmar_ratio","text":"Portfolio . calmar_ratio ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.calmar_ratio() .","title":"calmar_ratio()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.capture","text":"Portfolio . capture ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.capture() .","title":"capture()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.cash","text":"Portfolio . cash ( group_by = None , in_sim_order = False , free = False , wrap_kwargs = None ) Get cash balance series per column/group. See the explanation on in_sim_order in Portfolio.value() . For free , see Portfolio.cash_flow() .","title":"cash()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.cash_flow","text":"Portfolio . cash_flow ( group_by = None , free = False , wrap_kwargs = None ) Get cash flow series per column/group. Use free to return the flow of the free cash, which never goes above the initial level, because an operation always costs money.","title":"cash_flow()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.cash_sharing","text":"Whether to share cash within the same group.","title":"cash_sharing"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.close","text":"Price per unit series.","title":"close"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.cond_value_at_risk","text":"Portfolio . cond_value_at_risk ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.cond_value_at_risk() .","title":"cond_value_at_risk()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.cumulative_returns","text":"Portfolio . cumulative_returns ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.cumulative() .","title":"cumulative_returns()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.daily_returns","text":"Portfolio . daily_returns ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.daily() .","title":"daily_returns()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.deflated_sharpe_ratio","text":"Portfolio . deflated_sharpe_ratio ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.deflated_sharpe_ratio() .","title":"deflated_sharpe_ratio()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.down_capture","text":"Portfolio . down_capture ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.down_capture() .","title":"down_capture()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.downside_risk","text":"Portfolio . downside_risk ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.downside_risk() .","title":"downside_risk()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.drawdown","text":"Portfolio . drawdown ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.drawdown() .","title":"drawdown()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.drawdowns","text":"Portfolio.get_drawdowns() with default arguments.","title":"drawdowns"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.entry_trades","text":"Portfolio.get_entry_trades() with default arguments.","title":"entry_trades"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.exit_trades","text":"Portfolio.get_exit_trades() with default arguments.","title":"exit_trades"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.fillna_close","text":"Whether to forward-backward fill NaN values in Portfolio.close .","title":"fillna_close"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.final_value","text":"Portfolio . final_value ( group_by = None , wrap_kwargs = None ) Get total profit per column/group.","title":"final_value()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.from_holding","text":"Portfolio . from_holding ( close , ** kwargs ) Simulate portfolio from holding. Based on Portfolio.from_signals() . >>> close = pd . Series ([ 1 , 2 , 3 , 4 , 5 ]) >>> pf = vbt . Portfolio . from_holding ( close ) >>> pf . final_value () 500.0","title":"from_holding()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.from_order_func","text":"Portfolio . from_order_func ( close , order_func_nb , * order_args , flexible = None , init_cash = None , cash_sharing = None , call_seq = None , segment_mask = None , call_pre_segment = None , call_post_segment = None , pre_sim_func_nb = no_pre_func_nb , pre_sim_args = (), post_sim_func_nb = no_post_func_nb , post_sim_args = (), pre_group_func_nb = no_pre_func_nb , pre_group_args = (), post_group_func_nb = no_post_func_nb , post_group_args = (), pre_row_func_nb = no_pre_func_nb , pre_row_args = (), post_row_func_nb = no_post_func_nb , post_row_args = (), pre_segment_func_nb = no_pre_func_nb , pre_segment_args = (), post_segment_func_nb = no_post_func_nb , post_segment_args = (), post_order_func_nb = no_post_func_nb , post_order_args = (), ffill_val_price = None , update_value = None , fill_pos_record = None , row_wise = None , use_numba = None , max_orders = None , max_logs = None , seed = None , group_by = None , broadcast_named_args = None , broadcast_kwargs = None , template_mapping = None , wrapper_kwargs = None , freq = None , attach_call_seq = None , ** kwargs ) Build portfolio from a custom order function. Hint See simulate_nb() for illustrations and argument definitions. For more details on individual simulation functions: not row_wise and not flexible : See simulate_nb() not row_wise and flexible : See flex_simulate_nb() row_wise and not flexible : See simulate_row_wise_nb() row_wise and flexible : See flex_simulate_row_wise_nb() Args close :\u2002 array_like Last asset price at each time step. Will broadcast to target_shape . Used for calculating unrealized PnL and portfolio value. order_func_nb :\u2002 callable Order generation function. *order_args Arguments passed to order_func_nb . flexible :\u2002 bool Whether to simulate using a flexible order function. This lifts the limit of one order per tick and symbol. init_cash :\u2002 InitCashMode , float or array_like of float Initial capital. See init_cash in Portfolio.from_orders() . cash_sharing :\u2002 bool Whether to share cash within the same group. If group_by is None, group_by becomes True to form a single group with cash sharing. call_seq :\u2002 CallSeqType or array_like Default sequence of calls per row and group. Use CallSeqType to select a sequence type. Set to array to specify custom sequence. Will not broadcast. Note CallSeqType.Auto should be implemented manually. Use sort_call_seq_nb() or sort_call_seq_out_nb() in pre_segment_func_nb . segment_mask :\u2002 int or array_like of bool Mask of whether a particular segment should be executed. Supplying an integer will activate every n-th row. Supplying a boolean or an array of boolean will broadcast to the number of rows and groups. Does not broadcast together with close and broadcast_named_args , only against the final shape. call_pre_segment :\u2002 bool Whether to call pre_segment_func_nb regardless of segment_mask . call_post_segment :\u2002 bool Whether to call post_segment_func_nb regardless of segment_mask . pre_sim_func_nb :\u2002 callable Function called before simulation. Defaults to no_pre_func_nb() . pre_sim_args :\u2002 tuple Packed arguments passed to pre_sim_func_nb . Defaults to () . post_sim_func_nb :\u2002 callable Function called after simulation. Defaults to no_post_func_nb() . post_sim_args :\u2002 tuple Packed arguments passed to post_sim_func_nb . Defaults to () . pre_group_func_nb :\u2002 callable Function called before each group. Defaults to no_pre_func_nb() . Called only if row_wise is False. pre_group_args :\u2002 tuple Packed arguments passed to pre_group_func_nb . Defaults to () . post_group_func_nb :\u2002 callable Function called after each group. Defaults to no_post_func_nb() . Called only if row_wise is False. post_group_args :\u2002 tuple Packed arguments passed to post_group_func_nb . Defaults to () . pre_row_func_nb :\u2002 callable Function called before each row. Defaults to no_pre_func_nb() . Called only if row_wise is True. pre_row_args :\u2002 tuple Packed arguments passed to pre_row_func_nb . Defaults to () . post_row_func_nb :\u2002 callable Function called after each row. Defaults to no_post_func_nb() . Called only if row_wise is True. post_row_args :\u2002 tuple Packed arguments passed to post_row_func_nb . Defaults to () . pre_segment_func_nb :\u2002 callable Function called before each segment. Defaults to no_pre_func_nb() . pre_segment_args :\u2002 tuple Packed arguments passed to pre_segment_func_nb . Defaults to () . post_segment_func_nb :\u2002 callable Function called after each segment. Defaults to no_post_func_nb() . post_segment_args :\u2002 tuple Packed arguments passed to post_segment_func_nb . Defaults to () . post_order_func_nb :\u2002 callable Callback that is called after the order has been processed. post_order_args :\u2002 tuple Packed arguments passed to post_order_func_nb . Defaults to () . ffill_val_price :\u2002 bool Whether to track valuation price only if it's known. Otherwise, unknown close will lead to NaN in valuation price at the next timestamp. update_value :\u2002 bool Whether to update group value after each filled order. fill_pos_record :\u2002 bool Whether to fill position record. Disable this to make simulation a bit faster for simple use cases. row_wise :\u2002 bool Whether to iterate over rows rather than columns/groups. use_numba :\u2002 bool Whether to run the main simulation function using Numba. Note Disabling it does not disable Numba for other functions. If neccessary, you should ensure that every other function does not uses Numba as well. You can do this by using the py_func attribute of that function. Or, you could disable Numba globally by doing os.environ['NUMBA_DISABLE_JIT'] = '1' . max_orders :\u2002 int Size of the order records array. Defaults to the number of elements in the broadcasted shape. Set to a lower number if you run out of memory. max_logs :\u2002 int Size of the log records array. Defaults to the number of elements in the broadcasted shape. Set to a lower number if you run out of memory. seed :\u2002 int See Portfolio.from_orders() . group_by :\u2002 any See Portfolio.from_orders() . broadcast_named_args :\u2002 dict See Portfolio.from_signals() . broadcast_kwargs :\u2002 dict See Portfolio.from_orders() . template_mapping :\u2002 mapping See Portfolio.from_signals() . wrapper_kwargs :\u2002 dict See Portfolio.from_orders() . freq :\u2002 any See Portfolio.from_orders() . attach_call_seq :\u2002 bool See Portfolio.from_orders() . **kwargs Keyword arguments passed to the __init__ method. For defaults, see portfolio in settings . Note All passed functions should be Numba-compiled if Numba is enabled. Also see notes on Portfolio.from_orders() . Note In contrast to other methods, the valuation price is previous close instead of the order price since the price of an order is unknown before the call (which is more realistic by the way). You can still override the valuation price in pre_segment_func_nb . Usage Buy 10 units each tick using closing price: >>> @njit ... def order_func_nb ( c , size ): ... return nb . order_nb ( size = size ) >>> close = pd . Series ([ 1 , 2 , 3 , 4 , 5 ]) >>> pf = vbt . Portfolio . from_order_func ( close , order_func_nb , 10 ) >>> pf . assets () 0 10.0 1 20.0 2 30.0 3 40.0 4 40.0 dtype: float64 >>> pf . cash () 0 90.0 1 70.0 2 40.0 3 0.0 4 0.0 dtype: float64 Reverse each position by first closing it. Keep state of last position to determine which position to open next (just as an example, there are easier ways to do this): >>> @njit ... def pre_group_func_nb ( c ): ... last_pos_state = np . array ([ - 1 ]) ... return ( last_pos_state ,) >>> @njit ... def order_func_nb ( c , last_pos_state ): ... if c . position_now != 0 : ... return nb . close_position_nb () ... ... if last_pos_state [ 0 ] == 1 : ... size = - np . inf # open short ... last_pos_state [ 0 ] = - 1 ... else : ... size = np . inf # open long ... last_pos_state [ 0 ] = 1 ... return nb . order_nb ( size = size ) >>> pf = vbt . Portfolio . from_order_func ( ... close , ... order_func_nb , ... pre_group_func_nb = pre_group_func_nb ... ) >>> pf . assets () 0 100.000000 1 0.000000 2 -66.666667 3 0.000000 4 26.666667 dtype: float64 >>> pf . cash () 0 0.000000 1 200.000000 2 400.000000 3 133.333333 4 0.000000 dtype: float64 Equal-weighted portfolio as in the example under simulate_nb() : >>> @njit ... def pre_group_func_nb ( c ): ... order_value_out = np . empty ( c . group_len , dtype = np . float_ ) ... return ( order_value_out ,) >>> @njit ... def pre_segment_func_nb ( c , order_value_out , size , price , size_type , direction ): ... for col in range ( c . from_col , c . to_col ): ... c . last_val_price [ col ] = nb . get_col_elem_nb ( c , col , price ) ... nb . sort_call_seq_nb ( c , size , size_type , direction , order_value_out ) ... return () >>> @njit ... def order_func_nb ( c , size , price , size_type , direction , fees , fixed_fees , slippage ): ... return nb . order_nb ( ... size = nb . get_elem_nb ( c , size ), ... price = nb . get_elem_nb ( c , price ), ... size_type = nb . get_elem_nb ( c , size_type ), ... direction = nb . get_elem_nb ( c , direction ), ... fees = nb . get_elem_nb ( c , fees ), ... fixed_fees = nb . get_elem_nb ( c , fixed_fees ), ... slippage = nb . get_elem_nb ( c , slippage ) ... ) >>> np . random . seed ( 42 ) >>> close = np . random . uniform ( 1 , 10 , size = ( 5 , 3 )) >>> size_template = vbt . RepEval ( 'np.asarray(1 / group_lens[0])' ) >>> pf = vbt . Portfolio . from_order_func ( ... close , ... order_func_nb , ... size_template , # order_args as *args ... vbt . Rep ( 'price' ), ... vbt . Rep ( 'size_type' ), ... vbt . Rep ( 'direction' ), ... vbt . Rep ( 'fees' ), ... vbt . Rep ( 'fixed_fees' ), ... vbt . Rep ( 'slippage' ), ... segment_mask = 2 , # rebalance every second tick ... pre_group_func_nb = pre_group_func_nb , ... pre_segment_func_nb = pre_segment_func_nb , ... pre_segment_args = ( ... size_template , ... vbt . Rep ( 'price' ), ... vbt . Rep ( 'size_type' ), ... vbt . Rep ( 'direction' ) ... ), ... broadcast_named_args = dict ( # broadcast against each other ... price = close , ... size_type = SizeType . TargetPercent , ... direction = Direction . LongOnly , ... fees = 0.001 , ... fixed_fees = 1. , ... slippage = 0.001 ... ), ... template_mapping = dict ( np = np ), # required by size_template ... cash_sharing = True , group_by = True , # one group with cash sharing ... ) >>> pf . asset_value ( group_by = False ) . vbt . plot () Templates are a very powerful tool to prepare any custom arguments after they are broadcast and before they are passed to the simulation function. In the example above, we use broadcast_named_args to broadcast some arguments against each other and templates to pass those objects to callbacks. Additionally, we used an evaluation template to compute the size based on the number of assets in each group. You may ask: why should we bother using broadcasting and templates if we could just pass size=1/3 ? Because of flexibility those features provide: we can now pass whatever parameter combinations we want and it will work flawlessly. For example, to create two groups of equally-allocated positions, we need to change only two parameters: >>> close = np . random . uniform ( 1 , 10 , size = ( 5 , 6 )) # 6 columns instead of 3 >>> group_by = [ 'g1' , 'g1' , 'g1' , 'g2' , 'g2' , 'g2' ] # 2 groups instead of 1 >>> pf [ 'g1' ] . asset_value ( group_by = False ) . vbt . plot () >>> pf [ 'g2' ] . asset_value ( group_by = False ) . vbt . plot () Combine multiple exit conditions. Exit early if the price hits some threshold before an actual exit: >>> @njit ... def pre_sim_func_nb ( c ): ... # We need to define stop price per column once ... stop_price = np . full ( c . target_shape [ 1 ], np . nan , dtype = np . float_ ) ... return ( stop_price ,) >>> @njit ... def order_func_nb ( c , stop_price , entries , exits , size ): ... # Select info related to this order ... entry_now = nb . get_elem_nb ( c , entries ) ... exit_now = nb . get_elem_nb ( c , exits ) ... size_now = nb . get_elem_nb ( c , size ) ... price_now = nb . get_elem_nb ( c , c . close ) ... stop_price_now = stop_price [ c . col ] ... ... # Our logic ... if entry_now : ... if c . position_now == 0 : ... return nb . order_nb ( ... size = size_now , ... price = price_now , ... direction = Direction . LongOnly ) ... elif exit_now or price_now >= stop_price_now : ... if c . position_now > 0 : ... return nb . order_nb ( ... size =- size_now , ... price = price_now , ... direction = Direction . LongOnly ) ... return NoOrder >>> @njit ... def post_order_func_nb ( c , stop_price , stop ): ... # Same broadcasting as for size ... stop_now = nb . get_elem_nb ( c , stop ) ... ... if c . order_result . status == OrderStatus . Filled : ... if c . order_result . side == OrderSide . Buy : ... # Position entered: Set stop condition ... stop_price [ c . col ] = ( 1 + stop_now ) * c . order_result . price ... else : ... # Position exited: Remove stop condition ... stop_price [ c . col ] = np . nan >>> def simulate ( close , entries , exits , size , threshold ): ... return vbt . Portfolio . from_order_func ( ... close , ... order_func_nb , ... vbt . Rep ( 'entries' ), vbt . Rep ( 'exits' ), vbt . Rep ( 'size' ), # order_args ... pre_sim_func_nb = pre_sim_func_nb , ... post_order_func_nb = post_order_func_nb , ... post_order_args = ( vbt . Rep ( 'threshold' ),), ... broadcast_named_args = dict ( # broadcast against each other ... entries = entries , ... exits = exits , ... size = size , ... threshold = threshold ... ) ... ) >>> close = pd . Series ([ 10 , 11 , 12 , 13 , 14 ]) >>> entries = pd . Series ([ True , True , False , False , False ]) >>> exits = pd . Series ([ False , False , False , True , True ]) >>> simulate ( close , entries , exits , np . inf , 0.1 ) . asset_flow () 0 10.0 1 0.0 2 -10.0 3 0.0 4 0.0 dtype: float64 >>> simulate ( close , entries , exits , np . inf , 0.2 ) . asset_flow () 0 10.0 1 0.0 2 -10.0 3 0.0 4 0.0 dtype: float64 >>> simulate ( close , entries , exits , np . nan ) . asset_flow () 0 10.0 1 0.0 2 0.0 3 -10.0 4 0.0 dtype: float64 The reason why stop of 10% does not result in an order at the second time step is because it comes at the same time as entry, so it must wait until no entry is present. This can be changed by replacing the statement \"elif\" with \"if\", which would execute an exit regardless if an entry is present (similar to using ConflictMode.Opposite in Portfolio.from_signals() ). We can also test the parameter combinations above all at once (thanks to broadcasting): >>> size = pd . DataFrame ( ... [[ 0.1 , 0.2 , np . nan ]], ... columns = pd . Index ([ '0.1' , '0.2' , 'nan' ], name = 'size' ) ... ) >>> simulate ( close , entries , exits , np . inf , size ) . asset_flow () size 0.1 0.2 nan 0 10.0 10.0 10.0 1 0.0 0.0 0.0 2 -10.0 -10.0 0.0 3 0.0 0.0 -10.0 4 0.0 0.0 0.0 Let's illustrate how to generate multiple orders per symbol and bar. For each bar, buy at open and sell at close: >>> @njit ... def flex_order_func_nb ( c , open , size ): ... if c . call_idx == 0 : ... return c . from_col , nb . order_nb ( size = size , price = open [ c . i , c . from_col ]) ... if c . call_idx == 1 : ... return c . from_col , nb . close_position_nb ( price = c . close [ c . i , c . from_col ]) ... return - 1 , NoOrder >>> open = pd . DataFrame ({ 'a' : [ 1 , 2 , 3 ], 'b' : [ 4 , 5 , 6 ]}) >>> close = pd . DataFrame ({ 'a' : [ 2 , 3 , 4 ], 'b' : [ 3 , 4 , 5 ]}) >>> size = 1 >>> pf = vbt . Portfolio . from_order_func ( ... close , ... flex_order_func_nb , ... to_2d_array ( open ), size , ... flexible = True , max_orders = close . shape [ 0 ] * close . shape [ 1 ] * 2 ) >>> pf . orders . records_readable Order Id Timestamp Column Size Price Fees Side 0 0 0 a 1.0 1.0 0.0 Buy 1 1 0 a 1.0 2.0 0.0 Sell 2 2 1 a 1.0 2.0 0.0 Buy 3 3 1 a 1.0 3.0 0.0 Sell 4 4 2 a 1.0 3.0 0.0 Buy 5 5 2 a 1.0 4.0 0.0 Sell 6 6 0 b 1.0 4.0 0.0 Buy 7 7 0 b 1.0 3.0 0.0 Sell 8 8 1 b 1.0 5.0 0.0 Buy 9 9 1 b 1.0 4.0 0.0 Sell 10 10 2 b 1.0 6.0 0.0 Buy 11 11 2 b 1.0 5.0 0.0 Sell Warning Each bar is effectively a black box - we don't know how the price moves inside. Since trades must come in an order that replicates that of the real world, the only reliable pieces of information are the opening and the closing price.","title":"from_order_func()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.from_orders","text":"Portfolio . from_orders ( close , size = None , size_type = None , direction = None , price = None , fees = None , fixed_fees = None , slippage = None , min_size = None , max_size = None , size_granularity = None , reject_prob = None , lock_cash = None , allow_partial = None , raise_reject = None , log = None , val_price = None , init_cash = None , cash_sharing = None , call_seq = None , ffill_val_price = None , update_value = None , max_orders = None , max_logs = None , seed = None , group_by = None , broadcast_kwargs = None , wrapper_kwargs = None , freq = None , attach_call_seq = None , ** kwargs ) Simulate portfolio from orders - size, price, fees, and other information. Args close :\u2002 array_like Last asset price at each time step. Will broadcast. Used for calculating unrealized PnL and portfolio value. size :\u2002 float or array_like Size to order. See Order.size . Will broadcast. size_type :\u2002 SizeType or array_like See SizeType . See Order.size_type . Will broadcast. Note SizeType.Percent does not support position reversal. Switch to a single direction. Warning Be cautious using SizeType.Percent with call_seq set to 'auto'. To execute sell orders before buy orders, the value of each order in the group needs to be approximated in advance. But since SizeType.Percent depends upon the cash balance, which cannot be calculated in advance since it may change after each order, this can yield a non-optimal call sequence. direction :\u2002 Direction or array_like See Direction . See Order.direction . Will broadcast. price :\u2002 array_like of float Order price. See Order.price . Defaults to np.inf . Will broadcast. Note Make sure to use the same timestamp for all order prices in the group with cash sharing and call_seq set to CallSeqType.Auto . fees :\u2002 float or array_like Fees in percentage of the order value. See Order.fees . Will broadcast. fixed_fees :\u2002 float or array_like Fixed amount of fees to pay per order. See Order.fixed_fees . Will broadcast. slippage :\u2002 float or array_like Slippage in percentage of price. See Order.slippage . Will broadcast. min_size :\u2002 float or array_like Minimum size for an order to be accepted. See Order.min_size . Will broadcast. max_size :\u2002 float or array_like Maximum size for an order. See Order.max_size . Will broadcast. Will be partially filled if exceeded. size_granularity :\u2002 float or array_like Granularity of the size. See Order.size_granularity . Will broadcast. reject_prob :\u2002 float or array_like Order rejection probability. See Order.reject_prob . Will broadcast. lock_cash :\u2002 bool or array_like Whether to lock cash when shorting. See Order.lock_cash . Will broadcast. allow_partial :\u2002 bool or array_like Whether to allow partial fills. See Order.allow_partial . Will broadcast. Does not apply when size is np.inf . raise_reject :\u2002 bool or array_like Whether to raise an exception if order gets rejected. See Order.raise_reject . Will broadcast. log :\u2002 bool or array_like Whether to log orders. See Order.log . Will broadcast. val_price :\u2002 array_like of float Asset valuation price. Will broadcast. Any -np.inf element is replaced by the latest valuation price (the previous close or the latest known valuation price if ffill_val_price ). Any np.inf element is replaced by the current order price. Used at the time of decision making to calculate value of each asset in the group, for example, to convert target value into target amount. Note In contrast to Portfolio.from_order_func() , order price is known beforehand (kind of), thus val_price is set to the current order price (using np.inf ) by default. To valuate using previous close, set it in the settings to -np.inf . Note Make sure to use timestamp for val_price that comes before timestamps of all orders in the group with cash sharing (previous close for example), otherwise you're cheating yourself. init_cash :\u2002 InitCashMode , float or array_like of float Initial capital. By default, will broadcast to the number of columns. If cash sharing is enabled, will broadcast to the number of groups. See InitCashMode to find optimal initial cash. Note Mode InitCashMode.AutoAlign is applied after the portfolio is initialized to set the same initial cash for all columns/groups. Changing grouping will change the initial cash, so be aware when indexing. cash_sharing :\u2002 bool Whether to share cash within the same group. If group_by is None, group_by becomes True to form a single group with cash sharing. Warning Introduces cross-asset dependencies. This method presumes that in a group of assets that share the same capital all orders will be executed within the same tick and retain their price regardless of their position in the queue, even though they depend upon each other and thus cannot be executed in parallel. call_seq :\u2002 CallSeqType or array_like Default sequence of calls per row and group. Each value in this sequence should indicate the position of column in the group to call next. Processing of call_seq goes always from left to right. For example, [2, 0, 1] would first call column 'c', then 'a', and finally 'b'. Use CallSeqType to select a sequence type. Set to array to specify custom sequence. Will not broadcast. If CallSeqType.Auto selected, rearranges calls dynamically based on order value. Calculates value of all orders per row and group, and sorts them by this value. Sell orders will be executed first to release funds for buy orders. Warning CallSeqType.Auto should be used with caution: It not only presumes that order prices are known beforehand, but also that orders can be executed in arbitrary order and still retain their price. In reality, this is hardly the case: after processing one asset, some time has passed and the price for other assets might have already changed. Even if you're able to specify a slippage large enough to compensate for this behavior, slippage itself should depend upon execution order. This method doesn't let you do that. If one order is rejected, it still may execute next orders and possibly leave them without required funds. For more control, use Portfolio.from_order_func() . ffill_val_price :\u2002 bool Whether to track valuation price only if it's known. Otherwise, unknown close will lead to NaN in valuation price at the next timestamp. update_value :\u2002 bool Whether to update group value after each filled order. max_orders :\u2002 int Size of the order records array. Defaults to the number of elements in the broadcasted shape. Set to a lower number if you run out of memory. max_logs :\u2002 int Size of the log records array. Defaults to the number of elements in the broadcasted shape if any of the log is True, otherwise to 1. Set to a lower number if you run out of memory. seed :\u2002 int Seed to be set for both call_seq and at the beginning of the simulation. group_by :\u2002 any Group columns. See ColumnGrouper . broadcast_kwargs :\u2002 dict Keyword arguments passed to broadcast() . wrapper_kwargs :\u2002 dict Keyword arguments passed to ArrayWrapper . freq :\u2002 any Index frequency in case it cannot be parsed from close . attach_call_seq :\u2002 bool Whether to pass call_seq to the constructor. Makes sense if you want to analyze some metrics in the simulation order. Otherwise, just takes memory. **kwargs Keyword arguments passed to the __init__ method. All broadcastable arguments will broadcast using broadcast() but keep original shape to utilize flexible indexing and to save memory. For defaults, see portfolio in settings . Note When call_seq is not CallSeqType.Auto , at each timestamp, processing of the assets in a group goes strictly in order defined in call_seq . This order can't be changed dynamically. This has one big implication for this particular method: the last asset in the call stack cannot be processed until other assets are processed. This is the reason why rebalancing cannot work properly in this setting: one has to specify percentages for all assets beforehand and then tweak the processing order to sell to-be-sold assets first in order to release funds for to-be-bought assets. This can be automatically done by using CallSeqType.Auto . Hint All broadcastable arguments can be set per frame, series, row, column, or element. Usage Buy 10 units each tick: >>> close = pd . Series ([ 1 , 2 , 3 , 4 , 5 ]) >>> pf = vbt . Portfolio . from_orders ( close , 10 ) >>> pf . assets () 0 10.0 1 20.0 2 30.0 3 40.0 4 40.0 dtype: float64 >>> pf . cash () 0 90.0 1 70.0 2 40.0 3 0.0 4 0.0 dtype: float64 Reverse each position by first closing it: >>> size = [ 1 , 0 , - 1 , 0 , 1 ] >>> pf = vbt . Portfolio . from_orders ( close , size , size_type = 'targetpercent' ) >>> pf . assets () 0 100.000000 1 0.000000 2 -66.666667 3 0.000000 4 26.666667 dtype: float64 >>> pf . cash () 0 0.000000 1 200.000000 2 400.000000 3 133.333333 4 0.000000 dtype: float64 Equal-weighted portfolio as in simulate_nb() example (it's more compact but has less control over execution): >>> np . random . seed ( 42 ) >>> close = pd . DataFrame ( np . random . uniform ( 1 , 10 , size = ( 5 , 3 ))) >>> size = pd . Series ( np . full ( 5 , 1 / 3 )) # each column 33.3% >>> size [ 1 :: 2 ] = np . nan # skip every second tick >>> pf = vbt . Portfolio . from_orders ( ... close , # acts both as reference and order price here ... size , ... size_type = 'targetpercent' , ... call_seq = 'auto' , # first sell then buy ... group_by = True , # one group ... cash_sharing = True , # assets share the same cash ... fees = 0.001 , fixed_fees = 1. , slippage = 0.001 # costs ... ) >>> pf . asset_value ( group_by = False ) . vbt . plot ()","title":"from_orders()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.from_random_signals","text":"Portfolio . from_random_signals ( close , n = None , prob = None , entry_prob = None , exit_prob = None , param_product = False , seed = None , run_kwargs = None , ** kwargs ) Simulate portfolio from random entry and exit signals. Generates signals based either on the number of signals n or the probability of encountering a signal prob . If n is set, see RANDNX . If prob is set, see RPROBNX . Based on Portfolio.from_signals() . Note To generate random signals, the shape of close is used. Broadcasting with other arrays happens after the generation. Usage Test multiple combinations of random entries and exits: >>> close = pd . Series ([ 1 , 2 , 3 , 4 , 5 ]) >>> pf = vbt . Portfolio . from_random_signals ( close , n = [ 2 , 1 , 0 ], seed = 42 ) >>> pf . orders . count () randnx_n 2 4 1 2 0 0 Name: count, dtype: int64 Test the Cartesian product of entry and exit encounter probabilities: >>> pf = vbt . Portfolio . from_random_signals ( ... close , ... entry_prob = [ 0 , 0.5 , 1 ], ... exit_prob = [ 0 , 0.5 , 1 ], ... param_product = True , ... seed = 42 ) >>> pf . orders . count () rprobnx_entry_prob rprobnx_exit_prob 0.0 0.0 0 0.5 0 1.0 0 0.5 0.0 1 0.5 4 1.0 3 1.0 0.0 1 0.5 4 1.0 5 Name: count, dtype: int64","title":"from_random_signals()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.from_signals","text":"Portfolio . from_signals ( close , entries = None , exits = None , short_entries = None , short_exits = None , signal_func_nb = no_signal_func_nb , signal_args = (), size = None , size_type = None , price = None , fees = None , fixed_fees = None , slippage = None , min_size = None , max_size = None , size_granularity = None , reject_prob = None , lock_cash = None , allow_partial = None , raise_reject = None , log = None , accumulate = None , upon_long_conflict = None , upon_short_conflict = None , upon_dir_conflict = None , upon_opposite_entry = None , direction = None , val_price = None , open = None , high = None , low = None , sl_stop = None , sl_trail = None , tp_stop = None , stop_entry_price = None , stop_exit_price = None , upon_stop_exit = None , upon_stop_update = None , adjust_sl_func_nb = no_adjust_sl_func_nb , adjust_sl_args = (), adjust_tp_func_nb = no_adjust_tp_func_nb , adjust_tp_args = (), use_stops = None , init_cash = None , cash_sharing = None , call_seq = None , ffill_val_price = None , update_value = None , max_orders = None , max_logs = None , seed = None , group_by = None , broadcast_named_args = None , broadcast_kwargs = None , template_mapping = None , wrapper_kwargs = None , freq = None , attach_call_seq = None , ** kwargs ) Simulate portfolio from entry and exit signals. See simulate_from_signal_func_nb() . You have three options to provide signals: entries and exits : The direction of each pair of signals is taken from direction argument. Best to use when the direction doesn't change throughout time. Uses dir_enex_signal_func_nb() as signal_func_nb . Hint entries and exits can be easily translated to direction-aware signals: (True, True, 'longonly') -> True, True, False, False (True, True, 'shortonly') -> False, False, True, True (True, True, 'both') -> True, False, True, False entries (acting as long), exits (acting as long), short_entries , and short_exits : The direction is already built into the arrays. Best to use when the direction changes frequently (for example, if you have one indicator providing long signals and one providing short signals). Uses ls_enex_signal_func_nb() as signal_func_nb . signal_func_nb and signal_args : Custom signal function that returns direction-aware signals. Best to use when signals should be placed dynamically based on custom conditions. Args close :\u2002 array_like See Portfolio.from_orders() . entries :\u2002 array_like of bool Boolean array of entry signals. Defaults to True if all other signal arrays are not set, otherwise False. Will broadcast. If short_entries and short_exits are not set: Acts as a long signal if direction is all or longonly , otherwise short. If short_entries or short_exits are set: Acts as long_entries . exits :\u2002 array_like of bool Boolean array of exit signals. Defaults to False. Will broadcast. If short_entries and short_exits are not set: Acts as a short signal if direction is all or longonly , otherwise long. If short_entries or short_exits are set: Acts as long_exits . short_entries :\u2002 array_like of bool Boolean array of short entry signals. Defaults to False. Will broadcast. short_exits :\u2002 array_like of bool Boolean array of short exit signals. Defaults to False. Will broadcast. signal_func_nb :\u2002 callable Function called to generate signals. Should accept SignalContext and *signal_args . Should return long entry signal, long exit signal, short entry signal, and short exit signal. Note Stop signal has priority: signal_func_nb is executed only if there is no stop signal. signal_args :\u2002 tuple Packed arguments passed to signal_func_nb . Defaults to () . size :\u2002 float or array_like See Portfolio.from_orders() . Note Negative size is not allowed. You should express direction using signals. size_type :\u2002 SizeType or array_like See Portfolio.from_orders() . Only SizeType.Amount , SizeType.Value , and SizeType.Percent are supported. Other modes such as target percentage are not compatible with signals since their logic may contradict the direction of the signal. Note SizeType.Percent does not support position reversal. Switch to a single direction or use vectorbt.portfolio.enums.OppositeEntryMode.Close to close the position first. See warning in Portfolio.from_orders() . price :\u2002 array_like of float See Portfolio.from_orders() . fees :\u2002 float or array_like See Portfolio.from_orders() . fixed_fees :\u2002 float or array_like See Portfolio.from_orders() . slippage :\u2002 float or array_like See Portfolio.from_orders() . min_size :\u2002 float or array_like See Portfolio.from_orders() . max_size :\u2002 float or array_like See Portfolio.from_orders() . Will be partially filled if exceeded. You might not be able to properly close the position if accumulation is enabled and max_size is too low. size_granularity :\u2002 float or array_like See Portfolio.from_orders() . reject_prob :\u2002 float or array_like See Portfolio.from_orders() . lock_cash :\u2002 bool or array_like See Portfolio.from_orders() . allow_partial :\u2002 bool or array_like See Portfolio.from_orders() . raise_reject :\u2002 bool or array_like See Portfolio.from_orders() . log :\u2002 bool or array_like See Portfolio.from_orders() . accumulate :\u2002 bool , AccumulationMode or array_like See AccumulationMode . If True, becomes 'both'. If False, becomes 'disabled'. Will broadcast. When enabled, Portfolio.from_signals() behaves similarly to Portfolio.from_orders() . upon_long_conflict :\u2002 ConflictMode or array_like Conflict mode for long signals. See ConflictMode . Will broadcast. upon_short_conflict :\u2002 ConflictMode or array_like Conflict mode for short signals. See ConflictMode . Will broadcast. upon_dir_conflict :\u2002 DirectionConflictMode or array_like See DirectionConflictMode . Will broadcast. upon_opposite_entry :\u2002 OppositeEntryMode or array_like See OppositeEntryMode . Will broadcast. direction :\u2002 Direction or array_like See Portfolio.from_orders() . Takes only effect if short_entries and short_exits are not set. val_price :\u2002 array_like of float See Portfolio.from_orders() . open :\u2002 array_like of float First asset price at each time step. Defaults to np.nan , which gets replaced by close . Will broadcast. Used solely for stop signals. high :\u2002 array_like of float Highest asset price at each time step. Defaults to np.nan , which gets replaced by the maximum out of open and close . Will broadcast. Used solely for stop signals. low :\u2002 array_like of float Lowest asset price at each time step. Defaults to np.nan , which gets replaced by the minimum out of open and close . Will broadcast. Used solely for stop signals. sl_stop :\u2002 array_like of float Stop loss. Will broadcast. A percentage below/above the acquisition price for long/short position. Note that 0.01 = 1%. sl_trail :\u2002 array_like of bool Whether sl_stop should be trailing. Will broadcast. tp_stop :\u2002 array_like of float Take profit. Will broadcast. A percentage above/below the acquisition price for long/short position. Note that 0.01 = 1%. stop_entry_price :\u2002 StopEntryPrice or array_like See StopEntryPrice . Will broadcast. If provided on per-element basis, gets applied upon entry. stop_exit_price :\u2002 StopExitPrice or array_like See StopExitPrice . Will broadcast. If provided on per-element basis, gets applied upon exit. upon_stop_exit :\u2002 StopExitMode or array_like See StopExitMode . Will broadcast. If provided on per-element basis, gets applied upon exit. upon_stop_update :\u2002 StopUpdateMode or array_like See StopUpdateMode . Will broadcast. Only has effect if accumulation is enabled. If provided on per-element basis, gets applied upon repeated entry. adjust_sl_func_nb :\u2002 callable Function to adjust stop loss. Defaults to no_adjust_sl_func_nb() . Called for each element before each row. Should accept AdjustSLContext and *adjust_sl_args . Should return a tuple of a new stop value and trailing flag. adjust_sl_args :\u2002 tuple Packed arguments passed to adjust_sl_func_nb . Defaults to () . adjust_tp_func_nb :\u2002 callable Function to adjust take profit. Defaults to no_adjust_tp_func_nb() . Called for each element before each row. Should accept AdjustTPContext and *adjust_tp_args . of the stop, and *adjust_tp_args . Should return a new stop value. adjust_tp_args :\u2002 tuple Packed arguments passed to adjust_tp_func_nb . Defaults to () . use_stops :\u2002 bool Whether to use stops. Defaults to None, which becomes True if any of the stops are not NaN or any of the adjustment functions are custom. Disable this to make simulation a bit faster for simple use cases. init_cash :\u2002 InitCashMode , float or array_like of float See Portfolio.from_orders() . cash_sharing :\u2002 bool See Portfolio.from_orders() . call_seq :\u2002 CallSeqType or array_like See Portfolio.from_orders() . ffill_val_price :\u2002 bool See Portfolio.from_orders() . update_value :\u2002 bool See Portfolio.from_orders() . max_orders :\u2002 int See Portfolio.from_orders() . max_logs :\u2002 int See Portfolio.from_orders() . seed :\u2002 int See Portfolio.from_orders() . group_by :\u2002 any See Portfolio.from_orders() . broadcast_named_args :\u2002 dict Dictionary with named arguments to broadcast. You can then pass argument names to the functions and this method will substitute them by their corresponding broadcasted objects. broadcast_kwargs :\u2002 dict See Portfolio.from_orders() . template_mapping :\u2002 mapping Mapping to replace templates in arguments. wrapper_kwargs :\u2002 dict See Portfolio.from_orders() . freq :\u2002 any See Portfolio.from_orders() . attach_call_seq :\u2002 bool See Portfolio.from_orders() . **kwargs Keyword arguments passed to the __init__ method. All broadcastable arguments will broadcast using broadcast() but keep original shape to utilize flexible indexing and to save memory. For defaults, see portfolio in settings . Note Stop signal has priority - it's executed before other signals within the same bar. That is, if a stop signal is present, no other signals are generated and executed since there is a limit of one order per symbol and bar. Hint If you generated signals using close price, don't forget to shift your signals by one tick forward, for example, with signals.vbt.fshift(1) . In general, make sure to use a price that comes after the signal. Also see notes and hints for Portfolio.from_orders() . Usage By default, if all signal arrays are None, entries becomes True, which opens a position at the very first tick and does nothing else: >>> close = pd . Series ([ 1 , 2 , 3 , 4 , 5 ]) >>> pf = vbt . Portfolio . from_signals ( close , size = 1 ) >>> pf . asset_flow () 0 1.0 1 0.0 2 0.0 3 0.0 4 0.0 dtype: float64 Entry opens long, exit closes long: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... size = 1 , ... direction = 'longonly' ... ) >>> pf . asset_flow () 0 1.0 1 0.0 2 0.0 3 -1.0 4 0.0 dtype: float64 >>> # Using direction-aware arrays instead of `direction` >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), # long_entries ... exits = pd . Series ([ False , False , True , True , True ]), # long_exits ... short_entries = False , ... short_exits = False , ... size = 1 ... ) >>> pf . asset_flow () 0 1.0 1 0.0 2 0.0 3 -1.0 4 0.0 dtype: float64 Notice how both short_entries and short_exits are provided as constants - as any other broadcastable argument, they are treated as arrays where each element is False. Entry opens short, exit closes short: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... size = 1 , ... direction = 'shortonly' ... ) >>> pf . asset_flow () 0 -1.0 1 0.0 2 0.0 3 1.0 4 0.0 dtype: float64 >>> # Using direction-aware arrays instead of `direction` >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = False , # long_entries ... exits = False , # long_exits ... short_entries = pd . Series ([ True , True , True , False , False ]), ... short_exits = pd . Series ([ False , False , True , True , True ]), ... size = 1 ... ) >>> pf . asset_flow () 0 -1.0 1 0.0 2 0.0 3 1.0 4 0.0 dtype: float64 Entry opens long and closes short, exit closes long and opens short: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... size = 1 , ... direction = 'both' ... ) >>> pf . asset_flow () 0 1.0 1 0.0 2 0.0 3 -2.0 4 0.0 dtype: float64 >>> # Using direction-aware arrays instead of `direction` >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), # long_entries ... exits = False , # long_exits ... short_entries = pd . Series ([ False , False , True , True , True ]), ... short_exits = False , ... size = 1 ... ) >>> pf . asset_flow () 0 1.0 1 0.0 2 0.0 3 -2.0 4 0.0 dtype: float64 More complex signal combinations are best expressed using direction-aware arrays. For example, ignore opposite signals as long as the current position is open: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , False , False , False , False ]), # long_entries ... exits = pd . Series ([ False , False , True , False , False ]), # long_exits ... short_entries = pd . Series ([ False , True , False , True , False ]), ... short_exits = pd . Series ([ False , False , False , False , True ]), ... size = 1 , ... upon_opposite_entry = 'ignore' ... ) >>> pf . asset_flow () 0 1.0 1 0.0 2 -1.0 3 -1.0 4 1.0 dtype: float64 First opposite signal closes the position, second one opens a new position: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... size = 1 , ... direction = 'both' , ... upon_opposite_entry = 'close' ... ) >>> pf . asset_flow () 0 1.0 1 0.0 2 0.0 3 -1.0 4 -1.0 dtype: float64 If both long entry and exit signals are True (a signal conflict), choose exit: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... size = 1. , ... direction = 'longonly' , ... upon_long_conflict = 'exit' ) >>> pf . asset_flow () 0 1.0 1 0.0 2 -1.0 3 0.0 4 0.0 dtype: float64 If both long entry and short entry signal are True (a direction conflict), choose short: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... size = 1. , ... direction = 'both' , ... upon_dir_conflict = 'short' ) >>> pf . asset_flow () 0 1.0 1 0.0 2 -2.0 3 0.0 4 0.0 dtype: float64 Note Remember that when direction is set to 'both', entries become long_entries and exits become short_entries , so this becomes a conflict of directions rather than signals. If there are both signal and direction conflicts: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = True , # long_entries ... exits = True , # long_exits ... short_entries = True , ... short_exits = True , ... size = 1 , ... upon_long_conflict = 'entry' , ... upon_short_conflict = 'entry' , ... upon_dir_conflict = 'short' ... ) >>> pf . asset_flow () 0 -1.0 1 0.0 2 0.0 3 0.0 4 0.0 dtype: float64 Turn on accumulation of signals. Entry means long order, exit means short order (acts similar to from_orders ): >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... size = 1. , ... direction = 'both' , ... accumulate = True ) >>> pf . asset_flow () 0 1.0 1 1.0 2 0.0 3 -1.0 4 -1.0 dtype: float64 Allow increasing a position (of any direction), deny decreasing a position: >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... size = 1. , ... direction = 'both' , ... accumulate = 'addonly' ) >>> pf . asset_flow () 0 1.0 << open a long position 1 1.0 << add to the position 2 0.0 3 -3.0 << close and open a short position 4 -1.0 << add to the position dtype: float64 Testing multiple parameters (via broadcasting): >>> pf = vbt . Portfolio . from_signals ( ... close , ... entries = pd . Series ([ True , True , True , False , False ]), ... exits = pd . Series ([ False , False , True , True , True ]), ... direction = [ list ( Direction )], ... broadcast_kwargs = dict ( columns_from = Direction . _fields )) >>> pf . asset_flow () Long Short All 0 100.0 -100.0 100.0 1 0.0 0.0 0.0 2 0.0 0.0 0.0 3 -100.0 50.0 -200.0 4 0.0 0.0 0.0 Set risk/reward ratio by passing trailing stop loss and take profit thresholds: >>> close = pd . Series ([ 10 , 11 , 12 , 11 , 10 , 9 ]) >>> entries = pd . Series ([ True , False , False , False , False , False ]) >>> exits = pd . Series ([ False , False , False , False , False , True ]) >>> pf = vbt . Portfolio . from_signals ( ... close , entries , exits , ... sl_stop = 0.1 , sl_trail = True , tp_stop = 0.2 ) # take profit hit >>> pf . asset_flow () 0 10.0 1 0.0 2 -10.0 3 0.0 4 0.0 5 0.0 dtype: float64 >>> pf = vbt . Portfolio . from_signals ( ... close , entries , exits , ... sl_stop = 0.1 , sl_trail = True , tp_stop = 0.3 ) # stop loss hit >>> pf . asset_flow () 0 10.0 1 0.0 2 0.0 3 0.0 4 -10.0 5 0.0 dtype: float64 >>> pf = vbt . Portfolio . from_signals ( ... close , entries , exits , ... sl_stop = np . inf , sl_trail = True , tp_stop = np . inf ) # nothing hit, exit as usual >>> pf . asset_flow () 0 10.0 1 0.0 2 0.0 3 0.0 4 0.0 5 -10.0 dtype: float64 Note When the stop price is hit, the stop signal invalidates any other signal defined for this bar. Thus, make sure that your signaling logic happens at the very end of the bar (for example, by using the closing price), otherwise you may expose yourself to a look-ahead bias. See StopExitPrice for more details. We can implement our own stop loss or take profit, or adjust the existing one at each time step. Let's implement stepped stop-loss : >>> @njit ... def adjust_sl_func_nb ( c ): ... current_profit = ( c . val_price_now - c . init_price ) / c . init_price ... if current_profit >= 0.40 : ... return 0.25 , True ... elif current_profit >= 0.25 : ... return 0.15 , True ... elif current_profit >= 0.20 : ... return 0.07 , True ... return c . curr_stop , c . curr_trail >>> close = pd . Series ([ 10 , 11 , 12 , 11 , 10 ]) >>> pf = vbt . Portfolio . from_signals ( close , adjust_sl_func_nb = adjust_sl_func_nb ) >>> pf . asset_flow () 0 10.0 1 0.0 2 0.0 3 -10.0 # 7% from 12 hit 4 11.0 dtype: float64 Sometimes there is a need to provide or transform signals dynamically. For this, we can implement a custom signal function signal_func_nb . For example, let's implement a signal function that takes two numerical arrays - long and short one - and transforms them into 4 direction-aware boolean arrays that vectorbt understands: >>> @njit ... def signal_func_nb ( c , long_num_arr , short_num_arr ): ... long_num = nb . get_elem_nb ( c , long_num_arr ) ... short_num = nb . get_elem_nb ( c , short_num_arr ) ... is_long_entry = long_num > 0 ... is_long_exit = long_num < 0 ... is_short_entry = short_num > 0 ... is_short_exit = short_num < 0 ... return is_long_entry , is_long_exit , is_short_entry , is_short_exit >>> pf = vbt . Portfolio . from_signals ( ... pd . Series ([ 1 , 2 , 3 , 4 , 5 ]), ... signal_func_nb = signal_func_nb , ... signal_args = ( vbt . Rep ( 'long_num_arr' ), vbt . Rep ( 'short_num_arr' )), ... broadcast_named_args = dict ( ... long_num_arr = pd . Series ([ 1 , 0 , - 1 , 0 , 0 ]), ... short_num_arr = pd . Series ([ 0 , 1 , 0 , 1 , - 1 ]) ... ), ... size = 1 , ... upon_opposite_entry = 'ignore' ... ) >>> pf . asset_flow () 0 1.0 1 0.0 2 -1.0 3 -1.0 4 1.0 dtype: float64 Passing both arrays as broadcast_named_args broadcasts them internally as any other array, so we don't have to worry about their dimensions every time we change our data.","title":"from_signals()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.get_drawdowns","text":"Portfolio . get_drawdowns ( group_by = None , wrap_kwargs = None , wrapper_kwargs = None , ** kwargs ) Get drawdown records from Portfolio.value() . See Drawdowns .","title":"get_drawdowns()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.get_entry_trades","text":"Portfolio . get_entry_trades ( group_by = None , ** kwargs ) Get entry trade records. See EntryTrades .","title":"get_entry_trades()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.get_exit_trades","text":"Portfolio . get_exit_trades ( group_by = None , ** kwargs ) Get exit trade records. See ExitTrades .","title":"get_exit_trades()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.get_filled_close","text":"Portfolio . get_filled_close ( wrap_kwargs = None ) Forward-backward-fill NaN values in Portfolio.close","title":"get_filled_close()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.get_init_cash","text":"Portfolio . get_init_cash ( group_by = None , wrap_kwargs = None ) Initial amount of cash per column/group with default arguments. Note If the initial cash balance was found automatically and no own cash is used throughout the simulation (for example, when shorting), it will be set to 1 instead of 0 to enable smooth calculation of returns.","title":"get_init_cash()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.get_logs","text":"Portfolio . get_logs ( group_by = None , ** kwargs ) Get log records. See Logs .","title":"get_logs()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.get_orders","text":"Portfolio . get_orders ( group_by = None , ** kwargs ) Get order records. See Orders .","title":"get_orders()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.get_positions","text":"Portfolio . get_positions ( group_by = None , ** kwargs ) Get position records. See Positions .","title":"get_positions()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.get_qs","text":"Portfolio . get_qs ( group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) Get quantstats adapter of type QSAdapter . **kwargs are passed to the adapter constructor.","title":"get_qs()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.get_returns_acc","text":"Portfolio . get_returns_acc ( group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , defaults = None , ** kwargs ) Get returns accessor of type ReturnsAccessor . Hint You can find most methods of this accessor as (cacheable) attributes of this portfolio.","title":"get_returns_acc()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.get_trades","text":"Portfolio . get_trades ( group_by = None , ** kwargs ) Get trade/position records depending upon Portfolio.trades_type .","title":"get_trades()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.gross_exposure","text":"Portfolio . gross_exposure ( direction = 'both' , group_by = None , wrap_kwargs = None ) Get gross exposure.","title":"gross_exposure()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.indexing_func","text":"Portfolio . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on Portfolio .","title":"indexing_func()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.information_ratio","text":"Portfolio . information_ratio ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.information_ratio() .","title":"information_ratio()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.init_cash","text":"Portfolio.get_init_cash() with default arguments.","title":"init_cash"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.log_records","text":"A structured NumPy array of log records.","title":"log_records"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.logs","text":"Portfolio.get_logs() with default arguments.","title":"logs"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.max_drawdown","text":"Portfolio . max_drawdown ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.max_drawdown() .","title":"max_drawdown()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.metrics","text":"Metrics supported by Portfolio . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2c8c8>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2c950>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2c9d8>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"start_value\" : { \"title\" : \"Start Value\" , \"calc_func\" : \"get_init_cash\" , \"tags\" : \"portfolio\" }, \"end_value\" : { \"title\" : \"End Value\" , \"calc_func\" : \"final_value\" , \"tags\" : \"portfolio\" }, \"total_return\" : { \"title\" : \"Total Return [%]\" , \"calc_func\" : \"total_return\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2ca60>\" , \"tags\" : \"portfolio\" }, \"benchmark_return\" : { \"title\" : \"Benchmark Return [%]\" , \"calc_func\" : \"benchmark_rets.vbt.returns.total\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2cae8>\" , \"tags\" : \"portfolio\" }, \"max_gross_exposure\" : { \"title\" : \"Max Gross Exposure [%]\" , \"calc_func\" : \"gross_exposure.vbt.max\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2cb70>\" , \"tags\" : \"portfolio\" }, \"total_fees_paid\" : { \"title\" : \"Total Fees Paid\" , \"calc_func\" : \"orders.fees.sum\" , \"tags\" : [ \"portfolio\" , \"orders\" ] }, \"max_dd\" : { \"title\" : \"Max Drawdown [%]\" , \"calc_func\" : \"drawdowns.max_drawdown\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2cbf8>\" , \"tags\" : [ \"portfolio\" , \"drawdowns\" ] }, \"max_dd_duration\" : { \"title\" : \"Max Drawdown Duration\" , \"calc_func\" : \"drawdowns.max_duration\" , \"fill_wrap_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"drawdowns\" , \"duration\" ] }, \"total_trades\" : { \"title\" : \"Total Trades\" , \"calc_func\" : \"trades.count\" , \"incl_open\" : true , \"tags\" : [ \"portfolio\" , \"trades\" ] }, \"total_closed_trades\" : { \"title\" : \"Total Closed Trades\" , \"calc_func\" : \"trades.closed.count\" , \"tags\" : [ \"portfolio\" , \"trades\" , \"closed\" ] }, \"total_open_trades\" : { \"title\" : \"Total Open Trades\" , \"calc_func\" : \"trades.open.count\" , \"incl_open\" : true , \"tags\" : [ \"portfolio\" , \"trades\" , \"open\" ] }, \"open_trade_pnl\" : { \"title\" : \"Open Trade PnL\" , \"calc_func\" : \"trades.open.pnl.sum\" , \"incl_open\" : true , \"tags\" : [ \"portfolio\" , \"trades\" , \"open\" ] }, \"win_rate\" : { \"title\" : \"Win Rate [%]\" , \"calc_func\" : \"trades.win_rate\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2cc80>\" , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags]\\\", mapping={})\" }, \"best_trade\" : { \"title\" : \"Best Trade [%]\" , \"calc_func\" : \"trades.returns.max\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2cd08>\" , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags]\\\", mapping={})\" }, \"worst_trade\" : { \"title\" : \"Worst Trade [%]\" , \"calc_func\" : \"trades.returns.min\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2cd90>\" , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags]\\\", mapping={})\" }, \"avg_winning_trade\" : { \"title\" : \"Avg Winning Trade [%]\" , \"calc_func\" : \"trades.winning.returns.mean\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2ce18>\" , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags, 'winning']\\\", mapping={})\" }, \"avg_losing_trade\" : { \"title\" : \"Avg Losing Trade [%]\" , \"calc_func\" : \"trades.losing.returns.mean\" , \"post_calc_func\" : \"<function Portfolio.<lambda> at 0x7facbde2cea0>\" , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags, 'losing']\\\", mapping={})\" }, \"avg_winning_trade_duration\" : { \"title\" : \"Avg Winning Trade Duration\" , \"calc_func\" : \"trades.winning.duration.mean\" , \"apply_to_timedelta\" : true , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags, 'winning', 'duration']\\\", mapping={})\" }, \"avg_losing_trade_duration\" : { \"title\" : \"Avg Losing Trade Duration\" , \"calc_func\" : \"trades.losing.duration.mean\" , \"apply_to_timedelta\" : true , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags, 'losing', 'duration']\\\", mapping={})\" }, \"profit_factor\" : { \"title\" : \"Profit Factor\" , \"calc_func\" : \"trades.profit_factor\" , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags]\\\", mapping={})\" }, \"expectancy\" : { \"title\" : \"Expectancy\" , \"calc_func\" : \"trades.expectancy\" , \"tags\" : \"RepEval(expression=\\\"['portfolio', 'trades', *incl_open_tags]\\\", mapping={})\" }, \"sharpe_ratio\" : { \"title\" : \"Sharpe Ratio\" , \"calc_func\" : \"returns_acc.sharpe_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : [ \"portfolio\" , \"returns\" ] }, \"calmar_ratio\" : { \"title\" : \"Calmar Ratio\" , \"calc_func\" : \"returns_acc.calmar_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : [ \"portfolio\" , \"returns\" ] }, \"omega_ratio\" : { \"title\" : \"Omega Ratio\" , \"calc_func\" : \"returns_acc.omega_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : [ \"portfolio\" , \"returns\" ] }, \"sortino_ratio\" : { \"title\" : \"Sortino Ratio\" , \"calc_func\" : \"returns_acc.sortino_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : [ \"portfolio\" , \"returns\" ] } } ) Returns Portfolio._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable Portfolio._metrics .","title":"metrics"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.net_exposure","text":"Portfolio . net_exposure ( group_by = None , wrap_kwargs = None ) Get net exposure.","title":"net_exposure()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.omega_ratio","text":"Portfolio . omega_ratio ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.omega_ratio() .","title":"omega_ratio()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.order_records","text":"A structured NumPy array of order records.","title":"order_records"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.orders","text":"Portfolio.get_orders() with default arguments.","title":"orders"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plot","text":"PlotsBuilderMixin . plots ( subplots = None , tags = None , column = None , group_by = None , silence_warnings = None , template_mapping = None , settings = None , filters = None , subplot_settings = None , show_titles = None , hide_id_labels = None , group_id_labels = None , make_subplots_kwargs = None , ** layout_kwargs ) See PlotsBuilderMixin.plots() .","title":"plot()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plot_asset_flow","text":"Portfolio . plot_asset_flow ( column = None , direction = 'both' , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column of asset flow. Args column :\u2002 str Name of the column to plot. direction :\u2002 Direction See Direction . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericAccessor.plot() .","title":"plot_asset_flow()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plot_asset_value","text":"Portfolio . plot_asset_value ( column = None , group_by = None , direction = 'both' , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column/group of asset value. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . direction :\u2002 Direction See Direction . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericSRAccessor.plot_against() .","title":"plot_asset_value()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plot_assets","text":"Portfolio . plot_assets ( column = None , direction = 'both' , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column of assets. Args column :\u2002 str Name of the column to plot. direction :\u2002 Direction See Direction . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericSRAccessor.plot_against() .","title":"plot_assets()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plot_cash","text":"Portfolio . plot_cash ( column = None , group_by = None , free = False , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column/group of cash balance. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . free :\u2002 bool Whether to plot the flow of the free cash. xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericSRAccessor.plot_against() .","title":"plot_cash()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plot_cash_flow","text":"Portfolio . plot_cash_flow ( column = None , group_by = None , free = False , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column/group of cash flow. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . free :\u2002 bool Whether to plot the flow of the free cash. xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericAccessor.plot() .","title":"plot_cash_flow()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plot_cum_returns","text":"Portfolio . plot_cum_returns ( column = None , group_by = None , benchmark_rets = None , use_asset_returns = False , ** kwargs ) Plot one column/group of cumulative returns. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . benchmark_rets :\u2002 array_like Benchmark returns. If None, will use Portfolio.benchmark_returns() . use_asset_returns :\u2002 bool Whether to plot asset returns. **kwargs Keyword arguments passed to ReturnsSRAccessor.plot_cumulative() .","title":"plot_cum_returns()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plot_drawdowns","text":"Portfolio . plot_drawdowns ( column = None , group_by = None , ** kwargs ) Plot one column/group of drawdowns. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . **kwargs Keyword arguments passed to Drawdowns.plot() .","title":"plot_drawdowns()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plot_gross_exposure","text":"Portfolio . plot_gross_exposure ( column = None , group_by = None , direction = 'both' , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column/group of gross exposure. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . direction :\u2002 Direction See Direction . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericSRAccessor.plot_against() .","title":"plot_gross_exposure()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plot_net_exposure","text":"Portfolio . plot_net_exposure ( column = None , group_by = None , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column/group of net exposure. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericSRAccessor.plot_against() .","title":"plot_net_exposure()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plot_orders","text":"Portfolio . plot_orders ( column = None , ** kwargs ) Plot one column/group of orders.","title":"plot_orders()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plot_position_pnl","text":"Portfolio . plot_position_pnl ( column = None , ** kwargs ) Plot one column/group of position PnL.","title":"plot_position_pnl()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plot_positions","text":"Portfolio . plot_positions ( column = None , ** kwargs ) Plot one column/group of positions.","title":"plot_positions()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plot_trade_pnl","text":"Portfolio . plot_trade_pnl ( column = None , ** kwargs ) Plot one column/group of trade PnL.","title":"plot_trade_pnl()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plot_trades","text":"Portfolio . plot_trades ( column = None , ** kwargs ) Plot one column/group of trades.","title":"plot_trades()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plot_underwater","text":"Portfolio . plot_underwater ( column = None , group_by = None , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column/group of underwater. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericAccessor.plot() .","title":"plot_underwater()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plot_value","text":"Portfolio . plot_value ( column = None , group_by = None , xref = 'x' , yref = 'y' , hline_shape_kwargs = None , ** kwargs ) Plot one column/group of value. Args column :\u2002 str Name of the column/group to plot. group_by :\u2002 any Group or ungroup columns. See ColumnGrouper . free :\u2002 bool Whether to plot free cash flow. xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. **kwargs Keyword arguments passed to GenericSRAccessor.plot_against() .","title":"plot_value()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.plots_defaults","text":"Defaults for PlotsBuilderMixin.plots() . Merges PlotsBuilderMixin.plots_defaults and portfolio.plots from settings .","title":"plots_defaults"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.position_coverage","text":"Portfolio . position_coverage ( direction = 'both' , group_by = None , wrap_kwargs = None ) Get position coverage per column/group.","title":"position_coverage()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.position_mask","text":"Portfolio . position_mask ( direction = 'both' , group_by = None , wrap_kwargs = None ) Get position mask per column/group. An element is True if the asset is in the market at this tick.","title":"position_mask()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.positions","text":"Portfolio.get_positions() with default arguments.","title":"positions"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.post_resolve_attr","text":"Portfolio . post_resolve_attr ( attr , out , final_kwargs = None ) Post-process an object after resolution. Uses the following keys: incl_open : Whether to include open trades/positions when resolving an argument that is an instance of Trades .","title":"post_resolve_attr()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.pre_resolve_attr","text":"Portfolio . pre_resolve_attr ( attr , final_kwargs = None ) Pre-process an attribute before resolution. Uses the following keys: use_asset_returns : Whether to use Portfolio.asset_returns() when resolving returns argument. trades_type : Which trade type to use when resolving trades argument.","title":"pre_resolve_attr()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.qs","text":"Portfolio.get_qs() with default arguments.","title":"qs"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.regroup","text":"Portfolio . regroup ( group_by , ** kwargs ) Regroup this object. See Wrapping.regroup() . Note All cached objects will be lost.","title":"regroup()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.returns","text":"Portfolio . returns ( group_by = None , in_sim_order = False , wrap_kwargs = None ) Get return series per column/group based on portfolio value.","title":"returns()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.returns_acc","text":"Portfolio.get_returns_acc() with default arguments.","title":"returns_acc"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.returns_stats","text":"Portfolio . returns_stats ( group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , defaults = None , ** kwargs ) Compute various statistics on returns of this portfolio. See Portfolio.returns_acc and ReturnsAccessor.metrics . kwargs will be passed to StatsBuilderMixin.stats() method. If benchmark_rets is not set, uses Portfolio.benchmark_returns() .","title":"returns_stats()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.sharpe_ratio","text":"Portfolio . sharpe_ratio ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.sharpe_ratio() .","title":"sharpe_ratio()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.sortino_ratio","text":"Portfolio . sortino_ratio ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.sortino_ratio() .","title":"sortino_ratio()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.stats_defaults","text":"Defaults for StatsBuilderMixin.stats() . Merges StatsBuilderMixin.stats_defaults and portfolio.stats from settings .","title":"stats_defaults"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.subplots","text":"Subplots supported by Portfolio . Co nf ig( { \"orders\" : { \"title\" : \"Orders\" , \"yaxis_kwargs\" : { \"title\" : \"Price\" }, \"check_is_not_grouped\" : true , \"plot_func\" : \"orders.plot\" , \"tags\" : [ \"portfolio\" , \"orders\" ] }, \"trades\" : { \"title\" : \"Trades\" , \"yaxis_kwargs\" : { \"title\" : \"Price\" }, \"check_is_not_grouped\" : true , \"plot_func\" : \"trades.plot\" , \"tags\" : [ \"portfolio\" , \"trades\" ] }, \"trade_pnl\" : { \"title\" : \"Trade PnL\" , \"yaxis_kwargs\" : { \"title\" : \"Trade PnL\" }, \"check_is_not_grouped\" : true , \"plot_func\" : \"trades.plot_pnl\" , \"tags\" : [ \"portfolio\" , \"trades\" ] }, \"asset_flow\" : { \"title\" : \"Asset Flow\" , \"yaxis_kwargs\" : { \"title\" : \"Asset flow\" }, \"check_is_not_grouped\" : true , \"plot_func\" : \"plot_asset_flow\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"assets\" ] }, \"cash_flow\" : { \"title\" : \"Cash Flow\" , \"yaxis_kwargs\" : { \"title\" : \"Cash flow\" }, \"plot_func\" : \"plot_cash_flow\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"cash\" ] }, \"assets\" : { \"title\" : \"Assets\" , \"yaxis_kwargs\" : { \"title\" : \"Assets\" }, \"check_is_not_grouped\" : true , \"plot_func\" : \"plot_assets\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"assets\" ] }, \"cash\" : { \"title\" : \"Cash\" , \"yaxis_kwargs\" : { \"title\" : \"Cash\" }, \"plot_func\" : \"plot_cash\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"cash\" ] }, \"asset_value\" : { \"title\" : \"Asset Value\" , \"yaxis_kwargs\" : { \"title\" : \"Asset value\" }, \"plot_func\" : \"plot_asset_value\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"assets\" , \"value\" ] }, \"value\" : { \"title\" : \"Value\" , \"yaxis_kwargs\" : { \"title\" : \"Value\" }, \"plot_func\" : \"plot_value\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"value\" ] }, \"cum_returns\" : { \"title\" : \"Cumulative Returns\" , \"yaxis_kwargs\" : { \"title\" : \"Cumulative returns\" }, \"plot_func\" : \"plot_cum_returns\" , \"pass_hline_shape_kwargs\" : true , \"pass_add_trace_kwargs\" : true , \"pass_xref\" : true , \"pass_yref\" : true , \"tags\" : [ \"portfolio\" , \"returns\" ] }, \"drawdowns\" : { \"title\" : \"Drawdowns\" , \"yaxis_kwargs\" : { \"title\" : \"Value\" }, \"plot_func\" : \"plot_drawdowns\" , \"pass_add_trace_kwargs\" : true , \"pass_xref\" : true , \"pass_yref\" : true , \"tags\" : [ \"portfolio\" , \"value\" , \"drawdowns\" ] }, \"underwater\" : { \"title\" : \"Underwater\" , \"yaxis_kwargs\" : { \"title\" : \"Drawdown\" }, \"plot_func\" : \"plot_underwater\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"value\" , \"drawdowns\" ] }, \"gross_exposure\" : { \"title\" : \"Gross Exposure\" , \"yaxis_kwargs\" : { \"title\" : \"Gross exposure\" }, \"plot_func\" : \"plot_gross_exposure\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"exposure\" ] }, \"net_exposure\" : { \"title\" : \"Net Exposure\" , \"yaxis_kwargs\" : { \"title\" : \"Net exposure\" }, \"plot_func\" : \"plot_net_exposure\" , \"pass_add_trace_kwargs\" : true , \"tags\" : [ \"portfolio\" , \"exposure\" ] } } ) Returns Portfolio._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable Portfolio._subplots .","title":"subplots"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.tail_ratio","text":"Portfolio . tail_ratio ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.tail_ratio() .","title":"tail_ratio()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.total_benchmark_return","text":"Portfolio . total_benchmark_return ( group_by = None , wrap_kwargs = None ) Get total benchmark return.","title":"total_benchmark_return()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.total_profit","text":"Portfolio . total_profit ( group_by = None , wrap_kwargs = None ) Get total profit per column/group. Calculated directly from order records (fast).","title":"total_profit()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.total_return","text":"Portfolio . total_return ( group_by = None , wrap_kwargs = None ) Get total profit per column/group.","title":"total_return()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.trades","text":"Portfolio.get_trades() with default arguments.","title":"trades"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.trades_type","text":"Default Trades to use across Portfolio .","title":"trades_type"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.up_capture","text":"Portfolio . up_capture ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.up_capture() .","title":"up_capture()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.value","text":"Portfolio . value ( group_by = None , in_sim_order = False , wrap_kwargs = None ) Get portfolio value series per column/group. By default, will generate portfolio value for each asset based on cash flows and thus independent from other assets, with the initial cash balance and position being that of the entire group. Useful for generating returns and comparing assets within the same group. When group_by is False and in_sim_order is True, returns value generated in simulation order (see row-major order . This value cannot be used for generating returns as-is. Useful to analyze how value evolved throughout simulation.","title":"value()"},{"location":"api/portfolio/base/#vectorbt.portfolio.base.Portfolio.value_at_risk","text":"Portfolio . value_at_risk ( * , group_by = None , benchmark_rets = None , freq = None , year_freq = None , use_asset_returns = False , ** kwargs ) See ReturnsAccessor.value_at_risk() .","title":"value_at_risk()"},{"location":"api/portfolio/decorators/","text":"decorators module \u00b6 Class and function decorators. attach_returns_acc_methods function \u00b6 attach_returns_acc_methods ( config ) Class decorator to add returns accessor methods. config should contain target method names (keys) and dictionaries (values) with the following keys: source_name : Name of the source method. Defaults to the target name. docstring : Method docstring. Defaults to \"See vectorbt.returns.accessors.ReturnsAccessor.{source_name} .\". The class should be a subclass of Portfolio .","title":"decorators"},{"location":"api/portfolio/decorators/#vectorbt.portfolio.decorators","text":"Class and function decorators.","title":"vectorbt.portfolio.decorators"},{"location":"api/portfolio/decorators/#vectorbt.portfolio.decorators.attach_returns_acc_methods","text":"attach_returns_acc_methods ( config ) Class decorator to add returns accessor methods. config should contain target method names (keys) and dictionaries (values) with the following keys: source_name : Name of the source method. Defaults to the target name. docstring : Method docstring. Defaults to \"See vectorbt.returns.accessors.ReturnsAccessor.{source_name} .\". The class should be a subclass of Portfolio .","title":"attach_returns_acc_methods()"},{"location":"api/portfolio/enums/","text":"enums module \u00b6 Named tuples and enumerated types. Defines enums and other schemas for vectorbt.portfolio . AccumulationMode AccumulationModeT \u00b6 Accumulation mode. { \"Disabled\" : 0 , \"Both\" : 1 , \"AddOnly\" : 2 , \"RemoveOnly\" : 3 } Accumulation allows gradually increasing and decreasing positions by a size. Attributes Disabled Disable accumulation. Both Allow both adding to and removing from the position. AddOnly Allow accumulation to only add to the position. RemoveOnly Allow accumulation to only remove from the position. Note Accumulation acts differently for exits and opposite entries: exits reduce the current position but won't enter the opposite one, while opposite entries reduce the position by the same amount, but as soon as this position is closed, they begin to increase the opposite position. The behavior for opposite entries can be changed by OppositeEntryMode and for stop orders by StopExitMode . CallSeqType CallSeqTypeT \u00b6 Call sequence type. { \"Default\" : 0 , \"Reversed\" : 1 , \"Random\" : 2 , \"Auto\" : 3 } Attributes Default Place calls from left to right. Reversed Place calls from right to left. Random Place calls randomly. Auto Place calls dynamically based on order value. ConflictMode ConflictModeT \u00b6 Conflict mode. { \"Ignore\" : 0 , \"Entry\" : 1 , \"Exit\" : 2 , \"Adjacent\" : 3 , \"Opposite\" : 4 } What should happen if both entry and exit signals occur simultaneously? Attributes Ignore Ignore both signals. Entry Execute the entry signal. Exit Execute the exit signal. Adjacent Execute the adjacent signal. Takes effect only when in position, otherwise ignores. Opposite Execute the opposite signal. Takes effect only when in position, otherwise ignores. Direction DirectionT \u00b6 Position direction. { \"LongOnly\" : 0 , \"ShortOnly\" : 1 , \"Both\" : 2 } Attributes LongOnly Only long positions. ShortOnly Only short positions. Both Both long and short positions. DirectionConflictMode DirectionConflictModeT \u00b6 Direction conflict mode. { \"Ignore\" : 0 , \"Long\" : 1 , \"Short\" : 2 , \"Adjacent\" : 3 , \"Opposite\" : 4 } What should happen if both long and short entry signals occur simultaneously? Attributes Ignore Ignore both entry signals. Long Execute the long entry signal. Short Execute the short entry signal. Adjacent Execute the adjacent entry signal. Takes effect only when in position, otherwise ignores. Opposite Execute the opposite entry signal. Takes effect only when in position, otherwise ignores. InitCashMode InitCashModeT \u00b6 Initial cash mode. { \"Auto\" : 0 , \"AutoAlign\" : 1 } Attributes Auto Initial cash is infinite within simulation, and then set to the total cash spent. AutoAlign Initial cash is set to the total cash spent across all columns. NoOrder Order \u00b6 Order that should not be processed. OppositeEntryMode OppositeEntryModeT \u00b6 Opposite entry mode. { \"Ignore\" : 0 , \"Close\" : 1 , \"CloseReduce\" : 2 , \"Reverse\" : 3 , \"ReverseReduce\" : 4 } What should happen if an entry signal of opposite direction occurs before an exit signal? Attributes Ignore Ignore the opposite entry signal. Close Close the current position. CloseReduce Close the current position or reduce it if accumulation is enabled. Reverse Reverse the current position. ReverseReduce Reverse the current position or reduce it if accumulation is enabled. OrderSide OrderSideT \u00b6 Order side. { \"Buy\" : 0 , \"Sell\" : 1 } OrderStatus OrderStatusT \u00b6 Order status. { \"Filled\" : 0 , \"Ignored\" : 1 , \"Rejected\" : 2 } Attributes Filled Order has been filled. Ignored Order has been ignored. Rejected Order has been rejected. OrderStatusInfo OrderStatusInfoT \u00b6 Order status information. { \"SizeNaN\" : 0 , \"PriceNaN\" : 1 , \"ValPriceNaN\" : 2 , \"ValueNaN\" : 3 , \"ValueZeroNeg\" : 4 , \"SizeZero\" : 5 , \"NoCashShort\" : 6 , \"NoCashLong\" : 7 , \"NoOpenPosition\" : 8 , \"MaxSizeExceeded\" : 9 , \"RandomEvent\" : 10 , \"CantCoverFees\" : 11 , \"MinSizeNotReached\" : 12 , \"PartialFill\" : 13 } SizeType SizeTypeT \u00b6 Size type. { \"Amount\" : 0 , \"Value\" : 1 , \"Percent\" : 2 , \"TargetAmount\" : 3 , \"TargetValue\" : 4 , \"TargetPercent\" : 5 } Attributes Amount Amount of assets to trade. Value Asset value to trade. Gets converted into SizeType.Amount using OrderContext.val_price_now . Percent Percentage of available resources to use in either direction (not to be confused with the percentage of position value!) When buying, it's the percentage of OrderContext.cash_now . When selling, it's the percentage of OrderContext.position_now . When short selling, it's the percentage of OrderContext.free_cash_now . When selling and short selling (i.e. reversing position), it's the percentage of OrderContext.position_now and OrderContext.free_cash_now . Note Takes into account fees and slippage to find the limit. In reality, slippage and fees are not known beforehand. TargetAmount Target amount of assets to hold (= target position). Uses OrderContext.position_now to get the current position. Gets converted into SizeType.Amount . TargetValue Target asset value. Uses OrderContext.val_price_now to get the current asset value. Gets converted into SizeType.TargetAmount . TargetPercent Target percentage of total value. Uses OrderContext.value_now to get the current total value. Gets converted into SizeType.TargetValue . StopEntryPrice StopEntryPriceT \u00b6 Stop entry price. { \"ValPrice\" : 0 , \"Price\" : 1 , \"FillPrice\" : 2 , \"Close\" : 3 } Which price to use as an initial stop price? Attributes ValPrice Asset valuation price. Price Default price. FillPrice Fill price (that is, slippage is already applied). Close Closing price. StopExitMode StopExitModeT \u00b6 Stop exit mode. { \"Close\" : 0 , \"CloseReduce\" : 1 , \"Reverse\" : 2 , \"ReverseReduce\" : 3 } How to exit the current position upon a stop signal? Attributes Close Close the current position. CloseReduce Close the current position or reduce it if accumulation is enabled. Reverse Reverse the current position. ReverseReduce Reverse the current position or reduce it if accumulation is enabled. StopExitPrice StopExitPriceT \u00b6 Stop exit price. { \"StopLimit\" : 0 , \"StopMarket\" : 1 , \"Price\" : 2 , \"Close\" : 3 } Which price to use when exiting a position upon a stop signal? Attributes StopLimit Stop price as from a limit order. If the stop was hit before, the opening price at the next bar is used. User-defined slippage is not applied. StopMarket Stop price as from a market order. If the stop was hit before, the opening price at the next bar is used. User-defined slippage is applied. Price Default price. User-defined slippage is applied. Note Make sure to use StopExitPrice.Price only together with StopEntryPrice.Close . Otherwise, there is no proof that the price comes after the stop price. Close Closing price. User-defined slippage is applied. Note We can execute only one signal per asset and bar. This means the following: 1) Stop signal cannot be processed at the same bar as the entry signal. 2) When dealing with stop orders, we have another signal - stop signal - that may be in a conflict with the signals placed by the user. To choose between both, we assume that any stop signal comes before any other signal in time. Thus, make sure to always execute ordinary signals using the closing price when using stop orders. Otherwise, you're looking into the future. StopUpdateMode StopUpdateModeT \u00b6 Stop update mode. { \"Keep\" : 0 , \"Override\" : 1 , \"OverrideNaN\" : 2 } What to do with the old stop upon new acquisition? Attributes Keep Keep the old stop. Override Override the old stop, but only if the new stop is not NaN. OverrideNaN Override the old stop, even if the new stop is NaN. TradeDirection TradeDirectionT \u00b6 Event direction. { \"Long\" : 0 , \"Short\" : 1 } TradeStatus TradeStatusT \u00b6 Event status. { \"Open\" : 0 , \"Closed\" : 1 } TradesType TradesTypeT \u00b6 Trades type. { \"EntryTrades\" : 0 , \"ExitTrades\" : 1 , \"Positions\" : 2 } log_dt dtype[void] \u00b6 np.dtype of log records. { \"id\" : \"int64\" , \"group\" : \"int64\" , \"col\" : \"int64\" , \"idx\" : \"int64\" , \"cash\" : \"float64\" , \"position\" : \"float64\" , \"debt\" : \"float64\" , \"free_cash\" : \"float64\" , \"val_price\" : \"float64\" , \"value\" : \"float64\" , \"req_size\" : \"float64\" , \"req_price\" : \"float64\" , \"req_size_type\" : \"int64\" , \"req_direction\" : \"int64\" , \"req_fees\" : \"float64\" , \"req_fixed_fees\" : \"float64\" , \"req_slippage\" : \"float64\" , \"req_min_size\" : \"float64\" , \"req_max_size\" : \"float64\" , \"req_size_granularity\" : \"float64\" , \"req_reject_prob\" : \"float64\" , \"req_lock_cash\" : \"bool\" , \"req_allow_partial\" : \"bool\" , \"req_raise_reject\" : \"bool\" , \"req_log\" : \"bool\" , \"new_cash\" : \"float64\" , \"new_position\" : \"float64\" , \"new_debt\" : \"float64\" , \"new_free_cash\" : \"float64\" , \"new_val_price\" : \"float64\" , \"new_value\" : \"float64\" , \"res_size\" : \"float64\" , \"res_price\" : \"float64\" , \"res_fees\" : \"float64\" , \"res_side\" : \"int64\" , \"res_status\" : \"int64\" , \"res_status_info\" : \"int64\" , \"order_id\" : \"int64\" } order_dt dtype[void] \u00b6 np.dtype of order records. { \"id\" : \"int64\" , \"col\" : \"int64\" , \"idx\" : \"int64\" , \"size\" : \"float64\" , \"price\" : \"float64\" , \"fees\" : \"float64\" , \"side\" : \"int64\" } trade_dt dtype[void] \u00b6 np.dtype of trade records. { \"id\" : \"int64\" , \"col\" : \"int64\" , \"size\" : \"float64\" , \"entry_idx\" : \"int64\" , \"entry_price\" : \"float64\" , \"entry_fees\" : \"float64\" , \"exit_idx\" : \"int64\" , \"exit_price\" : \"float64\" , \"exit_fees\" : \"float64\" , \"pnl\" : \"float64\" , \"return\" : \"float64\" , \"direction\" : \"int64\" , \"status\" : \"int64\" , \"parent_id\" : \"int64\" } AdjustSLContext class \u00b6 A named tuple representing the context for generation of signals. Superclasses builtins.tuple col property \u00b6 Current column. Has range [0, target_shape[1]) and is always within [from_col, to_col) . curr_i property \u00b6 Index of the row of the updated stop. Gets updated once the price is updated. curr_price property \u00b6 Current stop price. Gets updated in trailing SL once a higher price is discovered. curr_stop property \u00b6 Current stop value. Can be updated by adjustment function. curr_trail property \u00b6 Current trailing flag. Can be updated by adjustment function. i property \u00b6 Index of the current row. Has range [0, target_shape[0]) . init_i property \u00b6 Index of the row of the initial stop. Doesn't change. init_price property \u00b6 Price of the initial stop. Doesn't change. position_now property \u00b6 Latest position. val_price_now property \u00b6 Latest valuation price. AdjustTPContext class \u00b6 A named tuple representing the context for adjusting take profit. Superclasses builtins.tuple col property \u00b6 See AdjustSLContext.col . curr_stop property \u00b6 See AdjustSLContext.curr_stop . i property \u00b6 See AdjustSLContext.i . init_i property \u00b6 See AdjustSLContext.init_i . init_price property \u00b6 See AdjustSLContext.curr_price . position_now property \u00b6 See AdjustSLContext.position_now . val_price_now property \u00b6 See AdjustSLContext.val_price_now . ExecuteOrderState class \u00b6 State after order execution. Superclasses builtins.tuple cash property \u00b6 See ProcessOrderState.cash . debt property \u00b6 See ProcessOrderState.debt . free_cash property \u00b6 See ProcessOrderState.free_cash . position property \u00b6 See ProcessOrderState.position . FlexOrderContext class \u00b6 A named tuple representing the context of a flexible order. Contains all fields from SegmentContext plus the current call index. Passed to flex_order_func_nb . Superclasses builtins.tuple call_idx property \u00b6 Index of the current call. call_post_segment property \u00b6 See SimulationContext.call_post_segment . call_pre_segment property \u00b6 See SimulationContext.call_pre_segment . call_seq property \u00b6 See SimulationContext.call_seq . call_seq_now property \u00b6 See SegmentContext.call_seq_now . cash_sharing property \u00b6 See SimulationContext.cash_sharing . close property \u00b6 See SimulationContext.close . ffill_val_price property \u00b6 See SimulationContext.ffill_val_price . fill_pos_record property \u00b6 See SimulationContext.fill_pos_record . flex_2d property \u00b6 See SimulationContext.flex_2d . from_col property \u00b6 See GroupContext.from_col . group property \u00b6 See GroupContext.group . group_len property \u00b6 See GroupContext.group_len . group_lens property \u00b6 See SimulationContext.group_lens . i property \u00b6 See RowContext.i . init_cash property \u00b6 See SimulationContext.init_cash . last_cash property \u00b6 See SimulationContext.last_cash . last_debt property \u00b6 See SimulationContext.last_debt . last_free_cash property \u00b6 See SimulationContext.last_free_cash . last_lidx property \u00b6 See SimulationContext.last_lidx . last_oidx property \u00b6 See SimulationContext.last_oidx . last_pos_record property \u00b6 See SimulationContext.last_pos_record . last_position property \u00b6 See SimulationContext.last_position . last_return property \u00b6 See SimulationContext.last_return . last_val_price property \u00b6 See SimulationContext.last_val_price . last_value property \u00b6 See SimulationContext.last_value . log_records property \u00b6 See SimulationContext.log_records . order_records property \u00b6 See SimulationContext.order_records . second_last_value property \u00b6 See SimulationContext.second_last_value . segment_mask property \u00b6 See SimulationContext.segment_mask . target_shape property \u00b6 See SimulationContext.target_shape . to_col property \u00b6 See GroupContext.to_col . update_value property \u00b6 See SimulationContext.update_value . GroupContext class \u00b6 A named tuple representing the context of a group. A group is a set of nearby columns that are somehow related (for example, by sharing the same capital). In each row, the columns under the same group are bound to the same segment. Contains all fields from SimulationContext plus fields describing the current group. Passed to pre_group_func_nb and post_group_func_nb . Example Consider a group of three columns, a group of two columns, and one more column: group group_len from_col to_col 0 3 0 3 1 2 3 5 2 1 5 6 Superclasses builtins.tuple call_post_segment property \u00b6 See SimulationContext.call_post_segment . call_pre_segment property \u00b6 See SimulationContext.call_pre_segment . call_seq property \u00b6 See SimulationContext.call_seq . cash_sharing property \u00b6 See SimulationContext.cash_sharing . close property \u00b6 See SimulationContext.close . ffill_val_price property \u00b6 See SimulationContext.ffill_val_price . fill_pos_record property \u00b6 See SimulationContext.fill_pos_record . flex_2d property \u00b6 See SimulationContext.flex_2d . from_col property \u00b6 Index of the first column in the current group. Has range [0, target_shape[1]) . group property \u00b6 Index of the current group. Has range [0, group_lens.shape[0]) . group_len property \u00b6 Number of columns in the current group. Scalar value. Same as group_lens[group] . group_lens property \u00b6 See SimulationContext.group_lens . init_cash property \u00b6 See SimulationContext.init_cash . last_cash property \u00b6 See SimulationContext.last_cash . last_debt property \u00b6 See SimulationContext.last_debt . last_free_cash property \u00b6 See SimulationContext.last_free_cash . last_lidx property \u00b6 See SimulationContext.last_lidx . last_oidx property \u00b6 See SimulationContext.last_oidx . last_pos_record property \u00b6 See SimulationContext.last_pos_record . last_position property \u00b6 See SimulationContext.last_position . last_return property \u00b6 See SimulationContext.last_return . last_val_price property \u00b6 See SimulationContext.last_val_price . last_value property \u00b6 See SimulationContext.last_value . log_records property \u00b6 See SimulationContext.log_records . order_records property \u00b6 See SimulationContext.order_records . second_last_value property \u00b6 See SimulationContext.second_last_value . segment_mask property \u00b6 See SimulationContext.segment_mask . target_shape property \u00b6 See SimulationContext.target_shape . to_col property \u00b6 Index of the last column in the current group plus one. Has range [1, target_shape[1] + 1) . If columns are not grouped, equals to from_col + 1 . Warning In the last group, to_col points at a column that doesn't exist. update_value property \u00b6 See SimulationContext.update_value . Order class \u00b6 A named tuple representing an order. Note Currently, Numba has issues with using defaults when filling named tuples. Use order_nb() to create an order. Superclasses builtins.tuple allow_partial property \u00b6 Whether to allow partial fill. Otherwise, the order gets rejected. Does not apply when Order.size is np.inf . direction property \u00b6 See Direction . fees property \u00b6 Fees in percentage of the order value. Negative trading fees like -0.05 means earning 0.05% per trade instead of paying a fee. Note 0.01 = 1%. fixed_fees property \u00b6 Fixed amount of fees to pay for this order. lock_cash property \u00b6 Whether to lock cash when shorting. If enabled, prevents free_cash from turning negative when buying or short selling. A negative free_cash means one column used collateral of another column, which is generally undesired. log property \u00b6 Whether to log this order by filling a log record. Remember to increase max_logs . max_size property \u00b6 Maximum size in both directions. Higher than that will be partly filled. min_size property \u00b6 Minimum size in both directions. Lower than that will be rejected. price property \u00b6 Price per unit. Final price will depend upon slippage. If -np.inf , replaced by the current open (if available) or the previous close (\u2248 the current open in crypto). If np.inf , replaced by the current close. Note Make sure to use timestamps that come between (and ideally not including) the current open and close. raise_reject property \u00b6 Whether to raise exception if order has been rejected. Terminates the simulation. reject_prob property \u00b6 Probability of rejecting this order to simulate a random rejection event. Not everything goes smoothly in real life. Use random rejections to test your order management for robustness. size property \u00b6 Size in units. Behavior depends upon Order.size_type and Order.direction . For any fixed size: Set to any number to buy/sell some fixed amount or value. Longs are limited by the current cash balance, while shorts are only limited if Order.lock_cash . Set to np.inf to buy for all cash, or -np.inf to sell for all free cash. If Order.direction is not Direction.Both , -np.inf will close the position. Set to np.nan or 0 to skip. For any target size: Set to any number to buy/sell an amount relative to the current position or value. Set to 0 to close the current position. Set to np.nan to skip. size_granularity property \u00b6 Granularity of the size. For example, granularity of 1.0 makes the quantity to behave like an integer. Placing an order of 12.5 shares (in any direction) will order exactly 12.0 shares. Note The filled size remains a floating number. size_type property \u00b6 See SizeType . slippage property \u00b6 Slippage in percentage of Order.price . Slippage is a penalty applied on the price. Note 0.01 = 1%. OrderContext class \u00b6 A named tuple representing the context of an order. Contains all fields from SegmentContext plus fields describing the current state. Passed to order_func_nb . Superclasses builtins.tuple call_idx property \u00b6 Index of the current call in SegmentContext.call_seq_now . Has range [0, group_len) . call_post_segment property \u00b6 See SimulationContext.call_post_segment . call_pre_segment property \u00b6 See SimulationContext.call_pre_segment . call_seq property \u00b6 See SimulationContext.call_seq . call_seq_now property \u00b6 See SegmentContext.call_seq_now . cash_now property \u00b6 SimulationContext.last_cash for the current column/group. cash_sharing property \u00b6 See SimulationContext.cash_sharing . close property \u00b6 See SimulationContext.close . col property \u00b6 Current column. Has range [0, target_shape[1]) and is always within [from_col, to_col) . debt_now property \u00b6 SimulationContext.last_debt for the current column. ffill_val_price property \u00b6 See SimulationContext.ffill_val_price . fill_pos_record property \u00b6 See SimulationContext.fill_pos_record . flex_2d property \u00b6 See SimulationContext.flex_2d . free_cash_now property \u00b6 SimulationContext.last_free_cash for the current column/group. from_col property \u00b6 See GroupContext.from_col . group property \u00b6 See GroupContext.group . group_len property \u00b6 See GroupContext.group_len . group_lens property \u00b6 See SimulationContext.group_lens . i property \u00b6 See RowContext.i . init_cash property \u00b6 See SimulationContext.init_cash . last_cash property \u00b6 See SimulationContext.last_cash . last_debt property \u00b6 See SimulationContext.last_debt . last_free_cash property \u00b6 See SimulationContext.last_free_cash . last_lidx property \u00b6 See SimulationContext.last_lidx . last_oidx property \u00b6 See SimulationContext.last_oidx . last_pos_record property \u00b6 See SimulationContext.last_pos_record . last_position property \u00b6 See SimulationContext.last_position . last_return property \u00b6 See SimulationContext.last_return . last_val_price property \u00b6 See SimulationContext.last_val_price . last_value property \u00b6 See SimulationContext.last_value . log_records property \u00b6 See SimulationContext.log_records . order_records property \u00b6 See SimulationContext.order_records . pos_record_now property \u00b6 SimulationContext.last_pos_record for the current column. position_now property \u00b6 SimulationContext.last_position for the current column. return_now property \u00b6 SimulationContext.last_return for the current column/group. second_last_value property \u00b6 See SimulationContext.second_last_value . segment_mask property \u00b6 See SimulationContext.segment_mask . target_shape property \u00b6 See SimulationContext.target_shape . to_col property \u00b6 See GroupContext.to_col . update_value property \u00b6 See SimulationContext.update_value . val_price_now property \u00b6 SimulationContext.last_val_price for the current column. value_now property \u00b6 SimulationContext.last_value for the current column/group. OrderResult class \u00b6 A named tuple representing an order result. Superclasses builtins.tuple fees property \u00b6 Total fees paid for this order. price property \u00b6 Filled price per unit, adjusted with slippage. side property \u00b6 See OrderSide . size property \u00b6 Filled size. status property \u00b6 See OrderStatus . status_info property \u00b6 See OrderStatusInfo . PostOrderContext class \u00b6 A named tuple representing the context after an order has been processed. Contains all fields from OrderContext plus fields describing the order result and the previous state. Passed to post_order_func_nb . Superclasses builtins.tuple call_idx property \u00b6 See OrderContext.call_idx . call_post_segment property \u00b6 See SimulationContext.call_post_segment . call_pre_segment property \u00b6 See SimulationContext.call_pre_segment . call_seq property \u00b6 See SimulationContext.call_seq . call_seq_now property \u00b6 See SegmentContext.call_seq_now . cash_before property \u00b6 OrderContext.cash_now before execution. cash_now property \u00b6 OrderContext.cash_now after execution. cash_sharing property \u00b6 See SimulationContext.cash_sharing . close property \u00b6 See SimulationContext.close . col property \u00b6 See OrderContext.col . debt_before property \u00b6 OrderContext.debt_now before execution. debt_now property \u00b6 OrderContext.debt_now after execution. ffill_val_price property \u00b6 See SimulationContext.ffill_val_price . fill_pos_record property \u00b6 See SimulationContext.fill_pos_record . flex_2d property \u00b6 See SimulationContext.flex_2d . free_cash_before property \u00b6 OrderContext.free_cash_now before execution. free_cash_now property \u00b6 OrderContext.free_cash_now after execution. from_col property \u00b6 See GroupContext.from_col . group property \u00b6 See GroupContext.group . group_len property \u00b6 See GroupContext.group_len . group_lens property \u00b6 See SimulationContext.group_lens . i property \u00b6 See RowContext.i . init_cash property \u00b6 See SimulationContext.init_cash . last_cash property \u00b6 See SimulationContext.last_cash . last_debt property \u00b6 See SimulationContext.last_debt . last_free_cash property \u00b6 See SimulationContext.last_free_cash . last_lidx property \u00b6 See SimulationContext.last_lidx . last_oidx property \u00b6 See SimulationContext.last_oidx . last_pos_record property \u00b6 See SimulationContext.last_pos_record . last_position property \u00b6 See SimulationContext.last_position . last_return property \u00b6 See SimulationContext.last_return . last_val_price property \u00b6 See SimulationContext.last_val_price . last_value property \u00b6 See SimulationContext.last_value . log_records property \u00b6 See SimulationContext.log_records . order_records property \u00b6 See SimulationContext.order_records . order_result property \u00b6 Order result of type OrderResult . Can be used to check whether the order has been filled, ignored, or rejected. pos_record_now property \u00b6 OrderContext.pos_record_now after execution. position_before property \u00b6 OrderContext.position_now before execution. position_now property \u00b6 OrderContext.position_now after execution. return_now property \u00b6 OrderContext.return_now after execution. second_last_value property \u00b6 See SimulationContext.second_last_value . segment_mask property \u00b6 See SimulationContext.segment_mask . target_shape property \u00b6 See SimulationContext.target_shape . to_col property \u00b6 See GroupContext.to_col . update_value property \u00b6 See SimulationContext.update_value . val_price_before property \u00b6 OrderContext.val_price_now before execution. val_price_now property \u00b6 OrderContext.val_price_now after execution. If SimulationContext.update_value , gets replaced with the fill price, as it becomes the most recently known price. Otherwise, stays the same. value_before property \u00b6 OrderContext.value_now before execution. value_now property \u00b6 OrderContext.value_now after execution. If SimulationContext.update_value , gets updated with the new cash and value of the column. Otherwise, stays the same. ProcessOrderState class \u00b6 State before or after order processing. Superclasses builtins.tuple cash property \u00b6 Cash in the current column or group with cash sharing. debt property \u00b6 Debt from shorting in the current column. free_cash property \u00b6 Free cash in the current column or group with cash sharing. lidx property \u00b6 Index of log record. oidx property \u00b6 Index of order record. position property \u00b6 Position in the current column. val_price property \u00b6 Valuation price in the current column. value property \u00b6 Value in the current column or group with cash sharing. RejectedOrderError class \u00b6 Rejected order error. Superclasses builtins.BaseException builtins.Exception RowContext class \u00b6 A named tuple representing the context of a row. A row is a time step in which segments are executed. Contains all fields from SimulationContext plus fields describing the current row. Passed to pre_row_func_nb and post_row_func_nb . Superclasses builtins.tuple call_post_segment property \u00b6 See SimulationContext.call_post_segment . call_pre_segment property \u00b6 See SimulationContext.call_pre_segment . call_seq property \u00b6 See SimulationContext.call_seq . cash_sharing property \u00b6 See SimulationContext.cash_sharing . close property \u00b6 See SimulationContext.close . ffill_val_price property \u00b6 See SimulationContext.ffill_val_price . fill_pos_record property \u00b6 See SimulationContext.fill_pos_record . flex_2d property \u00b6 See SimulationContext.flex_2d . group_lens property \u00b6 See SimulationContext.group_lens . i property \u00b6 Index of the current row. Has range [0, target_shape[0]) . init_cash property \u00b6 See SimulationContext.init_cash . last_cash property \u00b6 See SimulationContext.last_cash . last_debt property \u00b6 See SimulationContext.last_debt . last_free_cash property \u00b6 See SimulationContext.last_free_cash . last_lidx property \u00b6 See SimulationContext.last_lidx . last_oidx property \u00b6 See SimulationContext.last_oidx . last_pos_record property \u00b6 See SimulationContext.last_pos_record . last_position property \u00b6 See SimulationContext.last_position . last_return property \u00b6 See SimulationContext.last_return . last_val_price property \u00b6 See SimulationContext.last_val_price . last_value property \u00b6 See SimulationContext.last_value . log_records property \u00b6 See SimulationContext.log_records . order_records property \u00b6 See SimulationContext.order_records . second_last_value property \u00b6 See SimulationContext.second_last_value . segment_mask property \u00b6 See SimulationContext.segment_mask . target_shape property \u00b6 See SimulationContext.target_shape . update_value property \u00b6 See SimulationContext.update_value . SegmentContext class \u00b6 A named tuple representing the context of a segment. A segment is an intersection between groups and rows. It's an entity that defines how and in which order elements within the same group and row are processed. Contains all fields from SimulationContext , GroupContext , and RowContext , plus fields describing the current segment. Passed to pre_segment_func_nb and post_segment_func_nb . Superclasses builtins.tuple call_post_segment property \u00b6 See SimulationContext.call_post_segment . call_pre_segment property \u00b6 See SimulationContext.call_pre_segment . call_seq property \u00b6 See SimulationContext.call_seq . call_seq_now property \u00b6 Sequence of calls within the current segment. Has shape (group_len,) . Each value in this sequence should indicate the position of column in the group to call next. Processing goes always from left to right. You can use pre_segment_func_nb to override call_seq_now . Example [2, 0, 1] would first call column 2, then 0, and finally 1. cash_sharing property \u00b6 See SimulationContext.cash_sharing . close property \u00b6 See SimulationContext.close . ffill_val_price property \u00b6 See SimulationContext.ffill_val_price . fill_pos_record property \u00b6 See SimulationContext.fill_pos_record . flex_2d property \u00b6 See SimulationContext.flex_2d . from_col property \u00b6 See GroupContext.from_col . group property \u00b6 See GroupContext.group . group_len property \u00b6 See GroupContext.group_len . group_lens property \u00b6 See SimulationContext.group_lens . i property \u00b6 See RowContext.i . init_cash property \u00b6 See SimulationContext.init_cash . last_cash property \u00b6 See SimulationContext.last_cash . last_debt property \u00b6 See SimulationContext.last_debt . last_free_cash property \u00b6 See SimulationContext.last_free_cash . last_lidx property \u00b6 See SimulationContext.last_lidx . last_oidx property \u00b6 See SimulationContext.last_oidx . last_pos_record property \u00b6 See SimulationContext.last_pos_record . last_position property \u00b6 See SimulationContext.last_position . last_return property \u00b6 See SimulationContext.last_return . last_val_price property \u00b6 See SimulationContext.last_val_price . last_value property \u00b6 See SimulationContext.last_value . log_records property \u00b6 See SimulationContext.log_records . order_records property \u00b6 See SimulationContext.order_records . second_last_value property \u00b6 See SimulationContext.second_last_value . segment_mask property \u00b6 See SimulationContext.segment_mask . target_shape property \u00b6 See SimulationContext.target_shape . to_col property \u00b6 See GroupContext.to_col . update_value property \u00b6 See SimulationContext.update_value . SignalContext class \u00b6 SignalContext(i, col, position_now, val_price_now, flex_2d) Superclasses builtins.tuple col property \u00b6 Alias for field number 1 flex_2d property \u00b6 Alias for field number 4 i property \u00b6 Alias for field number 0 position_now property \u00b6 Alias for field number 2 val_price_now property \u00b6 Alias for field number 3 SimulationContext class \u00b6 A named tuple representing the context of a simulation. Contains general information available to all other contexts. Passed to pre_sim_func_nb and post_sim_func_nb . Superclasses builtins.tuple call_post_segment property \u00b6 Whether to call post_segment_func_nb regardless of SimulationContext.segment_mask . Allows, for example, to write user-defined arrays such as returns at the end of each segment. call_pre_segment property \u00b6 Whether to call pre_segment_func_nb regardless of SimulationContext.segment_mask . call_seq property \u00b6 Default sequence of calls per segment. Controls the sequence in which order_func_nb is executed within each segment. Has shape SimulationContext.target_shape and each value must exist in the range [0, group_len) . Note To use sort_call_seq_nb , should be generated using CallSeqType.Default . To change the call sequence dynamically, better change SegmentContext.call_seq_now in-place. Example The default call sequence for three data points and two groups with three columns each: np . array ([ [ 0 , 1 , 2 , 0 , 1 , 2 ], [ 0 , 1 , 2 , 0 , 1 , 2 ], [ 0 , 1 , 2 , 0 , 1 , 2 ] ]) cash_sharing property \u00b6 Whether cash sharing is enabled. close property \u00b6 Latest asset price at each time step. Utilizes flexible indexing using flex_select_auto_nb() and flex_2d , so it can be passed as 2-dim array, 1-dim array per column (requires flex_2d=True ), 1-dim array per row (requires flex_2d=False ), and a scalar. Broadcasts to the shape SimulationContext.target_shape . Note To modify the array in place, make sure to build an array of the full shape. ffill_val_price property \u00b6 Whether to track valuation price only if it's known. Otherwise, unknown SimulationContext.close will lead to NaN in valuation price at the next timestamp. fill_pos_record property \u00b6 Whether to fill position record. Disable this to make simulation a bit faster for simple use cases. flex_2d property \u00b6 Whether the elements in a 1-dim array should be treated per column rather than per row. This flag is set automatically when using Portfolio.from_order_func() depending upon whether there is any argument that has been broadcast to 2 dimensions. Has only effect when using flexible indexing, for example, with flex_select_auto_nb() . group_lens property \u00b6 Number of columns in each group. Even if columns are not grouped, group_lens contains ones - one column per group. Example In pairs trading, group_lens would be np.array([2]) , while three independent columns would be represented by group_lens of np.array([1, 1, 1]) . init_cash property \u00b6 Initial capital per column or group with cash sharing. If SimulationContext.cash_sharing , has shape (group_lens.shape[0],) , otherwise has shape (target_shape[1],) . Example Consider three columns, each having $100 of starting capital. If we built one group of two columns with cash sharing and one (imaginary) group with the last column, the init_cash would be np.array([200, 100]) . Without cash sharing, the init_cash would be np.array([100, 100, 100]) . last_cash property \u00b6 Latest cash per column or group with cash sharing. Has the same shape as SimulationContext.init_cash . Gets updated right after order_func_nb . last_debt property \u00b6 Latest debt from shorting per column. Debt is the total value from shorting that hasn't been covered yet. Used to update OrderContext.free_cash_now . Has shape (target_shape[1],) . Gets updated right after order_func_nb . last_free_cash property \u00b6 Latest free cash per column or group with cash sharing. Free cash never goes above the initial level, because an operation always costs money. Has shape (target_shape[1],) . Gets updated right after order_func_nb . last_lidx property \u00b6 Index of the latest log record of each column. Similar to SimulationContext.last_oidx but for log records. last_oidx property \u00b6 Index of the latest order record of each column. Points to SimulationContext.order_records and has shape (target_shape[1],) . Example last_oidx of np.array([1, 100, -1]) means the latest filled order is order_records[1] for the first column, order_records[100] for the second column, and no orders have been filled yet for the third column. last_pos_record property \u00b6 Latest position record of each column. It's a 1-dimensional array with records of type trade_dt . Has shape (target_shape[1],) . The array is initialized with empty records first (they contain random data) and the field id is set to -1. Once the first position is entered in a column, the id becomes 0 and the record materializes. Once the position is closed, the record fixes its identifier and other data until the next position is entered. The fields entry_price and exit_price are average entry and exit price respectively. The fields pnl and return contain statistics as if the position has been closed and are re-calculated using SimulationContext.last_val_price after pre_segment_func_nb (in case SimulationContext.last_val_price has been overridden) and before post_segment_func_nb . Note In an open position record, the field exit_price doesn't reflect the latest valuation price, but keeps the average price at which the position has been reduced. The position record is updated after successfully filling an order (after order_func_nb and before post_order_func_nb ). Example Consider a simulation that orders order_size for order_price and $1 fixed fees. Here's order info from order_func_nb and the updated position info from post_order_func_nb : order_size order_price id col size entry_idx entry_price \\ 0 NaN 1 -1 0 1.0 13 14.000000 1 0.5 2 0 0 0.5 1 2.000000 2 1.0 3 0 0 1.5 1 2.666667 3 NaN 4 0 0 1.5 1 2.666667 4 -1.0 5 0 0 1.5 1 2.666667 5 -0.5 6 0 0 1.5 1 2.666667 6 NaN 7 0 0 1.5 1 2.666667 7 -0.5 8 1 0 0.5 7 8.000000 8 -1.0 9 1 0 1.5 7 8.666667 9 1.0 10 1 0 1.5 7 8.666667 10 0.5 11 1 0 1.5 7 8.666667 11 1.0 12 2 0 1.0 11 12.000000 12 -2.0 13 3 0 1.0 12 13.000000 13 2.0 14 4 0 1.0 13 14.000000 entry_fees exit_idx exit_price exit_fees pnl return direction status 0 0.5 -1 NaN 0.0 -0.50 -0.035714 0 0 1 1.0 -1 NaN 0.0 -1.00 -1.000000 0 0 2 2.0 -1 NaN 0.0 -1.50 -0.375000 0 0 3 2.0 -1 NaN 0.0 -0.75 -0.187500 0 0 4 2.0 -1 5.000000 1.0 0.50 0.125000 0 0 5 2.0 5 5.333333 2.0 0.00 0.000000 0 1 6 2.0 5 5.333333 2.0 0.00 0.000000 0 1 7 1.0 -1 NaN 0.0 -1.00 -0.250000 1 0 8 2.0 -1 NaN 0.0 -2.50 -0.192308 1 0 9 2.0 -1 10.000000 1.0 -5.00 -0.384615 1 0 10 2.0 10 10.333333 2.0 -6.50 -0.500000 1 1 11 1.0 -1 NaN 0.0 -1.00 -0.083333 0 0 12 0.5 -1 NaN 0.0 -0.50 -0.038462 1 0 13 0.5 -1 NaN 0.0 -0.50 -0.035714 0 0 last_position property \u00b6 Latest position per column. Has shape (target_shape[1],) . Gets updated right after order_func_nb . last_return property \u00b6 Latest return per column or group with cash sharing. Has the same shape as SimulationContext.last_value . Calculated by comparing SimulationContext.last_value to SimulationContext.second_last_value . Gets updated each time SimulationContext.last_value is updated. last_val_price property \u00b6 Latest valuation price per column. Has shape (target_shape[1],) . Enables SizeType.Value , SizeType.TargetValue , and SizeType.TargetPercent . Gets multiplied by the current position to get the value of the column (see SimulationContext.last_value ). Defaults to the SimulationContext.close before post_segment_func_nb . If SimulationContext.ffill_val_price , gets updated only if SimulationContext.close is not NaN. For example, close of [1, 2, np.nan, np.nan, 5] yields valuation price of [1, 2, 2, 2, 5] . Also gets updated right after pre_segment_func_nb - you can use pre_segment_func_nb to override last_val_price in-place, such that order_func_nb can use the new group value. You are not allowed to use -np.inf or np.inf - only finite values. If SimulationContext.update_value , gets also updated right after order_func_nb using filled order price as the latest known price. Note Since the previous SimulationContext.close is NaN in the first row, the first last_val_price is also NaN. Overriding last_val_price with NaN won't apply SimulationContext.ffill_val_price , so your entire group will become NaN. Example Consider 10 units in column 1 and 20 units in column 2. The previous close of them is $40 and $50 respectively, which is also the default valuation price in the current row, available as last_val_price in pre_segment_func_nb . If both columns are in the same group with cash sharing, the group is valued at $1400 before any order_func_nb is called, and can be later accessed via OrderContext.value_now . last_value property \u00b6 Latest value per column or group with cash sharing. Has the same shape as SimulationContext.init_cash . Calculated by multiplying valuation price by the current position. The value of each column in a group with cash sharing is summed to get the value of the entire group. Gets updated using SimulationContext.last_val_price after pre_segment_func_nb and before post_segment_func_nb . If SimulationContext.update_value , gets also updated right after order_func_nb using filled order price as the latest known price (the difference will be minimal, only affected by costs). log_records property \u00b6 Log records. Similar to SimulationContext.order_records but of type log_dt and index SimulationContext.last_lidx . order_records property \u00b6 Order records. It's a 1-dimensional array with records of type order_dt . The array is initialized with empty records first (they contain random data), and then gradually filled with order data. The number of initialized records depends upon max_orders , but usually it's target_shape[0] * target_shape[1] , meaning there is maximal one order record per element. max_orders can be chosen lower if not every order_func_nb leads to a filled order, to save memory. You can use SimulationContext.last_oidx to get the index of the latest filled order of each column. Example Before filling, each order record looks like this: np . array ([( - 8070450532247928832 , - 8070450532247928832 , 4 , 0. , 0. , 0. , 5764616306889786413 )] After filling, it becomes like this: np . array ([( 0 , 0 , 1 , 50. , 1. , 0. , 1 )] second_last_value property \u00b6 Second-latest value per column or group with cash sharing. Has the same shape as SimulationContext.last_value . Contains the latest known value two rows before ( i - 2 ) to be compared either with the latest known value one row before ( i - 1 ) or now ( i ). Gets updated at the end of each segment/row. segment_mask property \u00b6 Mask of whether a particular segment should be executed. A segment is simply a sequence of order_func_nb calls under the same group and row. If a segment is inactive, any callback function inside of it will not be executed. You can still execute the segment's pre- and postprocessing function by enabling SimulationContext.call_pre_segment and SimulationContext.call_post_segment respectively. Utilizes flexible indexing using flex_select_auto_nb() and flex_2d , so it can be passed as 2-dim array, 1-dim array per column (requires flex_2d=True ), 1-dim array per row (requires flex_2d=False ), and a scalar. Broadcasts to the shape (target_shape[0], group_lens.shape[0]) . Note To modify the array in place, make sure to build an array of the full shape. Example Consider two groups with two columns each and the following activity mask: np . array ([[ True , False ], [ False , True ]]) The first group is only executed in the first row and the second group is only executed in the second row. target_shape property \u00b6 Target shape of the simulation. A tuple with exactly two elements: the number of rows and columns. Example One day of minute data for three assets would yield a target_shape of (1440, 3) , where the first axis are rows (minutes) and the second axis are columns (assets). update_value property \u00b6 Whether to update group value after each filled order. Otherwise, stays the same for all columns in the group (the value is calculated only once, before executing any order). The change is marginal and mostly driven by transaction costs and slippage.","title":"enums"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums","text":"Named tuples and enumerated types. Defines enums and other schemas for vectorbt.portfolio .","title":"vectorbt.portfolio.enums"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AccumulationMode","text":"Accumulation mode. { \"Disabled\" : 0 , \"Both\" : 1 , \"AddOnly\" : 2 , \"RemoveOnly\" : 3 } Accumulation allows gradually increasing and decreasing positions by a size. Attributes Disabled Disable accumulation. Both Allow both adding to and removing from the position. AddOnly Allow accumulation to only add to the position. RemoveOnly Allow accumulation to only remove from the position. Note Accumulation acts differently for exits and opposite entries: exits reduce the current position but won't enter the opposite one, while opposite entries reduce the position by the same amount, but as soon as this position is closed, they begin to increase the opposite position. The behavior for opposite entries can be changed by OppositeEntryMode and for stop orders by StopExitMode .","title":"AccumulationMode"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.CallSeqType","text":"Call sequence type. { \"Default\" : 0 , \"Reversed\" : 1 , \"Random\" : 2 , \"Auto\" : 3 } Attributes Default Place calls from left to right. Reversed Place calls from right to left. Random Place calls randomly. Auto Place calls dynamically based on order value.","title":"CallSeqType"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.ConflictMode","text":"Conflict mode. { \"Ignore\" : 0 , \"Entry\" : 1 , \"Exit\" : 2 , \"Adjacent\" : 3 , \"Opposite\" : 4 } What should happen if both entry and exit signals occur simultaneously? Attributes Ignore Ignore both signals. Entry Execute the entry signal. Exit Execute the exit signal. Adjacent Execute the adjacent signal. Takes effect only when in position, otherwise ignores. Opposite Execute the opposite signal. Takes effect only when in position, otherwise ignores.","title":"ConflictMode"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.Direction","text":"Position direction. { \"LongOnly\" : 0 , \"ShortOnly\" : 1 , \"Both\" : 2 } Attributes LongOnly Only long positions. ShortOnly Only short positions. Both Both long and short positions.","title":"Direction"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.DirectionConflictMode","text":"Direction conflict mode. { \"Ignore\" : 0 , \"Long\" : 1 , \"Short\" : 2 , \"Adjacent\" : 3 , \"Opposite\" : 4 } What should happen if both long and short entry signals occur simultaneously? Attributes Ignore Ignore both entry signals. Long Execute the long entry signal. Short Execute the short entry signal. Adjacent Execute the adjacent entry signal. Takes effect only when in position, otherwise ignores. Opposite Execute the opposite entry signal. Takes effect only when in position, otherwise ignores.","title":"DirectionConflictMode"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.InitCashMode","text":"Initial cash mode. { \"Auto\" : 0 , \"AutoAlign\" : 1 } Attributes Auto Initial cash is infinite within simulation, and then set to the total cash spent. AutoAlign Initial cash is set to the total cash spent across all columns.","title":"InitCashMode"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.NoOrder","text":"Order that should not be processed.","title":"NoOrder"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OppositeEntryMode","text":"Opposite entry mode. { \"Ignore\" : 0 , \"Close\" : 1 , \"CloseReduce\" : 2 , \"Reverse\" : 3 , \"ReverseReduce\" : 4 } What should happen if an entry signal of opposite direction occurs before an exit signal? Attributes Ignore Ignore the opposite entry signal. Close Close the current position. CloseReduce Close the current position or reduce it if accumulation is enabled. Reverse Reverse the current position. ReverseReduce Reverse the current position or reduce it if accumulation is enabled.","title":"OppositeEntryMode"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderSide","text":"Order side. { \"Buy\" : 0 , \"Sell\" : 1 }","title":"OrderSide"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderStatus","text":"Order status. { \"Filled\" : 0 , \"Ignored\" : 1 , \"Rejected\" : 2 } Attributes Filled Order has been filled. Ignored Order has been ignored. Rejected Order has been rejected.","title":"OrderStatus"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderStatusInfo","text":"Order status information. { \"SizeNaN\" : 0 , \"PriceNaN\" : 1 , \"ValPriceNaN\" : 2 , \"ValueNaN\" : 3 , \"ValueZeroNeg\" : 4 , \"SizeZero\" : 5 , \"NoCashShort\" : 6 , \"NoCashLong\" : 7 , \"NoOpenPosition\" : 8 , \"MaxSizeExceeded\" : 9 , \"RandomEvent\" : 10 , \"CantCoverFees\" : 11 , \"MinSizeNotReached\" : 12 , \"PartialFill\" : 13 }","title":"OrderStatusInfo"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SizeType","text":"Size type. { \"Amount\" : 0 , \"Value\" : 1 , \"Percent\" : 2 , \"TargetAmount\" : 3 , \"TargetValue\" : 4 , \"TargetPercent\" : 5 } Attributes Amount Amount of assets to trade. Value Asset value to trade. Gets converted into SizeType.Amount using OrderContext.val_price_now . Percent Percentage of available resources to use in either direction (not to be confused with the percentage of position value!) When buying, it's the percentage of OrderContext.cash_now . When selling, it's the percentage of OrderContext.position_now . When short selling, it's the percentage of OrderContext.free_cash_now . When selling and short selling (i.e. reversing position), it's the percentage of OrderContext.position_now and OrderContext.free_cash_now . Note Takes into account fees and slippage to find the limit. In reality, slippage and fees are not known beforehand. TargetAmount Target amount of assets to hold (= target position). Uses OrderContext.position_now to get the current position. Gets converted into SizeType.Amount . TargetValue Target asset value. Uses OrderContext.val_price_now to get the current asset value. Gets converted into SizeType.TargetAmount . TargetPercent Target percentage of total value. Uses OrderContext.value_now to get the current total value. Gets converted into SizeType.TargetValue .","title":"SizeType"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.StopEntryPrice","text":"Stop entry price. { \"ValPrice\" : 0 , \"Price\" : 1 , \"FillPrice\" : 2 , \"Close\" : 3 } Which price to use as an initial stop price? Attributes ValPrice Asset valuation price. Price Default price. FillPrice Fill price (that is, slippage is already applied). Close Closing price.","title":"StopEntryPrice"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.StopExitMode","text":"Stop exit mode. { \"Close\" : 0 , \"CloseReduce\" : 1 , \"Reverse\" : 2 , \"ReverseReduce\" : 3 } How to exit the current position upon a stop signal? Attributes Close Close the current position. CloseReduce Close the current position or reduce it if accumulation is enabled. Reverse Reverse the current position. ReverseReduce Reverse the current position or reduce it if accumulation is enabled.","title":"StopExitMode"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.StopExitPrice","text":"Stop exit price. { \"StopLimit\" : 0 , \"StopMarket\" : 1 , \"Price\" : 2 , \"Close\" : 3 } Which price to use when exiting a position upon a stop signal? Attributes StopLimit Stop price as from a limit order. If the stop was hit before, the opening price at the next bar is used. User-defined slippage is not applied. StopMarket Stop price as from a market order. If the stop was hit before, the opening price at the next bar is used. User-defined slippage is applied. Price Default price. User-defined slippage is applied. Note Make sure to use StopExitPrice.Price only together with StopEntryPrice.Close . Otherwise, there is no proof that the price comes after the stop price. Close Closing price. User-defined slippage is applied. Note We can execute only one signal per asset and bar. This means the following: 1) Stop signal cannot be processed at the same bar as the entry signal. 2) When dealing with stop orders, we have another signal - stop signal - that may be in a conflict with the signals placed by the user. To choose between both, we assume that any stop signal comes before any other signal in time. Thus, make sure to always execute ordinary signals using the closing price when using stop orders. Otherwise, you're looking into the future.","title":"StopExitPrice"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.StopUpdateMode","text":"Stop update mode. { \"Keep\" : 0 , \"Override\" : 1 , \"OverrideNaN\" : 2 } What to do with the old stop upon new acquisition? Attributes Keep Keep the old stop. Override Override the old stop, but only if the new stop is not NaN. OverrideNaN Override the old stop, even if the new stop is NaN.","title":"StopUpdateMode"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.TradeDirection","text":"Event direction. { \"Long\" : 0 , \"Short\" : 1 }","title":"TradeDirection"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.TradeStatus","text":"Event status. { \"Open\" : 0 , \"Closed\" : 1 }","title":"TradeStatus"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.TradesType","text":"Trades type. { \"EntryTrades\" : 0 , \"ExitTrades\" : 1 , \"Positions\" : 2 }","title":"TradesType"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.log_dt","text":"np.dtype of log records. { \"id\" : \"int64\" , \"group\" : \"int64\" , \"col\" : \"int64\" , \"idx\" : \"int64\" , \"cash\" : \"float64\" , \"position\" : \"float64\" , \"debt\" : \"float64\" , \"free_cash\" : \"float64\" , \"val_price\" : \"float64\" , \"value\" : \"float64\" , \"req_size\" : \"float64\" , \"req_price\" : \"float64\" , \"req_size_type\" : \"int64\" , \"req_direction\" : \"int64\" , \"req_fees\" : \"float64\" , \"req_fixed_fees\" : \"float64\" , \"req_slippage\" : \"float64\" , \"req_min_size\" : \"float64\" , \"req_max_size\" : \"float64\" , \"req_size_granularity\" : \"float64\" , \"req_reject_prob\" : \"float64\" , \"req_lock_cash\" : \"bool\" , \"req_allow_partial\" : \"bool\" , \"req_raise_reject\" : \"bool\" , \"req_log\" : \"bool\" , \"new_cash\" : \"float64\" , \"new_position\" : \"float64\" , \"new_debt\" : \"float64\" , \"new_free_cash\" : \"float64\" , \"new_val_price\" : \"float64\" , \"new_value\" : \"float64\" , \"res_size\" : \"float64\" , \"res_price\" : \"float64\" , \"res_fees\" : \"float64\" , \"res_side\" : \"int64\" , \"res_status\" : \"int64\" , \"res_status_info\" : \"int64\" , \"order_id\" : \"int64\" }","title":"log_dt"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.order_dt","text":"np.dtype of order records. { \"id\" : \"int64\" , \"col\" : \"int64\" , \"idx\" : \"int64\" , \"size\" : \"float64\" , \"price\" : \"float64\" , \"fees\" : \"float64\" , \"side\" : \"int64\" }","title":"order_dt"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.trade_dt","text":"np.dtype of trade records. { \"id\" : \"int64\" , \"col\" : \"int64\" , \"size\" : \"float64\" , \"entry_idx\" : \"int64\" , \"entry_price\" : \"float64\" , \"entry_fees\" : \"float64\" , \"exit_idx\" : \"int64\" , \"exit_price\" : \"float64\" , \"exit_fees\" : \"float64\" , \"pnl\" : \"float64\" , \"return\" : \"float64\" , \"direction\" : \"int64\" , \"status\" : \"int64\" , \"parent_id\" : \"int64\" }","title":"trade_dt"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustSLContext","text":"A named tuple representing the context for generation of signals. Superclasses builtins.tuple","title":"AdjustSLContext"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustSLContext.col","text":"Current column. Has range [0, target_shape[1]) and is always within [from_col, to_col) .","title":"col"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustSLContext.curr_i","text":"Index of the row of the updated stop. Gets updated once the price is updated.","title":"curr_i"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustSLContext.curr_price","text":"Current stop price. Gets updated in trailing SL once a higher price is discovered.","title":"curr_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustSLContext.curr_stop","text":"Current stop value. Can be updated by adjustment function.","title":"curr_stop"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustSLContext.curr_trail","text":"Current trailing flag. Can be updated by adjustment function.","title":"curr_trail"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustSLContext.i","text":"Index of the current row. Has range [0, target_shape[0]) .","title":"i"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustSLContext.init_i","text":"Index of the row of the initial stop. Doesn't change.","title":"init_i"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustSLContext.init_price","text":"Price of the initial stop. Doesn't change.","title":"init_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustSLContext.position_now","text":"Latest position.","title":"position_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustSLContext.val_price_now","text":"Latest valuation price.","title":"val_price_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustTPContext","text":"A named tuple representing the context for adjusting take profit. Superclasses builtins.tuple","title":"AdjustTPContext"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustTPContext.col","text":"See AdjustSLContext.col .","title":"col"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustTPContext.curr_stop","text":"See AdjustSLContext.curr_stop .","title":"curr_stop"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustTPContext.i","text":"See AdjustSLContext.i .","title":"i"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustTPContext.init_i","text":"See AdjustSLContext.init_i .","title":"init_i"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustTPContext.init_price","text":"See AdjustSLContext.curr_price .","title":"init_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustTPContext.position_now","text":"See AdjustSLContext.position_now .","title":"position_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.AdjustTPContext.val_price_now","text":"See AdjustSLContext.val_price_now .","title":"val_price_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.ExecuteOrderState","text":"State after order execution. Superclasses builtins.tuple","title":"ExecuteOrderState"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.ExecuteOrderState.cash","text":"See ProcessOrderState.cash .","title":"cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.ExecuteOrderState.debt","text":"See ProcessOrderState.debt .","title":"debt"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.ExecuteOrderState.free_cash","text":"See ProcessOrderState.free_cash .","title":"free_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.ExecuteOrderState.position","text":"See ProcessOrderState.position .","title":"position"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext","text":"A named tuple representing the context of a flexible order. Contains all fields from SegmentContext plus the current call index. Passed to flex_order_func_nb . Superclasses builtins.tuple","title":"FlexOrderContext"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.call_idx","text":"Index of the current call.","title":"call_idx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.call_post_segment","text":"See SimulationContext.call_post_segment .","title":"call_post_segment"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.call_pre_segment","text":"See SimulationContext.call_pre_segment .","title":"call_pre_segment"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.call_seq","text":"See SimulationContext.call_seq .","title":"call_seq"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.call_seq_now","text":"See SegmentContext.call_seq_now .","title":"call_seq_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.cash_sharing","text":"See SimulationContext.cash_sharing .","title":"cash_sharing"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.close","text":"See SimulationContext.close .","title":"close"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.ffill_val_price","text":"See SimulationContext.ffill_val_price .","title":"ffill_val_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.fill_pos_record","text":"See SimulationContext.fill_pos_record .","title":"fill_pos_record"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.flex_2d","text":"See SimulationContext.flex_2d .","title":"flex_2d"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.from_col","text":"See GroupContext.from_col .","title":"from_col"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.group","text":"See GroupContext.group .","title":"group"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.group_len","text":"See GroupContext.group_len .","title":"group_len"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.group_lens","text":"See SimulationContext.group_lens .","title":"group_lens"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.i","text":"See RowContext.i .","title":"i"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.init_cash","text":"See SimulationContext.init_cash .","title":"init_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.last_cash","text":"See SimulationContext.last_cash .","title":"last_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.last_debt","text":"See SimulationContext.last_debt .","title":"last_debt"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.last_free_cash","text":"See SimulationContext.last_free_cash .","title":"last_free_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.last_lidx","text":"See SimulationContext.last_lidx .","title":"last_lidx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.last_oidx","text":"See SimulationContext.last_oidx .","title":"last_oidx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.last_pos_record","text":"See SimulationContext.last_pos_record .","title":"last_pos_record"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.last_position","text":"See SimulationContext.last_position .","title":"last_position"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.last_return","text":"See SimulationContext.last_return .","title":"last_return"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.last_val_price","text":"See SimulationContext.last_val_price .","title":"last_val_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.last_value","text":"See SimulationContext.last_value .","title":"last_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.log_records","text":"See SimulationContext.log_records .","title":"log_records"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.order_records","text":"See SimulationContext.order_records .","title":"order_records"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.second_last_value","text":"See SimulationContext.second_last_value .","title":"second_last_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.segment_mask","text":"See SimulationContext.segment_mask .","title":"segment_mask"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.target_shape","text":"See SimulationContext.target_shape .","title":"target_shape"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.to_col","text":"See GroupContext.to_col .","title":"to_col"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.FlexOrderContext.update_value","text":"See SimulationContext.update_value .","title":"update_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext","text":"A named tuple representing the context of a group. A group is a set of nearby columns that are somehow related (for example, by sharing the same capital). In each row, the columns under the same group are bound to the same segment. Contains all fields from SimulationContext plus fields describing the current group. Passed to pre_group_func_nb and post_group_func_nb . Example Consider a group of three columns, a group of two columns, and one more column: group group_len from_col to_col 0 3 0 3 1 2 3 5 2 1 5 6 Superclasses builtins.tuple","title":"GroupContext"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.call_post_segment","text":"See SimulationContext.call_post_segment .","title":"call_post_segment"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.call_pre_segment","text":"See SimulationContext.call_pre_segment .","title":"call_pre_segment"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.call_seq","text":"See SimulationContext.call_seq .","title":"call_seq"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.cash_sharing","text":"See SimulationContext.cash_sharing .","title":"cash_sharing"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.close","text":"See SimulationContext.close .","title":"close"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.ffill_val_price","text":"See SimulationContext.ffill_val_price .","title":"ffill_val_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.fill_pos_record","text":"See SimulationContext.fill_pos_record .","title":"fill_pos_record"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.flex_2d","text":"See SimulationContext.flex_2d .","title":"flex_2d"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.from_col","text":"Index of the first column in the current group. Has range [0, target_shape[1]) .","title":"from_col"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.group","text":"Index of the current group. Has range [0, group_lens.shape[0]) .","title":"group"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.group_len","text":"Number of columns in the current group. Scalar value. Same as group_lens[group] .","title":"group_len"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.group_lens","text":"See SimulationContext.group_lens .","title":"group_lens"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.init_cash","text":"See SimulationContext.init_cash .","title":"init_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.last_cash","text":"See SimulationContext.last_cash .","title":"last_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.last_debt","text":"See SimulationContext.last_debt .","title":"last_debt"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.last_free_cash","text":"See SimulationContext.last_free_cash .","title":"last_free_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.last_lidx","text":"See SimulationContext.last_lidx .","title":"last_lidx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.last_oidx","text":"See SimulationContext.last_oidx .","title":"last_oidx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.last_pos_record","text":"See SimulationContext.last_pos_record .","title":"last_pos_record"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.last_position","text":"See SimulationContext.last_position .","title":"last_position"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.last_return","text":"See SimulationContext.last_return .","title":"last_return"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.last_val_price","text":"See SimulationContext.last_val_price .","title":"last_val_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.last_value","text":"See SimulationContext.last_value .","title":"last_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.log_records","text":"See SimulationContext.log_records .","title":"log_records"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.order_records","text":"See SimulationContext.order_records .","title":"order_records"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.second_last_value","text":"See SimulationContext.second_last_value .","title":"second_last_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.segment_mask","text":"See SimulationContext.segment_mask .","title":"segment_mask"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.target_shape","text":"See SimulationContext.target_shape .","title":"target_shape"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.to_col","text":"Index of the last column in the current group plus one. Has range [1, target_shape[1] + 1) . If columns are not grouped, equals to from_col + 1 . Warning In the last group, to_col points at a column that doesn't exist.","title":"to_col"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.GroupContext.update_value","text":"See SimulationContext.update_value .","title":"update_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.Order","text":"A named tuple representing an order. Note Currently, Numba has issues with using defaults when filling named tuples. Use order_nb() to create an order. Superclasses builtins.tuple","title":"Order"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.Order.allow_partial","text":"Whether to allow partial fill. Otherwise, the order gets rejected. Does not apply when Order.size is np.inf .","title":"allow_partial"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.Order.direction","text":"See Direction .","title":"direction"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.Order.fees","text":"Fees in percentage of the order value. Negative trading fees like -0.05 means earning 0.05% per trade instead of paying a fee. Note 0.01 = 1%.","title":"fees"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.Order.fixed_fees","text":"Fixed amount of fees to pay for this order.","title":"fixed_fees"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.Order.lock_cash","text":"Whether to lock cash when shorting. If enabled, prevents free_cash from turning negative when buying or short selling. A negative free_cash means one column used collateral of another column, which is generally undesired.","title":"lock_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.Order.log","text":"Whether to log this order by filling a log record. Remember to increase max_logs .","title":"log"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.Order.max_size","text":"Maximum size in both directions. Higher than that will be partly filled.","title":"max_size"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.Order.min_size","text":"Minimum size in both directions. Lower than that will be rejected.","title":"min_size"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.Order.price","text":"Price per unit. Final price will depend upon slippage. If -np.inf , replaced by the current open (if available) or the previous close (\u2248 the current open in crypto). If np.inf , replaced by the current close. Note Make sure to use timestamps that come between (and ideally not including) the current open and close.","title":"price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.Order.raise_reject","text":"Whether to raise exception if order has been rejected. Terminates the simulation.","title":"raise_reject"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.Order.reject_prob","text":"Probability of rejecting this order to simulate a random rejection event. Not everything goes smoothly in real life. Use random rejections to test your order management for robustness.","title":"reject_prob"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.Order.size","text":"Size in units. Behavior depends upon Order.size_type and Order.direction . For any fixed size: Set to any number to buy/sell some fixed amount or value. Longs are limited by the current cash balance, while shorts are only limited if Order.lock_cash . Set to np.inf to buy for all cash, or -np.inf to sell for all free cash. If Order.direction is not Direction.Both , -np.inf will close the position. Set to np.nan or 0 to skip. For any target size: Set to any number to buy/sell an amount relative to the current position or value. Set to 0 to close the current position. Set to np.nan to skip.","title":"size"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.Order.size_granularity","text":"Granularity of the size. For example, granularity of 1.0 makes the quantity to behave like an integer. Placing an order of 12.5 shares (in any direction) will order exactly 12.0 shares. Note The filled size remains a floating number.","title":"size_granularity"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.Order.size_type","text":"See SizeType .","title":"size_type"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.Order.slippage","text":"Slippage in percentage of Order.price . Slippage is a penalty applied on the price. Note 0.01 = 1%.","title":"slippage"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext","text":"A named tuple representing the context of an order. Contains all fields from SegmentContext plus fields describing the current state. Passed to order_func_nb . Superclasses builtins.tuple","title":"OrderContext"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.call_idx","text":"Index of the current call in SegmentContext.call_seq_now . Has range [0, group_len) .","title":"call_idx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.call_post_segment","text":"See SimulationContext.call_post_segment .","title":"call_post_segment"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.call_pre_segment","text":"See SimulationContext.call_pre_segment .","title":"call_pre_segment"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.call_seq","text":"See SimulationContext.call_seq .","title":"call_seq"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.call_seq_now","text":"See SegmentContext.call_seq_now .","title":"call_seq_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.cash_now","text":"SimulationContext.last_cash for the current column/group.","title":"cash_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.cash_sharing","text":"See SimulationContext.cash_sharing .","title":"cash_sharing"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.close","text":"See SimulationContext.close .","title":"close"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.col","text":"Current column. Has range [0, target_shape[1]) and is always within [from_col, to_col) .","title":"col"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.debt_now","text":"SimulationContext.last_debt for the current column.","title":"debt_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.ffill_val_price","text":"See SimulationContext.ffill_val_price .","title":"ffill_val_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.fill_pos_record","text":"See SimulationContext.fill_pos_record .","title":"fill_pos_record"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.flex_2d","text":"See SimulationContext.flex_2d .","title":"flex_2d"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.free_cash_now","text":"SimulationContext.last_free_cash for the current column/group.","title":"free_cash_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.from_col","text":"See GroupContext.from_col .","title":"from_col"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.group","text":"See GroupContext.group .","title":"group"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.group_len","text":"See GroupContext.group_len .","title":"group_len"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.group_lens","text":"See SimulationContext.group_lens .","title":"group_lens"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.i","text":"See RowContext.i .","title":"i"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.init_cash","text":"See SimulationContext.init_cash .","title":"init_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.last_cash","text":"See SimulationContext.last_cash .","title":"last_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.last_debt","text":"See SimulationContext.last_debt .","title":"last_debt"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.last_free_cash","text":"See SimulationContext.last_free_cash .","title":"last_free_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.last_lidx","text":"See SimulationContext.last_lidx .","title":"last_lidx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.last_oidx","text":"See SimulationContext.last_oidx .","title":"last_oidx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.last_pos_record","text":"See SimulationContext.last_pos_record .","title":"last_pos_record"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.last_position","text":"See SimulationContext.last_position .","title":"last_position"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.last_return","text":"See SimulationContext.last_return .","title":"last_return"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.last_val_price","text":"See SimulationContext.last_val_price .","title":"last_val_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.last_value","text":"See SimulationContext.last_value .","title":"last_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.log_records","text":"See SimulationContext.log_records .","title":"log_records"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.order_records","text":"See SimulationContext.order_records .","title":"order_records"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.pos_record_now","text":"SimulationContext.last_pos_record for the current column.","title":"pos_record_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.position_now","text":"SimulationContext.last_position for the current column.","title":"position_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.return_now","text":"SimulationContext.last_return for the current column/group.","title":"return_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.second_last_value","text":"See SimulationContext.second_last_value .","title":"second_last_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.segment_mask","text":"See SimulationContext.segment_mask .","title":"segment_mask"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.target_shape","text":"See SimulationContext.target_shape .","title":"target_shape"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.to_col","text":"See GroupContext.to_col .","title":"to_col"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.update_value","text":"See SimulationContext.update_value .","title":"update_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.val_price_now","text":"SimulationContext.last_val_price for the current column.","title":"val_price_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderContext.value_now","text":"SimulationContext.last_value for the current column/group.","title":"value_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderResult","text":"A named tuple representing an order result. Superclasses builtins.tuple","title":"OrderResult"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderResult.fees","text":"Total fees paid for this order.","title":"fees"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderResult.price","text":"Filled price per unit, adjusted with slippage.","title":"price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderResult.side","text":"See OrderSide .","title":"side"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderResult.size","text":"Filled size.","title":"size"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderResult.status","text":"See OrderStatus .","title":"status"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.OrderResult.status_info","text":"See OrderStatusInfo .","title":"status_info"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext","text":"A named tuple representing the context after an order has been processed. Contains all fields from OrderContext plus fields describing the order result and the previous state. Passed to post_order_func_nb . Superclasses builtins.tuple","title":"PostOrderContext"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.call_idx","text":"See OrderContext.call_idx .","title":"call_idx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.call_post_segment","text":"See SimulationContext.call_post_segment .","title":"call_post_segment"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.call_pre_segment","text":"See SimulationContext.call_pre_segment .","title":"call_pre_segment"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.call_seq","text":"See SimulationContext.call_seq .","title":"call_seq"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.call_seq_now","text":"See SegmentContext.call_seq_now .","title":"call_seq_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.cash_before","text":"OrderContext.cash_now before execution.","title":"cash_before"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.cash_now","text":"OrderContext.cash_now after execution.","title":"cash_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.cash_sharing","text":"See SimulationContext.cash_sharing .","title":"cash_sharing"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.close","text":"See SimulationContext.close .","title":"close"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.col","text":"See OrderContext.col .","title":"col"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.debt_before","text":"OrderContext.debt_now before execution.","title":"debt_before"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.debt_now","text":"OrderContext.debt_now after execution.","title":"debt_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.ffill_val_price","text":"See SimulationContext.ffill_val_price .","title":"ffill_val_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.fill_pos_record","text":"See SimulationContext.fill_pos_record .","title":"fill_pos_record"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.flex_2d","text":"See SimulationContext.flex_2d .","title":"flex_2d"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.free_cash_before","text":"OrderContext.free_cash_now before execution.","title":"free_cash_before"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.free_cash_now","text":"OrderContext.free_cash_now after execution.","title":"free_cash_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.from_col","text":"See GroupContext.from_col .","title":"from_col"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.group","text":"See GroupContext.group .","title":"group"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.group_len","text":"See GroupContext.group_len .","title":"group_len"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.group_lens","text":"See SimulationContext.group_lens .","title":"group_lens"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.i","text":"See RowContext.i .","title":"i"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.init_cash","text":"See SimulationContext.init_cash .","title":"init_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.last_cash","text":"See SimulationContext.last_cash .","title":"last_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.last_debt","text":"See SimulationContext.last_debt .","title":"last_debt"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.last_free_cash","text":"See SimulationContext.last_free_cash .","title":"last_free_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.last_lidx","text":"See SimulationContext.last_lidx .","title":"last_lidx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.last_oidx","text":"See SimulationContext.last_oidx .","title":"last_oidx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.last_pos_record","text":"See SimulationContext.last_pos_record .","title":"last_pos_record"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.last_position","text":"See SimulationContext.last_position .","title":"last_position"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.last_return","text":"See SimulationContext.last_return .","title":"last_return"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.last_val_price","text":"See SimulationContext.last_val_price .","title":"last_val_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.last_value","text":"See SimulationContext.last_value .","title":"last_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.log_records","text":"See SimulationContext.log_records .","title":"log_records"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.order_records","text":"See SimulationContext.order_records .","title":"order_records"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.order_result","text":"Order result of type OrderResult . Can be used to check whether the order has been filled, ignored, or rejected.","title":"order_result"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.pos_record_now","text":"OrderContext.pos_record_now after execution.","title":"pos_record_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.position_before","text":"OrderContext.position_now before execution.","title":"position_before"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.position_now","text":"OrderContext.position_now after execution.","title":"position_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.return_now","text":"OrderContext.return_now after execution.","title":"return_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.second_last_value","text":"See SimulationContext.second_last_value .","title":"second_last_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.segment_mask","text":"See SimulationContext.segment_mask .","title":"segment_mask"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.target_shape","text":"See SimulationContext.target_shape .","title":"target_shape"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.to_col","text":"See GroupContext.to_col .","title":"to_col"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.update_value","text":"See SimulationContext.update_value .","title":"update_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.val_price_before","text":"OrderContext.val_price_now before execution.","title":"val_price_before"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.val_price_now","text":"OrderContext.val_price_now after execution. If SimulationContext.update_value , gets replaced with the fill price, as it becomes the most recently known price. Otherwise, stays the same.","title":"val_price_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.value_before","text":"OrderContext.value_now before execution.","title":"value_before"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.PostOrderContext.value_now","text":"OrderContext.value_now after execution. If SimulationContext.update_value , gets updated with the new cash and value of the column. Otherwise, stays the same.","title":"value_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.ProcessOrderState","text":"State before or after order processing. Superclasses builtins.tuple","title":"ProcessOrderState"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.ProcessOrderState.cash","text":"Cash in the current column or group with cash sharing.","title":"cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.ProcessOrderState.debt","text":"Debt from shorting in the current column.","title":"debt"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.ProcessOrderState.free_cash","text":"Free cash in the current column or group with cash sharing.","title":"free_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.ProcessOrderState.lidx","text":"Index of log record.","title":"lidx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.ProcessOrderState.oidx","text":"Index of order record.","title":"oidx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.ProcessOrderState.position","text":"Position in the current column.","title":"position"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.ProcessOrderState.val_price","text":"Valuation price in the current column.","title":"val_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.ProcessOrderState.value","text":"Value in the current column or group with cash sharing.","title":"value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RejectedOrderError","text":"Rejected order error. Superclasses builtins.BaseException builtins.Exception","title":"RejectedOrderError"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext","text":"A named tuple representing the context of a row. A row is a time step in which segments are executed. Contains all fields from SimulationContext plus fields describing the current row. Passed to pre_row_func_nb and post_row_func_nb . Superclasses builtins.tuple","title":"RowContext"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.call_post_segment","text":"See SimulationContext.call_post_segment .","title":"call_post_segment"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.call_pre_segment","text":"See SimulationContext.call_pre_segment .","title":"call_pre_segment"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.call_seq","text":"See SimulationContext.call_seq .","title":"call_seq"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.cash_sharing","text":"See SimulationContext.cash_sharing .","title":"cash_sharing"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.close","text":"See SimulationContext.close .","title":"close"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.ffill_val_price","text":"See SimulationContext.ffill_val_price .","title":"ffill_val_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.fill_pos_record","text":"See SimulationContext.fill_pos_record .","title":"fill_pos_record"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.flex_2d","text":"See SimulationContext.flex_2d .","title":"flex_2d"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.group_lens","text":"See SimulationContext.group_lens .","title":"group_lens"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.i","text":"Index of the current row. Has range [0, target_shape[0]) .","title":"i"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.init_cash","text":"See SimulationContext.init_cash .","title":"init_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.last_cash","text":"See SimulationContext.last_cash .","title":"last_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.last_debt","text":"See SimulationContext.last_debt .","title":"last_debt"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.last_free_cash","text":"See SimulationContext.last_free_cash .","title":"last_free_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.last_lidx","text":"See SimulationContext.last_lidx .","title":"last_lidx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.last_oidx","text":"See SimulationContext.last_oidx .","title":"last_oidx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.last_pos_record","text":"See SimulationContext.last_pos_record .","title":"last_pos_record"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.last_position","text":"See SimulationContext.last_position .","title":"last_position"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.last_return","text":"See SimulationContext.last_return .","title":"last_return"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.last_val_price","text":"See SimulationContext.last_val_price .","title":"last_val_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.last_value","text":"See SimulationContext.last_value .","title":"last_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.log_records","text":"See SimulationContext.log_records .","title":"log_records"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.order_records","text":"See SimulationContext.order_records .","title":"order_records"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.second_last_value","text":"See SimulationContext.second_last_value .","title":"second_last_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.segment_mask","text":"See SimulationContext.segment_mask .","title":"segment_mask"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.target_shape","text":"See SimulationContext.target_shape .","title":"target_shape"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.RowContext.update_value","text":"See SimulationContext.update_value .","title":"update_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext","text":"A named tuple representing the context of a segment. A segment is an intersection between groups and rows. It's an entity that defines how and in which order elements within the same group and row are processed. Contains all fields from SimulationContext , GroupContext , and RowContext , plus fields describing the current segment. Passed to pre_segment_func_nb and post_segment_func_nb . Superclasses builtins.tuple","title":"SegmentContext"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.call_post_segment","text":"See SimulationContext.call_post_segment .","title":"call_post_segment"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.call_pre_segment","text":"See SimulationContext.call_pre_segment .","title":"call_pre_segment"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.call_seq","text":"See SimulationContext.call_seq .","title":"call_seq"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.call_seq_now","text":"Sequence of calls within the current segment. Has shape (group_len,) . Each value in this sequence should indicate the position of column in the group to call next. Processing goes always from left to right. You can use pre_segment_func_nb to override call_seq_now . Example [2, 0, 1] would first call column 2, then 0, and finally 1.","title":"call_seq_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.cash_sharing","text":"See SimulationContext.cash_sharing .","title":"cash_sharing"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.close","text":"See SimulationContext.close .","title":"close"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.ffill_val_price","text":"See SimulationContext.ffill_val_price .","title":"ffill_val_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.fill_pos_record","text":"See SimulationContext.fill_pos_record .","title":"fill_pos_record"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.flex_2d","text":"See SimulationContext.flex_2d .","title":"flex_2d"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.from_col","text":"See GroupContext.from_col .","title":"from_col"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.group","text":"See GroupContext.group .","title":"group"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.group_len","text":"See GroupContext.group_len .","title":"group_len"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.group_lens","text":"See SimulationContext.group_lens .","title":"group_lens"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.i","text":"See RowContext.i .","title":"i"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.init_cash","text":"See SimulationContext.init_cash .","title":"init_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.last_cash","text":"See SimulationContext.last_cash .","title":"last_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.last_debt","text":"See SimulationContext.last_debt .","title":"last_debt"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.last_free_cash","text":"See SimulationContext.last_free_cash .","title":"last_free_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.last_lidx","text":"See SimulationContext.last_lidx .","title":"last_lidx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.last_oidx","text":"See SimulationContext.last_oidx .","title":"last_oidx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.last_pos_record","text":"See SimulationContext.last_pos_record .","title":"last_pos_record"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.last_position","text":"See SimulationContext.last_position .","title":"last_position"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.last_return","text":"See SimulationContext.last_return .","title":"last_return"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.last_val_price","text":"See SimulationContext.last_val_price .","title":"last_val_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.last_value","text":"See SimulationContext.last_value .","title":"last_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.log_records","text":"See SimulationContext.log_records .","title":"log_records"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.order_records","text":"See SimulationContext.order_records .","title":"order_records"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.second_last_value","text":"See SimulationContext.second_last_value .","title":"second_last_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.segment_mask","text":"See SimulationContext.segment_mask .","title":"segment_mask"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.target_shape","text":"See SimulationContext.target_shape .","title":"target_shape"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.to_col","text":"See GroupContext.to_col .","title":"to_col"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SegmentContext.update_value","text":"See SimulationContext.update_value .","title":"update_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SignalContext","text":"SignalContext(i, col, position_now, val_price_now, flex_2d) Superclasses builtins.tuple","title":"SignalContext"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SignalContext.col","text":"Alias for field number 1","title":"col"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SignalContext.flex_2d","text":"Alias for field number 4","title":"flex_2d"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SignalContext.i","text":"Alias for field number 0","title":"i"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SignalContext.position_now","text":"Alias for field number 2","title":"position_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SignalContext.val_price_now","text":"Alias for field number 3","title":"val_price_now"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext","text":"A named tuple representing the context of a simulation. Contains general information available to all other contexts. Passed to pre_sim_func_nb and post_sim_func_nb . Superclasses builtins.tuple","title":"SimulationContext"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.call_post_segment","text":"Whether to call post_segment_func_nb regardless of SimulationContext.segment_mask . Allows, for example, to write user-defined arrays such as returns at the end of each segment.","title":"call_post_segment"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.call_pre_segment","text":"Whether to call pre_segment_func_nb regardless of SimulationContext.segment_mask .","title":"call_pre_segment"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.call_seq","text":"Default sequence of calls per segment. Controls the sequence in which order_func_nb is executed within each segment. Has shape SimulationContext.target_shape and each value must exist in the range [0, group_len) . Note To use sort_call_seq_nb , should be generated using CallSeqType.Default . To change the call sequence dynamically, better change SegmentContext.call_seq_now in-place. Example The default call sequence for three data points and two groups with three columns each: np . array ([ [ 0 , 1 , 2 , 0 , 1 , 2 ], [ 0 , 1 , 2 , 0 , 1 , 2 ], [ 0 , 1 , 2 , 0 , 1 , 2 ] ])","title":"call_seq"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.cash_sharing","text":"Whether cash sharing is enabled.","title":"cash_sharing"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.close","text":"Latest asset price at each time step. Utilizes flexible indexing using flex_select_auto_nb() and flex_2d , so it can be passed as 2-dim array, 1-dim array per column (requires flex_2d=True ), 1-dim array per row (requires flex_2d=False ), and a scalar. Broadcasts to the shape SimulationContext.target_shape . Note To modify the array in place, make sure to build an array of the full shape.","title":"close"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.ffill_val_price","text":"Whether to track valuation price only if it's known. Otherwise, unknown SimulationContext.close will lead to NaN in valuation price at the next timestamp.","title":"ffill_val_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.fill_pos_record","text":"Whether to fill position record. Disable this to make simulation a bit faster for simple use cases.","title":"fill_pos_record"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.flex_2d","text":"Whether the elements in a 1-dim array should be treated per column rather than per row. This flag is set automatically when using Portfolio.from_order_func() depending upon whether there is any argument that has been broadcast to 2 dimensions. Has only effect when using flexible indexing, for example, with flex_select_auto_nb() .","title":"flex_2d"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.group_lens","text":"Number of columns in each group. Even if columns are not grouped, group_lens contains ones - one column per group. Example In pairs trading, group_lens would be np.array([2]) , while three independent columns would be represented by group_lens of np.array([1, 1, 1]) .","title":"group_lens"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.init_cash","text":"Initial capital per column or group with cash sharing. If SimulationContext.cash_sharing , has shape (group_lens.shape[0],) , otherwise has shape (target_shape[1],) . Example Consider three columns, each having $100 of starting capital. If we built one group of two columns with cash sharing and one (imaginary) group with the last column, the init_cash would be np.array([200, 100]) . Without cash sharing, the init_cash would be np.array([100, 100, 100]) .","title":"init_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.last_cash","text":"Latest cash per column or group with cash sharing. Has the same shape as SimulationContext.init_cash . Gets updated right after order_func_nb .","title":"last_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.last_debt","text":"Latest debt from shorting per column. Debt is the total value from shorting that hasn't been covered yet. Used to update OrderContext.free_cash_now . Has shape (target_shape[1],) . Gets updated right after order_func_nb .","title":"last_debt"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.last_free_cash","text":"Latest free cash per column or group with cash sharing. Free cash never goes above the initial level, because an operation always costs money. Has shape (target_shape[1],) . Gets updated right after order_func_nb .","title":"last_free_cash"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.last_lidx","text":"Index of the latest log record of each column. Similar to SimulationContext.last_oidx but for log records.","title":"last_lidx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.last_oidx","text":"Index of the latest order record of each column. Points to SimulationContext.order_records and has shape (target_shape[1],) . Example last_oidx of np.array([1, 100, -1]) means the latest filled order is order_records[1] for the first column, order_records[100] for the second column, and no orders have been filled yet for the third column.","title":"last_oidx"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.last_pos_record","text":"Latest position record of each column. It's a 1-dimensional array with records of type trade_dt . Has shape (target_shape[1],) . The array is initialized with empty records first (they contain random data) and the field id is set to -1. Once the first position is entered in a column, the id becomes 0 and the record materializes. Once the position is closed, the record fixes its identifier and other data until the next position is entered. The fields entry_price and exit_price are average entry and exit price respectively. The fields pnl and return contain statistics as if the position has been closed and are re-calculated using SimulationContext.last_val_price after pre_segment_func_nb (in case SimulationContext.last_val_price has been overridden) and before post_segment_func_nb . Note In an open position record, the field exit_price doesn't reflect the latest valuation price, but keeps the average price at which the position has been reduced. The position record is updated after successfully filling an order (after order_func_nb and before post_order_func_nb ). Example Consider a simulation that orders order_size for order_price and $1 fixed fees. Here's order info from order_func_nb and the updated position info from post_order_func_nb : order_size order_price id col size entry_idx entry_price \\ 0 NaN 1 -1 0 1.0 13 14.000000 1 0.5 2 0 0 0.5 1 2.000000 2 1.0 3 0 0 1.5 1 2.666667 3 NaN 4 0 0 1.5 1 2.666667 4 -1.0 5 0 0 1.5 1 2.666667 5 -0.5 6 0 0 1.5 1 2.666667 6 NaN 7 0 0 1.5 1 2.666667 7 -0.5 8 1 0 0.5 7 8.000000 8 -1.0 9 1 0 1.5 7 8.666667 9 1.0 10 1 0 1.5 7 8.666667 10 0.5 11 1 0 1.5 7 8.666667 11 1.0 12 2 0 1.0 11 12.000000 12 -2.0 13 3 0 1.0 12 13.000000 13 2.0 14 4 0 1.0 13 14.000000 entry_fees exit_idx exit_price exit_fees pnl return direction status 0 0.5 -1 NaN 0.0 -0.50 -0.035714 0 0 1 1.0 -1 NaN 0.0 -1.00 -1.000000 0 0 2 2.0 -1 NaN 0.0 -1.50 -0.375000 0 0 3 2.0 -1 NaN 0.0 -0.75 -0.187500 0 0 4 2.0 -1 5.000000 1.0 0.50 0.125000 0 0 5 2.0 5 5.333333 2.0 0.00 0.000000 0 1 6 2.0 5 5.333333 2.0 0.00 0.000000 0 1 7 1.0 -1 NaN 0.0 -1.00 -0.250000 1 0 8 2.0 -1 NaN 0.0 -2.50 -0.192308 1 0 9 2.0 -1 10.000000 1.0 -5.00 -0.384615 1 0 10 2.0 10 10.333333 2.0 -6.50 -0.500000 1 1 11 1.0 -1 NaN 0.0 -1.00 -0.083333 0 0 12 0.5 -1 NaN 0.0 -0.50 -0.038462 1 0 13 0.5 -1 NaN 0.0 -0.50 -0.035714 0 0","title":"last_pos_record"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.last_position","text":"Latest position per column. Has shape (target_shape[1],) . Gets updated right after order_func_nb .","title":"last_position"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.last_return","text":"Latest return per column or group with cash sharing. Has the same shape as SimulationContext.last_value . Calculated by comparing SimulationContext.last_value to SimulationContext.second_last_value . Gets updated each time SimulationContext.last_value is updated.","title":"last_return"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.last_val_price","text":"Latest valuation price per column. Has shape (target_shape[1],) . Enables SizeType.Value , SizeType.TargetValue , and SizeType.TargetPercent . Gets multiplied by the current position to get the value of the column (see SimulationContext.last_value ). Defaults to the SimulationContext.close before post_segment_func_nb . If SimulationContext.ffill_val_price , gets updated only if SimulationContext.close is not NaN. For example, close of [1, 2, np.nan, np.nan, 5] yields valuation price of [1, 2, 2, 2, 5] . Also gets updated right after pre_segment_func_nb - you can use pre_segment_func_nb to override last_val_price in-place, such that order_func_nb can use the new group value. You are not allowed to use -np.inf or np.inf - only finite values. If SimulationContext.update_value , gets also updated right after order_func_nb using filled order price as the latest known price. Note Since the previous SimulationContext.close is NaN in the first row, the first last_val_price is also NaN. Overriding last_val_price with NaN won't apply SimulationContext.ffill_val_price , so your entire group will become NaN. Example Consider 10 units in column 1 and 20 units in column 2. The previous close of them is $40 and $50 respectively, which is also the default valuation price in the current row, available as last_val_price in pre_segment_func_nb . If both columns are in the same group with cash sharing, the group is valued at $1400 before any order_func_nb is called, and can be later accessed via OrderContext.value_now .","title":"last_val_price"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.last_value","text":"Latest value per column or group with cash sharing. Has the same shape as SimulationContext.init_cash . Calculated by multiplying valuation price by the current position. The value of each column in a group with cash sharing is summed to get the value of the entire group. Gets updated using SimulationContext.last_val_price after pre_segment_func_nb and before post_segment_func_nb . If SimulationContext.update_value , gets also updated right after order_func_nb using filled order price as the latest known price (the difference will be minimal, only affected by costs).","title":"last_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.log_records","text":"Log records. Similar to SimulationContext.order_records but of type log_dt and index SimulationContext.last_lidx .","title":"log_records"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.order_records","text":"Order records. It's a 1-dimensional array with records of type order_dt . The array is initialized with empty records first (they contain random data), and then gradually filled with order data. The number of initialized records depends upon max_orders , but usually it's target_shape[0] * target_shape[1] , meaning there is maximal one order record per element. max_orders can be chosen lower if not every order_func_nb leads to a filled order, to save memory. You can use SimulationContext.last_oidx to get the index of the latest filled order of each column. Example Before filling, each order record looks like this: np . array ([( - 8070450532247928832 , - 8070450532247928832 , 4 , 0. , 0. , 0. , 5764616306889786413 )] After filling, it becomes like this: np . array ([( 0 , 0 , 1 , 50. , 1. , 0. , 1 )]","title":"order_records"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.second_last_value","text":"Second-latest value per column or group with cash sharing. Has the same shape as SimulationContext.last_value . Contains the latest known value two rows before ( i - 2 ) to be compared either with the latest known value one row before ( i - 1 ) or now ( i ). Gets updated at the end of each segment/row.","title":"second_last_value"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.segment_mask","text":"Mask of whether a particular segment should be executed. A segment is simply a sequence of order_func_nb calls under the same group and row. If a segment is inactive, any callback function inside of it will not be executed. You can still execute the segment's pre- and postprocessing function by enabling SimulationContext.call_pre_segment and SimulationContext.call_post_segment respectively. Utilizes flexible indexing using flex_select_auto_nb() and flex_2d , so it can be passed as 2-dim array, 1-dim array per column (requires flex_2d=True ), 1-dim array per row (requires flex_2d=False ), and a scalar. Broadcasts to the shape (target_shape[0], group_lens.shape[0]) . Note To modify the array in place, make sure to build an array of the full shape. Example Consider two groups with two columns each and the following activity mask: np . array ([[ True , False ], [ False , True ]]) The first group is only executed in the first row and the second group is only executed in the second row.","title":"segment_mask"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.target_shape","text":"Target shape of the simulation. A tuple with exactly two elements: the number of rows and columns. Example One day of minute data for three assets would yield a target_shape of (1440, 3) , where the first axis are rows (minutes) and the second axis are columns (assets).","title":"target_shape"},{"location":"api/portfolio/enums/#vectorbt.portfolio.enums.SimulationContext.update_value","text":"Whether to update group value after each filled order. Otherwise, stays the same for all columns in the group (the value is calculated only once, before executing any order). The change is marginal and mostly driven by transaction costs and slippage.","title":"update_value"},{"location":"api/portfolio/logs/","text":"logs module \u00b6 Base class for working with log records. Order records capture information on simulation logs. Logs are populated when simulating a portfolio and can be accessed as Portfolio.logs . >>> import pandas as pd >>> import numpy as np >>> from datetime import datetime , timedelta >>> import vectorbt as vbt >>> np . random . seed ( 42 ) >>> price = pd . DataFrame ({ ... 'a' : np . random . uniform ( 1 , 2 , size = 100 ), ... 'b' : np . random . uniform ( 1 , 2 , size = 100 ) ... }, index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 100 )]) >>> size = pd . DataFrame ({ ... 'a' : np . random . uniform ( - 100 , 100 , size = 100 ), ... 'b' : np . random . uniform ( - 100 , 100 , size = 100 ), ... }, index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 100 )]) >>> pf = vbt . Portfolio . from_orders ( price , size , fees = 0.01 , freq = 'd' , log = True ) >>> logs = pf . logs >>> logs . filled . count () a 88 b 99 Name: count, dtype: int64 >>> logs . ignored . count () a 0 b 0 Name: count, dtype: int64 >>> logs . rejected . count () a 12 b 1 Name: count, dtype: int64 Stats \u00b6 Hint See StatsBuilderMixin.stats() and Logs.metrics . >>> logs [ 'a' ] . stats () Start 2020-01-01 00:00:00 End 2020-04-09 00:00:00 Period 100 days 00:00:00 Total Records 100 Status Counts: None 0 Status Counts: Filled 88 Status Counts: Ignored 0 Status Counts: Rejected 12 Status Info Counts: None 88 Status Info Counts: NoCashLong 12 Name: a, dtype: object StatsBuilderMixin.stats() also supports (re-)grouping: >>> logs . stats ( group_by = True ) Start 2020-01-01 00:00:00 End 2020-04-09 00:00:00 Period 100 days 00:00:00 Total Records 200 Status Counts: None 0 Status Counts: Filled 187 Status Counts: Ignored 0 Status Counts: Rejected 13 Status Info Counts: None 187 Status Info Counts: NoCashLong 13 Name: group, dtype: object Plots \u00b6 Hint See PlotsBuilderMixin.plots() and Logs.subplots . This class does not have any subplots. logs_attach_field_config Config \u00b6 Config of fields to be attached to Logs . Co nf ig( { \"res_side\" : { \"attach_filters\" : true }, \"res_status\" : { \"attach_filters\" : true }, \"res_status_info\" : { \"attach_filters\" : true } } ) logs_field_config Config \u00b6 Field config for Logs . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"group\" : \"int64\" , \"col\" : \"int64\" , \"idx\" : \"int64\" , \"cash\" : \"float64\" , \"position\" : \"float64\" , \"debt\" : \"float64\" , \"free_cash\" : \"float64\" , \"val_price\" : \"float64\" , \"value\" : \"float64\" , \"req_size\" : \"float64\" , \"req_price\" : \"float64\" , \"req_size_type\" : \"int64\" , \"req_direction\" : \"int64\" , \"req_fees\" : \"float64\" , \"req_fixed_fees\" : \"float64\" , \"req_slippage\" : \"float64\" , \"req_min_size\" : \"float64\" , \"req_max_size\" : \"float64\" , \"req_size_granularity\" : \"float64\" , \"req_reject_prob\" : \"float64\" , \"req_lock_cash\" : \"bool\" , \"req_allow_partial\" : \"bool\" , \"req_raise_reject\" : \"bool\" , \"req_log\" : \"bool\" , \"new_cash\" : \"float64\" , \"new_position\" : \"float64\" , \"new_debt\" : \"float64\" , \"new_free_cash\" : \"float64\" , \"new_val_price\" : \"float64\" , \"new_value\" : \"float64\" , \"res_size\" : \"float64\" , \"res_price\" : \"float64\" , \"res_fees\" : \"float64\" , \"res_side\" : \"int64\" , \"res_status\" : \"int64\" , \"res_status_info\" : \"int64\" , \"order_id\" : \"int64\" }, \"settings\" : { \"id\" : { \"title\" : \"Log Id\" }, \"group\" : { \"title\" : \"Group\" }, \"cash\" : { \"title\" : \"Cash\" }, \"position\" : { \"title\" : \"Position\" }, \"debt\" : { \"title\" : \"Debt\" }, \"free_cash\" : { \"title\" : \"Free Cash\" }, \"val_price\" : { \"title\" : \"Val Price\" }, \"value\" : { \"title\" : \"Value\" }, \"req_size\" : { \"title\" : \"Request Size\" }, \"req_price\" : { \"title\" : \"Request Price\" }, \"req_size_type\" : { \"title\" : \"Request Size Type\" , \"mapping\" : { \"Amount\" : 0 , \"Value\" : 1 , \"Percent\" : 2 , \"TargetAmount\" : 3 , \"TargetValue\" : 4 , \"TargetPercent\" : 5 } }, \"req_direction\" : { \"title\" : \"Request Direction\" , \"mapping\" : { \"LongOnly\" : 0 , \"ShortOnly\" : 1 , \"Both\" : 2 } }, \"req_fees\" : { \"title\" : \"Request Fees\" }, \"req_fixed_fees\" : { \"title\" : \"Request Fixed Fees\" }, \"req_slippage\" : { \"title\" : \"Request Slippage\" }, \"req_min_size\" : { \"title\" : \"Request Min Size\" }, \"req_max_size\" : { \"title\" : \"Request Max Size\" }, \"req_size_granularity\" : { \"title\" : \"Request Size Granularity\" }, \"req_reject_prob\" : { \"title\" : \"Request Rejection Prob\" }, \"req_lock_cash\" : { \"title\" : \"Request Lock Cash\" }, \"req_allow_partial\" : { \"title\" : \"Request Allow Partial\" }, \"req_raise_reject\" : { \"title\" : \"Request Raise Rejection\" }, \"req_log\" : { \"title\" : \"Request Log\" }, \"new_cash\" : { \"title\" : \"New Cash\" }, \"new_position\" : { \"title\" : \"New Position\" }, \"new_debt\" : { \"title\" : \"New Debt\" }, \"new_free_cash\" : { \"title\" : \"New Free Cash\" }, \"new_val_price\" : { \"title\" : \"New Val Price\" }, \"new_value\" : { \"title\" : \"New Value\" }, \"res_size\" : { \"title\" : \"Result Size\" }, \"res_price\" : { \"title\" : \"Result Price\" }, \"res_fees\" : { \"title\" : \"Result Fees\" }, \"res_side\" : { \"title\" : \"Result Side\" , \"mapping\" : { \"Buy\" : 0 , \"Sell\" : 1 } }, \"res_status\" : { \"title\" : \"Result Status\" , \"mapping\" : { \"Filled\" : 0 , \"Ignored\" : 1 , \"Rejected\" : 2 } }, \"res_status_info\" : { \"title\" : \"Result Status Info\" , \"mapping\" : { \"SizeNaN\" : 0 , \"PriceNaN\" : 1 , \"ValPriceNaN\" : 2 , \"ValueNaN\" : 3 , \"ValueZeroNeg\" : 4 , \"SizeZero\" : 5 , \"NoCashShort\" : 6 , \"NoCashLong\" : 7 , \"NoOpenPosition\" : 8 , \"MaxSizeExceeded\" : 9 , \"RandomEvent\" : 10 , \"CantCoverFees\" : 11 , \"MinSizeNotReached\" : 12 , \"PartialFill\" : 13 } }, \"order_id\" : { \"title\" : \"Order Id\" } } } ) Logs class \u00b6 Extends Records for working with log records. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin Records RecordsWithFields StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() Records.apply() Records.apply_mask() Records.build_field_config_doc() Records.col_arr Records.col_mapper Records.config Records.count() Records.get_apply_mapping_arr() Records.get_by_col_idxs() Records.get_field_arr() Records.get_field_mapping() Records.get_field_name() Records.get_field_setting() Records.get_field_title() Records.get_map_field() Records.get_map_field_to_index() Records.id_arr Records.idx_arr Records.iloc Records.indexing_func() Records.indexing_func_meta() Records.indexing_kwargs Records.is_sorted() Records.loc Records.map() Records.map_array() Records.map_field() Records.override_field_config_doc() Records.records Records.records_arr Records.records_readable Records.replace() Records.self_aliases Records.sort() Records.values Records.wrapper Records.writeable_attrs StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() buy method \u00b6 Records filtered by res_side == 0 . cant_cover_fees method \u00b6 Records filtered by res_status_info == 11 . cash method \u00b6 Mapped array of the field cash . col method \u00b6 Mapped array of the field col . debt method \u00b6 Mapped array of the field debt . field_config class variable \u00b6 Field config of Logs . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"group\" : \"int64\" , \"col\" : \"int64\" , \"idx\" : \"int64\" , \"cash\" : \"float64\" , \"position\" : \"float64\" , \"debt\" : \"float64\" , \"free_cash\" : \"float64\" , \"val_price\" : \"float64\" , \"value\" : \"float64\" , \"req_size\" : \"float64\" , \"req_price\" : \"float64\" , \"req_size_type\" : \"int64\" , \"req_direction\" : \"int64\" , \"req_fees\" : \"float64\" , \"req_fixed_fees\" : \"float64\" , \"req_slippage\" : \"float64\" , \"req_min_size\" : \"float64\" , \"req_max_size\" : \"float64\" , \"req_size_granularity\" : \"float64\" , \"req_reject_prob\" : \"float64\" , \"req_lock_cash\" : \"bool\" , \"req_allow_partial\" : \"bool\" , \"req_raise_reject\" : \"bool\" , \"req_log\" : \"bool\" , \"new_cash\" : \"float64\" , \"new_position\" : \"float64\" , \"new_debt\" : \"float64\" , \"new_free_cash\" : \"float64\" , \"new_val_price\" : \"float64\" , \"new_value\" : \"float64\" , \"res_size\" : \"float64\" , \"res_price\" : \"float64\" , \"res_fees\" : \"float64\" , \"res_side\" : \"int64\" , \"res_status\" : \"int64\" , \"res_status_info\" : \"int64\" , \"order_id\" : \"int64\" }, \"settings\" : { \"id\" : { \"name\" : \"id\" , \"title\" : \"Log Id\" }, \"col\" : { \"name\" : \"col\" , \"title\" : \"Column\" , \"mapping\" : \"columns\" }, \"idx\" : { \"name\" : \"idx\" , \"title\" : \"Timestamp\" , \"mapping\" : \"index\" }, \"group\" : { \"title\" : \"Group\" }, \"cash\" : { \"title\" : \"Cash\" }, \"position\" : { \"title\" : \"Position\" }, \"debt\" : { \"title\" : \"Debt\" }, \"free_cash\" : { \"title\" : \"Free Cash\" }, \"val_price\" : { \"title\" : \"Val Price\" }, \"value\" : { \"title\" : \"Value\" }, \"req_size\" : { \"title\" : \"Request Size\" }, \"req_price\" : { \"title\" : \"Request Price\" }, \"req_size_type\" : { \"title\" : \"Request Size Type\" , \"mapping\" : { \"Amount\" : 0 , \"Value\" : 1 , \"Percent\" : 2 , \"TargetAmount\" : 3 , \"TargetValue\" : 4 , \"TargetPercent\" : 5 } }, \"req_direction\" : { \"title\" : \"Request Direction\" , \"mapping\" : { \"LongOnly\" : 0 , \"ShortOnly\" : 1 , \"Both\" : 2 } }, \"req_fees\" : { \"title\" : \"Request Fees\" }, \"req_fixed_fees\" : { \"title\" : \"Request Fixed Fees\" }, \"req_slippage\" : { \"title\" : \"Request Slippage\" }, \"req_min_size\" : { \"title\" : \"Request Min Size\" }, \"req_max_size\" : { \"title\" : \"Request Max Size\" }, \"req_size_granularity\" : { \"title\" : \"Request Size Granularity\" }, \"req_reject_prob\" : { \"title\" : \"Request Rejection Prob\" }, \"req_lock_cash\" : { \"title\" : \"Request Lock Cash\" }, \"req_allow_partial\" : { \"title\" : \"Request Allow Partial\" }, \"req_raise_reject\" : { \"title\" : \"Request Raise Rejection\" }, \"req_log\" : { \"title\" : \"Request Log\" }, \"new_cash\" : { \"title\" : \"New Cash\" }, \"new_position\" : { \"title\" : \"New Position\" }, \"new_debt\" : { \"title\" : \"New Debt\" }, \"new_free_cash\" : { \"title\" : \"New Free Cash\" }, \"new_val_price\" : { \"title\" : \"New Val Price\" }, \"new_value\" : { \"title\" : \"New Value\" }, \"res_size\" : { \"title\" : \"Result Size\" }, \"res_price\" : { \"title\" : \"Result Price\" }, \"res_fees\" : { \"title\" : \"Result Fees\" }, \"res_side\" : { \"title\" : \"Result Side\" , \"mapping\" : { \"Buy\" : 0 , \"Sell\" : 1 } }, \"res_status\" : { \"title\" : \"Result Status\" , \"mapping\" : { \"Filled\" : 0 , \"Ignored\" : 1 , \"Rejected\" : 2 } }, \"res_status_info\" : { \"title\" : \"Result Status Info\" , \"mapping\" : { \"SizeNaN\" : 0 , \"PriceNaN\" : 1 , \"ValPriceNaN\" : 2 , \"ValueNaN\" : 3 , \"ValueZeroNeg\" : 4 , \"SizeZero\" : 5 , \"NoCashShort\" : 6 , \"NoCashLong\" : 7 , \"NoOpenPosition\" : 8 , \"MaxSizeExceeded\" : 9 , \"RandomEvent\" : 10 , \"CantCoverFees\" : 11 , \"MinSizeNotReached\" : 12 , \"PartialFill\" : 13 } }, \"order_id\" : { \"title\" : \"Order Id\" } } } ) filled method \u00b6 Records filtered by res_status == 0 . free_cash method \u00b6 Mapped array of the field free_cash . group method \u00b6 Mapped array of the field group . id method \u00b6 Mapped array of the field id . idx method \u00b6 Mapped array of the field idx . ignored method \u00b6 Records filtered by res_status == 1 . max_size_exceeded method \u00b6 Records filtered by res_status_info == 9 . metrics class variable \u00b6 Metrics supported by Logs . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function Logs.<lambda> at 0x7fac99030510>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function Logs.<lambda> at 0x7fac99030598>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function Logs.<lambda> at 0x7fac99030620>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"total_records\" : { \"title\" : \"Total Records\" , \"calc_func\" : \"count\" , \"tags\" : \"records\" }, \"res_status_counts\" : { \"title\" : \"Status Counts\" , \"calc_func\" : \"res_status.value_counts\" , \"incl_all_keys\" : true , \"post_calc_func\" : \"<function Logs.<lambda> at 0x7fac990306a8>\" , \"tags\" : [ \"logs\" , \"res_status\" , \"value_counts\" ] }, \"res_status_info_counts\" : { \"title\" : \"Status Info Counts\" , \"calc_func\" : \"res_status_info.value_counts\" , \"post_calc_func\" : \"<function Logs.<lambda> at 0x7fac99030730>\" , \"tags\" : [ \"logs\" , \"res_status_info\" , \"value_counts\" ] } } ) Returns Logs._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable Logs._metrics . min_size_not_reached method \u00b6 Records filtered by res_status_info == 12 . new_cash method \u00b6 Mapped array of the field new_cash . new_debt method \u00b6 Mapped array of the field new_debt . new_free_cash method \u00b6 Mapped array of the field new_free_cash . new_position method \u00b6 Mapped array of the field new_position . new_val_price method \u00b6 Mapped array of the field new_val_price . new_value method \u00b6 Mapped array of the field new_value . no_cash_long method \u00b6 Records filtered by res_status_info == 7 . no_cash_short method \u00b6 Records filtered by res_status_info == 6 . no_open_position method \u00b6 Records filtered by res_status_info == 8 . order_id method \u00b6 Mapped array of the field order_id . partial_fill method \u00b6 Records filtered by res_status_info == 13 . plots_defaults property \u00b6 Defaults for PlotsBuilderMixin.plots() . Merges Records.plots_defaults and logs.plots from settings . position method \u00b6 Mapped array of the field position . price_nan method \u00b6 Records filtered by res_status_info == 1 . random_event method \u00b6 Records filtered by res_status_info == 10 . rejected method \u00b6 Records filtered by res_status == 2 . req_allow_partial method \u00b6 Mapped array of the field req_allow_partial . req_direction method \u00b6 Mapped array of the field req_direction . req_fees method \u00b6 Mapped array of the field req_fees . req_fixed_fees method \u00b6 Mapped array of the field req_fixed_fees . req_lock_cash method \u00b6 Mapped array of the field req_lock_cash . req_log method \u00b6 Mapped array of the field req_log . req_max_size method \u00b6 Mapped array of the field req_max_size . req_min_size method \u00b6 Mapped array of the field req_min_size . req_price method \u00b6 Mapped array of the field req_price . req_raise_reject method \u00b6 Mapped array of the field req_raise_reject . req_reject_prob method \u00b6 Mapped array of the field req_reject_prob . req_size method \u00b6 Mapped array of the field req_size . req_size_granularity method \u00b6 Mapped array of the field req_size_granularity . req_size_type method \u00b6 Mapped array of the field req_size_type . req_slippage method \u00b6 Mapped array of the field req_slippage . res_fees method \u00b6 Mapped array of the field res_fees . res_price method \u00b6 Mapped array of the field res_price . res_side method \u00b6 Mapped array of the field res_side . res_size method \u00b6 Mapped array of the field res_size . res_status method \u00b6 Mapped array of the field res_status . res_status_info method \u00b6 Mapped array of the field res_status_info . sell method \u00b6 Records filtered by res_side == 1 . size_nan method \u00b6 Records filtered by res_status_info == 0 . size_zero method \u00b6 Records filtered by res_status_info == 5 . stats_defaults property \u00b6 Defaults for StatsBuilderMixin.stats() . Merges Records.stats_defaults and logs.stats from settings . subplots class variable \u00b6 Subplots supported by Logs . Co nf ig( {} ) Returns Logs._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable Logs._subplots . val_price method \u00b6 Mapped array of the field val_price . val_price_nan method \u00b6 Records filtered by res_status_info == 2 . value method \u00b6 Mapped array of the field value . value_nan method \u00b6 Records filtered by res_status_info == 3 . value_zero_neg method \u00b6 Records filtered by res_status_info == 4 .","title":"logs"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs","text":"Base class for working with log records. Order records capture information on simulation logs. Logs are populated when simulating a portfolio and can be accessed as Portfolio.logs . >>> import pandas as pd >>> import numpy as np >>> from datetime import datetime , timedelta >>> import vectorbt as vbt >>> np . random . seed ( 42 ) >>> price = pd . DataFrame ({ ... 'a' : np . random . uniform ( 1 , 2 , size = 100 ), ... 'b' : np . random . uniform ( 1 , 2 , size = 100 ) ... }, index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 100 )]) >>> size = pd . DataFrame ({ ... 'a' : np . random . uniform ( - 100 , 100 , size = 100 ), ... 'b' : np . random . uniform ( - 100 , 100 , size = 100 ), ... }, index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 100 )]) >>> pf = vbt . Portfolio . from_orders ( price , size , fees = 0.01 , freq = 'd' , log = True ) >>> logs = pf . logs >>> logs . filled . count () a 88 b 99 Name: count, dtype: int64 >>> logs . ignored . count () a 0 b 0 Name: count, dtype: int64 >>> logs . rejected . count () a 12 b 1 Name: count, dtype: int64","title":"vectorbt.portfolio.logs"},{"location":"api/portfolio/logs/#stats","text":"Hint See StatsBuilderMixin.stats() and Logs.metrics . >>> logs [ 'a' ] . stats () Start 2020-01-01 00:00:00 End 2020-04-09 00:00:00 Period 100 days 00:00:00 Total Records 100 Status Counts: None 0 Status Counts: Filled 88 Status Counts: Ignored 0 Status Counts: Rejected 12 Status Info Counts: None 88 Status Info Counts: NoCashLong 12 Name: a, dtype: object StatsBuilderMixin.stats() also supports (re-)grouping: >>> logs . stats ( group_by = True ) Start 2020-01-01 00:00:00 End 2020-04-09 00:00:00 Period 100 days 00:00:00 Total Records 200 Status Counts: None 0 Status Counts: Filled 187 Status Counts: Ignored 0 Status Counts: Rejected 13 Status Info Counts: None 187 Status Info Counts: NoCashLong 13 Name: group, dtype: object","title":"Stats"},{"location":"api/portfolio/logs/#plots","text":"Hint See PlotsBuilderMixin.plots() and Logs.subplots . This class does not have any subplots.","title":"Plots"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.logs_attach_field_config","text":"Config of fields to be attached to Logs . Co nf ig( { \"res_side\" : { \"attach_filters\" : true }, \"res_status\" : { \"attach_filters\" : true }, \"res_status_info\" : { \"attach_filters\" : true } } )","title":"logs_attach_field_config"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.logs_field_config","text":"Field config for Logs . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"group\" : \"int64\" , \"col\" : \"int64\" , \"idx\" : \"int64\" , \"cash\" : \"float64\" , \"position\" : \"float64\" , \"debt\" : \"float64\" , \"free_cash\" : \"float64\" , \"val_price\" : \"float64\" , \"value\" : \"float64\" , \"req_size\" : \"float64\" , \"req_price\" : \"float64\" , \"req_size_type\" : \"int64\" , \"req_direction\" : \"int64\" , \"req_fees\" : \"float64\" , \"req_fixed_fees\" : \"float64\" , \"req_slippage\" : \"float64\" , \"req_min_size\" : \"float64\" , \"req_max_size\" : \"float64\" , \"req_size_granularity\" : \"float64\" , \"req_reject_prob\" : \"float64\" , \"req_lock_cash\" : \"bool\" , \"req_allow_partial\" : \"bool\" , \"req_raise_reject\" : \"bool\" , \"req_log\" : \"bool\" , \"new_cash\" : \"float64\" , \"new_position\" : \"float64\" , \"new_debt\" : \"float64\" , \"new_free_cash\" : \"float64\" , \"new_val_price\" : \"float64\" , \"new_value\" : \"float64\" , \"res_size\" : \"float64\" , \"res_price\" : \"float64\" , \"res_fees\" : \"float64\" , \"res_side\" : \"int64\" , \"res_status\" : \"int64\" , \"res_status_info\" : \"int64\" , \"order_id\" : \"int64\" }, \"settings\" : { \"id\" : { \"title\" : \"Log Id\" }, \"group\" : { \"title\" : \"Group\" }, \"cash\" : { \"title\" : \"Cash\" }, \"position\" : { \"title\" : \"Position\" }, \"debt\" : { \"title\" : \"Debt\" }, \"free_cash\" : { \"title\" : \"Free Cash\" }, \"val_price\" : { \"title\" : \"Val Price\" }, \"value\" : { \"title\" : \"Value\" }, \"req_size\" : { \"title\" : \"Request Size\" }, \"req_price\" : { \"title\" : \"Request Price\" }, \"req_size_type\" : { \"title\" : \"Request Size Type\" , \"mapping\" : { \"Amount\" : 0 , \"Value\" : 1 , \"Percent\" : 2 , \"TargetAmount\" : 3 , \"TargetValue\" : 4 , \"TargetPercent\" : 5 } }, \"req_direction\" : { \"title\" : \"Request Direction\" , \"mapping\" : { \"LongOnly\" : 0 , \"ShortOnly\" : 1 , \"Both\" : 2 } }, \"req_fees\" : { \"title\" : \"Request Fees\" }, \"req_fixed_fees\" : { \"title\" : \"Request Fixed Fees\" }, \"req_slippage\" : { \"title\" : \"Request Slippage\" }, \"req_min_size\" : { \"title\" : \"Request Min Size\" }, \"req_max_size\" : { \"title\" : \"Request Max Size\" }, \"req_size_granularity\" : { \"title\" : \"Request Size Granularity\" }, \"req_reject_prob\" : { \"title\" : \"Request Rejection Prob\" }, \"req_lock_cash\" : { \"title\" : \"Request Lock Cash\" }, \"req_allow_partial\" : { \"title\" : \"Request Allow Partial\" }, \"req_raise_reject\" : { \"title\" : \"Request Raise Rejection\" }, \"req_log\" : { \"title\" : \"Request Log\" }, \"new_cash\" : { \"title\" : \"New Cash\" }, \"new_position\" : { \"title\" : \"New Position\" }, \"new_debt\" : { \"title\" : \"New Debt\" }, \"new_free_cash\" : { \"title\" : \"New Free Cash\" }, \"new_val_price\" : { \"title\" : \"New Val Price\" }, \"new_value\" : { \"title\" : \"New Value\" }, \"res_size\" : { \"title\" : \"Result Size\" }, \"res_price\" : { \"title\" : \"Result Price\" }, \"res_fees\" : { \"title\" : \"Result Fees\" }, \"res_side\" : { \"title\" : \"Result Side\" , \"mapping\" : { \"Buy\" : 0 , \"Sell\" : 1 } }, \"res_status\" : { \"title\" : \"Result Status\" , \"mapping\" : { \"Filled\" : 0 , \"Ignored\" : 1 , \"Rejected\" : 2 } }, \"res_status_info\" : { \"title\" : \"Result Status Info\" , \"mapping\" : { \"SizeNaN\" : 0 , \"PriceNaN\" : 1 , \"ValPriceNaN\" : 2 , \"ValueNaN\" : 3 , \"ValueZeroNeg\" : 4 , \"SizeZero\" : 5 , \"NoCashShort\" : 6 , \"NoCashLong\" : 7 , \"NoOpenPosition\" : 8 , \"MaxSizeExceeded\" : 9 , \"RandomEvent\" : 10 , \"CantCoverFees\" : 11 , \"MinSizeNotReached\" : 12 , \"PartialFill\" : 13 } }, \"order_id\" : { \"title\" : \"Order Id\" } } } )","title":"logs_field_config"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs","text":"Extends Records for working with log records. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin Records RecordsWithFields StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() Records.apply() Records.apply_mask() Records.build_field_config_doc() Records.col_arr Records.col_mapper Records.config Records.count() Records.get_apply_mapping_arr() Records.get_by_col_idxs() Records.get_field_arr() Records.get_field_mapping() Records.get_field_name() Records.get_field_setting() Records.get_field_title() Records.get_map_field() Records.get_map_field_to_index() Records.id_arr Records.idx_arr Records.iloc Records.indexing_func() Records.indexing_func_meta() Records.indexing_kwargs Records.is_sorted() Records.loc Records.map() Records.map_array() Records.map_field() Records.override_field_config_doc() Records.records Records.records_arr Records.records_readable Records.replace() Records.self_aliases Records.sort() Records.values Records.wrapper Records.writeable_attrs StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"Logs"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.buy","text":"Records filtered by res_side == 0 .","title":"buy"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.cant_cover_fees","text":"Records filtered by res_status_info == 11 .","title":"cant_cover_fees"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.cash","text":"Mapped array of the field cash .","title":"cash"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.col","text":"Mapped array of the field col .","title":"col"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.debt","text":"Mapped array of the field debt .","title":"debt"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.field_config","text":"Field config of Logs . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"group\" : \"int64\" , \"col\" : \"int64\" , \"idx\" : \"int64\" , \"cash\" : \"float64\" , \"position\" : \"float64\" , \"debt\" : \"float64\" , \"free_cash\" : \"float64\" , \"val_price\" : \"float64\" , \"value\" : \"float64\" , \"req_size\" : \"float64\" , \"req_price\" : \"float64\" , \"req_size_type\" : \"int64\" , \"req_direction\" : \"int64\" , \"req_fees\" : \"float64\" , \"req_fixed_fees\" : \"float64\" , \"req_slippage\" : \"float64\" , \"req_min_size\" : \"float64\" , \"req_max_size\" : \"float64\" , \"req_size_granularity\" : \"float64\" , \"req_reject_prob\" : \"float64\" , \"req_lock_cash\" : \"bool\" , \"req_allow_partial\" : \"bool\" , \"req_raise_reject\" : \"bool\" , \"req_log\" : \"bool\" , \"new_cash\" : \"float64\" , \"new_position\" : \"float64\" , \"new_debt\" : \"float64\" , \"new_free_cash\" : \"float64\" , \"new_val_price\" : \"float64\" , \"new_value\" : \"float64\" , \"res_size\" : \"float64\" , \"res_price\" : \"float64\" , \"res_fees\" : \"float64\" , \"res_side\" : \"int64\" , \"res_status\" : \"int64\" , \"res_status_info\" : \"int64\" , \"order_id\" : \"int64\" }, \"settings\" : { \"id\" : { \"name\" : \"id\" , \"title\" : \"Log Id\" }, \"col\" : { \"name\" : \"col\" , \"title\" : \"Column\" , \"mapping\" : \"columns\" }, \"idx\" : { \"name\" : \"idx\" , \"title\" : \"Timestamp\" , \"mapping\" : \"index\" }, \"group\" : { \"title\" : \"Group\" }, \"cash\" : { \"title\" : \"Cash\" }, \"position\" : { \"title\" : \"Position\" }, \"debt\" : { \"title\" : \"Debt\" }, \"free_cash\" : { \"title\" : \"Free Cash\" }, \"val_price\" : { \"title\" : \"Val Price\" }, \"value\" : { \"title\" : \"Value\" }, \"req_size\" : { \"title\" : \"Request Size\" }, \"req_price\" : { \"title\" : \"Request Price\" }, \"req_size_type\" : { \"title\" : \"Request Size Type\" , \"mapping\" : { \"Amount\" : 0 , \"Value\" : 1 , \"Percent\" : 2 , \"TargetAmount\" : 3 , \"TargetValue\" : 4 , \"TargetPercent\" : 5 } }, \"req_direction\" : { \"title\" : \"Request Direction\" , \"mapping\" : { \"LongOnly\" : 0 , \"ShortOnly\" : 1 , \"Both\" : 2 } }, \"req_fees\" : { \"title\" : \"Request Fees\" }, \"req_fixed_fees\" : { \"title\" : \"Request Fixed Fees\" }, \"req_slippage\" : { \"title\" : \"Request Slippage\" }, \"req_min_size\" : { \"title\" : \"Request Min Size\" }, \"req_max_size\" : { \"title\" : \"Request Max Size\" }, \"req_size_granularity\" : { \"title\" : \"Request Size Granularity\" }, \"req_reject_prob\" : { \"title\" : \"Request Rejection Prob\" }, \"req_lock_cash\" : { \"title\" : \"Request Lock Cash\" }, \"req_allow_partial\" : { \"title\" : \"Request Allow Partial\" }, \"req_raise_reject\" : { \"title\" : \"Request Raise Rejection\" }, \"req_log\" : { \"title\" : \"Request Log\" }, \"new_cash\" : { \"title\" : \"New Cash\" }, \"new_position\" : { \"title\" : \"New Position\" }, \"new_debt\" : { \"title\" : \"New Debt\" }, \"new_free_cash\" : { \"title\" : \"New Free Cash\" }, \"new_val_price\" : { \"title\" : \"New Val Price\" }, \"new_value\" : { \"title\" : \"New Value\" }, \"res_size\" : { \"title\" : \"Result Size\" }, \"res_price\" : { \"title\" : \"Result Price\" }, \"res_fees\" : { \"title\" : \"Result Fees\" }, \"res_side\" : { \"title\" : \"Result Side\" , \"mapping\" : { \"Buy\" : 0 , \"Sell\" : 1 } }, \"res_status\" : { \"title\" : \"Result Status\" , \"mapping\" : { \"Filled\" : 0 , \"Ignored\" : 1 , \"Rejected\" : 2 } }, \"res_status_info\" : { \"title\" : \"Result Status Info\" , \"mapping\" : { \"SizeNaN\" : 0 , \"PriceNaN\" : 1 , \"ValPriceNaN\" : 2 , \"ValueNaN\" : 3 , \"ValueZeroNeg\" : 4 , \"SizeZero\" : 5 , \"NoCashShort\" : 6 , \"NoCashLong\" : 7 , \"NoOpenPosition\" : 8 , \"MaxSizeExceeded\" : 9 , \"RandomEvent\" : 10 , \"CantCoverFees\" : 11 , \"MinSizeNotReached\" : 12 , \"PartialFill\" : 13 } }, \"order_id\" : { \"title\" : \"Order Id\" } } } )","title":"field_config"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.filled","text":"Records filtered by res_status == 0 .","title":"filled"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.free_cash","text":"Mapped array of the field free_cash .","title":"free_cash"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.group","text":"Mapped array of the field group .","title":"group"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.id","text":"Mapped array of the field id .","title":"id"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.idx","text":"Mapped array of the field idx .","title":"idx"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.ignored","text":"Records filtered by res_status == 1 .","title":"ignored"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.max_size_exceeded","text":"Records filtered by res_status_info == 9 .","title":"max_size_exceeded"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.metrics","text":"Metrics supported by Logs . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function Logs.<lambda> at 0x7fac99030510>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function Logs.<lambda> at 0x7fac99030598>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function Logs.<lambda> at 0x7fac99030620>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"total_records\" : { \"title\" : \"Total Records\" , \"calc_func\" : \"count\" , \"tags\" : \"records\" }, \"res_status_counts\" : { \"title\" : \"Status Counts\" , \"calc_func\" : \"res_status.value_counts\" , \"incl_all_keys\" : true , \"post_calc_func\" : \"<function Logs.<lambda> at 0x7fac990306a8>\" , \"tags\" : [ \"logs\" , \"res_status\" , \"value_counts\" ] }, \"res_status_info_counts\" : { \"title\" : \"Status Info Counts\" , \"calc_func\" : \"res_status_info.value_counts\" , \"post_calc_func\" : \"<function Logs.<lambda> at 0x7fac99030730>\" , \"tags\" : [ \"logs\" , \"res_status_info\" , \"value_counts\" ] } } ) Returns Logs._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable Logs._metrics .","title":"metrics"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.min_size_not_reached","text":"Records filtered by res_status_info == 12 .","title":"min_size_not_reached"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.new_cash","text":"Mapped array of the field new_cash .","title":"new_cash"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.new_debt","text":"Mapped array of the field new_debt .","title":"new_debt"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.new_free_cash","text":"Mapped array of the field new_free_cash .","title":"new_free_cash"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.new_position","text":"Mapped array of the field new_position .","title":"new_position"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.new_val_price","text":"Mapped array of the field new_val_price .","title":"new_val_price"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.new_value","text":"Mapped array of the field new_value .","title":"new_value"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.no_cash_long","text":"Records filtered by res_status_info == 7 .","title":"no_cash_long"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.no_cash_short","text":"Records filtered by res_status_info == 6 .","title":"no_cash_short"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.no_open_position","text":"Records filtered by res_status_info == 8 .","title":"no_open_position"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.order_id","text":"Mapped array of the field order_id .","title":"order_id"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.partial_fill","text":"Records filtered by res_status_info == 13 .","title":"partial_fill"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.plots_defaults","text":"Defaults for PlotsBuilderMixin.plots() . Merges Records.plots_defaults and logs.plots from settings .","title":"plots_defaults"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.position","text":"Mapped array of the field position .","title":"position"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.price_nan","text":"Records filtered by res_status_info == 1 .","title":"price_nan"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.random_event","text":"Records filtered by res_status_info == 10 .","title":"random_event"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.rejected","text":"Records filtered by res_status == 2 .","title":"rejected"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.req_allow_partial","text":"Mapped array of the field req_allow_partial .","title":"req_allow_partial"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.req_direction","text":"Mapped array of the field req_direction .","title":"req_direction"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.req_fees","text":"Mapped array of the field req_fees .","title":"req_fees"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.req_fixed_fees","text":"Mapped array of the field req_fixed_fees .","title":"req_fixed_fees"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.req_lock_cash","text":"Mapped array of the field req_lock_cash .","title":"req_lock_cash"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.req_log","text":"Mapped array of the field req_log .","title":"req_log"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.req_max_size","text":"Mapped array of the field req_max_size .","title":"req_max_size"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.req_min_size","text":"Mapped array of the field req_min_size .","title":"req_min_size"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.req_price","text":"Mapped array of the field req_price .","title":"req_price"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.req_raise_reject","text":"Mapped array of the field req_raise_reject .","title":"req_raise_reject"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.req_reject_prob","text":"Mapped array of the field req_reject_prob .","title":"req_reject_prob"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.req_size","text":"Mapped array of the field req_size .","title":"req_size"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.req_size_granularity","text":"Mapped array of the field req_size_granularity .","title":"req_size_granularity"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.req_size_type","text":"Mapped array of the field req_size_type .","title":"req_size_type"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.req_slippage","text":"Mapped array of the field req_slippage .","title":"req_slippage"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.res_fees","text":"Mapped array of the field res_fees .","title":"res_fees"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.res_price","text":"Mapped array of the field res_price .","title":"res_price"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.res_side","text":"Mapped array of the field res_side .","title":"res_side"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.res_size","text":"Mapped array of the field res_size .","title":"res_size"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.res_status","text":"Mapped array of the field res_status .","title":"res_status"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.res_status_info","text":"Mapped array of the field res_status_info .","title":"res_status_info"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.sell","text":"Records filtered by res_side == 1 .","title":"sell"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.size_nan","text":"Records filtered by res_status_info == 0 .","title":"size_nan"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.size_zero","text":"Records filtered by res_status_info == 5 .","title":"size_zero"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.stats_defaults","text":"Defaults for StatsBuilderMixin.stats() . Merges Records.stats_defaults and logs.stats from settings .","title":"stats_defaults"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.subplots","text":"Subplots supported by Logs . Co nf ig( {} ) Returns Logs._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable Logs._subplots .","title":"subplots"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.val_price","text":"Mapped array of the field val_price .","title":"val_price"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.val_price_nan","text":"Records filtered by res_status_info == 2 .","title":"val_price_nan"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.value","text":"Mapped array of the field value .","title":"value"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.value_nan","text":"Records filtered by res_status_info == 3 .","title":"value_nan"},{"location":"api/portfolio/logs/#vectorbt.portfolio.logs.Logs.value_zero_neg","text":"Records filtered by res_status_info == 4 .","title":"value_zero_neg"},{"location":"api/portfolio/nb/","text":"nb module \u00b6 Numba-compiled functions. Provides an arsenal of Numba-compiled functions that are used for portfolio modeling, such as generating and filling orders. These only accept NumPy arrays and other Numba-compatible types. Note vectorbt treats matrices as first-class citizens and expects input arrays to be 2-dim, unless function has suffix _1d or is meant to be input to another function. All functions passed as argument should be Numba-compiled. Records should retain the order they were created in. Warning Accumulation of roundoff error possible. See here for explanation. Rounding errors can cause trades and positions to not close properly: >>> print ( ' %.50f ' % 0.1 ) # has positive error 0.10000000000000000555111512312578270211815834045410 >>> # many buy transactions with positive error -> cannot close position >>> sum ([ 0.1 for _ in range ( 1000000 )]) - 100000 1.3328826753422618e-06 >>> print ( ' %.50f ' % 0.3 ) # has negative error 0.29999999999999998889776975374843459576368331909180 >>> # many sell transactions with negative error -> cannot close position >>> 300000 - sum ([ 0.3 for _ in range ( 1000000 )]) 5.657668225467205e-06 While vectorbt has implemented tolerance checks when comparing floats for equality, adding/subtracting small amounts large number of times may still introduce a noticable error that cannot be corrected post factum. To mitigate this issue, avoid repeating lots of micro-transactions of the same sign. For example, reduce by np.inf or position_now to close a long/short position. See vectorbt.utils.math_ for current tolerance values. approx_order_value_nb function \u00b6 approx_order_value_nb ( size , size_type , direction , cash_now , position_now , free_cash_now , val_price_now , value_now ) Approximate value of an order. asset_flow_nb function \u00b6 asset_flow_nb ( target_shape , order_records , col_map , direction ) Get asset flow series per column. Returns the total transacted amount of assets at each time step. asset_returns_nb function \u00b6 asset_returns_nb ( cash_flow , asset_value ) Get asset return series per column/group. asset_value_grouped_nb function \u00b6 asset_value_grouped_nb ( asset_value , group_lens ) Get asset value series per group. asset_value_nb function \u00b6 asset_value_nb ( close , assets ) Get asset value series per column. assets_nb function \u00b6 assets_nb ( asset_flow ) Get asset series per column. Returns the current position at each time step. benchmark_value_grouped_nb function \u00b6 benchmark_value_grouped_nb ( close , group_lens , init_cash_grouped ) Get market value per group. benchmark_value_nb function \u00b6 benchmark_value_nb ( close , init_cash ) Get market value per column. build_call_seq function \u00b6 build_call_seq ( target_shape , group_lens , call_seq_type = 0 ) Not compiled but faster version of build_call_seq_nb() . build_call_seq_nb function \u00b6 build_call_seq_nb ( target_shape , group_lens , call_seq_type = 0 ) Build a new call sequence array. buy_nb function \u00b6 buy_nb ( exec_state , size , price , direction = 2 , fees = 0.0 , fixed_fees = 0.0 , slippage = 0.0 , min_size = 0.0 , max_size = inf , size_granularity = nan , lock_cash = False , allow_partial = True , percent = nan ) Buy or/and cover. cash_flow_grouped_nb function \u00b6 cash_flow_grouped_nb ( cash_flow , group_lens ) Get cash flow series per group. cash_flow_nb function \u00b6 cash_flow_nb ( target_shape , order_records , col_map , free ) Get (free) cash flow series per column. cash_grouped_nb function \u00b6 cash_grouped_nb ( target_shape , cash_flow_grouped , group_lens , init_cash_grouped ) Get cash series per group. cash_in_sim_order_nb function \u00b6 cash_in_sim_order_nb ( cash_flow , group_lens , init_cash_grouped , call_seq ) Get cash series in simulation order. cash_nb function \u00b6 cash_nb ( cash_flow , init_cash ) Get cash series per column. check_group_init_cash_nb function \u00b6 check_group_init_cash_nb ( group_lens , n_cols , init_cash , cash_sharing ) Check init_cash . check_group_lens_nb function \u00b6 check_group_lens_nb ( group_lens , n_cols ) Check group_lens . close_position_nb function \u00b6 close_position_nb ( price = inf , fees = 0.0 , fixed_fees = 0.0 , slippage = 0.0 , min_size = 0.0 , max_size = inf , size_granularity = nan , reject_prob = 0.0 , lock_cash = False , allow_partial = True , raise_reject = False , log = False ) Close the current position. copy_trade_record_nb function \u00b6 copy_trade_record_nb ( record , trade_record ) Copy a trade record. dir_enex_signal_func_nb function \u00b6 dir_enex_signal_func_nb ( c , entries , exits , direction ) Resolve direction-aware signals out of entries, exits, and direction. execute_order_nb function \u00b6 execute_order_nb ( state , order ) Execute an order given the current state. Args state :\u2002 ProcessOrderState See ProcessOrderState . order :\u2002 Order See Order . Error is thrown if an input has value that is not expected. Order is ignored if its execution has no effect on current balance. Order is rejected if an input goes over a limit/restriction. fill_entry_trades_in_position_nb function \u00b6 fill_entry_trades_in_position_nb ( order_records , col_map , col , first_c , last_c , first_entry_size , first_entry_fees , exit_idx , exit_size_sum , exit_gross_sum , exit_fees_sum , direction , status , parent_id , trade_records , tidx ) Fill entry trades located within a single position. fill_log_record_nb function \u00b6 fill_log_record_nb ( record , record_id , i , col , group , cash , position , debt , free_cash , val_price , value , order , new_cash , new_position , new_debt , new_free_cash , new_val_price , new_value , order_result , order_id ) Fill a log record. fill_order_record_nb function \u00b6 fill_order_record_nb ( record , record_id , i , col , order_result ) Fill an order record. fill_position_record_nb function \u00b6 fill_position_record_nb ( record , id_ , trade_records ) Fill a position record by aggregating trade records. fill_trade_record_nb function \u00b6 fill_trade_record_nb ( record , id_ , col , size , entry_idx , entry_price , entry_fees , exit_idx , exit_price , exit_fees , direction , status , parent_id ) Fill a trade record. final_value_nb function \u00b6 final_value_nb ( total_profit , init_cash ) Get total profit per column/group. flex_simulate_nb function \u00b6 flex_simulate_nb ( target_shape , group_lens , init_cash , cash_sharing , segment_mask = array ( True ), call_pre_segment = False , call_post_segment = False , pre_sim_func_nb = no_pre_func_nb , pre_sim_args = (), post_sim_func_nb = no_post_func_nb , post_sim_args = (), pre_group_func_nb = no_pre_func_nb , pre_group_args = (), post_group_func_nb = no_post_func_nb , post_group_args = (), pre_segment_func_nb = no_pre_func_nb , pre_segment_args = (), post_segment_func_nb = no_post_func_nb , post_segment_args = (), flex_order_func_nb = no_flex_order_func_nb , flex_order_args = (), post_order_func_nb = no_post_func_nb , post_order_args = (), close = array ( nan ), ffill_val_price = True , update_value = False , fill_pos_record = True , max_orders = None , max_logs = 0 , flex_2d = True ) Same as simulate_nb() , but with no predefined call sequence. In contrast to order_func_nb in simulate_nb() , post_order_func_nb is a segment-level order function that returns a column along with the order, and gets repeatedly called until some condition is met. This allows multiple orders to be issued within a single element and in an arbitrary order. The order function should accept FlexOrderContext , unpacked tuple from pre_segment_func_nb , and *flex_order_args . Should return column and Order . To break out of the loop, return column of -1. Note Since one element can now accommodate multiple orders, you may run into \"order_records index out of range\" exception. In this case, you should increase max_orders . This cannot be done automatically and dynamically to avoid performance degradation. Usage The same example as in simulate_nb() : >>> import numpy as np >>> from numba import njit >>> from vectorbt.portfolio.enums import SizeType , Direction >>> from vectorbt.portfolio.nb import ( ... get_col_elem_nb , ... order_nb , ... order_nothing_nb , ... flex_simulate_nb , ... flex_simulate_row_wise_nb , ... sort_call_seq_out_nb ... ) >>> @njit ... def pre_sim_func_nb ( c ): ... print ( 'before simulation' ) ... return () >>> @njit ... def pre_group_func_nb ( c ): ... print ( ' \\t before group' , c . group ) ... # Create temporary arrays and pass them down the stack ... order_value_out = np . empty ( c . group_len , dtype = np . float_ ) ... call_seq_out = np . empty ( c . group_len , dtype = np . int_ ) ... # Forward down the stack ... return ( order_value_out , call_seq_out ) >>> @njit ... def pre_segment_func_nb ( c , order_value_out , call_seq_out , size , price , size_type , direction ): ... print ( ' \\t\\t before segment' , c . i ) ... for col in range ( c . from_col , c . to_col ): ... # Here we use order price for group valuation ... c . last_val_price [ col ] = get_col_elem_nb ( c , col , price ) ... ... # Same as for simulate_nb, but since we don't have a predefined c.call_seq_now anymore, ... # we need to store our new call sequence somewhere else ... call_seq_out [:] = np . arange ( c . group_len ) ... sort_call_seq_out_nb ( c , size , size_type , direction , order_value_out , call_seq_out ) ... ... # Forward the sorted call sequence ... return ( call_seq_out ,) >>> @njit ... def flex_order_func_nb ( c , call_seq_out , size , price , size_type , direction , fees , fixed_fees , slippage ): ... if c . call_idx < c . group_len : ... col = c . from_col + call_seq_out [ c . call_idx ] ... print ( ' \\t\\t\\t creating order' , c . call_idx , 'at column' , col ) ... # # Create and return an order ... return col , order_nb ( ... size = get_col_elem_nb ( c , col , size ), ... price = get_col_elem_nb ( c , col , price ), ... size_type = get_col_elem_nb ( c , col , size_type ), ... direction = get_col_elem_nb ( c , col , direction ), ... fees = get_col_elem_nb ( c , col , fees ), ... fixed_fees = get_col_elem_nb ( c , col , fixed_fees ), ... slippage = get_col_elem_nb ( c , col , slippage ) ... ) ... # All columns already processed -> break the loop ... print ( ' \\t\\t\\t breaking out of the loop' ) ... return - 1 , order_nothing_nb () >>> @njit ... def post_order_func_nb ( c , call_seq_out ): ... print ( ' \\t\\t\\t\\t order status:' , c . order_result . status ) ... return None >>> @njit ... def post_segment_func_nb ( c , order_value_out , call_seq_out ): ... print ( ' \\t\\t after segment' , c . i ) ... return None >>> @njit ... def post_group_func_nb ( c ): ... print ( ' \\t after group' , c . group ) ... return None >>> @njit ... def post_sim_func_nb ( c ): ... print ( 'after simulation' ) ... return None >>> target_shape = ( 5 , 3 ) >>> np . random . seed ( 42 ) >>> group_lens = np . array ([ 3 ]) # one group of three columns >>> init_cash = np . array ([ 100. ]) # one capital per group >>> cash_sharing = True >>> call_seq = build_call_seq ( target_shape , group_lens ) # will be overridden >>> segment_mask = np . array ([ True , False , True , False , True ])[:, None ] >>> segment_mask = np . copy ( np . broadcast_to ( segment_mask , target_shape )) >>> size = np . asarray ( 1 / target_shape [ 1 ]) # scalars must become 0-dim arrays >>> price = close = np . random . uniform ( 1 , 10 , size = target_shape ) >>> size_type = np . asarray ( SizeType . TargetPercent ) >>> direction = np . asarray ( Direction . LongOnly ) >>> fees = np . asarray ( 0.001 ) >>> fixed_fees = np . asarray ( 1. ) >>> slippage = np . asarray ( 0.001 ) >>> order_records , log_records = flex_simulate_nb ( ... target_shape , ... group_lens , ... init_cash , ... cash_sharing , ... segment_mask = segment_mask , ... pre_sim_func_nb = pre_sim_func_nb , ... post_sim_func_nb = post_sim_func_nb , ... pre_group_func_nb = pre_group_func_nb , ... post_group_func_nb = post_group_func_nb , ... pre_segment_func_nb = pre_segment_func_nb , ... pre_segment_args = ( size , price , size_type , direction ), ... post_segment_func_nb = post_segment_func_nb , ... flex_order_func_nb = flex_order_func_nb , ... flex_order_args = ( size , price , size_type , direction , fees , fixed_fees , slippage ), ... post_order_func_nb = post_order_func_nb ... ) before simulation before group 0 before segment 0 creating order 0 at column 0 order status: 0 creating order 1 at column 1 order status: 0 creating order 2 at column 2 order status: 0 breaking out of the loop after segment 0 before segment 2 creating order 0 at column 1 order status: 0 creating order 1 at column 2 order status: 0 creating order 2 at column 0 order status: 0 breaking out of the loop after segment 2 before segment 4 creating order 0 at column 0 order status: 0 creating order 1 at column 2 order status: 0 creating order 2 at column 1 order status: 0 breaking out of the loop after segment 4 after group 0 after simulation flex_simulate_row_wise_nb function \u00b6 flex_simulate_row_wise_nb ( target_shape , group_lens , init_cash , cash_sharing , segment_mask = array ( True ), call_pre_segment = False , call_post_segment = False , pre_sim_func_nb = no_pre_func_nb , pre_sim_args = (), post_sim_func_nb = no_post_func_nb , post_sim_args = (), pre_row_func_nb = no_pre_func_nb , pre_row_args = (), post_row_func_nb = no_post_func_nb , post_row_args = (), pre_segment_func_nb = no_pre_func_nb , pre_segment_args = (), post_segment_func_nb = no_post_func_nb , post_segment_args = (), flex_order_func_nb = no_flex_order_func_nb , flex_order_args = (), post_order_func_nb = no_post_func_nb , post_order_args = (), close = array ( nan ), ffill_val_price = True , update_value = False , fill_pos_record = True , max_orders = None , max_logs = 0 , flex_2d = True ) Same as flex_simulate_nb() , but iterates using row-major order, with the rows changing fastest, and the columns/groups changing slowest. generate_stop_signal_nb function \u00b6 generate_stop_signal_nb ( position_now , upon_stop_exit , accumulate ) Generate stop signal and change accumulation if needed. get_col_elem_nb function \u00b6 get_col_elem_nb ( ctx , col , a ) Get the current element using flexible indexing given the context and the column. get_elem_nb function \u00b6 get_elem_nb ( ctx , a ) Get the current element using flexible indexing given just the context. get_entry_trades_nb function \u00b6 get_entry_trades_nb ( order_records , close , col_map ) Fill entry trade records by aggregating order records. Entry trade records are buy orders in a long position and sell orders in a short position. Usage >>> import numpy as np >>> import pandas as pd >>> from numba import njit >>> from vectorbt.records.nb import col_map_nb >>> from vectorbt.portfolio.nb import simulate_from_orders_nb , get_entry_trades_nb >>> close = order_price = np . array ([ ... [ 1 , 6 ], ... [ 2 , 5 ], ... [ 3 , 4 ], ... [ 4 , 3 ], ... [ 5 , 2 ], ... [ 6 , 1 ] ... ]) >>> size = np . asarray ([ ... [ 1 , - 1 ], ... [ 0.1 , - 0.1 ], ... [ - 1 , 1 ], ... [ - 0.1 , 0.1 ], ... [ 1 , - 1 ], ... [ - 2 , 2 ] ... ]) >>> target_shape = close . shape >>> group_lens = np . full ( target_shape [ 1 ], 1 ) >>> init_cash = np . full ( target_shape [ 1 ], 100 ) >>> call_seq = np . full ( target_shape , 0 ) >>> order_records , log_records = simulate_from_orders_nb ( ... target_shape , ... group_lens , ... init_cash , ... call_seq , ... size = size , ... price = close , ... fees = np . asarray ( 0.01 ), ... slippage = np . asarray ( 0.01 ) ... ) >>> col_map = col_map_nb ( order_records [ 'col' ], target_shape [ 1 ]) >>> entry_trade_records = get_entry_trades_nb ( order_records , close , col_map ) >>> pd . DataFrame . from_records ( entry_trade_records ) id col size entry_idx entry_price entry_fees exit_idx exit_price \\ 0 0 0 1.0 0 1.01 0.01010 3 3.060000 1 1 0 0.1 1 2.02 0.00202 3 3.060000 2 2 0 1.0 4 5.05 0.05050 5 5.940000 3 3 0 1.0 5 5.94 0.05940 5 6.000000 4 4 1 1.0 0 5.94 0.05940 3 3.948182 5 5 1 0.1 1 4.95 0.00495 3 3.948182 6 6 1 1.0 4 1.98 0.01980 5 1.010000 7 7 1 1.0 5 1.01 0.01010 5 1.000000 exit_fees pnl return direction status parent_id 0 0.030600 2.009300 1.989406 0 1 0 1 0.003060 0.098920 0.489703 0 1 0 2 0.059400 0.780100 0.154475 0 1 1 3 0.000000 -0.119400 -0.020101 1 0 2 4 0.039482 1.892936 0.318676 1 1 3 5 0.003948 0.091284 0.184411 1 1 3 6 0.010100 0.940100 0.474798 1 1 4 7 0.000000 -0.020100 -0.019901 0 0 5 get_exit_trades_nb function \u00b6 get_exit_trades_nb ( order_records , close , col_map ) Fill exit trade records by aggregating order records. Exit trade records are sell orders in a long position and buy orders in a short position. Usage >>> import numpy as np >>> import pandas as pd >>> from numba import njit >>> from vectorbt.records.nb import col_map_nb >>> from vectorbt.portfolio.nb import simulate_from_orders_nb , get_exit_trades_nb >>> close = order_price = np . array ([ ... [ 1 , 6 ], ... [ 2 , 5 ], ... [ 3 , 4 ], ... [ 4 , 3 ], ... [ 5 , 2 ], ... [ 6 , 1 ] ... ]) >>> size = np . asarray ([ ... [ 1 , - 1 ], ... [ 0.1 , - 0.1 ], ... [ - 1 , 1 ], ... [ - 0.1 , 0.1 ], ... [ 1 , - 1 ], ... [ - 2 , 2 ] ... ]) >>> target_shape = close . shape >>> group_lens = np . full ( target_shape [ 1 ], 1 ) >>> init_cash = np . full ( target_shape [ 1 ], 100 ) >>> call_seq = np . full ( target_shape , 0 ) >>> order_records , log_records = simulate_from_orders_nb ( ... target_shape , ... group_lens , ... init_cash , ... call_seq , ... size = size , ... price = close , ... fees = np . asarray ( 0.01 ), ... slippage = np . asarray ( 0.01 ) ... ) >>> col_map = col_map_nb ( order_records [ 'col' ], target_shape [ 1 ]) >>> exit_trade_records = get_exit_trades_nb ( order_records , close , col_map ) >>> pd . DataFrame . from_records ( exit_trade_records ) id col size entry_idx entry_price entry_fees exit_idx exit_price \\ 0 0 0 1.0 0 1.101818 0.011018 2 2.97 1 1 0 0.1 0 1.101818 0.001102 3 3.96 2 2 0 1.0 4 5.050000 0.050500 5 5.94 3 3 0 1.0 5 5.940000 0.059400 5 6.00 4 4 1 1.0 0 5.850000 0.058500 2 4.04 5 5 1 0.1 0 5.850000 0.005850 3 3.03 6 6 1 1.0 4 1.980000 0.019800 5 1.01 7 7 1 1.0 5 1.010000 0.010100 5 1.00 exit_fees pnl return direction status parent_id 0 0.02970 1.827464 1.658589 0 1 0 1 0.00396 0.280756 2.548119 0 1 0 2 0.05940 0.780100 0.154475 0 1 1 3 0.00000 -0.119400 -0.020101 1 0 2 4 0.04040 1.711100 0.292496 1 1 3 5 0.00303 0.273120 0.466872 1 1 3 6 0.01010 0.940100 0.474798 1 1 4 7 0.00000 -0.020100 -0.019901 0 0 5 get_free_cash_diff_nb function \u00b6 get_free_cash_diff_nb ( position_before , position_now , debt_now , price , fees ) Get updated debt and free cash flow. get_group_value_ctx_nb function \u00b6 get_group_value_ctx_nb ( seg_ctx ) Get group value from context. Accepts SegmentContext . Best called once from pre_segment_func_nb . To set the valuation price, change last_val_price of the context in-place. Note Cash sharing must be enabled. get_group_value_nb function \u00b6 get_group_value_nb ( from_col , to_col , cash_now , last_position , last_val_price ) Get group value. get_long_size_nb function \u00b6 get_long_size_nb ( position_before , position_now ) Get long size. get_positions_nb function \u00b6 get_positions_nb ( trade_records , col_map ) Fill position records by aggregating trade records. Trades can be entry trades, exit trades, and even positions themselves - all will produce the same results. Usage Building upon the example in get_exit_trades_nb() : >>> from vectorbt.portfolio.nb import get_positions_nb >>> col_map = col_map_nb ( exit_trade_records [ 'col' ], target_shape [ 1 ]) >>> position_records = get_positions_nb ( exit_trade_records , col_map ) >>> pd . DataFrame . from_records ( position_records ) id col size entry_idx entry_price entry_fees exit_idx exit_price \\ 0 0 0 1.1 0 1.101818 0.01212 3 3.060000 1 1 0 1.0 4 5.050000 0.05050 5 5.940000 2 2 0 1.0 5 5.940000 0.05940 5 6.000000 3 3 1 1.1 0 5.850000 0.06435 3 3.948182 4 4 1 1.0 4 1.980000 0.01980 5 1.010000 5 5 1 1.0 5 1.010000 0.01010 5 1.000000 exit_fees pnl return direction status parent_id 0 0.03366 2.10822 1.739455 0 1 0 1 0.05940 0.78010 0.154475 0 1 1 2 0.00000 -0.11940 -0.020101 1 0 2 3 0.04343 1.98422 0.308348 1 1 3 4 0.01010 0.94010 0.474798 1 1 4 5 0.00000 -0.02010 -0.019901 0 0 5 get_short_size_nb function \u00b6 get_short_size_nb ( position_before , position_now ) Get short size. get_stop_price_nb function \u00b6 get_stop_price_nb ( position_now , stop_price , stop , open , low , high , hit_below ) Get stop price. If hit before open, returns open. get_trade_stats_nb function \u00b6 get_trade_stats_nb ( size , entry_price , entry_fees , exit_price , exit_fees , direction ) Get trade statistics. gross_exposure_nb function \u00b6 gross_exposure_nb ( asset_value , cash ) Get gross exposure per column/group. group_mean_reduce_nb function \u00b6 group_mean_reduce_nb ( group , a ) Mean reducer for grouped columns. i_group_any_reduce_nb function \u00b6 i_group_any_reduce_nb ( i , group , a ) Boolean \"any\" reducer for grouped columns. init_cash_grouped_nb function \u00b6 init_cash_grouped_nb ( init_cash , group_lens , cash_sharing ) Get initial cash per group. init_cash_nb function \u00b6 init_cash_nb ( init_cash , group_lens , cash_sharing ) Get initial cash per column. init_records_nb function \u00b6 init_records_nb ( target_shape , max_orders = None , max_logs = 0 ) Initialize order and log records. is_grouped_nb function \u00b6 is_grouped_nb ( group_lens ) Check if columm,ns are grouped, that is, more than one column per group. ls_enex_signal_func_nb function \u00b6 ls_enex_signal_func_nb ( c , long_entries , long_exits , short_entries , short_exits ) Get an element of direction-aware signals. no_adjust_sl_func_nb function \u00b6 no_adjust_sl_func_nb ( c , * args ) Placeholder function that returns the initial stop-loss value and trailing flag. no_adjust_tp_func_nb function \u00b6 no_adjust_tp_func_nb ( c , * args ) Placeholder function that returns the initial take-profit value. no_flex_order_func_nb function \u00b6 no_flex_order_func_nb ( c , * args ) Placeholder flexible order function that returns break column and no order. no_order_func_nb function \u00b6 no_order_func_nb ( c , * args ) Placeholder order function that returns no order. no_post_func_nb function \u00b6 no_post_func_nb ( c , * args ) Placeholder postprocessing function that returns nothing. no_pre_func_nb function \u00b6 no_pre_func_nb ( c , * args ) Placeholder preprocessing function that forwards received arguments down the stack. no_signal_func_nb function \u00b6 no_signal_func_nb ( c , * args ) Placeholder signal function that returns no signal. order_nb function \u00b6 order_nb ( size = nan , price = inf , size_type = 0 , direction = 2 , fees = 0.0 , fixed_fees = 0.0 , slippage = 0.0 , min_size = 0.0 , max_size = inf , size_granularity = nan , reject_prob = 0.0 , lock_cash = False , allow_partial = True , raise_reject = False , log = False ) Create an order. See Order for details on arguments. order_not_filled_nb function \u00b6 order_not_filled_nb ( status , status_info ) Return OrderResult for order that hasn't been filled. order_nothing_nb function \u00b6 order_nothing_nb () Convenience function to order nothing. position_coverage_grouped_nb function \u00b6 position_coverage_grouped_nb ( position_mask , group_lens ) Get coverage of position for each row and group. position_mask_grouped_nb function \u00b6 position_mask_grouped_nb ( position_mask , group_lens ) Get whether in position for each row and group. process_order_nb function \u00b6 process_order_nb ( i , col , group , state , update_value , order , order_records , log_records ) Process an order by executing it, saving relevant information to the logs, and returning a new state. raise_rejected_order_nb function \u00b6 raise_rejected_order_nb ( order_result ) Raise an RejectedOrderError . replace_inf_price_nb function \u00b6 replace_inf_price_nb ( prev_close , close , order ) Replace infinity price in an order. require_call_seq function \u00b6 require_call_seq ( call_seq ) Force the call sequence array to pass our requirements. resolve_dir_conflict_nb function \u00b6 resolve_dir_conflict_nb ( position_now , is_long_entry , is_short_entry , upon_dir_conflict ) Resolve any direction conflict between a long entry and a short entry. resolve_opposite_entry_nb function \u00b6 resolve_opposite_entry_nb ( position_now , is_long_entry , is_long_exit , is_short_entry , is_short_exit , upon_opposite_entry , accumulate ) Resolve opposite entry. resolve_signal_conflict_nb function \u00b6 resolve_signal_conflict_nb ( position_now , is_entry , is_exit , direction , conflict_mode ) Resolve any conflict between an entry and an exit. resolve_stop_price_and_slippage_nb function \u00b6 resolve_stop_price_and_slippage_nb ( stop_price , price , close , slippage , stop_exit_price ) Resolve price and slippage of a stop order. returns_in_sim_order_nb function \u00b6 returns_in_sim_order_nb ( value_iso , group_lens , init_cash_grouped , call_seq ) Get portfolio return series in simulation order. sell_nb function \u00b6 sell_nb ( exec_state , size , price , direction = 2 , fees = 0.0 , fixed_fees = 0.0 , slippage = 0.0 , min_size = 0.0 , max_size = inf , size_granularity = nan , lock_cash = False , allow_partial = True , percent = nan ) Sell or/and short sell. should_update_stop_nb function \u00b6 should_update_stop_nb ( stop , upon_stop_update ) Whether to update stop. shuffle_call_seq_nb function \u00b6 shuffle_call_seq_nb ( call_seq , group_lens ) Shuffle the call sequence array. signals_to_size_nb function \u00b6 signals_to_size_nb ( position_now , is_long_entry , is_long_exit , is_short_entry , is_short_exit , size , size_type , accumulate , val_price_now ) Translate direction-aware signals into size, size type, and direction. simulate_from_orders_nb function \u00b6 simulate_from_orders_nb ( target_shape , group_lens , init_cash , call_seq , size = array ( inf ), price = array ( inf ), size_type = array ( 0 ), direction = array ( 2 ), fees = array ( 0. ), fixed_fees = array ( 0. ), slippage = array ( 0. ), min_size = array ( 0. ), max_size = array ( inf ), size_granularity = array ( nan ), reject_prob = array ( 0. ), lock_cash = array ( False ), allow_partial = array ( True ), raise_reject = array ( False ), log = array ( False ), val_price = array ( inf ), close = array ( nan ), auto_call_seq = False , ffill_val_price = True , update_value = False , max_orders = None , max_logs = 0 , flex_2d = True ) Creates on order out of each element. Iterates in the column-major order. Utilizes flexible broadcasting. Note Should be only grouped if cash sharing is enabled. If auto_call_seq is True, make sure that call_seq follows CallSeqType.Default . Single value should be passed as a 0-dim array (for example, by using np.asarray(value) ). Usage Buy and hold using all cash and closing price (default): >>> import numpy as np >>> from vectorbt.records.nb import col_map_nb >>> from vectorbt.portfolio.nb import simulate_from_orders_nb , asset_flow_nb >>> from vectorbt.portfolio.enums import Direction >>> close = np . array ([ 1 , 2 , 3 , 4 , 5 ])[:, None ] >>> order_records , _ = simulate_from_orders_nb ( ... target_shape = close . shape , ... close = close , ... group_lens = np . array ([ 1 ]), ... init_cash = np . array ([ 100 ]), ... call_seq = np . full ( close . shape , 0 ) ... ) >>> col_map = col_map_nb ( order_records [ 'col' ], close . shape [ 1 ]) >>> asset_flow = asset_flow_nb ( close . shape , order_records , col_map , Direction . Both ) >>> asset_flow array([[100.], [ 0.], [ 0.], [ 0.], [ 0.]]) simulate_from_signal_func_nb function \u00b6 simulate_from_signal_func_nb ( target_shape , group_lens , init_cash , call_seq , signal_func_nb = no_signal_func_nb , signal_args = (), size = array ( inf ), price = array ( inf ), size_type = array ( 0 ), fees = array ( 0. ), fixed_fees = array ( 0. ), slippage = array ( 0. ), min_size = array ( 0. ), max_size = array ( inf ), size_granularity = array ( nan ), reject_prob = array ( 0. ), lock_cash = array ( False ), allow_partial = array ( True ), raise_reject = array ( False ), log = array ( False ), accumulate = array ( 0 ), upon_long_conflict = array ( 0 ), upon_short_conflict = array ( 0 ), upon_dir_conflict = array ( 0 ), upon_opposite_entry = array ( 4 ), val_price = array ( inf ), open = array ( nan ), high = array ( nan ), low = array ( nan ), close = array ( nan ), sl_stop = array ( nan ), sl_trail = array ( False ), tp_stop = array ( nan ), stop_entry_price = array ( 3 ), stop_exit_price = array ( 0 ), upon_stop_exit = array ( 0 ), upon_stop_update = array ( 1 ), adjust_sl_func_nb = no_adjust_sl_func_nb , adjust_sl_args = (), adjust_tp_func_nb = no_adjust_tp_func_nb , adjust_tp_args = (), use_stops = True , auto_call_seq = False , ffill_val_price = True , update_value = False , max_orders = None , max_logs = 0 , flex_2d = True ) Creates an order out of each element by resolving entry and exit signals returned by signal_func_nb . Iterates in the column-major order. Utilizes flexible broadcasting. Signals are processed using the following pipeline: 1) If there is a stop signal, convert it to direction-aware signals and proceed to 7) 2) Get direction-aware signals using signal_func_nb 3) Resolve any entry and exit conflict of each direction using resolve_signal_conflict_nb() 4) Resolve any direction conflict using resolve_dir_conflict_nb() 5) Resolve an opposite entry signal scenario using resolve_opposite_entry_nb() 7) Convert the final signals into size, size type, and direction using signals_to_size_nb() Note Should be only grouped if cash sharing is enabled. If auto_call_seq is True, make sure that call_seq follows CallSeqType.Default . Single value should be passed as a 0-dim array (for example, by using np.asarray(value) ). Usage Buy and hold using all cash and closing price (default): >>> import numpy as np >>> from vectorbt.records.nb import col_map_nb >>> from vectorbt.portfolio import nb >>> from vectorbt.portfolio.enums import Direction >>> close = np . array ([ 1 , 2 , 3 , 4 , 5 ])[:, None ] >>> order_records , _ = nb . simulate_from_signal_func_nb ( ... target_shape = close . shape , ... close = close , ... group_lens = np . array ([ 1 ]), ... init_cash = np . array ([ 100 ]), ... call_seq = np . full ( close . shape , 0 ), ... signal_func_nb = nb . dir_enex_signal_func_nb , ... signal_args = ( np . asarray ( True ), np . asarray ( False ), np . asarray ( Direction . LongOnly )) ... ) >>> col_map = col_map_nb ( order_records [ 'col' ], close . shape [ 1 ]) >>> asset_flow = nb . asset_flow_nb ( close . shape , order_records , col_map , Direction . Both ) >>> asset_flow array([[100.], [ 0.], [ 0.], [ 0.], [ 0.]]) simulate_nb function \u00b6 simulate_nb ( target_shape , group_lens , init_cash , cash_sharing , call_seq , segment_mask = array ( True ), call_pre_segment = False , call_post_segment = False , pre_sim_func_nb = no_pre_func_nb , pre_sim_args = (), post_sim_func_nb = no_post_func_nb , post_sim_args = (), pre_group_func_nb = no_pre_func_nb , pre_group_args = (), post_group_func_nb = no_post_func_nb , post_group_args = (), pre_segment_func_nb = no_pre_func_nb , pre_segment_args = (), post_segment_func_nb = no_post_func_nb , post_segment_args = (), order_func_nb = no_order_func_nb , order_args = (), post_order_func_nb = no_post_func_nb , post_order_args = (), close = array ( nan ), ffill_val_price = True , update_value = False , fill_pos_record = True , max_orders = None , max_logs = 0 , flex_2d = True ) Fill order and log records by iterating over a shape and calling a range of user-defined functions. Starting with initial cash init_cash , iterates over each group and column in target_shape , and for each data point, generates an order using order_func_nb . Tries then to fulfill that order. Upon success, updates the current state including the cash balance and the position. Returns order records of layout order_dt and log records of layout log_dt . As opposed to simulate_row_wise_nb() , order processing happens in column-major order. Column-major order means processing the entire column/group with all rows before moving to the next one. See Row- and column-major order . Args target_shape :\u2002 tuple See SimulationContext.target_shape . group_lens :\u2002 array_like of int See SimulationContext.group_lens . init_cash :\u2002 array_like of float See SimulationContext.init_cash . cash_sharing :\u2002 bool See SimulationContext.cash_sharing . call_seq :\u2002 array_like of int See SimulationContext.call_seq . segment_mask :\u2002 array_like of bool See SimulationContext.segment_mask . call_pre_segment :\u2002 bool See SimulationContext.call_pre_segment . call_post_segment :\u2002 bool See SimulationContext.call_post_segment . pre_sim_func_nb :\u2002 callable Function called before simulation. Can be used for creation of global arrays and setting the seed. Should accept SimulationContext and *pre_sim_args . Should return a tuple of any content, which is then passed to pre_group_func_nb and post_group_func_nb . pre_sim_args :\u2002 tuple Packed arguments passed to pre_sim_func_nb . post_sim_func_nb :\u2002 callable Function called after simulation. Should accept SimulationContext and *post_sim_args . Should return nothing. post_sim_args :\u2002 tuple Packed arguments passed to post_sim_func_nb . pre_group_func_nb :\u2002 callable Function called before each group. Should accept GroupContext , unpacked tuple from pre_sim_func_nb , and *pre_group_args . Should return a tuple of any content, which is then passed to pre_segment_func_nb and post_segment_func_nb . pre_group_args :\u2002 tuple Packed arguments passed to pre_group_func_nb . post_group_func_nb :\u2002 callable Function called after each group. Should accept GroupContext , unpacked tuple from pre_sim_func_nb , and *post_group_args . Should return nothing. post_group_args :\u2002 tuple Packed arguments passed to post_group_func_nb . pre_segment_func_nb :\u2002 callable Function called before each segment. Called if segment_mask or call_pre_segment is True. Should accept SegmentContext , unpacked tuple from pre_group_func_nb , and *pre_segment_args . Should return a tuple of any content, which is then passed to order_func_nb and post_order_func_nb . This is the right place to change call sequence and set the valuation price. Group re-valuation and update of the open position stats happens right after this function, regardless of whether it has been called. Note To change the call sequence of a segment, access SegmentContext.call_seq_now and change it in-place. Make sure to not generate any new arrays as it may negatively impact performance. Assigning SegmentContext.call_seq_now as any other context (named tuple) value is not supported. See SegmentContext.call_seq_now . Note You can override elements of last_val_price to manipulate group valuation. See SimulationContext.last_val_price . pre_segment_args :\u2002 tuple Packed arguments passed to pre_segment_func_nb . post_segment_func_nb :\u2002 callable Function called after each segment. Called if segment_mask or call_post_segment is True. The last group re-valuation and update of the open position stats happens right before this function, regardless of whether it has been called. Should accept SegmentContext , unpacked tuple from pre_group_func_nb , and *post_segment_args . Should return nothing. post_segment_args :\u2002 tuple Packed arguments passed to post_segment_func_nb . order_func_nb :\u2002 callable Order generation function. Used for either generating an order or skipping. Should accept OrderContext , unpacked tuple from pre_segment_func_nb , and *order_args . Should return Order . Note If the returned order has been rejected, there is no way of issuing a new order. You should make sure that the order passes, for example, by using try_order_nb() . To have a greater freedom in order management, use flex_simulate_nb() . order_args :\u2002 tuple Arguments passed to order_func_nb . post_order_func_nb :\u2002 callable Callback that is called after the order has been processed. Used for checking the order status and doing some post-processing. Should accept PostOrderContext , unpacked tuple from pre_segment_func_nb , and *post_order_args . Should return nothing. post_order_args :\u2002 tuple Arguments passed to post_order_func_nb . close :\u2002 array_like of float See SimulationContext.close . ffill_val_price :\u2002 bool See SimulationContext.ffill_val_price . update_value :\u2002 bool See SimulationContext.update_value . fill_pos_record :\u2002 bool See SimulationContext.fill_pos_record . max_orders :\u2002 int Size of the order records array. max_logs :\u2002 int Size of the log records array. flex_2d :\u2002 bool See SimulationContext.flex_2d . Note Remember that indexing of 2-dim arrays in vectorbt follows that of pandas: a[i, col] . Warning You can only safely access data of columns that are to the left of the current group and rows that are to the top of the current row within the same group. Other data points have not been processed yet and thus empty. Accessing them will not trigger any errors or warnings, but provide you with arbitrary data (see np.empty ). Call hierarchy Like most things in the vectorbt universe, simulation is also done by iterating over a (imaginary) frame. This frame consists of two dimensions: time (rows) and assets/features (columns). Each element of this frame is a potential order, which gets generated by calling an order function. The question is: how do we move across this frame to simulate trading? There are two movement patterns: column-major (as done by simulate_nb() ) and row-major order (as done by simulate_row_wise_nb() ). In each of these patterns, we are always moving from top to bottom (time axis) and from left to right (asset/feature axis); the only difference between them is across which axis we are moving faster: do we want to process each column first (thus assuming that columns are independent) or each row? Choosing between them is mostly a matter of preference, but it also makes different data being available when generating an order. The frame is further divided into \"blocks\": columns, groups, rows, segments, and elements. For example, columns can be grouped into groups that may or may not share the same capital. Regardless of capital sharing, each collection of elements within a group and a time step is called a segment, which simply defines a single context (such as shared capital) for one or multiple orders. Each segment can also define a custom sequence (a so-called call sequence) in which orders are executed. You can imagine each of these blocks as a rectangle drawn over different parts of the frame, and having its own context and pre/post-processing function. The pre-processing function is a simple callback that is called before entering the block, and can be provided by the user to, for example, prepare arrays or do some custom calculations. It must return a tuple (can be empty) that is then unpacked and passed as arguments to the pre- and postprocessing function coming next in the call hierarchy. The postprocessing function can be used, for example, to write user-defined arrays such as returns. Let's demonstrate a frame with one group of two columns and one group of one column, and the following call sequence: array([[0, 1, 0], [1, 0, 0]]) And here is the context information available at each step: Usage Create a group of three assets together sharing 100$ and simulate an equal-weighted portfolio that rebalances every second tick, all without leaving Numba: >>> import numpy as np >>> import pandas as pd >>> from collections import namedtuple >>> from numba import njit >>> from vectorbt.generic.plotting import Scatter >>> from vectorbt.records.nb import col_map_nb >>> from vectorbt.portfolio.enums import SizeType , Direction >>> from vectorbt.portfolio.nb import ( ... get_col_elem_nb , ... get_elem_nb , ... order_nb , ... simulate_nb , ... simulate_row_wise_nb , ... build_call_seq , ... sort_call_seq_nb , ... asset_flow_nb , ... assets_nb , ... asset_value_nb ... ) >>> @njit ... def pre_sim_func_nb ( c ): ... print ( 'before simulation' ) ... # Create a temporary array and pass it down the stack ... order_value_out = np . empty ( c . target_shape [ 1 ], dtype = np . float_ ) ... return ( order_value_out ,) >>> @njit ... def pre_group_func_nb ( c , order_value_out ): ... print ( ' \\t before group' , c . group ) ... # Forward down the stack (you can omit pre_group_func_nb entirely) ... return ( order_value_out ,) >>> @njit ... def pre_segment_func_nb ( c , order_value_out , size , price , size_type , direction ): ... print ( ' \\t\\t before segment' , c . i ) ... for col in range ( c . from_col , c . to_col ): ... # Here we use order price for group valuation ... c . last_val_price [ col ] = get_col_elem_nb ( c , col , price ) ... ... # Reorder call sequence of this segment such that selling orders come first and buying last ... # Rearranges c.call_seq_now based on order value (size, size_type, direction, and val_price) ... # Utilizes flexible indexing using get_col_elem_nb (as we did above) ... sort_call_seq_nb ( c , size , size_type , direction , order_value_out [ c . from_col : c . to_col ]) ... # Forward nothing ... return () >>> @njit ... def order_func_nb ( c , size , price , size_type , direction , fees , fixed_fees , slippage ): ... print ( ' \\t\\t\\t creating order' , c . call_idx , 'at column' , c . col ) ... # Create and return an order ... return order_nb ( ... size = get_elem_nb ( c , size ), ... price = get_elem_nb ( c , price ), ... size_type = get_elem_nb ( c , size_type ), ... direction = get_elem_nb ( c , direction ), ... fees = get_elem_nb ( c , fees ), ... fixed_fees = get_elem_nb ( c , fixed_fees ), ... slippage = get_elem_nb ( c , slippage ) ... ) >>> @njit ... def post_order_func_nb ( c ): ... print ( ' \\t\\t\\t\\t order status:' , c . order_result . status ) ... return None >>> @njit ... def post_segment_func_nb ( c , order_value_out ): ... print ( ' \\t\\t after segment' , c . i ) ... return None >>> @njit ... def post_group_func_nb ( c , order_value_out ): ... print ( ' \\t after group' , c . group ) ... return None >>> @njit ... def post_sim_func_nb ( c ): ... print ( 'after simulation' ) ... return None >>> target_shape = ( 5 , 3 ) >>> np . random . seed ( 42 ) >>> group_lens = np . array ([ 3 ]) # one group of three columns >>> init_cash = np . array ([ 100. ]) # one capital per group >>> cash_sharing = True >>> call_seq = build_call_seq ( target_shape , group_lens ) # will be overridden >>> segment_mask = np . array ([ True , False , True , False , True ])[:, None ] >>> segment_mask = np . copy ( np . broadcast_to ( segment_mask , target_shape )) >>> size = np . asarray ( 1 / target_shape [ 1 ]) # scalars must become 0-dim arrays >>> price = close = np . random . uniform ( 1 , 10 , size = target_shape ) >>> size_type = np . asarray ( SizeType . TargetPercent ) >>> direction = np . asarray ( Direction . LongOnly ) >>> fees = np . asarray ( 0.001 ) >>> fixed_fees = np . asarray ( 1. ) >>> slippage = np . asarray ( 0.001 ) >>> order_records , log_records = simulate_nb ( ... target_shape , ... group_lens , ... init_cash , ... cash_sharing , ... call_seq , ... segment_mask = segment_mask , ... pre_sim_func_nb = pre_sim_func_nb , ... post_sim_func_nb = post_sim_func_nb , ... pre_group_func_nb = pre_group_func_nb , ... post_group_func_nb = post_group_func_nb , ... pre_segment_func_nb = pre_segment_func_nb , ... pre_segment_args = ( size , price , size_type , direction ), ... post_segment_func_nb = post_segment_func_nb , ... order_func_nb = order_func_nb , ... order_args = ( size , price , size_type , direction , fees , fixed_fees , slippage ), ... post_order_func_nb = post_order_func_nb ... ) before simulation before group 0 before segment 0 creating order 0 at column 0 order status: 0 creating order 1 at column 1 order status: 0 creating order 2 at column 2 order status: 0 after segment 0 before segment 2 creating order 0 at column 1 order status: 0 creating order 1 at column 2 order status: 0 creating order 2 at column 0 order status: 0 after segment 2 before segment 4 creating order 0 at column 0 order status: 0 creating order 1 at column 2 order status: 0 creating order 2 at column 1 order status: 0 after segment 4 after group 0 after simulation >>> pd . DataFrame . from_records ( order_records ) id col idx size price fees side 0 0 0 0 7.626262 4.375232 1.033367 0 1 1 1 0 3.488053 9.565985 1.033367 0 2 2 2 0 3.972040 7.595533 1.030170 0 3 3 1 2 0.920352 8.786790 1.008087 1 4 4 2 2 0.448747 6.403625 1.002874 1 5 5 0 2 5.210115 1.524275 1.007942 0 6 6 0 4 7.899568 8.483492 1.067016 1 7 7 2 4 12.378281 2.639061 1.032667 0 8 8 1 4 10.713236 2.913963 1.031218 0 >>> call_seq array([[0, 1, 2], [0, 1, 2], [1, 2, 0], [0, 1, 2], [0, 2, 1]]) >>> col_map = col_map_nb ( order_records [ 'col' ], target_shape [ 1 ]) >>> asset_flow = asset_flow_nb ( target_shape , order_records , col_map , Direction . Both ) >>> assets = assets_nb ( asset_flow ) >>> asset_value = asset_value_nb ( close , assets ) >>> Scatter ( data = asset_value ) . fig . show () Note that the last order in a group with cash sharing is always disadvantaged as it has a bit less funds than the previous orders due to costs, which are not included when valuating the group. simulate_row_wise_nb function \u00b6 simulate_row_wise_nb ( target_shape , group_lens , init_cash , cash_sharing , call_seq , segment_mask = array ( True ), call_pre_segment = False , call_post_segment = False , pre_sim_func_nb = no_pre_func_nb , pre_sim_args = (), post_sim_func_nb = no_post_func_nb , post_sim_args = (), pre_row_func_nb = no_pre_func_nb , pre_row_args = (), post_row_func_nb = no_post_func_nb , post_row_args = (), pre_segment_func_nb = no_pre_func_nb , pre_segment_args = (), post_segment_func_nb = no_post_func_nb , post_segment_args = (), order_func_nb = no_order_func_nb , order_args = (), post_order_func_nb = no_post_func_nb , post_order_args = (), close = array ( nan ), ffill_val_price = True , update_value = False , fill_pos_record = True , max_orders = None , max_logs = 0 , flex_2d = True ) Same as simulate_nb() , but iterates in row-major order. Row-major order means processing the entire row with all groups/columns before moving to the next one. The main difference is that instead of pre_group_func_nb it now exposes pre_row_func_nb , which is executed per entire row. It should accept RowContext . Note Function pre_row_func_nb is only called if there is at least on active segment in the row. Functions pre_segment_func_nb and order_func_nb are only called if their segment is active. If the main task of pre_row_func_nb is to activate/deactivate segments, all segments should be activated by default to allow pre_row_func_nb to be called. Warning You can only safely access data points that are to the left of the current group and rows that are to the top of the current row. Call hierarchy Let's illustrate the same example as in simulate_nb() but adapted for this function: Usage Running the same example as in simulate_nb() but adapted for this function: >>> @njit ... def pre_row_func_nb ( c , order_value_out ): ... print ( ' \\t before row' , c . i ) ... # Forward down the stack ... return ( order_value_out ,) >>> @njit ... def post_row_func_nb ( c , order_value_out ): ... print ( ' \\t after row' , c . i ) ... return None >>> call_seq = build_call_seq ( target_shape , group_lens ) >>> order_records , log_records = simulate_row_wise_nb ( ... target_shape , ... group_lens , ... init_cash , ... cash_sharing , ... call_seq , ... segment_mask = segment_mask , ... pre_sim_func_nb = pre_sim_func_nb , ... post_sim_func_nb = post_sim_func_nb , ... pre_row_func_nb = pre_row_func_nb , ... post_row_func_nb = post_row_func_nb , ... pre_segment_func_nb = pre_segment_func_nb , ... pre_segment_args = ( size , price , size_type , direction ), ... post_segment_func_nb = post_segment_func_nb , ... order_func_nb = order_func_nb , ... order_args = ( size , price , size_type , direction , fees , fixed_fees , slippage ), ... post_order_func_nb = post_order_func_nb ... ) before simulation before row 0 before segment 0 creating order 0 at column 0 order status: 0 creating order 1 at column 1 order status: 0 creating order 2 at column 2 order status: 0 after segment 0 after row 0 before row 1 after row 1 before row 2 before segment 2 creating order 0 at column 1 order status: 0 creating order 1 at column 2 order status: 0 creating order 2 at column 0 order status: 0 after segment 2 after row 2 before row 3 after row 3 before row 4 before segment 4 creating order 0 at column 0 order status: 0 creating order 1 at column 2 order status: 0 creating order 2 at column 1 order status: 0 after segment 4 after row 4 after simulation sort_call_seq_nb function \u00b6 sort_call_seq_nb ( ctx , size , size_type , direction , order_value_out , ctx_select = True ) Sort call sequence attached to SegmentContext . See sort_call_seq_out_nb() . Note Can only be used in non-flexible simulation functions. sort_call_seq_out_nb function \u00b6 sort_call_seq_out_nb ( ctx , size , size_type , direction , order_value_out , call_seq_out , ctx_select = True ) Sort call sequence call_seq_out based on the value of each potential order. Accepts SegmentContext and other arguments, sorts call_seq_out in place, and returns nothing. Arrays size , size_type , and direction utilize flexible indexing. If ctx_select is True, selects the elements of each size , size_type , and direction using get_col_elem_nb() assuming that each array can broadcast to target_shape . Otherwise, selects using flex_select_auto_nb() assuming that each array can broadcast to group_len . The lengths of order_value_out and call_seq_out should match the number of columns in the group. Array order_value_out should be empty and will contain sorted order values after execution. Array call_seq_out should be filled with integers ranging from 0 to the number of columns in the group (in this exact order). Best called once from pre_segment_func_nb . Note Cash sharing must be enabled and call_seq_out should follow CallSeqType.Default . Should be used in flexible simulation functions. sum_grouped_nb function \u00b6 sum_grouped_nb ( a , group_lens ) Squeeze each group of columns into a single column using sum operation. total_benchmark_return_nb function \u00b6 total_benchmark_return_nb ( benchmark_value ) Get total market return per column/group. total_profit_grouped_nb function \u00b6 total_profit_grouped_nb ( total_profit , group_lens ) Get total profit per group. total_profit_nb function \u00b6 total_profit_nb ( target_shape , close , order_records , col_map ) Get total profit per column. A much faster version than the one based on value_nb() . total_return_nb function \u00b6 total_return_nb ( total_profit , init_cash ) Get total return per column/group. trade_losing_streak_nb function \u00b6 trade_losing_streak_nb ( records ) Return the current losing streak of each trade. trade_winning_streak_nb function \u00b6 trade_winning_streak_nb ( records ) Return the current winning streak of each trade. try_order_nb function \u00b6 try_order_nb ( ctx , order ) Execute an order without persistence. update_open_pos_stats_nb function \u00b6 update_open_pos_stats_nb ( record , position_now , price ) Update statistics of an open position record using custom price. update_pos_record_nb function \u00b6 update_pos_record_nb ( record , i , col , position_before , position_now , order_result ) Update position record after filling an order. update_value_nb function \u00b6 update_value_nb ( cash_before , cash_now , position_before , position_now , val_price_before , price , value_before ) Update valuation price and value. value_in_sim_order_nb function \u00b6 value_in_sim_order_nb ( cash , asset_value , group_lens , call_seq ) Get portfolio value series in simulation order. value_nb function \u00b6 value_nb ( cash , asset_value ) Get portfolio value series per column/group.","title":"nb"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb","text":"Numba-compiled functions. Provides an arsenal of Numba-compiled functions that are used for portfolio modeling, such as generating and filling orders. These only accept NumPy arrays and other Numba-compatible types. Note vectorbt treats matrices as first-class citizens and expects input arrays to be 2-dim, unless function has suffix _1d or is meant to be input to another function. All functions passed as argument should be Numba-compiled. Records should retain the order they were created in. Warning Accumulation of roundoff error possible. See here for explanation. Rounding errors can cause trades and positions to not close properly: >>> print ( ' %.50f ' % 0.1 ) # has positive error 0.10000000000000000555111512312578270211815834045410 >>> # many buy transactions with positive error -> cannot close position >>> sum ([ 0.1 for _ in range ( 1000000 )]) - 100000 1.3328826753422618e-06 >>> print ( ' %.50f ' % 0.3 ) # has negative error 0.29999999999999998889776975374843459576368331909180 >>> # many sell transactions with negative error -> cannot close position >>> 300000 - sum ([ 0.3 for _ in range ( 1000000 )]) 5.657668225467205e-06 While vectorbt has implemented tolerance checks when comparing floats for equality, adding/subtracting small amounts large number of times may still introduce a noticable error that cannot be corrected post factum. To mitigate this issue, avoid repeating lots of micro-transactions of the same sign. For example, reduce by np.inf or position_now to close a long/short position. See vectorbt.utils.math_ for current tolerance values.","title":"vectorbt.portfolio.nb"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.approx_order_value_nb","text":"approx_order_value_nb ( size , size_type , direction , cash_now , position_now , free_cash_now , val_price_now , value_now ) Approximate value of an order.","title":"approx_order_value_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.asset_flow_nb","text":"asset_flow_nb ( target_shape , order_records , col_map , direction ) Get asset flow series per column. Returns the total transacted amount of assets at each time step.","title":"asset_flow_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.asset_returns_nb","text":"asset_returns_nb ( cash_flow , asset_value ) Get asset return series per column/group.","title":"asset_returns_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.asset_value_grouped_nb","text":"asset_value_grouped_nb ( asset_value , group_lens ) Get asset value series per group.","title":"asset_value_grouped_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.asset_value_nb","text":"asset_value_nb ( close , assets ) Get asset value series per column.","title":"asset_value_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.assets_nb","text":"assets_nb ( asset_flow ) Get asset series per column. Returns the current position at each time step.","title":"assets_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.benchmark_value_grouped_nb","text":"benchmark_value_grouped_nb ( close , group_lens , init_cash_grouped ) Get market value per group.","title":"benchmark_value_grouped_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.benchmark_value_nb","text":"benchmark_value_nb ( close , init_cash ) Get market value per column.","title":"benchmark_value_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.build_call_seq","text":"build_call_seq ( target_shape , group_lens , call_seq_type = 0 ) Not compiled but faster version of build_call_seq_nb() .","title":"build_call_seq()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.build_call_seq_nb","text":"build_call_seq_nb ( target_shape , group_lens , call_seq_type = 0 ) Build a new call sequence array.","title":"build_call_seq_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.buy_nb","text":"buy_nb ( exec_state , size , price , direction = 2 , fees = 0.0 , fixed_fees = 0.0 , slippage = 0.0 , min_size = 0.0 , max_size = inf , size_granularity = nan , lock_cash = False , allow_partial = True , percent = nan ) Buy or/and cover.","title":"buy_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.cash_flow_grouped_nb","text":"cash_flow_grouped_nb ( cash_flow , group_lens ) Get cash flow series per group.","title":"cash_flow_grouped_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.cash_flow_nb","text":"cash_flow_nb ( target_shape , order_records , col_map , free ) Get (free) cash flow series per column.","title":"cash_flow_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.cash_grouped_nb","text":"cash_grouped_nb ( target_shape , cash_flow_grouped , group_lens , init_cash_grouped ) Get cash series per group.","title":"cash_grouped_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.cash_in_sim_order_nb","text":"cash_in_sim_order_nb ( cash_flow , group_lens , init_cash_grouped , call_seq ) Get cash series in simulation order.","title":"cash_in_sim_order_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.cash_nb","text":"cash_nb ( cash_flow , init_cash ) Get cash series per column.","title":"cash_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.check_group_init_cash_nb","text":"check_group_init_cash_nb ( group_lens , n_cols , init_cash , cash_sharing ) Check init_cash .","title":"check_group_init_cash_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.check_group_lens_nb","text":"check_group_lens_nb ( group_lens , n_cols ) Check group_lens .","title":"check_group_lens_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.close_position_nb","text":"close_position_nb ( price = inf , fees = 0.0 , fixed_fees = 0.0 , slippage = 0.0 , min_size = 0.0 , max_size = inf , size_granularity = nan , reject_prob = 0.0 , lock_cash = False , allow_partial = True , raise_reject = False , log = False ) Close the current position.","title":"close_position_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.copy_trade_record_nb","text":"copy_trade_record_nb ( record , trade_record ) Copy a trade record.","title":"copy_trade_record_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.dir_enex_signal_func_nb","text":"dir_enex_signal_func_nb ( c , entries , exits , direction ) Resolve direction-aware signals out of entries, exits, and direction.","title":"dir_enex_signal_func_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.execute_order_nb","text":"execute_order_nb ( state , order ) Execute an order given the current state. Args state :\u2002 ProcessOrderState See ProcessOrderState . order :\u2002 Order See Order . Error is thrown if an input has value that is not expected. Order is ignored if its execution has no effect on current balance. Order is rejected if an input goes over a limit/restriction.","title":"execute_order_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.fill_entry_trades_in_position_nb","text":"fill_entry_trades_in_position_nb ( order_records , col_map , col , first_c , last_c , first_entry_size , first_entry_fees , exit_idx , exit_size_sum , exit_gross_sum , exit_fees_sum , direction , status , parent_id , trade_records , tidx ) Fill entry trades located within a single position.","title":"fill_entry_trades_in_position_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.fill_log_record_nb","text":"fill_log_record_nb ( record , record_id , i , col , group , cash , position , debt , free_cash , val_price , value , order , new_cash , new_position , new_debt , new_free_cash , new_val_price , new_value , order_result , order_id ) Fill a log record.","title":"fill_log_record_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.fill_order_record_nb","text":"fill_order_record_nb ( record , record_id , i , col , order_result ) Fill an order record.","title":"fill_order_record_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.fill_position_record_nb","text":"fill_position_record_nb ( record , id_ , trade_records ) Fill a position record by aggregating trade records.","title":"fill_position_record_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.fill_trade_record_nb","text":"fill_trade_record_nb ( record , id_ , col , size , entry_idx , entry_price , entry_fees , exit_idx , exit_price , exit_fees , direction , status , parent_id ) Fill a trade record.","title":"fill_trade_record_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.final_value_nb","text":"final_value_nb ( total_profit , init_cash ) Get total profit per column/group.","title":"final_value_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.flex_simulate_nb","text":"flex_simulate_nb ( target_shape , group_lens , init_cash , cash_sharing , segment_mask = array ( True ), call_pre_segment = False , call_post_segment = False , pre_sim_func_nb = no_pre_func_nb , pre_sim_args = (), post_sim_func_nb = no_post_func_nb , post_sim_args = (), pre_group_func_nb = no_pre_func_nb , pre_group_args = (), post_group_func_nb = no_post_func_nb , post_group_args = (), pre_segment_func_nb = no_pre_func_nb , pre_segment_args = (), post_segment_func_nb = no_post_func_nb , post_segment_args = (), flex_order_func_nb = no_flex_order_func_nb , flex_order_args = (), post_order_func_nb = no_post_func_nb , post_order_args = (), close = array ( nan ), ffill_val_price = True , update_value = False , fill_pos_record = True , max_orders = None , max_logs = 0 , flex_2d = True ) Same as simulate_nb() , but with no predefined call sequence. In contrast to order_func_nb in simulate_nb() , post_order_func_nb is a segment-level order function that returns a column along with the order, and gets repeatedly called until some condition is met. This allows multiple orders to be issued within a single element and in an arbitrary order. The order function should accept FlexOrderContext , unpacked tuple from pre_segment_func_nb , and *flex_order_args . Should return column and Order . To break out of the loop, return column of -1. Note Since one element can now accommodate multiple orders, you may run into \"order_records index out of range\" exception. In this case, you should increase max_orders . This cannot be done automatically and dynamically to avoid performance degradation. Usage The same example as in simulate_nb() : >>> import numpy as np >>> from numba import njit >>> from vectorbt.portfolio.enums import SizeType , Direction >>> from vectorbt.portfolio.nb import ( ... get_col_elem_nb , ... order_nb , ... order_nothing_nb , ... flex_simulate_nb , ... flex_simulate_row_wise_nb , ... sort_call_seq_out_nb ... ) >>> @njit ... def pre_sim_func_nb ( c ): ... print ( 'before simulation' ) ... return () >>> @njit ... def pre_group_func_nb ( c ): ... print ( ' \\t before group' , c . group ) ... # Create temporary arrays and pass them down the stack ... order_value_out = np . empty ( c . group_len , dtype = np . float_ ) ... call_seq_out = np . empty ( c . group_len , dtype = np . int_ ) ... # Forward down the stack ... return ( order_value_out , call_seq_out ) >>> @njit ... def pre_segment_func_nb ( c , order_value_out , call_seq_out , size , price , size_type , direction ): ... print ( ' \\t\\t before segment' , c . i ) ... for col in range ( c . from_col , c . to_col ): ... # Here we use order price for group valuation ... c . last_val_price [ col ] = get_col_elem_nb ( c , col , price ) ... ... # Same as for simulate_nb, but since we don't have a predefined c.call_seq_now anymore, ... # we need to store our new call sequence somewhere else ... call_seq_out [:] = np . arange ( c . group_len ) ... sort_call_seq_out_nb ( c , size , size_type , direction , order_value_out , call_seq_out ) ... ... # Forward the sorted call sequence ... return ( call_seq_out ,) >>> @njit ... def flex_order_func_nb ( c , call_seq_out , size , price , size_type , direction , fees , fixed_fees , slippage ): ... if c . call_idx < c . group_len : ... col = c . from_col + call_seq_out [ c . call_idx ] ... print ( ' \\t\\t\\t creating order' , c . call_idx , 'at column' , col ) ... # # Create and return an order ... return col , order_nb ( ... size = get_col_elem_nb ( c , col , size ), ... price = get_col_elem_nb ( c , col , price ), ... size_type = get_col_elem_nb ( c , col , size_type ), ... direction = get_col_elem_nb ( c , col , direction ), ... fees = get_col_elem_nb ( c , col , fees ), ... fixed_fees = get_col_elem_nb ( c , col , fixed_fees ), ... slippage = get_col_elem_nb ( c , col , slippage ) ... ) ... # All columns already processed -> break the loop ... print ( ' \\t\\t\\t breaking out of the loop' ) ... return - 1 , order_nothing_nb () >>> @njit ... def post_order_func_nb ( c , call_seq_out ): ... print ( ' \\t\\t\\t\\t order status:' , c . order_result . status ) ... return None >>> @njit ... def post_segment_func_nb ( c , order_value_out , call_seq_out ): ... print ( ' \\t\\t after segment' , c . i ) ... return None >>> @njit ... def post_group_func_nb ( c ): ... print ( ' \\t after group' , c . group ) ... return None >>> @njit ... def post_sim_func_nb ( c ): ... print ( 'after simulation' ) ... return None >>> target_shape = ( 5 , 3 ) >>> np . random . seed ( 42 ) >>> group_lens = np . array ([ 3 ]) # one group of three columns >>> init_cash = np . array ([ 100. ]) # one capital per group >>> cash_sharing = True >>> call_seq = build_call_seq ( target_shape , group_lens ) # will be overridden >>> segment_mask = np . array ([ True , False , True , False , True ])[:, None ] >>> segment_mask = np . copy ( np . broadcast_to ( segment_mask , target_shape )) >>> size = np . asarray ( 1 / target_shape [ 1 ]) # scalars must become 0-dim arrays >>> price = close = np . random . uniform ( 1 , 10 , size = target_shape ) >>> size_type = np . asarray ( SizeType . TargetPercent ) >>> direction = np . asarray ( Direction . LongOnly ) >>> fees = np . asarray ( 0.001 ) >>> fixed_fees = np . asarray ( 1. ) >>> slippage = np . asarray ( 0.001 ) >>> order_records , log_records = flex_simulate_nb ( ... target_shape , ... group_lens , ... init_cash , ... cash_sharing , ... segment_mask = segment_mask , ... pre_sim_func_nb = pre_sim_func_nb , ... post_sim_func_nb = post_sim_func_nb , ... pre_group_func_nb = pre_group_func_nb , ... post_group_func_nb = post_group_func_nb , ... pre_segment_func_nb = pre_segment_func_nb , ... pre_segment_args = ( size , price , size_type , direction ), ... post_segment_func_nb = post_segment_func_nb , ... flex_order_func_nb = flex_order_func_nb , ... flex_order_args = ( size , price , size_type , direction , fees , fixed_fees , slippage ), ... post_order_func_nb = post_order_func_nb ... ) before simulation before group 0 before segment 0 creating order 0 at column 0 order status: 0 creating order 1 at column 1 order status: 0 creating order 2 at column 2 order status: 0 breaking out of the loop after segment 0 before segment 2 creating order 0 at column 1 order status: 0 creating order 1 at column 2 order status: 0 creating order 2 at column 0 order status: 0 breaking out of the loop after segment 2 before segment 4 creating order 0 at column 0 order status: 0 creating order 1 at column 2 order status: 0 creating order 2 at column 1 order status: 0 breaking out of the loop after segment 4 after group 0 after simulation","title":"flex_simulate_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.flex_simulate_row_wise_nb","text":"flex_simulate_row_wise_nb ( target_shape , group_lens , init_cash , cash_sharing , segment_mask = array ( True ), call_pre_segment = False , call_post_segment = False , pre_sim_func_nb = no_pre_func_nb , pre_sim_args = (), post_sim_func_nb = no_post_func_nb , post_sim_args = (), pre_row_func_nb = no_pre_func_nb , pre_row_args = (), post_row_func_nb = no_post_func_nb , post_row_args = (), pre_segment_func_nb = no_pre_func_nb , pre_segment_args = (), post_segment_func_nb = no_post_func_nb , post_segment_args = (), flex_order_func_nb = no_flex_order_func_nb , flex_order_args = (), post_order_func_nb = no_post_func_nb , post_order_args = (), close = array ( nan ), ffill_val_price = True , update_value = False , fill_pos_record = True , max_orders = None , max_logs = 0 , flex_2d = True ) Same as flex_simulate_nb() , but iterates using row-major order, with the rows changing fastest, and the columns/groups changing slowest.","title":"flex_simulate_row_wise_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.generate_stop_signal_nb","text":"generate_stop_signal_nb ( position_now , upon_stop_exit , accumulate ) Generate stop signal and change accumulation if needed.","title":"generate_stop_signal_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.get_col_elem_nb","text":"get_col_elem_nb ( ctx , col , a ) Get the current element using flexible indexing given the context and the column.","title":"get_col_elem_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.get_elem_nb","text":"get_elem_nb ( ctx , a ) Get the current element using flexible indexing given just the context.","title":"get_elem_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.get_entry_trades_nb","text":"get_entry_trades_nb ( order_records , close , col_map ) Fill entry trade records by aggregating order records. Entry trade records are buy orders in a long position and sell orders in a short position. Usage >>> import numpy as np >>> import pandas as pd >>> from numba import njit >>> from vectorbt.records.nb import col_map_nb >>> from vectorbt.portfolio.nb import simulate_from_orders_nb , get_entry_trades_nb >>> close = order_price = np . array ([ ... [ 1 , 6 ], ... [ 2 , 5 ], ... [ 3 , 4 ], ... [ 4 , 3 ], ... [ 5 , 2 ], ... [ 6 , 1 ] ... ]) >>> size = np . asarray ([ ... [ 1 , - 1 ], ... [ 0.1 , - 0.1 ], ... [ - 1 , 1 ], ... [ - 0.1 , 0.1 ], ... [ 1 , - 1 ], ... [ - 2 , 2 ] ... ]) >>> target_shape = close . shape >>> group_lens = np . full ( target_shape [ 1 ], 1 ) >>> init_cash = np . full ( target_shape [ 1 ], 100 ) >>> call_seq = np . full ( target_shape , 0 ) >>> order_records , log_records = simulate_from_orders_nb ( ... target_shape , ... group_lens , ... init_cash , ... call_seq , ... size = size , ... price = close , ... fees = np . asarray ( 0.01 ), ... slippage = np . asarray ( 0.01 ) ... ) >>> col_map = col_map_nb ( order_records [ 'col' ], target_shape [ 1 ]) >>> entry_trade_records = get_entry_trades_nb ( order_records , close , col_map ) >>> pd . DataFrame . from_records ( entry_trade_records ) id col size entry_idx entry_price entry_fees exit_idx exit_price \\ 0 0 0 1.0 0 1.01 0.01010 3 3.060000 1 1 0 0.1 1 2.02 0.00202 3 3.060000 2 2 0 1.0 4 5.05 0.05050 5 5.940000 3 3 0 1.0 5 5.94 0.05940 5 6.000000 4 4 1 1.0 0 5.94 0.05940 3 3.948182 5 5 1 0.1 1 4.95 0.00495 3 3.948182 6 6 1 1.0 4 1.98 0.01980 5 1.010000 7 7 1 1.0 5 1.01 0.01010 5 1.000000 exit_fees pnl return direction status parent_id 0 0.030600 2.009300 1.989406 0 1 0 1 0.003060 0.098920 0.489703 0 1 0 2 0.059400 0.780100 0.154475 0 1 1 3 0.000000 -0.119400 -0.020101 1 0 2 4 0.039482 1.892936 0.318676 1 1 3 5 0.003948 0.091284 0.184411 1 1 3 6 0.010100 0.940100 0.474798 1 1 4 7 0.000000 -0.020100 -0.019901 0 0 5","title":"get_entry_trades_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.get_exit_trades_nb","text":"get_exit_trades_nb ( order_records , close , col_map ) Fill exit trade records by aggregating order records. Exit trade records are sell orders in a long position and buy orders in a short position. Usage >>> import numpy as np >>> import pandas as pd >>> from numba import njit >>> from vectorbt.records.nb import col_map_nb >>> from vectorbt.portfolio.nb import simulate_from_orders_nb , get_exit_trades_nb >>> close = order_price = np . array ([ ... [ 1 , 6 ], ... [ 2 , 5 ], ... [ 3 , 4 ], ... [ 4 , 3 ], ... [ 5 , 2 ], ... [ 6 , 1 ] ... ]) >>> size = np . asarray ([ ... [ 1 , - 1 ], ... [ 0.1 , - 0.1 ], ... [ - 1 , 1 ], ... [ - 0.1 , 0.1 ], ... [ 1 , - 1 ], ... [ - 2 , 2 ] ... ]) >>> target_shape = close . shape >>> group_lens = np . full ( target_shape [ 1 ], 1 ) >>> init_cash = np . full ( target_shape [ 1 ], 100 ) >>> call_seq = np . full ( target_shape , 0 ) >>> order_records , log_records = simulate_from_orders_nb ( ... target_shape , ... group_lens , ... init_cash , ... call_seq , ... size = size , ... price = close , ... fees = np . asarray ( 0.01 ), ... slippage = np . asarray ( 0.01 ) ... ) >>> col_map = col_map_nb ( order_records [ 'col' ], target_shape [ 1 ]) >>> exit_trade_records = get_exit_trades_nb ( order_records , close , col_map ) >>> pd . DataFrame . from_records ( exit_trade_records ) id col size entry_idx entry_price entry_fees exit_idx exit_price \\ 0 0 0 1.0 0 1.101818 0.011018 2 2.97 1 1 0 0.1 0 1.101818 0.001102 3 3.96 2 2 0 1.0 4 5.050000 0.050500 5 5.94 3 3 0 1.0 5 5.940000 0.059400 5 6.00 4 4 1 1.0 0 5.850000 0.058500 2 4.04 5 5 1 0.1 0 5.850000 0.005850 3 3.03 6 6 1 1.0 4 1.980000 0.019800 5 1.01 7 7 1 1.0 5 1.010000 0.010100 5 1.00 exit_fees pnl return direction status parent_id 0 0.02970 1.827464 1.658589 0 1 0 1 0.00396 0.280756 2.548119 0 1 0 2 0.05940 0.780100 0.154475 0 1 1 3 0.00000 -0.119400 -0.020101 1 0 2 4 0.04040 1.711100 0.292496 1 1 3 5 0.00303 0.273120 0.466872 1 1 3 6 0.01010 0.940100 0.474798 1 1 4 7 0.00000 -0.020100 -0.019901 0 0 5","title":"get_exit_trades_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.get_free_cash_diff_nb","text":"get_free_cash_diff_nb ( position_before , position_now , debt_now , price , fees ) Get updated debt and free cash flow.","title":"get_free_cash_diff_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.get_group_value_ctx_nb","text":"get_group_value_ctx_nb ( seg_ctx ) Get group value from context. Accepts SegmentContext . Best called once from pre_segment_func_nb . To set the valuation price, change last_val_price of the context in-place. Note Cash sharing must be enabled.","title":"get_group_value_ctx_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.get_group_value_nb","text":"get_group_value_nb ( from_col , to_col , cash_now , last_position , last_val_price ) Get group value.","title":"get_group_value_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.get_long_size_nb","text":"get_long_size_nb ( position_before , position_now ) Get long size.","title":"get_long_size_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.get_positions_nb","text":"get_positions_nb ( trade_records , col_map ) Fill position records by aggregating trade records. Trades can be entry trades, exit trades, and even positions themselves - all will produce the same results. Usage Building upon the example in get_exit_trades_nb() : >>> from vectorbt.portfolio.nb import get_positions_nb >>> col_map = col_map_nb ( exit_trade_records [ 'col' ], target_shape [ 1 ]) >>> position_records = get_positions_nb ( exit_trade_records , col_map ) >>> pd . DataFrame . from_records ( position_records ) id col size entry_idx entry_price entry_fees exit_idx exit_price \\ 0 0 0 1.1 0 1.101818 0.01212 3 3.060000 1 1 0 1.0 4 5.050000 0.05050 5 5.940000 2 2 0 1.0 5 5.940000 0.05940 5 6.000000 3 3 1 1.1 0 5.850000 0.06435 3 3.948182 4 4 1 1.0 4 1.980000 0.01980 5 1.010000 5 5 1 1.0 5 1.010000 0.01010 5 1.000000 exit_fees pnl return direction status parent_id 0 0.03366 2.10822 1.739455 0 1 0 1 0.05940 0.78010 0.154475 0 1 1 2 0.00000 -0.11940 -0.020101 1 0 2 3 0.04343 1.98422 0.308348 1 1 3 4 0.01010 0.94010 0.474798 1 1 4 5 0.00000 -0.02010 -0.019901 0 0 5","title":"get_positions_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.get_short_size_nb","text":"get_short_size_nb ( position_before , position_now ) Get short size.","title":"get_short_size_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.get_stop_price_nb","text":"get_stop_price_nb ( position_now , stop_price , stop , open , low , high , hit_below ) Get stop price. If hit before open, returns open.","title":"get_stop_price_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.get_trade_stats_nb","text":"get_trade_stats_nb ( size , entry_price , entry_fees , exit_price , exit_fees , direction ) Get trade statistics.","title":"get_trade_stats_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.gross_exposure_nb","text":"gross_exposure_nb ( asset_value , cash ) Get gross exposure per column/group.","title":"gross_exposure_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.group_mean_reduce_nb","text":"group_mean_reduce_nb ( group , a ) Mean reducer for grouped columns.","title":"group_mean_reduce_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.i_group_any_reduce_nb","text":"i_group_any_reduce_nb ( i , group , a ) Boolean \"any\" reducer for grouped columns.","title":"i_group_any_reduce_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.init_cash_grouped_nb","text":"init_cash_grouped_nb ( init_cash , group_lens , cash_sharing ) Get initial cash per group.","title":"init_cash_grouped_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.init_cash_nb","text":"init_cash_nb ( init_cash , group_lens , cash_sharing ) Get initial cash per column.","title":"init_cash_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.init_records_nb","text":"init_records_nb ( target_shape , max_orders = None , max_logs = 0 ) Initialize order and log records.","title":"init_records_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.is_grouped_nb","text":"is_grouped_nb ( group_lens ) Check if columm,ns are grouped, that is, more than one column per group.","title":"is_grouped_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.ls_enex_signal_func_nb","text":"ls_enex_signal_func_nb ( c , long_entries , long_exits , short_entries , short_exits ) Get an element of direction-aware signals.","title":"ls_enex_signal_func_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.no_adjust_sl_func_nb","text":"no_adjust_sl_func_nb ( c , * args ) Placeholder function that returns the initial stop-loss value and trailing flag.","title":"no_adjust_sl_func_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.no_adjust_tp_func_nb","text":"no_adjust_tp_func_nb ( c , * args ) Placeholder function that returns the initial take-profit value.","title":"no_adjust_tp_func_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.no_flex_order_func_nb","text":"no_flex_order_func_nb ( c , * args ) Placeholder flexible order function that returns break column and no order.","title":"no_flex_order_func_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.no_order_func_nb","text":"no_order_func_nb ( c , * args ) Placeholder order function that returns no order.","title":"no_order_func_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.no_post_func_nb","text":"no_post_func_nb ( c , * args ) Placeholder postprocessing function that returns nothing.","title":"no_post_func_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.no_pre_func_nb","text":"no_pre_func_nb ( c , * args ) Placeholder preprocessing function that forwards received arguments down the stack.","title":"no_pre_func_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.no_signal_func_nb","text":"no_signal_func_nb ( c , * args ) Placeholder signal function that returns no signal.","title":"no_signal_func_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.order_nb","text":"order_nb ( size = nan , price = inf , size_type = 0 , direction = 2 , fees = 0.0 , fixed_fees = 0.0 , slippage = 0.0 , min_size = 0.0 , max_size = inf , size_granularity = nan , reject_prob = 0.0 , lock_cash = False , allow_partial = True , raise_reject = False , log = False ) Create an order. See Order for details on arguments.","title":"order_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.order_not_filled_nb","text":"order_not_filled_nb ( status , status_info ) Return OrderResult for order that hasn't been filled.","title":"order_not_filled_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.order_nothing_nb","text":"order_nothing_nb () Convenience function to order nothing.","title":"order_nothing_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.position_coverage_grouped_nb","text":"position_coverage_grouped_nb ( position_mask , group_lens ) Get coverage of position for each row and group.","title":"position_coverage_grouped_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.position_mask_grouped_nb","text":"position_mask_grouped_nb ( position_mask , group_lens ) Get whether in position for each row and group.","title":"position_mask_grouped_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.process_order_nb","text":"process_order_nb ( i , col , group , state , update_value , order , order_records , log_records ) Process an order by executing it, saving relevant information to the logs, and returning a new state.","title":"process_order_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.raise_rejected_order_nb","text":"raise_rejected_order_nb ( order_result ) Raise an RejectedOrderError .","title":"raise_rejected_order_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.replace_inf_price_nb","text":"replace_inf_price_nb ( prev_close , close , order ) Replace infinity price in an order.","title":"replace_inf_price_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.require_call_seq","text":"require_call_seq ( call_seq ) Force the call sequence array to pass our requirements.","title":"require_call_seq()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.resolve_dir_conflict_nb","text":"resolve_dir_conflict_nb ( position_now , is_long_entry , is_short_entry , upon_dir_conflict ) Resolve any direction conflict between a long entry and a short entry.","title":"resolve_dir_conflict_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.resolve_opposite_entry_nb","text":"resolve_opposite_entry_nb ( position_now , is_long_entry , is_long_exit , is_short_entry , is_short_exit , upon_opposite_entry , accumulate ) Resolve opposite entry.","title":"resolve_opposite_entry_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.resolve_signal_conflict_nb","text":"resolve_signal_conflict_nb ( position_now , is_entry , is_exit , direction , conflict_mode ) Resolve any conflict between an entry and an exit.","title":"resolve_signal_conflict_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.resolve_stop_price_and_slippage_nb","text":"resolve_stop_price_and_slippage_nb ( stop_price , price , close , slippage , stop_exit_price ) Resolve price and slippage of a stop order.","title":"resolve_stop_price_and_slippage_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.returns_in_sim_order_nb","text":"returns_in_sim_order_nb ( value_iso , group_lens , init_cash_grouped , call_seq ) Get portfolio return series in simulation order.","title":"returns_in_sim_order_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.sell_nb","text":"sell_nb ( exec_state , size , price , direction = 2 , fees = 0.0 , fixed_fees = 0.0 , slippage = 0.0 , min_size = 0.0 , max_size = inf , size_granularity = nan , lock_cash = False , allow_partial = True , percent = nan ) Sell or/and short sell.","title":"sell_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.should_update_stop_nb","text":"should_update_stop_nb ( stop , upon_stop_update ) Whether to update stop.","title":"should_update_stop_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.shuffle_call_seq_nb","text":"shuffle_call_seq_nb ( call_seq , group_lens ) Shuffle the call sequence array.","title":"shuffle_call_seq_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.signals_to_size_nb","text":"signals_to_size_nb ( position_now , is_long_entry , is_long_exit , is_short_entry , is_short_exit , size , size_type , accumulate , val_price_now ) Translate direction-aware signals into size, size type, and direction.","title":"signals_to_size_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.simulate_from_orders_nb","text":"simulate_from_orders_nb ( target_shape , group_lens , init_cash , call_seq , size = array ( inf ), price = array ( inf ), size_type = array ( 0 ), direction = array ( 2 ), fees = array ( 0. ), fixed_fees = array ( 0. ), slippage = array ( 0. ), min_size = array ( 0. ), max_size = array ( inf ), size_granularity = array ( nan ), reject_prob = array ( 0. ), lock_cash = array ( False ), allow_partial = array ( True ), raise_reject = array ( False ), log = array ( False ), val_price = array ( inf ), close = array ( nan ), auto_call_seq = False , ffill_val_price = True , update_value = False , max_orders = None , max_logs = 0 , flex_2d = True ) Creates on order out of each element. Iterates in the column-major order. Utilizes flexible broadcasting. Note Should be only grouped if cash sharing is enabled. If auto_call_seq is True, make sure that call_seq follows CallSeqType.Default . Single value should be passed as a 0-dim array (for example, by using np.asarray(value) ). Usage Buy and hold using all cash and closing price (default): >>> import numpy as np >>> from vectorbt.records.nb import col_map_nb >>> from vectorbt.portfolio.nb import simulate_from_orders_nb , asset_flow_nb >>> from vectorbt.portfolio.enums import Direction >>> close = np . array ([ 1 , 2 , 3 , 4 , 5 ])[:, None ] >>> order_records , _ = simulate_from_orders_nb ( ... target_shape = close . shape , ... close = close , ... group_lens = np . array ([ 1 ]), ... init_cash = np . array ([ 100 ]), ... call_seq = np . full ( close . shape , 0 ) ... ) >>> col_map = col_map_nb ( order_records [ 'col' ], close . shape [ 1 ]) >>> asset_flow = asset_flow_nb ( close . shape , order_records , col_map , Direction . Both ) >>> asset_flow array([[100.], [ 0.], [ 0.], [ 0.], [ 0.]])","title":"simulate_from_orders_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.simulate_from_signal_func_nb","text":"simulate_from_signal_func_nb ( target_shape , group_lens , init_cash , call_seq , signal_func_nb = no_signal_func_nb , signal_args = (), size = array ( inf ), price = array ( inf ), size_type = array ( 0 ), fees = array ( 0. ), fixed_fees = array ( 0. ), slippage = array ( 0. ), min_size = array ( 0. ), max_size = array ( inf ), size_granularity = array ( nan ), reject_prob = array ( 0. ), lock_cash = array ( False ), allow_partial = array ( True ), raise_reject = array ( False ), log = array ( False ), accumulate = array ( 0 ), upon_long_conflict = array ( 0 ), upon_short_conflict = array ( 0 ), upon_dir_conflict = array ( 0 ), upon_opposite_entry = array ( 4 ), val_price = array ( inf ), open = array ( nan ), high = array ( nan ), low = array ( nan ), close = array ( nan ), sl_stop = array ( nan ), sl_trail = array ( False ), tp_stop = array ( nan ), stop_entry_price = array ( 3 ), stop_exit_price = array ( 0 ), upon_stop_exit = array ( 0 ), upon_stop_update = array ( 1 ), adjust_sl_func_nb = no_adjust_sl_func_nb , adjust_sl_args = (), adjust_tp_func_nb = no_adjust_tp_func_nb , adjust_tp_args = (), use_stops = True , auto_call_seq = False , ffill_val_price = True , update_value = False , max_orders = None , max_logs = 0 , flex_2d = True ) Creates an order out of each element by resolving entry and exit signals returned by signal_func_nb . Iterates in the column-major order. Utilizes flexible broadcasting. Signals are processed using the following pipeline: 1) If there is a stop signal, convert it to direction-aware signals and proceed to 7) 2) Get direction-aware signals using signal_func_nb 3) Resolve any entry and exit conflict of each direction using resolve_signal_conflict_nb() 4) Resolve any direction conflict using resolve_dir_conflict_nb() 5) Resolve an opposite entry signal scenario using resolve_opposite_entry_nb() 7) Convert the final signals into size, size type, and direction using signals_to_size_nb() Note Should be only grouped if cash sharing is enabled. If auto_call_seq is True, make sure that call_seq follows CallSeqType.Default . Single value should be passed as a 0-dim array (for example, by using np.asarray(value) ). Usage Buy and hold using all cash and closing price (default): >>> import numpy as np >>> from vectorbt.records.nb import col_map_nb >>> from vectorbt.portfolio import nb >>> from vectorbt.portfolio.enums import Direction >>> close = np . array ([ 1 , 2 , 3 , 4 , 5 ])[:, None ] >>> order_records , _ = nb . simulate_from_signal_func_nb ( ... target_shape = close . shape , ... close = close , ... group_lens = np . array ([ 1 ]), ... init_cash = np . array ([ 100 ]), ... call_seq = np . full ( close . shape , 0 ), ... signal_func_nb = nb . dir_enex_signal_func_nb , ... signal_args = ( np . asarray ( True ), np . asarray ( False ), np . asarray ( Direction . LongOnly )) ... ) >>> col_map = col_map_nb ( order_records [ 'col' ], close . shape [ 1 ]) >>> asset_flow = nb . asset_flow_nb ( close . shape , order_records , col_map , Direction . Both ) >>> asset_flow array([[100.], [ 0.], [ 0.], [ 0.], [ 0.]])","title":"simulate_from_signal_func_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.simulate_nb","text":"simulate_nb ( target_shape , group_lens , init_cash , cash_sharing , call_seq , segment_mask = array ( True ), call_pre_segment = False , call_post_segment = False , pre_sim_func_nb = no_pre_func_nb , pre_sim_args = (), post_sim_func_nb = no_post_func_nb , post_sim_args = (), pre_group_func_nb = no_pre_func_nb , pre_group_args = (), post_group_func_nb = no_post_func_nb , post_group_args = (), pre_segment_func_nb = no_pre_func_nb , pre_segment_args = (), post_segment_func_nb = no_post_func_nb , post_segment_args = (), order_func_nb = no_order_func_nb , order_args = (), post_order_func_nb = no_post_func_nb , post_order_args = (), close = array ( nan ), ffill_val_price = True , update_value = False , fill_pos_record = True , max_orders = None , max_logs = 0 , flex_2d = True ) Fill order and log records by iterating over a shape and calling a range of user-defined functions. Starting with initial cash init_cash , iterates over each group and column in target_shape , and for each data point, generates an order using order_func_nb . Tries then to fulfill that order. Upon success, updates the current state including the cash balance and the position. Returns order records of layout order_dt and log records of layout log_dt . As opposed to simulate_row_wise_nb() , order processing happens in column-major order. Column-major order means processing the entire column/group with all rows before moving to the next one. See Row- and column-major order . Args target_shape :\u2002 tuple See SimulationContext.target_shape . group_lens :\u2002 array_like of int See SimulationContext.group_lens . init_cash :\u2002 array_like of float See SimulationContext.init_cash . cash_sharing :\u2002 bool See SimulationContext.cash_sharing . call_seq :\u2002 array_like of int See SimulationContext.call_seq . segment_mask :\u2002 array_like of bool See SimulationContext.segment_mask . call_pre_segment :\u2002 bool See SimulationContext.call_pre_segment . call_post_segment :\u2002 bool See SimulationContext.call_post_segment . pre_sim_func_nb :\u2002 callable Function called before simulation. Can be used for creation of global arrays and setting the seed. Should accept SimulationContext and *pre_sim_args . Should return a tuple of any content, which is then passed to pre_group_func_nb and post_group_func_nb . pre_sim_args :\u2002 tuple Packed arguments passed to pre_sim_func_nb . post_sim_func_nb :\u2002 callable Function called after simulation. Should accept SimulationContext and *post_sim_args . Should return nothing. post_sim_args :\u2002 tuple Packed arguments passed to post_sim_func_nb . pre_group_func_nb :\u2002 callable Function called before each group. Should accept GroupContext , unpacked tuple from pre_sim_func_nb , and *pre_group_args . Should return a tuple of any content, which is then passed to pre_segment_func_nb and post_segment_func_nb . pre_group_args :\u2002 tuple Packed arguments passed to pre_group_func_nb . post_group_func_nb :\u2002 callable Function called after each group. Should accept GroupContext , unpacked tuple from pre_sim_func_nb , and *post_group_args . Should return nothing. post_group_args :\u2002 tuple Packed arguments passed to post_group_func_nb . pre_segment_func_nb :\u2002 callable Function called before each segment. Called if segment_mask or call_pre_segment is True. Should accept SegmentContext , unpacked tuple from pre_group_func_nb , and *pre_segment_args . Should return a tuple of any content, which is then passed to order_func_nb and post_order_func_nb . This is the right place to change call sequence and set the valuation price. Group re-valuation and update of the open position stats happens right after this function, regardless of whether it has been called. Note To change the call sequence of a segment, access SegmentContext.call_seq_now and change it in-place. Make sure to not generate any new arrays as it may negatively impact performance. Assigning SegmentContext.call_seq_now as any other context (named tuple) value is not supported. See SegmentContext.call_seq_now . Note You can override elements of last_val_price to manipulate group valuation. See SimulationContext.last_val_price . pre_segment_args :\u2002 tuple Packed arguments passed to pre_segment_func_nb . post_segment_func_nb :\u2002 callable Function called after each segment. Called if segment_mask or call_post_segment is True. The last group re-valuation and update of the open position stats happens right before this function, regardless of whether it has been called. Should accept SegmentContext , unpacked tuple from pre_group_func_nb , and *post_segment_args . Should return nothing. post_segment_args :\u2002 tuple Packed arguments passed to post_segment_func_nb . order_func_nb :\u2002 callable Order generation function. Used for either generating an order or skipping. Should accept OrderContext , unpacked tuple from pre_segment_func_nb , and *order_args . Should return Order . Note If the returned order has been rejected, there is no way of issuing a new order. You should make sure that the order passes, for example, by using try_order_nb() . To have a greater freedom in order management, use flex_simulate_nb() . order_args :\u2002 tuple Arguments passed to order_func_nb . post_order_func_nb :\u2002 callable Callback that is called after the order has been processed. Used for checking the order status and doing some post-processing. Should accept PostOrderContext , unpacked tuple from pre_segment_func_nb , and *post_order_args . Should return nothing. post_order_args :\u2002 tuple Arguments passed to post_order_func_nb . close :\u2002 array_like of float See SimulationContext.close . ffill_val_price :\u2002 bool See SimulationContext.ffill_val_price . update_value :\u2002 bool See SimulationContext.update_value . fill_pos_record :\u2002 bool See SimulationContext.fill_pos_record . max_orders :\u2002 int Size of the order records array. max_logs :\u2002 int Size of the log records array. flex_2d :\u2002 bool See SimulationContext.flex_2d . Note Remember that indexing of 2-dim arrays in vectorbt follows that of pandas: a[i, col] . Warning You can only safely access data of columns that are to the left of the current group and rows that are to the top of the current row within the same group. Other data points have not been processed yet and thus empty. Accessing them will not trigger any errors or warnings, but provide you with arbitrary data (see np.empty ). Call hierarchy Like most things in the vectorbt universe, simulation is also done by iterating over a (imaginary) frame. This frame consists of two dimensions: time (rows) and assets/features (columns). Each element of this frame is a potential order, which gets generated by calling an order function. The question is: how do we move across this frame to simulate trading? There are two movement patterns: column-major (as done by simulate_nb() ) and row-major order (as done by simulate_row_wise_nb() ). In each of these patterns, we are always moving from top to bottom (time axis) and from left to right (asset/feature axis); the only difference between them is across which axis we are moving faster: do we want to process each column first (thus assuming that columns are independent) or each row? Choosing between them is mostly a matter of preference, but it also makes different data being available when generating an order. The frame is further divided into \"blocks\": columns, groups, rows, segments, and elements. For example, columns can be grouped into groups that may or may not share the same capital. Regardless of capital sharing, each collection of elements within a group and a time step is called a segment, which simply defines a single context (such as shared capital) for one or multiple orders. Each segment can also define a custom sequence (a so-called call sequence) in which orders are executed. You can imagine each of these blocks as a rectangle drawn over different parts of the frame, and having its own context and pre/post-processing function. The pre-processing function is a simple callback that is called before entering the block, and can be provided by the user to, for example, prepare arrays or do some custom calculations. It must return a tuple (can be empty) that is then unpacked and passed as arguments to the pre- and postprocessing function coming next in the call hierarchy. The postprocessing function can be used, for example, to write user-defined arrays such as returns. Let's demonstrate a frame with one group of two columns and one group of one column, and the following call sequence: array([[0, 1, 0], [1, 0, 0]]) And here is the context information available at each step: Usage Create a group of three assets together sharing 100$ and simulate an equal-weighted portfolio that rebalances every second tick, all without leaving Numba: >>> import numpy as np >>> import pandas as pd >>> from collections import namedtuple >>> from numba import njit >>> from vectorbt.generic.plotting import Scatter >>> from vectorbt.records.nb import col_map_nb >>> from vectorbt.portfolio.enums import SizeType , Direction >>> from vectorbt.portfolio.nb import ( ... get_col_elem_nb , ... get_elem_nb , ... order_nb , ... simulate_nb , ... simulate_row_wise_nb , ... build_call_seq , ... sort_call_seq_nb , ... asset_flow_nb , ... assets_nb , ... asset_value_nb ... ) >>> @njit ... def pre_sim_func_nb ( c ): ... print ( 'before simulation' ) ... # Create a temporary array and pass it down the stack ... order_value_out = np . empty ( c . target_shape [ 1 ], dtype = np . float_ ) ... return ( order_value_out ,) >>> @njit ... def pre_group_func_nb ( c , order_value_out ): ... print ( ' \\t before group' , c . group ) ... # Forward down the stack (you can omit pre_group_func_nb entirely) ... return ( order_value_out ,) >>> @njit ... def pre_segment_func_nb ( c , order_value_out , size , price , size_type , direction ): ... print ( ' \\t\\t before segment' , c . i ) ... for col in range ( c . from_col , c . to_col ): ... # Here we use order price for group valuation ... c . last_val_price [ col ] = get_col_elem_nb ( c , col , price ) ... ... # Reorder call sequence of this segment such that selling orders come first and buying last ... # Rearranges c.call_seq_now based on order value (size, size_type, direction, and val_price) ... # Utilizes flexible indexing using get_col_elem_nb (as we did above) ... sort_call_seq_nb ( c , size , size_type , direction , order_value_out [ c . from_col : c . to_col ]) ... # Forward nothing ... return () >>> @njit ... def order_func_nb ( c , size , price , size_type , direction , fees , fixed_fees , slippage ): ... print ( ' \\t\\t\\t creating order' , c . call_idx , 'at column' , c . col ) ... # Create and return an order ... return order_nb ( ... size = get_elem_nb ( c , size ), ... price = get_elem_nb ( c , price ), ... size_type = get_elem_nb ( c , size_type ), ... direction = get_elem_nb ( c , direction ), ... fees = get_elem_nb ( c , fees ), ... fixed_fees = get_elem_nb ( c , fixed_fees ), ... slippage = get_elem_nb ( c , slippage ) ... ) >>> @njit ... def post_order_func_nb ( c ): ... print ( ' \\t\\t\\t\\t order status:' , c . order_result . status ) ... return None >>> @njit ... def post_segment_func_nb ( c , order_value_out ): ... print ( ' \\t\\t after segment' , c . i ) ... return None >>> @njit ... def post_group_func_nb ( c , order_value_out ): ... print ( ' \\t after group' , c . group ) ... return None >>> @njit ... def post_sim_func_nb ( c ): ... print ( 'after simulation' ) ... return None >>> target_shape = ( 5 , 3 ) >>> np . random . seed ( 42 ) >>> group_lens = np . array ([ 3 ]) # one group of three columns >>> init_cash = np . array ([ 100. ]) # one capital per group >>> cash_sharing = True >>> call_seq = build_call_seq ( target_shape , group_lens ) # will be overridden >>> segment_mask = np . array ([ True , False , True , False , True ])[:, None ] >>> segment_mask = np . copy ( np . broadcast_to ( segment_mask , target_shape )) >>> size = np . asarray ( 1 / target_shape [ 1 ]) # scalars must become 0-dim arrays >>> price = close = np . random . uniform ( 1 , 10 , size = target_shape ) >>> size_type = np . asarray ( SizeType . TargetPercent ) >>> direction = np . asarray ( Direction . LongOnly ) >>> fees = np . asarray ( 0.001 ) >>> fixed_fees = np . asarray ( 1. ) >>> slippage = np . asarray ( 0.001 ) >>> order_records , log_records = simulate_nb ( ... target_shape , ... group_lens , ... init_cash , ... cash_sharing , ... call_seq , ... segment_mask = segment_mask , ... pre_sim_func_nb = pre_sim_func_nb , ... post_sim_func_nb = post_sim_func_nb , ... pre_group_func_nb = pre_group_func_nb , ... post_group_func_nb = post_group_func_nb , ... pre_segment_func_nb = pre_segment_func_nb , ... pre_segment_args = ( size , price , size_type , direction ), ... post_segment_func_nb = post_segment_func_nb , ... order_func_nb = order_func_nb , ... order_args = ( size , price , size_type , direction , fees , fixed_fees , slippage ), ... post_order_func_nb = post_order_func_nb ... ) before simulation before group 0 before segment 0 creating order 0 at column 0 order status: 0 creating order 1 at column 1 order status: 0 creating order 2 at column 2 order status: 0 after segment 0 before segment 2 creating order 0 at column 1 order status: 0 creating order 1 at column 2 order status: 0 creating order 2 at column 0 order status: 0 after segment 2 before segment 4 creating order 0 at column 0 order status: 0 creating order 1 at column 2 order status: 0 creating order 2 at column 1 order status: 0 after segment 4 after group 0 after simulation >>> pd . DataFrame . from_records ( order_records ) id col idx size price fees side 0 0 0 0 7.626262 4.375232 1.033367 0 1 1 1 0 3.488053 9.565985 1.033367 0 2 2 2 0 3.972040 7.595533 1.030170 0 3 3 1 2 0.920352 8.786790 1.008087 1 4 4 2 2 0.448747 6.403625 1.002874 1 5 5 0 2 5.210115 1.524275 1.007942 0 6 6 0 4 7.899568 8.483492 1.067016 1 7 7 2 4 12.378281 2.639061 1.032667 0 8 8 1 4 10.713236 2.913963 1.031218 0 >>> call_seq array([[0, 1, 2], [0, 1, 2], [1, 2, 0], [0, 1, 2], [0, 2, 1]]) >>> col_map = col_map_nb ( order_records [ 'col' ], target_shape [ 1 ]) >>> asset_flow = asset_flow_nb ( target_shape , order_records , col_map , Direction . Both ) >>> assets = assets_nb ( asset_flow ) >>> asset_value = asset_value_nb ( close , assets ) >>> Scatter ( data = asset_value ) . fig . show () Note that the last order in a group with cash sharing is always disadvantaged as it has a bit less funds than the previous orders due to costs, which are not included when valuating the group.","title":"simulate_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.simulate_row_wise_nb","text":"simulate_row_wise_nb ( target_shape , group_lens , init_cash , cash_sharing , call_seq , segment_mask = array ( True ), call_pre_segment = False , call_post_segment = False , pre_sim_func_nb = no_pre_func_nb , pre_sim_args = (), post_sim_func_nb = no_post_func_nb , post_sim_args = (), pre_row_func_nb = no_pre_func_nb , pre_row_args = (), post_row_func_nb = no_post_func_nb , post_row_args = (), pre_segment_func_nb = no_pre_func_nb , pre_segment_args = (), post_segment_func_nb = no_post_func_nb , post_segment_args = (), order_func_nb = no_order_func_nb , order_args = (), post_order_func_nb = no_post_func_nb , post_order_args = (), close = array ( nan ), ffill_val_price = True , update_value = False , fill_pos_record = True , max_orders = None , max_logs = 0 , flex_2d = True ) Same as simulate_nb() , but iterates in row-major order. Row-major order means processing the entire row with all groups/columns before moving to the next one. The main difference is that instead of pre_group_func_nb it now exposes pre_row_func_nb , which is executed per entire row. It should accept RowContext . Note Function pre_row_func_nb is only called if there is at least on active segment in the row. Functions pre_segment_func_nb and order_func_nb are only called if their segment is active. If the main task of pre_row_func_nb is to activate/deactivate segments, all segments should be activated by default to allow pre_row_func_nb to be called. Warning You can only safely access data points that are to the left of the current group and rows that are to the top of the current row. Call hierarchy Let's illustrate the same example as in simulate_nb() but adapted for this function: Usage Running the same example as in simulate_nb() but adapted for this function: >>> @njit ... def pre_row_func_nb ( c , order_value_out ): ... print ( ' \\t before row' , c . i ) ... # Forward down the stack ... return ( order_value_out ,) >>> @njit ... def post_row_func_nb ( c , order_value_out ): ... print ( ' \\t after row' , c . i ) ... return None >>> call_seq = build_call_seq ( target_shape , group_lens ) >>> order_records , log_records = simulate_row_wise_nb ( ... target_shape , ... group_lens , ... init_cash , ... cash_sharing , ... call_seq , ... segment_mask = segment_mask , ... pre_sim_func_nb = pre_sim_func_nb , ... post_sim_func_nb = post_sim_func_nb , ... pre_row_func_nb = pre_row_func_nb , ... post_row_func_nb = post_row_func_nb , ... pre_segment_func_nb = pre_segment_func_nb , ... pre_segment_args = ( size , price , size_type , direction ), ... post_segment_func_nb = post_segment_func_nb , ... order_func_nb = order_func_nb , ... order_args = ( size , price , size_type , direction , fees , fixed_fees , slippage ), ... post_order_func_nb = post_order_func_nb ... ) before simulation before row 0 before segment 0 creating order 0 at column 0 order status: 0 creating order 1 at column 1 order status: 0 creating order 2 at column 2 order status: 0 after segment 0 after row 0 before row 1 after row 1 before row 2 before segment 2 creating order 0 at column 1 order status: 0 creating order 1 at column 2 order status: 0 creating order 2 at column 0 order status: 0 after segment 2 after row 2 before row 3 after row 3 before row 4 before segment 4 creating order 0 at column 0 order status: 0 creating order 1 at column 2 order status: 0 creating order 2 at column 1 order status: 0 after segment 4 after row 4 after simulation","title":"simulate_row_wise_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.sort_call_seq_nb","text":"sort_call_seq_nb ( ctx , size , size_type , direction , order_value_out , ctx_select = True ) Sort call sequence attached to SegmentContext . See sort_call_seq_out_nb() . Note Can only be used in non-flexible simulation functions.","title":"sort_call_seq_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.sort_call_seq_out_nb","text":"sort_call_seq_out_nb ( ctx , size , size_type , direction , order_value_out , call_seq_out , ctx_select = True ) Sort call sequence call_seq_out based on the value of each potential order. Accepts SegmentContext and other arguments, sorts call_seq_out in place, and returns nothing. Arrays size , size_type , and direction utilize flexible indexing. If ctx_select is True, selects the elements of each size , size_type , and direction using get_col_elem_nb() assuming that each array can broadcast to target_shape . Otherwise, selects using flex_select_auto_nb() assuming that each array can broadcast to group_len . The lengths of order_value_out and call_seq_out should match the number of columns in the group. Array order_value_out should be empty and will contain sorted order values after execution. Array call_seq_out should be filled with integers ranging from 0 to the number of columns in the group (in this exact order). Best called once from pre_segment_func_nb . Note Cash sharing must be enabled and call_seq_out should follow CallSeqType.Default . Should be used in flexible simulation functions.","title":"sort_call_seq_out_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.sum_grouped_nb","text":"sum_grouped_nb ( a , group_lens ) Squeeze each group of columns into a single column using sum operation.","title":"sum_grouped_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.total_benchmark_return_nb","text":"total_benchmark_return_nb ( benchmark_value ) Get total market return per column/group.","title":"total_benchmark_return_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.total_profit_grouped_nb","text":"total_profit_grouped_nb ( total_profit , group_lens ) Get total profit per group.","title":"total_profit_grouped_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.total_profit_nb","text":"total_profit_nb ( target_shape , close , order_records , col_map ) Get total profit per column. A much faster version than the one based on value_nb() .","title":"total_profit_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.total_return_nb","text":"total_return_nb ( total_profit , init_cash ) Get total return per column/group.","title":"total_return_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.trade_losing_streak_nb","text":"trade_losing_streak_nb ( records ) Return the current losing streak of each trade.","title":"trade_losing_streak_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.trade_winning_streak_nb","text":"trade_winning_streak_nb ( records ) Return the current winning streak of each trade.","title":"trade_winning_streak_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.try_order_nb","text":"try_order_nb ( ctx , order ) Execute an order without persistence.","title":"try_order_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.update_open_pos_stats_nb","text":"update_open_pos_stats_nb ( record , position_now , price ) Update statistics of an open position record using custom price.","title":"update_open_pos_stats_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.update_pos_record_nb","text":"update_pos_record_nb ( record , i , col , position_before , position_now , order_result ) Update position record after filling an order.","title":"update_pos_record_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.update_value_nb","text":"update_value_nb ( cash_before , cash_now , position_before , position_now , val_price_before , price , value_before ) Update valuation price and value.","title":"update_value_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.value_in_sim_order_nb","text":"value_in_sim_order_nb ( cash , asset_value , group_lens , call_seq ) Get portfolio value series in simulation order.","title":"value_in_sim_order_nb()"},{"location":"api/portfolio/nb/#vectorbt.portfolio.nb.value_nb","text":"value_nb ( cash , asset_value ) Get portfolio value series per column/group.","title":"value_nb()"},{"location":"api/portfolio/orders/","text":"orders module \u00b6 Base class for working with order records. Order records capture information on filled orders. Orders are mainly populated when simulating a portfolio and can be accessed as Portfolio.orders . >>> import pandas as pd >>> import numpy as np >>> from datetime import datetime , timedelta >>> import vectorbt as vbt >>> np . random . seed ( 42 ) >>> price = pd . DataFrame ({ ... 'a' : np . random . uniform ( 1 , 2 , size = 100 ), ... 'b' : np . random . uniform ( 1 , 2 , size = 100 ) ... }, index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 100 )]) >>> size = pd . DataFrame ({ ... 'a' : np . random . uniform ( - 1 , 1 , size = 100 ), ... 'b' : np . random . uniform ( - 1 , 1 , size = 100 ), ... }, index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 100 )]) >>> pf = vbt . Portfolio . from_orders ( price , size , fees = 0.01 , freq = 'd' ) >>> orders = pf . orders >>> orders . buy . count () a 58 b 51 Name: count, dtype: int64 >>> orders . sell . count () a 42 b 49 Name: count, dtype: int64 Stats \u00b6 Hint See StatsBuilderMixin.stats() and Orders.metrics . >>> orders [ 'a' ] . stats () Start 2020-01-01 00:00:00 End 2020-04-09 00:00:00 Period 100 days 00:00:00 Total Records 100 Total Buy Orders 58 Total Sell Orders 42 Min Size 0.003033 Max Size 0.989877 Avg Size 0.508608 Avg Buy Size 0.468802 Avg Sell Size 0.563577 Avg Buy Price 1.437037 Avg Sell Price 1.515951 Total Fees 0.740177 Min Fees 0.000052 Max Fees 0.016224 Avg Fees 0.007402 Avg Buy Fees 0.006771 Avg Sell Fees 0.008273 Name: a, dtype: object StatsBuilderMixin.stats() also supports (re-)grouping: >>> orders . stats ( group_by = True ) Start 2020-01-01 00:00:00 End 2020-04-09 00:00:00 Period 100 days 00:00:00 Total Records 200 Total Buy Orders 109 Total Sell Orders 91 Min Size 0.003033 Max Size 0.989877 Avg Size 0.506279 Avg Buy Size 0.472504 Avg Sell Size 0.546735 Avg Buy Price 1.47336 Avg Sell Price 1.496759 Total Fees 1.483343 Min Fees 0.000052 Max Fees 0.018319 Avg Fees 0.007417 Avg Buy Fees 0.006881 Avg Sell Fees 0.008058 Name: group, dtype: object Plots \u00b6 Hint See PlotsBuilderMixin.plots() and Orders.subplots . Orders class has a single subplot based on Orders.plot() : >>> orders [ 'a' ] . plots () orders_attach_field_config Config \u00b6 Config of fields to be attached to Orders . Co nf ig( { \"side\" : { \"attach_filters\" : true } } ) orders_field_config Config \u00b6 Field config for Orders . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"col\" : \"int64\" , \"idx\" : \"int64\" , \"size\" : \"float64\" , \"price\" : \"float64\" , \"fees\" : \"float64\" , \"side\" : \"int64\" }, \"settings\" : { \"id\" : { \"title\" : \"Order Id\" }, \"size\" : { \"title\" : \"Size\" }, \"price\" : { \"title\" : \"Price\" }, \"fees\" : { \"title\" : \"Fees\" }, \"side\" : { \"title\" : \"Side\" , \"mapping\" : { \"Buy\" : 0 , \"Sell\" : 1 } } } } ) Orders class \u00b6 Extends Records for working with order records. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin Records RecordsWithFields StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() Records.apply() Records.apply_mask() Records.build_field_config_doc() Records.col_arr Records.col_mapper Records.config Records.count() Records.get_apply_mapping_arr() Records.get_by_col_idxs() Records.get_field_arr() Records.get_field_mapping() Records.get_field_name() Records.get_field_setting() Records.get_field_title() Records.get_map_field() Records.get_map_field_to_index() Records.id_arr Records.idx_arr Records.iloc Records.indexing_func_meta() Records.indexing_kwargs Records.is_sorted() Records.loc Records.map() Records.map_array() Records.map_field() Records.override_field_config_doc() Records.records Records.records_arr Records.records_readable Records.replace() Records.self_aliases Records.sort() Records.values Records.wrapper Records.writeable_attrs StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() buy method \u00b6 Records filtered by side == 0 . close property \u00b6 Reference price such as close (optional). col method \u00b6 Mapped array of the field col . fees method \u00b6 Mapped array of the field fees . field_config class variable \u00b6 Field config of Orders . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"col\" : \"int64\" , \"idx\" : \"int64\" , \"size\" : \"float64\" , \"price\" : \"float64\" , \"fees\" : \"float64\" , \"side\" : \"int64\" }, \"settings\" : { \"id\" : { \"name\" : \"id\" , \"title\" : \"Order Id\" }, \"col\" : { \"name\" : \"col\" , \"title\" : \"Column\" , \"mapping\" : \"columns\" }, \"idx\" : { \"name\" : \"idx\" , \"title\" : \"Timestamp\" , \"mapping\" : \"index\" }, \"size\" : { \"title\" : \"Size\" }, \"price\" : { \"title\" : \"Price\" }, \"fees\" : { \"title\" : \"Fees\" }, \"side\" : { \"title\" : \"Side\" , \"mapping\" : { \"Buy\" : 0 , \"Sell\" : 1 } } } } ) id method \u00b6 Mapped array of the field id . idx method \u00b6 Mapped array of the field idx . indexing_func method \u00b6 Orders . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on Orders . metrics class variable \u00b6 Metrics supported by Orders . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function Orders.<lambda> at 0x7fac99054048>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function Orders.<lambda> at 0x7fac990540d0>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function Orders.<lambda> at 0x7fac99054158>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"total_records\" : { \"title\" : \"Total Records\" , \"calc_func\" : \"count\" , \"tags\" : \"records\" }, \"total_buy_orders\" : { \"title\" : \"Total Buy Orders\" , \"calc_func\" : \"buy.count\" , \"tags\" : [ \"orders\" , \"buy\" ] }, \"total_sell_orders\" : { \"title\" : \"Total Sell Orders\" , \"calc_func\" : \"sell.count\" , \"tags\" : [ \"orders\" , \"sell\" ] }, \"min_size\" : { \"title\" : \"Min Size\" , \"calc_func\" : \"size.min\" , \"tags\" : [ \"orders\" , \"size\" ] }, \"max_size\" : { \"title\" : \"Max Size\" , \"calc_func\" : \"size.max\" , \"tags\" : [ \"orders\" , \"size\" ] }, \"avg_size\" : { \"title\" : \"Avg Size\" , \"calc_func\" : \"size.mean\" , \"tags\" : [ \"orders\" , \"size\" ] }, \"avg_buy_size\" : { \"title\" : \"Avg Buy Size\" , \"calc_func\" : \"buy.size.mean\" , \"tags\" : [ \"orders\" , \"buy\" , \"size\" ] }, \"avg_sell_size\" : { \"title\" : \"Avg Sell Size\" , \"calc_func\" : \"sell.size.mean\" , \"tags\" : [ \"orders\" , \"sell\" , \"size\" ] }, \"avg_buy_price\" : { \"title\" : \"Avg Buy Price\" , \"calc_func\" : \"buy.price.mean\" , \"tags\" : [ \"orders\" , \"buy\" , \"price\" ] }, \"avg_sell_price\" : { \"title\" : \"Avg Sell Price\" , \"calc_func\" : \"sell.price.mean\" , \"tags\" : [ \"orders\" , \"sell\" , \"price\" ] }, \"total_fees\" : { \"title\" : \"Total Fees\" , \"calc_func\" : \"fees.sum\" , \"tags\" : [ \"orders\" , \"fees\" ] }, \"min_fees\" : { \"title\" : \"Min Fees\" , \"calc_func\" : \"fees.min\" , \"tags\" : [ \"orders\" , \"fees\" ] }, \"max_fees\" : { \"title\" : \"Max Fees\" , \"calc_func\" : \"fees.max\" , \"tags\" : [ \"orders\" , \"fees\" ] }, \"avg_fees\" : { \"title\" : \"Avg Fees\" , \"calc_func\" : \"fees.mean\" , \"tags\" : [ \"orders\" , \"fees\" ] }, \"avg_buy_fees\" : { \"title\" : \"Avg Buy Fees\" , \"calc_func\" : \"buy.fees.mean\" , \"tags\" : [ \"orders\" , \"buy\" , \"fees\" ] }, \"avg_sell_fees\" : { \"title\" : \"Avg Sell Fees\" , \"calc_func\" : \"sell.fees.mean\" , \"tags\" : [ \"orders\" , \"sell\" , \"fees\" ] } } ) Returns Orders._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable Orders._metrics . plot method \u00b6 Orders . plot ( column = None , close_trace_kwargs = None , buy_trace_kwargs = None , sell_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot orders. Args column :\u2002 str Name of the column to plot. close_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for Orders.close . buy_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Buy\" markers. sell_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Sell\" markers. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import pandas as pd >>> from datetime import datetime , timedelta >>> import vectorbt as vbt >>> price = pd . Series ([ 1. , 2. , 3. , 2. , 1. ], name = 'Price' ) >>> price . index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( len ( price ))] >>> size = pd . Series ([ 1. , 1. , 1. , 1. , - 1. ]) >>> orders = vbt . Portfolio . from_orders ( price , size ) . orders >>> orders . plot () plots_defaults property \u00b6 Defaults for PlotsBuilderMixin.plots() . Merges Records.plots_defaults and orders.plots from settings . price method \u00b6 Mapped array of the field price . sell method \u00b6 Records filtered by side == 1 . side method \u00b6 Mapped array of the field side . size method \u00b6 Mapped array of the field size . stats_defaults property \u00b6 Defaults for StatsBuilderMixin.stats() . Merges Records.stats_defaults and orders.stats from settings . subplots class variable \u00b6 Subplots supported by Orders . Co nf ig( { \"plot\" : { \"title\" : \"Orders\" , \"yaxis_kwargs\" : { \"title\" : \"Price\" }, \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"tags\" : \"orders\" } } ) Returns Orders._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable Orders._subplots .","title":"orders"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders","text":"Base class for working with order records. Order records capture information on filled orders. Orders are mainly populated when simulating a portfolio and can be accessed as Portfolio.orders . >>> import pandas as pd >>> import numpy as np >>> from datetime import datetime , timedelta >>> import vectorbt as vbt >>> np . random . seed ( 42 ) >>> price = pd . DataFrame ({ ... 'a' : np . random . uniform ( 1 , 2 , size = 100 ), ... 'b' : np . random . uniform ( 1 , 2 , size = 100 ) ... }, index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 100 )]) >>> size = pd . DataFrame ({ ... 'a' : np . random . uniform ( - 1 , 1 , size = 100 ), ... 'b' : np . random . uniform ( - 1 , 1 , size = 100 ), ... }, index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 100 )]) >>> pf = vbt . Portfolio . from_orders ( price , size , fees = 0.01 , freq = 'd' ) >>> orders = pf . orders >>> orders . buy . count () a 58 b 51 Name: count, dtype: int64 >>> orders . sell . count () a 42 b 49 Name: count, dtype: int64","title":"vectorbt.portfolio.orders"},{"location":"api/portfolio/orders/#stats","text":"Hint See StatsBuilderMixin.stats() and Orders.metrics . >>> orders [ 'a' ] . stats () Start 2020-01-01 00:00:00 End 2020-04-09 00:00:00 Period 100 days 00:00:00 Total Records 100 Total Buy Orders 58 Total Sell Orders 42 Min Size 0.003033 Max Size 0.989877 Avg Size 0.508608 Avg Buy Size 0.468802 Avg Sell Size 0.563577 Avg Buy Price 1.437037 Avg Sell Price 1.515951 Total Fees 0.740177 Min Fees 0.000052 Max Fees 0.016224 Avg Fees 0.007402 Avg Buy Fees 0.006771 Avg Sell Fees 0.008273 Name: a, dtype: object StatsBuilderMixin.stats() also supports (re-)grouping: >>> orders . stats ( group_by = True ) Start 2020-01-01 00:00:00 End 2020-04-09 00:00:00 Period 100 days 00:00:00 Total Records 200 Total Buy Orders 109 Total Sell Orders 91 Min Size 0.003033 Max Size 0.989877 Avg Size 0.506279 Avg Buy Size 0.472504 Avg Sell Size 0.546735 Avg Buy Price 1.47336 Avg Sell Price 1.496759 Total Fees 1.483343 Min Fees 0.000052 Max Fees 0.018319 Avg Fees 0.007417 Avg Buy Fees 0.006881 Avg Sell Fees 0.008058 Name: group, dtype: object","title":"Stats"},{"location":"api/portfolio/orders/#plots","text":"Hint See PlotsBuilderMixin.plots() and Orders.subplots . Orders class has a single subplot based on Orders.plot() : >>> orders [ 'a' ] . plots ()","title":"Plots"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.orders_attach_field_config","text":"Config of fields to be attached to Orders . Co nf ig( { \"side\" : { \"attach_filters\" : true } } )","title":"orders_attach_field_config"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.orders_field_config","text":"Field config for Orders . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"col\" : \"int64\" , \"idx\" : \"int64\" , \"size\" : \"float64\" , \"price\" : \"float64\" , \"fees\" : \"float64\" , \"side\" : \"int64\" }, \"settings\" : { \"id\" : { \"title\" : \"Order Id\" }, \"size\" : { \"title\" : \"Size\" }, \"price\" : { \"title\" : \"Price\" }, \"fees\" : { \"title\" : \"Fees\" }, \"side\" : { \"title\" : \"Side\" , \"mapping\" : { \"Buy\" : 0 , \"Sell\" : 1 } } } } )","title":"orders_field_config"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders","text":"Extends Records for working with order records. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin Records RecordsWithFields StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() Records.apply() Records.apply_mask() Records.build_field_config_doc() Records.col_arr Records.col_mapper Records.config Records.count() Records.get_apply_mapping_arr() Records.get_by_col_idxs() Records.get_field_arr() Records.get_field_mapping() Records.get_field_name() Records.get_field_setting() Records.get_field_title() Records.get_map_field() Records.get_map_field_to_index() Records.id_arr Records.idx_arr Records.iloc Records.indexing_func_meta() Records.indexing_kwargs Records.is_sorted() Records.loc Records.map() Records.map_array() Records.map_field() Records.override_field_config_doc() Records.records Records.records_arr Records.records_readable Records.replace() Records.self_aliases Records.sort() Records.values Records.wrapper Records.writeable_attrs StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"Orders"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders.buy","text":"Records filtered by side == 0 .","title":"buy"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders.close","text":"Reference price such as close (optional).","title":"close"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders.col","text":"Mapped array of the field col .","title":"col"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders.fees","text":"Mapped array of the field fees .","title":"fees"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders.field_config","text":"Field config of Orders . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"col\" : \"int64\" , \"idx\" : \"int64\" , \"size\" : \"float64\" , \"price\" : \"float64\" , \"fees\" : \"float64\" , \"side\" : \"int64\" }, \"settings\" : { \"id\" : { \"name\" : \"id\" , \"title\" : \"Order Id\" }, \"col\" : { \"name\" : \"col\" , \"title\" : \"Column\" , \"mapping\" : \"columns\" }, \"idx\" : { \"name\" : \"idx\" , \"title\" : \"Timestamp\" , \"mapping\" : \"index\" }, \"size\" : { \"title\" : \"Size\" }, \"price\" : { \"title\" : \"Price\" }, \"fees\" : { \"title\" : \"Fees\" }, \"side\" : { \"title\" : \"Side\" , \"mapping\" : { \"Buy\" : 0 , \"Sell\" : 1 } } } } )","title":"field_config"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders.id","text":"Mapped array of the field id .","title":"id"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders.idx","text":"Mapped array of the field idx .","title":"idx"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders.indexing_func","text":"Orders . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on Orders .","title":"indexing_func()"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders.metrics","text":"Metrics supported by Orders . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function Orders.<lambda> at 0x7fac99054048>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function Orders.<lambda> at 0x7fac990540d0>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function Orders.<lambda> at 0x7fac99054158>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"total_records\" : { \"title\" : \"Total Records\" , \"calc_func\" : \"count\" , \"tags\" : \"records\" }, \"total_buy_orders\" : { \"title\" : \"Total Buy Orders\" , \"calc_func\" : \"buy.count\" , \"tags\" : [ \"orders\" , \"buy\" ] }, \"total_sell_orders\" : { \"title\" : \"Total Sell Orders\" , \"calc_func\" : \"sell.count\" , \"tags\" : [ \"orders\" , \"sell\" ] }, \"min_size\" : { \"title\" : \"Min Size\" , \"calc_func\" : \"size.min\" , \"tags\" : [ \"orders\" , \"size\" ] }, \"max_size\" : { \"title\" : \"Max Size\" , \"calc_func\" : \"size.max\" , \"tags\" : [ \"orders\" , \"size\" ] }, \"avg_size\" : { \"title\" : \"Avg Size\" , \"calc_func\" : \"size.mean\" , \"tags\" : [ \"orders\" , \"size\" ] }, \"avg_buy_size\" : { \"title\" : \"Avg Buy Size\" , \"calc_func\" : \"buy.size.mean\" , \"tags\" : [ \"orders\" , \"buy\" , \"size\" ] }, \"avg_sell_size\" : { \"title\" : \"Avg Sell Size\" , \"calc_func\" : \"sell.size.mean\" , \"tags\" : [ \"orders\" , \"sell\" , \"size\" ] }, \"avg_buy_price\" : { \"title\" : \"Avg Buy Price\" , \"calc_func\" : \"buy.price.mean\" , \"tags\" : [ \"orders\" , \"buy\" , \"price\" ] }, \"avg_sell_price\" : { \"title\" : \"Avg Sell Price\" , \"calc_func\" : \"sell.price.mean\" , \"tags\" : [ \"orders\" , \"sell\" , \"price\" ] }, \"total_fees\" : { \"title\" : \"Total Fees\" , \"calc_func\" : \"fees.sum\" , \"tags\" : [ \"orders\" , \"fees\" ] }, \"min_fees\" : { \"title\" : \"Min Fees\" , \"calc_func\" : \"fees.min\" , \"tags\" : [ \"orders\" , \"fees\" ] }, \"max_fees\" : { \"title\" : \"Max Fees\" , \"calc_func\" : \"fees.max\" , \"tags\" : [ \"orders\" , \"fees\" ] }, \"avg_fees\" : { \"title\" : \"Avg Fees\" , \"calc_func\" : \"fees.mean\" , \"tags\" : [ \"orders\" , \"fees\" ] }, \"avg_buy_fees\" : { \"title\" : \"Avg Buy Fees\" , \"calc_func\" : \"buy.fees.mean\" , \"tags\" : [ \"orders\" , \"buy\" , \"fees\" ] }, \"avg_sell_fees\" : { \"title\" : \"Avg Sell Fees\" , \"calc_func\" : \"sell.fees.mean\" , \"tags\" : [ \"orders\" , \"sell\" , \"fees\" ] } } ) Returns Orders._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable Orders._metrics .","title":"metrics"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders.plot","text":"Orders . plot ( column = None , close_trace_kwargs = None , buy_trace_kwargs = None , sell_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot orders. Args column :\u2002 str Name of the column to plot. close_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for Orders.close . buy_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Buy\" markers. sell_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Sell\" markers. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import pandas as pd >>> from datetime import datetime , timedelta >>> import vectorbt as vbt >>> price = pd . Series ([ 1. , 2. , 3. , 2. , 1. ], name = 'Price' ) >>> price . index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( len ( price ))] >>> size = pd . Series ([ 1. , 1. , 1. , 1. , - 1. ]) >>> orders = vbt . Portfolio . from_orders ( price , size ) . orders >>> orders . plot ()","title":"plot()"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders.plots_defaults","text":"Defaults for PlotsBuilderMixin.plots() . Merges Records.plots_defaults and orders.plots from settings .","title":"plots_defaults"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders.price","text":"Mapped array of the field price .","title":"price"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders.sell","text":"Records filtered by side == 1 .","title":"sell"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders.side","text":"Mapped array of the field side .","title":"side"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders.size","text":"Mapped array of the field size .","title":"size"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders.stats_defaults","text":"Defaults for StatsBuilderMixin.stats() . Merges Records.stats_defaults and orders.stats from settings .","title":"stats_defaults"},{"location":"api/portfolio/orders/#vectorbt.portfolio.orders.Orders.subplots","text":"Subplots supported by Orders . Co nf ig( { \"plot\" : { \"title\" : \"Orders\" , \"yaxis_kwargs\" : { \"title\" : \"Price\" }, \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"tags\" : \"orders\" } } ) Returns Orders._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable Orders._subplots .","title":"subplots"},{"location":"api/portfolio/trades/","text":"trades module \u00b6 Base class for working with trade records. Trade records capture information on trades. In vectorbt, a trade is a sequence of orders that starts with an opening order and optionally ends with a closing order. Every pair of opposite orders can be represented by a trade. Each trade has a PnL info attached to quickly assess its performance. An interesting effect of this representation is the ability to aggregate trades: if two or more trades are happening one after another in time, they can be aggregated into a bigger trade. This way, for example, single-order trades can be aggregated into positions; but also multiple positions can be aggregated into a single blob that reflects the performance of the entire symbol. Warning All classes return both closed AND open trades/positions, which may skew your performance results. To only consider closed trades/positions, you should explicitly query the closed attribute. Trade types \u00b6 There are three main types of trades. Entry trades \u00b6 An entry trade is created from each order that opens or adds to a position. For example, if we have a single large buy order and 100 smaller sell orders, we will see a single trade with the entry information copied from the buy order and the exit information being a size-weighted average over the exit information of all sell orders. On the other hand, if we have 100 smaller buy orders and a single sell order, we will see 100 trades, each with the entry information copied from the buy order and the exit information being a size-based fraction of the exit information of the sell order. Use EntryTrades.from_orders() to build entry trades from orders. Also available as Portfolio.entry_trades . Exit trades \u00b6 An exit trade is created from each order that closes or removes from a position. Use ExitTrades.from_orders() to build exit trades from orders. Also available as Portfolio.exit_trades . Positions \u00b6 A position is created from a sequence of entry or exit trades. Use Positions.from_trades() to build positions from entry or exit trades. Also available as Portfolio.positions . Example \u00b6 Increasing position: >>> import pandas as pd >>> import numpy as np >>> from datetime import datetime , timedelta >>> import vectorbt as vbt >>> # Entry trades >>> pf_kwargs = dict ( ... close = pd . Series ([ 1. , 2. , 3. , 4. , 5. ]), ... size = pd . Series ([ 1. , 1. , 1. , 1. , - 4. ]), ... fixed_fees = 1. ... ) >>> entry_trades = vbt . Portfolio . from_orders ( ** pf_kwargs ) . entry_trades >>> entry_trades . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 1.0 0 1.0 1.0 1 1 0 1.0 1 2.0 1.0 2 2 0 1.0 2 3.0 1.0 3 3 0 1.0 3 4.0 1.0 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 4 5.0 0.25 2.75 2.7500 Long Closed 1 4 5.0 0.25 1.75 0.8750 Long Closed 2 4 5.0 0.25 0.75 0.2500 Long Closed 3 4 5.0 0.25 -0.25 -0.0625 Long Closed Parent Id 0 0 1 0 2 0 3 0 >>> # Exit trades >>> exit_trades = vbt . Portfolio . from_orders ( ** pf_kwargs ) . exit_trades >>> exit_trades . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 4.0 0 2.5 4.0 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 4 5.0 1.0 5.0 0.5 Long Closed Parent Id 0 0 >>> # Positions >>> positions = vbt . Portfolio . from_orders ( ** pf_kwargs ) . positions >>> positions . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 4.0 0 2.5 4.0 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 4 5.0 1.0 5.0 0.5 Long Closed Parent Id 0 0 >>> entry_trades . pnl . sum () == exit_trades . pnl . sum () == positions . pnl . sum () True Decreasing position: >>> # Entry trades >>> pf_kwargs = dict ( ... close = pd . Series ([ 1. , 2. , 3. , 4. , 5. ]), ... size = pd . Series ([ 4. , - 1. , - 1. , - 1. , - 1. ]), ... fixed_fees = 1. ... ) >>> entry_trades = vbt . Portfolio . from_orders ( ** pf_kwargs ) . entry_trades >>> entry_trades . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 4.0 0 1.0 1.0 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 4 3.5 4.0 5.0 1.25 Long Closed Parent Id 0 0 >>> # Exit trades >>> exit_trades = vbt . Portfolio . from_orders ( ** pf_kwargs ) . exit_trades >>> exit_trades . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 1.0 0 1.0 0.25 1 1 0 1.0 0 1.0 0.25 2 2 0 1.0 0 1.0 0.25 3 3 0 1.0 0 1.0 0.25 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 1 2.0 1.0 -0.25 -0.25 Long Closed 1 2 3.0 1.0 0.75 0.75 Long Closed 2 3 4.0 1.0 1.75 1.75 Long Closed 3 4 5.0 1.0 2.75 2.75 Long Closed Parent Id 0 0 1 0 2 0 3 0 >>> # Positions >>> positions = vbt . Portfolio . from_orders ( ** pf_kwargs ) . positions >>> positions . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 4.0 0 1.0 1.0 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 4 3.5 4.0 5.0 1.25 Long Closed Parent Id 0 0 >>> entry_trades . pnl . sum () == exit_trades . pnl . sum () == positions . pnl . sum () True Multiple reversing positions: >>> # Entry trades >>> pf_kwargs = dict ( ... close = pd . Series ([ 1. , 2. , 3. , 4. , 5. ]), ... size = pd . Series ([ 1. , - 2. , 2. , - 2. , 1. ]), ... fixed_fees = 1. ... ) >>> entry_trades = vbt . Portfolio . from_orders ( ** pf_kwargs ) . entry_trades >>> entry_trades . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 1.0 0 1.0 1.0 1 1 0 1.0 1 2.0 0.5 2 2 0 1.0 2 3.0 0.5 3 3 0 1.0 3 4.0 0.5 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 1 2.0 0.5 -0.5 -0.500 Long Closed 1 2 3.0 0.5 -2.0 -1.000 Short Closed 2 3 4.0 0.5 0.0 0.000 Long Closed 3 4 5.0 1.0 -2.5 -0.625 Short Closed Parent Id 0 0 1 1 2 2 3 3 >>> # Exit trades >>> exit_trades = vbt . Portfolio . from_orders ( ** pf_kwargs ) . exit_trades >>> exit_trades . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 1.0 0 1.0 1.0 1 1 0 1.0 1 2.0 0.5 2 2 0 1.0 2 3.0 0.5 3 3 0 1.0 3 4.0 0.5 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 1 2.0 0.5 -0.5 -0.500 Long Closed 1 2 3.0 0.5 -2.0 -1.000 Short Closed 2 3 4.0 0.5 0.0 0.000 Long Closed 3 4 5.0 1.0 -2.5 -0.625 Short Closed Parent Id 0 0 1 1 2 2 3 3 >>> # Positions >>> positions = vbt . Portfolio . from_orders ( ** pf_kwargs ) . positions >>> positions . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 1.0 0 1.0 1.0 1 1 0 1.0 1 2.0 0.5 2 2 0 1.0 2 3.0 0.5 3 3 0 1.0 3 4.0 0.5 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 1 2.0 0.5 -0.5 -0.500 Long Closed 1 2 3.0 0.5 -2.0 -1.000 Short Closed 2 3 4.0 0.5 0.0 0.000 Long Closed 3 4 5.0 1.0 -2.5 -0.625 Short Closed Parent Id 0 0 1 1 2 2 3 3 >>> entry_trades . pnl . sum () == exit_trades . pnl . sum () == positions . pnl . sum () True Open position: >>> # Entry trades >>> pf_kwargs = dict ( ... close = pd . Series ([ 1. , 2. , 3. , 4. , 5. ]), ... size = pd . Series ([ 1. , 0. , 0. , 0. , 0. ]), ... fixed_fees = 1. ... ) >>> entry_trades = vbt . Portfolio . from_orders ( ** pf_kwargs ) . entry_trades >>> entry_trades . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 1.0 0 1.0 1.0 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 4 5.0 0.0 3.0 3.0 Long Open Parent Id 0 0 >>> # Exit trades >>> exit_trades = vbt . Portfolio . from_orders ( ** pf_kwargs ) . exit_trades >>> exit_trades . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 1.0 0 1.0 1.0 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 4 5.0 0.0 3.0 3.0 Long Open Parent Id 0 0 >>> # Positions >>> positions = vbt . Portfolio . from_orders ( ** pf_kwargs ) . positions >>> positions . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 1.0 0 1.0 1.0 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 4 5.0 0.0 3.0 3.0 Long Open Parent Id 0 0 >>> entry_trades . pnl . sum () == exit_trades . pnl . sum () == positions . pnl . sum () True Get trade count, trade PnL, and winning trade PnL: >>> price = pd . Series ([ 1. , 2. , 3. , 4. , 3. , 2. , 1. ]) >>> size = pd . Series ([ 1. , - 0.5 , - 0.5 , 2. , - 0.5 , - 0.5 , - 0.5 ]) >>> trades = vbt . Portfolio . from_orders ( price , size ) . trades >>> trades . count () 6 >>> trades . pnl . sum () -3.0 >>> trades . winning . count () 2 >>> trades . winning . pnl . sum () 1.5 Get count and PnL of trades with duration of more than 2 days: >>> mask = ( trades . records [ 'exit_idx' ] - trades . records [ 'entry_idx' ]) > 2 >>> trades_filtered = trades . apply_mask ( mask ) >>> trades_filtered . count () 2 >>> trades_filtered . pnl . sum () -3.0 Stats \u00b6 Hint See StatsBuilderMixin.stats() and Trades.metrics . >>> np . random . seed ( 42 ) >>> price = pd . DataFrame ({ ... 'a' : np . random . uniform ( 1 , 2 , size = 100 ), ... 'b' : np . random . uniform ( 1 , 2 , size = 100 ) ... }, index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 100 )]) >>> size = pd . DataFrame ({ ... 'a' : np . random . uniform ( - 1 , 1 , size = 100 ), ... 'b' : np . random . uniform ( - 1 , 1 , size = 100 ), ... }, index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 100 )]) >>> pf = vbt . Portfolio . from_orders ( price , size , fees = 0.01 , freq = 'd' ) >>> pf . trades [ 'a' ] . stats () Start 2020-01-01 00:00:00 End 2020-04-09 00:00:00 Period 100 days 00:00:00 First Trade Start 2020-01-01 00:00:00 Last Trade End 2020-04-09 00:00:00 Coverage 100 days 00:00:00 Overlap Coverage 97 days 00:00:00 Total Records 48 Total Long Trades 22 Total Short Trades 26 Total Closed Trades 47 Total Open Trades 1 Open Trade PnL -1.290981 Win Rate [%] 51.06383 Max Win Streak 3 Max Loss Streak 3 Best Trade [%] 43.326077 Worst Trade [%] -59.478304 Avg Winning Trade [%] 21.418522 Avg Losing Trade [%] -18.856792 Avg Winning Trade Duration 22 days 22:00:00 Avg Losing Trade Duration 29 days 01:02:36.521739130 Profit Factor 0.976634 Expectancy -0.001569 SQN -0.064929 Name: a, dtype: object Positions share almost identical metrics with trades: >>> pf . positions [ 'a' ] . stats () Start 2020-01-01 00:00:00 End 2020-04-09 00:00:00 Period 100 days 00:00:00 Coverage [%] 100.0 First Position Start 2020-01-01 00:00:00 Last Position End 2020-04-09 00:00:00 Total Records 3 Total Long Positions 2 Total Short Positions 1 Total Closed Positions 2 Total Open Positions 1 Open Position PnL -0.929746 Win Rate [%] 50.0 Max Win Streak 1 Max Loss Streak 1 Best Position [%] 39.498421 Worst Position [%] -3.32533 Avg Winning Position [%] 39.498421 Avg Losing Position [%] -3.32533 Avg Winning Position Duration 1 days 00:00:00 Avg Losing Position Duration 47 days 00:00:00 Profit Factor 0.261748 Expectancy -0.217492 SQN -0.585103 Name: a, dtype: object To also include open trades/positions when calculating metrics such as win rate, pass incl_open=True : >>> pf . trades [ 'a' ] . stats ( settings = dict ( incl_open = True )) Start 2020-01-01 00:00:00 End 2020-04-09 00:00:00 Period 100 days 00:00:00 First Trade Start 2020-01-01 00:00:00 Last Trade End 2020-04-09 00:00:00 Coverage 100 days 00:00:00 Overlap Coverage 97 days 00:00:00 Total Records 48 Total Long Trades 22 Total Short Trades 26 Total Closed Trades 47 Total Open Trades 1 Open Trade PnL -1.290981 Win Rate [%] 51.06383 Max Win Streak 3 Max Loss Streak 3 Best Trade [%] 43.326077 Worst Trade [%] -59.478304 Avg Winning Trade [%] 21.418522 Avg Losing Trade [%] -19.117677 Avg Winning Trade Duration 22 days 22:00:00 Avg Losing Trade Duration 30 days 00:00:00 Profit Factor 0.693135 Expectancy -0.028432 SQN -0.794284 Name: a, dtype: object StatsBuilderMixin.stats() also supports (re-)grouping: >>> pf . trades . stats ( group_by = True ) Start 2020-01-01 00:00:00 End 2020-04-09 00:00:00 Period 100 days 00:00:00 First Trade Start 2020-01-01 00:00:00 Last Trade End 2020-04-09 00:00:00 Coverage 100 days 00:00:00 Overlap Coverage 100 days 00:00:00 Total Records 104 Total Long Trades 32 Total Short Trades 72 Total Closed Trades 102 Total Open Trades 2 Open Trade PnL -1.790938 Win Rate [%] 46.078431 Max Win Streak 5 Max Loss Streak 5 Best Trade [%] 43.326077 Worst Trade [%] -87.793448 Avg Winning Trade [%] 19.023926 Avg Losing Trade [%] -20.605892 Avg Winning Trade Duration 24 days 08:40:51.063829787 Avg Losing Trade Duration 25 days 11:20:43.636363636 Profit Factor 0.909581 Expectancy -0.006035 SQN -0.365593 Name: group, dtype: object Plots \u00b6 Hint See PlotsBuilderMixin.plots() and Trades.subplots . Trades class has two subplots based on Trades.plot() and Trades.plot_pnl() : >>> pf . trades [ 'a' ] . plots ( settings = dict ( plot_zones = False )) . show_svg () entry_trades_field_config Config \u00b6 Field config for EntryTrades . Co nf ig( { \"settings\" : { \"id\" : { \"title\" : \"Entry Trade Id\" }, \"idx\" : { \"name\" : \"entry_idx\" } } } ) exit_trades_field_config Config \u00b6 Field config for ExitTrades . Co nf ig( { \"settings\" : { \"id\" : { \"title\" : \"Exit Trade Id\" } } } ) positions_field_config Config \u00b6 Field config for Positions . Co nf ig( { \"settings\" : { \"id\" : { \"title\" : \"Position Id\" }, \"parent_id\" : { \"title\" : \"Parent Id\" , \"ignore\" : true } } } ) trades_attach_field_config Config \u00b6 Config of fields to be attached to Trades . Co nf ig( { \"return\" : { \"attach\" : \"returns\" }, \"direction\" : { \"attach_filters\" : true }, \"status\" : { \"attach_filters\" : true , \"on_conflict\" : \"ignore\" } } ) trades_field_config Config \u00b6 Field config for Trades . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"col\" : \"int64\" , \"size\" : \"float64\" , \"entry_idx\" : \"int64\" , \"entry_price\" : \"float64\" , \"entry_fees\" : \"float64\" , \"exit_idx\" : \"int64\" , \"exit_price\" : \"float64\" , \"exit_fees\" : \"float64\" , \"pnl\" : \"float64\" , \"return\" : \"float64\" , \"direction\" : \"int64\" , \"status\" : \"int64\" , \"parent_id\" : \"int64\" }, \"settings\" : { \"id\" : { \"title\" : \"Trade Id\" }, \"idx\" : { \"name\" : \"exit_idx\" }, \"start_idx\" : { \"name\" : \"entry_idx\" }, \"end_idx\" : { \"name\" : \"exit_idx\" }, \"size\" : { \"title\" : \"Size\" }, \"entry_idx\" : { \"title\" : \"Entry Timestamp\" , \"mapping\" : \"index\" }, \"entry_price\" : { \"title\" : \"Avg Entry Price\" }, \"entry_fees\" : { \"title\" : \"Entry Fees\" }, \"exit_idx\" : { \"title\" : \"Exit Timestamp\" , \"mapping\" : \"index\" }, \"exit_price\" : { \"title\" : \"Avg Exit Price\" }, \"exit_fees\" : { \"title\" : \"Exit Fees\" }, \"pnl\" : { \"title\" : \"PnL\" }, \"return\" : { \"title\" : \"Return\" }, \"direction\" : { \"title\" : \"Direction\" , \"mapping\" : { \"Long\" : 0 , \"Short\" : 1 } }, \"status\" : { \"title\" : \"Status\" , \"mapping\" : { \"Open\" : 0 , \"Closed\" : 1 } }, \"parent_id\" : { \"title\" : \"Position Id\" } } } ) EntryTrades class \u00b6 Extends Trades for working with entry trade records. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin Ranges Records RecordsWithFields StatsBuilderMixin Trades Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() Ranges.avg_duration() Ranges.coverage() Ranges.max_duration() Ranges.to_mask() Records.apply() Records.apply_mask() Records.build_field_config_doc() Records.count() Records.get_apply_mapping_arr() Records.get_by_col_idxs() Records.get_field_arr() Records.get_field_mapping() Records.get_field_name() Records.get_field_setting() Records.get_field_title() Records.get_map_field() Records.get_map_field_to_index() Records.indexing_func_meta() Records.is_sorted() Records.map() Records.map_array() Records.map_field() Records.override_field_config_doc() Records.replace() Records.sort() RecordsWithFields.field_config StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Trades.close Trades.closed Trades.col Trades.col_arr Trades.col_mapper Trades.config Trades.direction Trades.duration Trades.end_idx Trades.entry_fees Trades.entry_idx Trades.entry_price Trades.exit_fees Trades.exit_idx Trades.exit_price Trades.expectancy() Trades.from_ts() Trades.id Trades.id_arr Trades.idx_arr Trades.iloc Trades.indexing_func() Trades.indexing_kwargs Trades.loc Trades.long Trades.losing Trades.losing_streak Trades.open Trades.parent_id Trades.plot() Trades.plot_pnl() Trades.plots_defaults Trades.pnl Trades.profit_factor() Trades.records Trades.records_arr Trades.records_readable Trades.returns Trades.self_aliases Trades.short Trades.size Trades.sqn() Trades.start_idx Trades.stats_defaults Trades.status Trades.ts Trades.values Trades.win_rate() Trades.winning Trades.winning_streak Trades.wrapper Trades.writeable_attrs Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() from_orders class method \u00b6 EntryTrades . from_orders ( orders , close = None , attach_close = True , ** kwargs ) Build EntryTrades from Orders . ExitTrades class \u00b6 Extends Trades for working with exit trade records. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin Ranges Records RecordsWithFields StatsBuilderMixin Trades Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() Ranges.avg_duration() Ranges.coverage() Ranges.max_duration() Ranges.to_mask() Records.apply() Records.apply_mask() Records.build_field_config_doc() Records.count() Records.get_apply_mapping_arr() Records.get_by_col_idxs() Records.get_field_arr() Records.get_field_mapping() Records.get_field_name() Records.get_field_setting() Records.get_field_title() Records.get_map_field() Records.get_map_field_to_index() Records.indexing_func_meta() Records.is_sorted() Records.map() Records.map_array() Records.map_field() Records.override_field_config_doc() Records.replace() Records.sort() RecordsWithFields.field_config StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Trades.close Trades.closed Trades.col Trades.col_arr Trades.col_mapper Trades.config Trades.direction Trades.duration Trades.end_idx Trades.entry_fees Trades.entry_idx Trades.entry_price Trades.exit_fees Trades.exit_idx Trades.exit_price Trades.expectancy() Trades.from_ts() Trades.id Trades.id_arr Trades.idx_arr Trades.iloc Trades.indexing_func() Trades.indexing_kwargs Trades.loc Trades.long Trades.losing Trades.losing_streak Trades.open Trades.parent_id Trades.plot() Trades.plot_pnl() Trades.plots_defaults Trades.pnl Trades.profit_factor() Trades.records Trades.records_arr Trades.records_readable Trades.returns Trades.self_aliases Trades.short Trades.size Trades.sqn() Trades.start_idx Trades.stats_defaults Trades.status Trades.ts Trades.values Trades.win_rate() Trades.winning Trades.winning_streak Trades.wrapper Trades.writeable_attrs Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() from_orders class method \u00b6 ExitTrades . from_orders ( orders , close = None , attach_close = True , ** kwargs ) Build ExitTrades from Orders . Positions class \u00b6 Extends Trades for working with position records. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin Ranges Records RecordsWithFields StatsBuilderMixin Trades Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() Ranges.avg_duration() Ranges.coverage() Ranges.max_duration() Ranges.to_mask() Records.apply() Records.apply_mask() Records.build_field_config_doc() Records.count() Records.get_apply_mapping_arr() Records.get_by_col_idxs() Records.get_field_arr() Records.get_field_mapping() Records.get_field_name() Records.get_field_setting() Records.get_field_title() Records.get_map_field() Records.get_map_field_to_index() Records.indexing_func_meta() Records.is_sorted() Records.map() Records.map_array() Records.map_field() Records.override_field_config_doc() Records.replace() Records.sort() RecordsWithFields.field_config StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Trades.close Trades.closed Trades.col Trades.col_arr Trades.col_mapper Trades.config Trades.direction Trades.duration Trades.end_idx Trades.entry_fees Trades.entry_idx Trades.entry_price Trades.exit_fees Trades.exit_idx Trades.exit_price Trades.expectancy() Trades.from_ts() Trades.id Trades.id_arr Trades.idx_arr Trades.iloc Trades.indexing_func() Trades.indexing_kwargs Trades.loc Trades.long Trades.losing Trades.losing_streak Trades.open Trades.parent_id Trades.plot() Trades.plot_pnl() Trades.plots_defaults Trades.pnl Trades.profit_factor() Trades.records Trades.records_arr Trades.records_readable Trades.returns Trades.self_aliases Trades.short Trades.size Trades.sqn() Trades.start_idx Trades.stats_defaults Trades.status Trades.ts Trades.values Trades.win_rate() Trades.winning Trades.winning_streak Trades.wrapper Trades.writeable_attrs Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() from_trades class method \u00b6 Positions . from_trades ( trades , close = None , attach_close = True , ** kwargs ) Build Positions from Trades . Trades class \u00b6 Extends Ranges for working with trade-like records, such as entry trades, exit trades, and positions. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin Ranges Records RecordsWithFields StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() Ranges.avg_duration() Ranges.closed Ranges.col Ranges.col_arr Ranges.col_mapper Ranges.config Ranges.coverage() Ranges.duration Ranges.end_idx Ranges.from_ts() Ranges.id Ranges.id_arr Ranges.idx_arr Ranges.iloc Ranges.indexing_kwargs Ranges.loc Ranges.max_duration() Ranges.open Ranges.records Ranges.records_arr Ranges.records_readable Ranges.self_aliases Ranges.start_idx Ranges.status Ranges.to_mask() Ranges.ts Ranges.values Ranges.wrapper Ranges.writeable_attrs Records.apply() Records.apply_mask() Records.build_field_config_doc() Records.count() Records.get_apply_mapping_arr() Records.get_by_col_idxs() Records.get_field_arr() Records.get_field_mapping() Records.get_field_name() Records.get_field_setting() Records.get_field_title() Records.get_map_field() Records.get_map_field_to_index() Records.indexing_func_meta() Records.is_sorted() Records.map() Records.map_array() Records.map_field() Records.override_field_config_doc() Records.replace() Records.sort() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses EntryTrades ExitTrades Positions close property \u00b6 Reference price such as close (optional). direction method \u00b6 Mapped array of the field direction . entry_fees method \u00b6 Mapped array of the field entry_fees . entry_idx method \u00b6 Mapped array of the field entry_idx . entry_price method \u00b6 Mapped array of the field entry_price . exit_fees method \u00b6 Mapped array of the field exit_fees . exit_idx method \u00b6 Mapped array of the field exit_idx . exit_price method \u00b6 Mapped array of the field exit_price . expectancy method \u00b6 Trades . expectancy ( group_by = None , wrap_kwargs = None ) Average profitability. field_config class variable \u00b6 Field config of Trades . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"col\" : \"int64\" , \"size\" : \"float64\" , \"entry_idx\" : \"int64\" , \"entry_price\" : \"float64\" , \"entry_fees\" : \"float64\" , \"exit_idx\" : \"int64\" , \"exit_price\" : \"float64\" , \"exit_fees\" : \"float64\" , \"pnl\" : \"float64\" , \"return\" : \"float64\" , \"direction\" : \"int64\" , \"status\" : \"int64\" , \"parent_id\" : \"int64\" }, \"settings\" : { \"id\" : { \"name\" : \"id\" , \"title\" : \"Trade Id\" }, \"col\" : { \"name\" : \"col\" , \"title\" : \"Column\" , \"mapping\" : \"columns\" }, \"idx\" : { \"name\" : \"exit_idx\" , \"title\" : \"Timestamp\" , \"mapping\" : \"index\" }, \"start_idx\" : { \"title\" : \"Start Timestamp\" , \"mapping\" : \"index\" , \"name\" : \"entry_idx\" }, \"end_idx\" : { \"title\" : \"End Timestamp\" , \"mapping\" : \"index\" , \"name\" : \"exit_idx\" }, \"status\" : { \"title\" : \"Status\" , \"mapping\" : { \"Open\" : 0 , \"Closed\" : 1 } }, \"size\" : { \"title\" : \"Size\" }, \"entry_idx\" : { \"title\" : \"Entry Timestamp\" , \"mapping\" : \"index\" }, \"entry_price\" : { \"title\" : \"Avg Entry Price\" }, \"entry_fees\" : { \"title\" : \"Entry Fees\" }, \"exit_idx\" : { \"title\" : \"Exit Timestamp\" , \"mapping\" : \"index\" }, \"exit_price\" : { \"title\" : \"Avg Exit Price\" }, \"exit_fees\" : { \"title\" : \"Exit Fees\" }, \"pnl\" : { \"title\" : \"PnL\" }, \"return\" : { \"title\" : \"Return\" }, \"direction\" : { \"title\" : \"Direction\" , \"mapping\" : { \"Long\" : 0 , \"Short\" : 1 } }, \"parent_id\" : { \"title\" : \"Position Id\" } } } ) indexing_func method \u00b6 Trades . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on Trades . long method \u00b6 Records filtered by direction == 0 . losing method \u00b6 Losing trades. losing_streak method \u00b6 Losing streak at each trade in the current column. See trade_losing_streak_nb() . metrics class variable \u00b6 Metrics supported by Trades . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function Trades.<lambda> at 0x7fac99068378>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function Trades.<lambda> at 0x7fac99068400>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function Trades.<lambda> at 0x7fac99068488>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"first_trade_start\" : { \"title\" : \"First Trade Start\" , \"calc_func\" : \"entry_idx.nth\" , \"n\" : 0 , \"wrap_kwargs\" : { \"to_index\" : true }, \"tags\" : [ \"trades\" , \"index\" ] }, \"last_trade_end\" : { \"title\" : \"Last Trade End\" , \"calc_func\" : \"exit_idx.nth\" , \"n\" : -1 , \"wrap_kwargs\" : { \"to_index\" : true }, \"tags\" : [ \"trades\" , \"index\" ] }, \"coverage\" : { \"title\" : \"Coverage\" , \"calc_func\" : \"coverage\" , \"overlapping\" : false , \"normalize\" : false , \"apply_to_timedelta\" : true , \"tags\" : [ \"ranges\" , \"coverage\" ] }, \"overlap_coverage\" : { \"title\" : \"Overlap Coverage\" , \"calc_func\" : \"coverage\" , \"overlapping\" : true , \"normalize\" : false , \"apply_to_timedelta\" : true , \"tags\" : [ \"ranges\" , \"coverage\" ] }, \"total_records\" : { \"title\" : \"Total Records\" , \"calc_func\" : \"count\" , \"tags\" : \"records\" }, \"total_long_trades\" : { \"title\" : \"Total Long Trades\" , \"calc_func\" : \"long.count\" , \"tags\" : [ \"trades\" , \"long\" ] }, \"total_short_trades\" : { \"title\" : \"Total Short Trades\" , \"calc_func\" : \"short.count\" , \"tags\" : [ \"trades\" , \"short\" ] }, \"total_closed_trades\" : { \"title\" : \"Total Closed Trades\" , \"calc_func\" : \"closed.count\" , \"tags\" : [ \"trades\" , \"closed\" ] }, \"total_open_trades\" : { \"title\" : \"Total Open Trades\" , \"calc_func\" : \"open.count\" , \"tags\" : [ \"trades\" , \"open\" ] }, \"open_trade_pnl\" : { \"title\" : \"Open Trade PnL\" , \"calc_func\" : \"open.pnl.sum\" , \"tags\" : [ \"trades\" , \"open\" ] }, \"win_rate\" : { \"title\" : \"Win Rate [%]\" , \"calc_func\" : \"closed.win_rate\" , \"post_calc_func\" : \"<function Trades.<lambda> at 0x7fac99068510>\" , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags]\\\", mapping={})\" }, \"winning_streak\" : { \"title\" : \"Max Win Streak\" , \"calc_func\" : \"RepEval(expression=\\\"'winning_streak.max' if incl_open else 'closed.winning_streak.max'\\\", mapping={})\" , \"wrap_kwargs\" : { \"dtype\" : \"Int64\" }, \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags, 'streak']\\\", mapping={})\" }, \"losing_streak\" : { \"title\" : \"Max Loss Streak\" , \"calc_func\" : \"RepEval(expression=\\\"'losing_streak.max' if incl_open else 'closed.losing_streak.max'\\\", mapping={})\" , \"wrap_kwargs\" : { \"dtype\" : \"Int64\" }, \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags, 'streak']\\\", mapping={})\" }, \"best_trade\" : { \"title\" : \"Best Trade [%]\" , \"calc_func\" : \"RepEval(expression=\\\"'returns.max' if incl_open else 'closed.returns.max'\\\", mapping={})\" , \"post_calc_func\" : \"<function Trades.<lambda> at 0x7fac99068598>\" , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags]\\\", mapping={})\" }, \"worst_trade\" : { \"title\" : \"Worst Trade [%]\" , \"calc_func\" : \"RepEval(expression=\\\"'returns.min' if incl_open else 'closed.returns.min'\\\", mapping={})\" , \"post_calc_func\" : \"<function Trades.<lambda> at 0x7fac99068620>\" , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags]\\\", mapping={})\" }, \"avg_winning_trade\" : { \"title\" : \"Avg Winning Trade [%]\" , \"calc_func\" : \"RepEval(expression=\\\"'winning.returns.mean' if incl_open else 'closed.winning.returns.mean'\\\", mapping={})\" , \"post_calc_func\" : \"<function Trades.<lambda> at 0x7fac990686a8>\" , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags, 'winning']\\\", mapping={})\" }, \"avg_losing_trade\" : { \"title\" : \"Avg Losing Trade [%]\" , \"calc_func\" : \"RepEval(expression=\\\"'losing.returns.mean' if incl_open else 'closed.losing.returns.mean'\\\", mapping={})\" , \"post_calc_func\" : \"<function Trades.<lambda> at 0x7fac99068730>\" , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags, 'losing']\\\", mapping={})\" }, \"avg_winning_trade_duration\" : { \"title\" : \"Avg Winning Trade Duration\" , \"calc_func\" : \"RepEval(expression=\\\"'winning.avg_duration' if incl_open else 'closed.winning.avg_duration'\\\", mapping={})\" , \"fill_wrap_kwargs\" : true , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags, 'winning', 'duration']\\\", mapping={})\" }, \"avg_losing_trade_duration\" : { \"title\" : \"Avg Losing Trade Duration\" , \"calc_func\" : \"RepEval(expression=\\\"'losing.avg_duration' if incl_open else 'closed.losing.avg_duration'\\\", mapping={})\" , \"fill_wrap_kwargs\" : true , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags, 'losing', 'duration']\\\", mapping={})\" }, \"profit_factor\" : { \"title\" : \"Profit Factor\" , \"calc_func\" : \"RepEval(expression=\\\"'profit_factor' if incl_open else 'closed.profit_factor'\\\", mapping={})\" , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags]\\\", mapping={})\" }, \"expectancy\" : { \"title\" : \"Expectancy\" , \"calc_func\" : \"RepEval(expression=\\\"'expectancy' if incl_open else 'closed.expectancy'\\\", mapping={})\" , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags]\\\", mapping={})\" }, \"sqn\" : { \"title\" : \"SQN\" , \"calc_func\" : \"RepEval(expression=\\\"'sqn' if incl_open else 'closed.sqn'\\\", mapping={})\" , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags]\\\", mapping={})\" } } ) Returns Trades._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable Trades._metrics . parent_id method \u00b6 Mapped array of the field parent_id . plot method \u00b6 Trades . plot ( column = None , plot_zones = True , close_trace_kwargs = None , entry_trace_kwargs = None , exit_trace_kwargs = None , exit_profit_trace_kwargs = None , exit_loss_trace_kwargs = None , active_trace_kwargs = None , profit_shape_kwargs = None , loss_shape_kwargs = None , add_trace_kwargs = None , xref = 'x' , yref = 'y' , fig = None , ** layout_kwargs ) Plot orders. Args column :\u2002 str Name of the column to plot. plot_zones :\u2002 bool Whether to plot zones. Set to False if there are many trades within one position. close_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for Trades.close . entry_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Entry\" markers. exit_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Exit\" markers. exit_profit_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Exit - Profit\" markers. exit_loss_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Exit - Loss\" markers. active_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Active\" markers. profit_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for profit zones. loss_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for loss zones. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import pandas as pd >>> from datetime import datetime , timedelta >>> import vectorbt as vbt >>> price = pd . Series ([ 1. , 2. , 3. , 4. , 3. , 2. , 1. ], name = 'Price' ) >>> price . index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( len ( price ))] >>> orders = pd . Series ([ 1. , - 0.5 , - 0.5 , 2. , - 0.5 , - 0.5 , - 0.5 ]) >>> pf = vbt . Portfolio . from_orders ( price , orders ) >>> pf . trades . plot () plot_pnl method \u00b6 Trades . plot_pnl ( column = None , pct_scale = True , marker_size_range = ( 7 , 14 ), opacity_range = ( 0.75 , 0.9 ), closed_profit_trace_kwargs = None , closed_loss_trace_kwargs = None , open_trace_kwargs = None , hline_shape_kwargs = None , add_trace_kwargs = None , xref = 'x' , yref = 'y' , fig = None , ** layout_kwargs ) Plot trade PnL and returns. Args column :\u2002 str Name of the column to plot. pct_scale :\u2002 bool Whether to set y-axis to Trades.returns , otherwise to Trades.pnl . marker_size_range :\u2002 tuple Range of marker size. opacity_range :\u2002 tuple Range of marker opacity. closed_profit_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Closed - Profit\" markers. closed_loss_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Closed - Loss\" markers. open_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Open\" markers. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import pandas as pd >>> from datetime import datetime , timedelta >>> import vectorbt as vbt >>> price = pd . Series ([ 1. , 2. , 3. , 4. , 3. , 2. , 1. ]) >>> price . index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( len ( price ))] >>> orders = pd . Series ([ 1. , - 0.5 , - 0.5 , 2. , - 0.5 , - 0.5 , - 0.5 ]) >>> pf = vbt . Portfolio . from_orders ( price , orders ) >>> pf . trades . plot_pnl () plots_defaults property \u00b6 Defaults for PlotsBuilderMixin.plots() . Merges Ranges.plots_defaults and trades.plots from settings . pnl method \u00b6 Mapped array of the field pnl . profit_factor method \u00b6 Trades . profit_factor ( group_by = None , wrap_kwargs = None ) Profit factor. returns method \u00b6 Mapped array of the field return . short method \u00b6 Records filtered by direction == 1 . size method \u00b6 Mapped array of the field size . sqn method \u00b6 Trades . sqn ( group_by = None , wrap_kwargs = None ) System Quality Number (SQN). stats_defaults property \u00b6 Defaults for StatsBuilderMixin.stats() . Merges Ranges.stats_defaults and trades.stats from settings . subplots class variable \u00b6 Subplots supported by Trades . Co nf ig( { \"plot\" : { \"title\" : \"Trades\" , \"yaxis_kwargs\" : { \"title\" : \"Price\" }, \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"tags\" : \"trades\" }, \"plot_pnl\" : { \"title\" : \"Trade PnL\" , \"yaxis_kwargs\" : { \"title\" : \"Trade PnL\" }, \"check_is_not_grouped\" : true , \"plot_func\" : \"plot_pnl\" , \"tags\" : \"trades\" } } ) Returns Trades._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable Trades._subplots . win_rate method \u00b6 Trades . win_rate ( group_by = None , wrap_kwargs = None ) Rate of winning trades. winning method \u00b6 Winning trades. winning_streak method \u00b6 Winning streak at each trade in the current column. See trade_winning_streak_nb() .","title":"trades"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades","text":"Base class for working with trade records. Trade records capture information on trades. In vectorbt, a trade is a sequence of orders that starts with an opening order and optionally ends with a closing order. Every pair of opposite orders can be represented by a trade. Each trade has a PnL info attached to quickly assess its performance. An interesting effect of this representation is the ability to aggregate trades: if two or more trades are happening one after another in time, they can be aggregated into a bigger trade. This way, for example, single-order trades can be aggregated into positions; but also multiple positions can be aggregated into a single blob that reflects the performance of the entire symbol. Warning All classes return both closed AND open trades/positions, which may skew your performance results. To only consider closed trades/positions, you should explicitly query the closed attribute.","title":"vectorbt.portfolio.trades"},{"location":"api/portfolio/trades/#trade-types","text":"There are three main types of trades.","title":"Trade types"},{"location":"api/portfolio/trades/#entry-trades","text":"An entry trade is created from each order that opens or adds to a position. For example, if we have a single large buy order and 100 smaller sell orders, we will see a single trade with the entry information copied from the buy order and the exit information being a size-weighted average over the exit information of all sell orders. On the other hand, if we have 100 smaller buy orders and a single sell order, we will see 100 trades, each with the entry information copied from the buy order and the exit information being a size-based fraction of the exit information of the sell order. Use EntryTrades.from_orders() to build entry trades from orders. Also available as Portfolio.entry_trades .","title":"Entry trades"},{"location":"api/portfolio/trades/#exit-trades","text":"An exit trade is created from each order that closes or removes from a position. Use ExitTrades.from_orders() to build exit trades from orders. Also available as Portfolio.exit_trades .","title":"Exit trades"},{"location":"api/portfolio/trades/#positions","text":"A position is created from a sequence of entry or exit trades. Use Positions.from_trades() to build positions from entry or exit trades. Also available as Portfolio.positions .","title":"Positions"},{"location":"api/portfolio/trades/#example","text":"Increasing position: >>> import pandas as pd >>> import numpy as np >>> from datetime import datetime , timedelta >>> import vectorbt as vbt >>> # Entry trades >>> pf_kwargs = dict ( ... close = pd . Series ([ 1. , 2. , 3. , 4. , 5. ]), ... size = pd . Series ([ 1. , 1. , 1. , 1. , - 4. ]), ... fixed_fees = 1. ... ) >>> entry_trades = vbt . Portfolio . from_orders ( ** pf_kwargs ) . entry_trades >>> entry_trades . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 1.0 0 1.0 1.0 1 1 0 1.0 1 2.0 1.0 2 2 0 1.0 2 3.0 1.0 3 3 0 1.0 3 4.0 1.0 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 4 5.0 0.25 2.75 2.7500 Long Closed 1 4 5.0 0.25 1.75 0.8750 Long Closed 2 4 5.0 0.25 0.75 0.2500 Long Closed 3 4 5.0 0.25 -0.25 -0.0625 Long Closed Parent Id 0 0 1 0 2 0 3 0 >>> # Exit trades >>> exit_trades = vbt . Portfolio . from_orders ( ** pf_kwargs ) . exit_trades >>> exit_trades . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 4.0 0 2.5 4.0 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 4 5.0 1.0 5.0 0.5 Long Closed Parent Id 0 0 >>> # Positions >>> positions = vbt . Portfolio . from_orders ( ** pf_kwargs ) . positions >>> positions . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 4.0 0 2.5 4.0 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 4 5.0 1.0 5.0 0.5 Long Closed Parent Id 0 0 >>> entry_trades . pnl . sum () == exit_trades . pnl . sum () == positions . pnl . sum () True Decreasing position: >>> # Entry trades >>> pf_kwargs = dict ( ... close = pd . Series ([ 1. , 2. , 3. , 4. , 5. ]), ... size = pd . Series ([ 4. , - 1. , - 1. , - 1. , - 1. ]), ... fixed_fees = 1. ... ) >>> entry_trades = vbt . Portfolio . from_orders ( ** pf_kwargs ) . entry_trades >>> entry_trades . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 4.0 0 1.0 1.0 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 4 3.5 4.0 5.0 1.25 Long Closed Parent Id 0 0 >>> # Exit trades >>> exit_trades = vbt . Portfolio . from_orders ( ** pf_kwargs ) . exit_trades >>> exit_trades . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 1.0 0 1.0 0.25 1 1 0 1.0 0 1.0 0.25 2 2 0 1.0 0 1.0 0.25 3 3 0 1.0 0 1.0 0.25 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 1 2.0 1.0 -0.25 -0.25 Long Closed 1 2 3.0 1.0 0.75 0.75 Long Closed 2 3 4.0 1.0 1.75 1.75 Long Closed 3 4 5.0 1.0 2.75 2.75 Long Closed Parent Id 0 0 1 0 2 0 3 0 >>> # Positions >>> positions = vbt . Portfolio . from_orders ( ** pf_kwargs ) . positions >>> positions . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 4.0 0 1.0 1.0 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 4 3.5 4.0 5.0 1.25 Long Closed Parent Id 0 0 >>> entry_trades . pnl . sum () == exit_trades . pnl . sum () == positions . pnl . sum () True Multiple reversing positions: >>> # Entry trades >>> pf_kwargs = dict ( ... close = pd . Series ([ 1. , 2. , 3. , 4. , 5. ]), ... size = pd . Series ([ 1. , - 2. , 2. , - 2. , 1. ]), ... fixed_fees = 1. ... ) >>> entry_trades = vbt . Portfolio . from_orders ( ** pf_kwargs ) . entry_trades >>> entry_trades . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 1.0 0 1.0 1.0 1 1 0 1.0 1 2.0 0.5 2 2 0 1.0 2 3.0 0.5 3 3 0 1.0 3 4.0 0.5 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 1 2.0 0.5 -0.5 -0.500 Long Closed 1 2 3.0 0.5 -2.0 -1.000 Short Closed 2 3 4.0 0.5 0.0 0.000 Long Closed 3 4 5.0 1.0 -2.5 -0.625 Short Closed Parent Id 0 0 1 1 2 2 3 3 >>> # Exit trades >>> exit_trades = vbt . Portfolio . from_orders ( ** pf_kwargs ) . exit_trades >>> exit_trades . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 1.0 0 1.0 1.0 1 1 0 1.0 1 2.0 0.5 2 2 0 1.0 2 3.0 0.5 3 3 0 1.0 3 4.0 0.5 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 1 2.0 0.5 -0.5 -0.500 Long Closed 1 2 3.0 0.5 -2.0 -1.000 Short Closed 2 3 4.0 0.5 0.0 0.000 Long Closed 3 4 5.0 1.0 -2.5 -0.625 Short Closed Parent Id 0 0 1 1 2 2 3 3 >>> # Positions >>> positions = vbt . Portfolio . from_orders ( ** pf_kwargs ) . positions >>> positions . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 1.0 0 1.0 1.0 1 1 0 1.0 1 2.0 0.5 2 2 0 1.0 2 3.0 0.5 3 3 0 1.0 3 4.0 0.5 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 1 2.0 0.5 -0.5 -0.500 Long Closed 1 2 3.0 0.5 -2.0 -1.000 Short Closed 2 3 4.0 0.5 0.0 0.000 Long Closed 3 4 5.0 1.0 -2.5 -0.625 Short Closed Parent Id 0 0 1 1 2 2 3 3 >>> entry_trades . pnl . sum () == exit_trades . pnl . sum () == positions . pnl . sum () True Open position: >>> # Entry trades >>> pf_kwargs = dict ( ... close = pd . Series ([ 1. , 2. , 3. , 4. , 5. ]), ... size = pd . Series ([ 1. , 0. , 0. , 0. , 0. ]), ... fixed_fees = 1. ... ) >>> entry_trades = vbt . Portfolio . from_orders ( ** pf_kwargs ) . entry_trades >>> entry_trades . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 1.0 0 1.0 1.0 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 4 5.0 0.0 3.0 3.0 Long Open Parent Id 0 0 >>> # Exit trades >>> exit_trades = vbt . Portfolio . from_orders ( ** pf_kwargs ) . exit_trades >>> exit_trades . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 1.0 0 1.0 1.0 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 4 5.0 0.0 3.0 3.0 Long Open Parent Id 0 0 >>> # Positions >>> positions = vbt . Portfolio . from_orders ( ** pf_kwargs ) . positions >>> positions . records_readable Trade Id Column Size Entry Timestamp Avg Entry Price Entry Fees \\ 0 0 0 1.0 0 1.0 1.0 Exit Timestamp Avg Exit Price Exit Fees PnL Return Direction Status \\ 0 4 5.0 0.0 3.0 3.0 Long Open Parent Id 0 0 >>> entry_trades . pnl . sum () == exit_trades . pnl . sum () == positions . pnl . sum () True Get trade count, trade PnL, and winning trade PnL: >>> price = pd . Series ([ 1. , 2. , 3. , 4. , 3. , 2. , 1. ]) >>> size = pd . Series ([ 1. , - 0.5 , - 0.5 , 2. , - 0.5 , - 0.5 , - 0.5 ]) >>> trades = vbt . Portfolio . from_orders ( price , size ) . trades >>> trades . count () 6 >>> trades . pnl . sum () -3.0 >>> trades . winning . count () 2 >>> trades . winning . pnl . sum () 1.5 Get count and PnL of trades with duration of more than 2 days: >>> mask = ( trades . records [ 'exit_idx' ] - trades . records [ 'entry_idx' ]) > 2 >>> trades_filtered = trades . apply_mask ( mask ) >>> trades_filtered . count () 2 >>> trades_filtered . pnl . sum () -3.0","title":"Example"},{"location":"api/portfolio/trades/#stats","text":"Hint See StatsBuilderMixin.stats() and Trades.metrics . >>> np . random . seed ( 42 ) >>> price = pd . DataFrame ({ ... 'a' : np . random . uniform ( 1 , 2 , size = 100 ), ... 'b' : np . random . uniform ( 1 , 2 , size = 100 ) ... }, index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 100 )]) >>> size = pd . DataFrame ({ ... 'a' : np . random . uniform ( - 1 , 1 , size = 100 ), ... 'b' : np . random . uniform ( - 1 , 1 , size = 100 ), ... }, index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 100 )]) >>> pf = vbt . Portfolio . from_orders ( price , size , fees = 0.01 , freq = 'd' ) >>> pf . trades [ 'a' ] . stats () Start 2020-01-01 00:00:00 End 2020-04-09 00:00:00 Period 100 days 00:00:00 First Trade Start 2020-01-01 00:00:00 Last Trade End 2020-04-09 00:00:00 Coverage 100 days 00:00:00 Overlap Coverage 97 days 00:00:00 Total Records 48 Total Long Trades 22 Total Short Trades 26 Total Closed Trades 47 Total Open Trades 1 Open Trade PnL -1.290981 Win Rate [%] 51.06383 Max Win Streak 3 Max Loss Streak 3 Best Trade [%] 43.326077 Worst Trade [%] -59.478304 Avg Winning Trade [%] 21.418522 Avg Losing Trade [%] -18.856792 Avg Winning Trade Duration 22 days 22:00:00 Avg Losing Trade Duration 29 days 01:02:36.521739130 Profit Factor 0.976634 Expectancy -0.001569 SQN -0.064929 Name: a, dtype: object Positions share almost identical metrics with trades: >>> pf . positions [ 'a' ] . stats () Start 2020-01-01 00:00:00 End 2020-04-09 00:00:00 Period 100 days 00:00:00 Coverage [%] 100.0 First Position Start 2020-01-01 00:00:00 Last Position End 2020-04-09 00:00:00 Total Records 3 Total Long Positions 2 Total Short Positions 1 Total Closed Positions 2 Total Open Positions 1 Open Position PnL -0.929746 Win Rate [%] 50.0 Max Win Streak 1 Max Loss Streak 1 Best Position [%] 39.498421 Worst Position [%] -3.32533 Avg Winning Position [%] 39.498421 Avg Losing Position [%] -3.32533 Avg Winning Position Duration 1 days 00:00:00 Avg Losing Position Duration 47 days 00:00:00 Profit Factor 0.261748 Expectancy -0.217492 SQN -0.585103 Name: a, dtype: object To also include open trades/positions when calculating metrics such as win rate, pass incl_open=True : >>> pf . trades [ 'a' ] . stats ( settings = dict ( incl_open = True )) Start 2020-01-01 00:00:00 End 2020-04-09 00:00:00 Period 100 days 00:00:00 First Trade Start 2020-01-01 00:00:00 Last Trade End 2020-04-09 00:00:00 Coverage 100 days 00:00:00 Overlap Coverage 97 days 00:00:00 Total Records 48 Total Long Trades 22 Total Short Trades 26 Total Closed Trades 47 Total Open Trades 1 Open Trade PnL -1.290981 Win Rate [%] 51.06383 Max Win Streak 3 Max Loss Streak 3 Best Trade [%] 43.326077 Worst Trade [%] -59.478304 Avg Winning Trade [%] 21.418522 Avg Losing Trade [%] -19.117677 Avg Winning Trade Duration 22 days 22:00:00 Avg Losing Trade Duration 30 days 00:00:00 Profit Factor 0.693135 Expectancy -0.028432 SQN -0.794284 Name: a, dtype: object StatsBuilderMixin.stats() also supports (re-)grouping: >>> pf . trades . stats ( group_by = True ) Start 2020-01-01 00:00:00 End 2020-04-09 00:00:00 Period 100 days 00:00:00 First Trade Start 2020-01-01 00:00:00 Last Trade End 2020-04-09 00:00:00 Coverage 100 days 00:00:00 Overlap Coverage 100 days 00:00:00 Total Records 104 Total Long Trades 32 Total Short Trades 72 Total Closed Trades 102 Total Open Trades 2 Open Trade PnL -1.790938 Win Rate [%] 46.078431 Max Win Streak 5 Max Loss Streak 5 Best Trade [%] 43.326077 Worst Trade [%] -87.793448 Avg Winning Trade [%] 19.023926 Avg Losing Trade [%] -20.605892 Avg Winning Trade Duration 24 days 08:40:51.063829787 Avg Losing Trade Duration 25 days 11:20:43.636363636 Profit Factor 0.909581 Expectancy -0.006035 SQN -0.365593 Name: group, dtype: object","title":"Stats"},{"location":"api/portfolio/trades/#plots","text":"Hint See PlotsBuilderMixin.plots() and Trades.subplots . Trades class has two subplots based on Trades.plot() and Trades.plot_pnl() : >>> pf . trades [ 'a' ] . plots ( settings = dict ( plot_zones = False )) . show_svg ()","title":"Plots"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.entry_trades_field_config","text":"Field config for EntryTrades . Co nf ig( { \"settings\" : { \"id\" : { \"title\" : \"Entry Trade Id\" }, \"idx\" : { \"name\" : \"entry_idx\" } } } )","title":"entry_trades_field_config"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.exit_trades_field_config","text":"Field config for ExitTrades . Co nf ig( { \"settings\" : { \"id\" : { \"title\" : \"Exit Trade Id\" } } } )","title":"exit_trades_field_config"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.positions_field_config","text":"Field config for Positions . Co nf ig( { \"settings\" : { \"id\" : { \"title\" : \"Position Id\" }, \"parent_id\" : { \"title\" : \"Parent Id\" , \"ignore\" : true } } } )","title":"positions_field_config"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.trades_attach_field_config","text":"Config of fields to be attached to Trades . Co nf ig( { \"return\" : { \"attach\" : \"returns\" }, \"direction\" : { \"attach_filters\" : true }, \"status\" : { \"attach_filters\" : true , \"on_conflict\" : \"ignore\" } } )","title":"trades_attach_field_config"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.trades_field_config","text":"Field config for Trades . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"col\" : \"int64\" , \"size\" : \"float64\" , \"entry_idx\" : \"int64\" , \"entry_price\" : \"float64\" , \"entry_fees\" : \"float64\" , \"exit_idx\" : \"int64\" , \"exit_price\" : \"float64\" , \"exit_fees\" : \"float64\" , \"pnl\" : \"float64\" , \"return\" : \"float64\" , \"direction\" : \"int64\" , \"status\" : \"int64\" , \"parent_id\" : \"int64\" }, \"settings\" : { \"id\" : { \"title\" : \"Trade Id\" }, \"idx\" : { \"name\" : \"exit_idx\" }, \"start_idx\" : { \"name\" : \"entry_idx\" }, \"end_idx\" : { \"name\" : \"exit_idx\" }, \"size\" : { \"title\" : \"Size\" }, \"entry_idx\" : { \"title\" : \"Entry Timestamp\" , \"mapping\" : \"index\" }, \"entry_price\" : { \"title\" : \"Avg Entry Price\" }, \"entry_fees\" : { \"title\" : \"Entry Fees\" }, \"exit_idx\" : { \"title\" : \"Exit Timestamp\" , \"mapping\" : \"index\" }, \"exit_price\" : { \"title\" : \"Avg Exit Price\" }, \"exit_fees\" : { \"title\" : \"Exit Fees\" }, \"pnl\" : { \"title\" : \"PnL\" }, \"return\" : { \"title\" : \"Return\" }, \"direction\" : { \"title\" : \"Direction\" , \"mapping\" : { \"Long\" : 0 , \"Short\" : 1 } }, \"status\" : { \"title\" : \"Status\" , \"mapping\" : { \"Open\" : 0 , \"Closed\" : 1 } }, \"parent_id\" : { \"title\" : \"Position Id\" } } } )","title":"trades_field_config"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.EntryTrades","text":"Extends Trades for working with entry trade records. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin Ranges Records RecordsWithFields StatsBuilderMixin Trades Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() Ranges.avg_duration() Ranges.coverage() Ranges.max_duration() Ranges.to_mask() Records.apply() Records.apply_mask() Records.build_field_config_doc() Records.count() Records.get_apply_mapping_arr() Records.get_by_col_idxs() Records.get_field_arr() Records.get_field_mapping() Records.get_field_name() Records.get_field_setting() Records.get_field_title() Records.get_map_field() Records.get_map_field_to_index() Records.indexing_func_meta() Records.is_sorted() Records.map() Records.map_array() Records.map_field() Records.override_field_config_doc() Records.replace() Records.sort() RecordsWithFields.field_config StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Trades.close Trades.closed Trades.col Trades.col_arr Trades.col_mapper Trades.config Trades.direction Trades.duration Trades.end_idx Trades.entry_fees Trades.entry_idx Trades.entry_price Trades.exit_fees Trades.exit_idx Trades.exit_price Trades.expectancy() Trades.from_ts() Trades.id Trades.id_arr Trades.idx_arr Trades.iloc Trades.indexing_func() Trades.indexing_kwargs Trades.loc Trades.long Trades.losing Trades.losing_streak Trades.open Trades.parent_id Trades.plot() Trades.plot_pnl() Trades.plots_defaults Trades.pnl Trades.profit_factor() Trades.records Trades.records_arr Trades.records_readable Trades.returns Trades.self_aliases Trades.short Trades.size Trades.sqn() Trades.start_idx Trades.stats_defaults Trades.status Trades.ts Trades.values Trades.win_rate() Trades.winning Trades.winning_streak Trades.wrapper Trades.writeable_attrs Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"EntryTrades"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.EntryTrades.from_orders","text":"EntryTrades . from_orders ( orders , close = None , attach_close = True , ** kwargs ) Build EntryTrades from Orders .","title":"from_orders()"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.ExitTrades","text":"Extends Trades for working with exit trade records. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin Ranges Records RecordsWithFields StatsBuilderMixin Trades Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() Ranges.avg_duration() Ranges.coverage() Ranges.max_duration() Ranges.to_mask() Records.apply() Records.apply_mask() Records.build_field_config_doc() Records.count() Records.get_apply_mapping_arr() Records.get_by_col_idxs() Records.get_field_arr() Records.get_field_mapping() Records.get_field_name() Records.get_field_setting() Records.get_field_title() Records.get_map_field() Records.get_map_field_to_index() Records.indexing_func_meta() Records.is_sorted() Records.map() Records.map_array() Records.map_field() Records.override_field_config_doc() Records.replace() Records.sort() RecordsWithFields.field_config StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Trades.close Trades.closed Trades.col Trades.col_arr Trades.col_mapper Trades.config Trades.direction Trades.duration Trades.end_idx Trades.entry_fees Trades.entry_idx Trades.entry_price Trades.exit_fees Trades.exit_idx Trades.exit_price Trades.expectancy() Trades.from_ts() Trades.id Trades.id_arr Trades.idx_arr Trades.iloc Trades.indexing_func() Trades.indexing_kwargs Trades.loc Trades.long Trades.losing Trades.losing_streak Trades.open Trades.parent_id Trades.plot() Trades.plot_pnl() Trades.plots_defaults Trades.pnl Trades.profit_factor() Trades.records Trades.records_arr Trades.records_readable Trades.returns Trades.self_aliases Trades.short Trades.size Trades.sqn() Trades.start_idx Trades.stats_defaults Trades.status Trades.ts Trades.values Trades.win_rate() Trades.winning Trades.winning_streak Trades.wrapper Trades.writeable_attrs Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"ExitTrades"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.ExitTrades.from_orders","text":"ExitTrades . from_orders ( orders , close = None , attach_close = True , ** kwargs ) Build ExitTrades from Orders .","title":"from_orders()"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Positions","text":"Extends Trades for working with position records. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin Ranges Records RecordsWithFields StatsBuilderMixin Trades Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() Ranges.avg_duration() Ranges.coverage() Ranges.max_duration() Ranges.to_mask() Records.apply() Records.apply_mask() Records.build_field_config_doc() Records.count() Records.get_apply_mapping_arr() Records.get_by_col_idxs() Records.get_field_arr() Records.get_field_mapping() Records.get_field_name() Records.get_field_setting() Records.get_field_title() Records.get_map_field() Records.get_map_field_to_index() Records.indexing_func_meta() Records.is_sorted() Records.map() Records.map_array() Records.map_field() Records.override_field_config_doc() Records.replace() Records.sort() RecordsWithFields.field_config StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Trades.close Trades.closed Trades.col Trades.col_arr Trades.col_mapper Trades.config Trades.direction Trades.duration Trades.end_idx Trades.entry_fees Trades.entry_idx Trades.entry_price Trades.exit_fees Trades.exit_idx Trades.exit_price Trades.expectancy() Trades.from_ts() Trades.id Trades.id_arr Trades.idx_arr Trades.iloc Trades.indexing_func() Trades.indexing_kwargs Trades.loc Trades.long Trades.losing Trades.losing_streak Trades.open Trades.parent_id Trades.plot() Trades.plot_pnl() Trades.plots_defaults Trades.pnl Trades.profit_factor() Trades.records Trades.records_arr Trades.records_readable Trades.returns Trades.self_aliases Trades.short Trades.size Trades.sqn() Trades.start_idx Trades.stats_defaults Trades.status Trades.ts Trades.values Trades.win_rate() Trades.winning Trades.winning_streak Trades.wrapper Trades.writeable_attrs Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"Positions"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Positions.from_trades","text":"Positions . from_trades ( trades , close = None , attach_close = True , ** kwargs ) Build Positions from Trades .","title":"from_trades()"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades","text":"Extends Ranges for working with trade-like records, such as entry trades, exit trades, and positions. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin Ranges Records RecordsWithFields StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() Ranges.avg_duration() Ranges.closed Ranges.col Ranges.col_arr Ranges.col_mapper Ranges.config Ranges.coverage() Ranges.duration Ranges.end_idx Ranges.from_ts() Ranges.id Ranges.id_arr Ranges.idx_arr Ranges.iloc Ranges.indexing_kwargs Ranges.loc Ranges.max_duration() Ranges.open Ranges.records Ranges.records_arr Ranges.records_readable Ranges.self_aliases Ranges.start_idx Ranges.status Ranges.to_mask() Ranges.ts Ranges.values Ranges.wrapper Ranges.writeable_attrs Records.apply() Records.apply_mask() Records.build_field_config_doc() Records.count() Records.get_apply_mapping_arr() Records.get_by_col_idxs() Records.get_field_arr() Records.get_field_mapping() Records.get_field_name() Records.get_field_setting() Records.get_field_title() Records.get_map_field() Records.get_map_field_to_index() Records.indexing_func_meta() Records.is_sorted() Records.map() Records.map_array() Records.map_field() Records.override_field_config_doc() Records.replace() Records.sort() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses EntryTrades ExitTrades Positions","title":"Trades"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.close","text":"Reference price such as close (optional).","title":"close"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.direction","text":"Mapped array of the field direction .","title":"direction"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.entry_fees","text":"Mapped array of the field entry_fees .","title":"entry_fees"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.entry_idx","text":"Mapped array of the field entry_idx .","title":"entry_idx"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.entry_price","text":"Mapped array of the field entry_price .","title":"entry_price"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.exit_fees","text":"Mapped array of the field exit_fees .","title":"exit_fees"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.exit_idx","text":"Mapped array of the field exit_idx .","title":"exit_idx"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.exit_price","text":"Mapped array of the field exit_price .","title":"exit_price"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.expectancy","text":"Trades . expectancy ( group_by = None , wrap_kwargs = None ) Average profitability.","title":"expectancy()"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.field_config","text":"Field config of Trades . Co nf ig( { \"dtype\" : { \"id\" : \"int64\" , \"col\" : \"int64\" , \"size\" : \"float64\" , \"entry_idx\" : \"int64\" , \"entry_price\" : \"float64\" , \"entry_fees\" : \"float64\" , \"exit_idx\" : \"int64\" , \"exit_price\" : \"float64\" , \"exit_fees\" : \"float64\" , \"pnl\" : \"float64\" , \"return\" : \"float64\" , \"direction\" : \"int64\" , \"status\" : \"int64\" , \"parent_id\" : \"int64\" }, \"settings\" : { \"id\" : { \"name\" : \"id\" , \"title\" : \"Trade Id\" }, \"col\" : { \"name\" : \"col\" , \"title\" : \"Column\" , \"mapping\" : \"columns\" }, \"idx\" : { \"name\" : \"exit_idx\" , \"title\" : \"Timestamp\" , \"mapping\" : \"index\" }, \"start_idx\" : { \"title\" : \"Start Timestamp\" , \"mapping\" : \"index\" , \"name\" : \"entry_idx\" }, \"end_idx\" : { \"title\" : \"End Timestamp\" , \"mapping\" : \"index\" , \"name\" : \"exit_idx\" }, \"status\" : { \"title\" : \"Status\" , \"mapping\" : { \"Open\" : 0 , \"Closed\" : 1 } }, \"size\" : { \"title\" : \"Size\" }, \"entry_idx\" : { \"title\" : \"Entry Timestamp\" , \"mapping\" : \"index\" }, \"entry_price\" : { \"title\" : \"Avg Entry Price\" }, \"entry_fees\" : { \"title\" : \"Entry Fees\" }, \"exit_idx\" : { \"title\" : \"Exit Timestamp\" , \"mapping\" : \"index\" }, \"exit_price\" : { \"title\" : \"Avg Exit Price\" }, \"exit_fees\" : { \"title\" : \"Exit Fees\" }, \"pnl\" : { \"title\" : \"PnL\" }, \"return\" : { \"title\" : \"Return\" }, \"direction\" : { \"title\" : \"Direction\" , \"mapping\" : { \"Long\" : 0 , \"Short\" : 1 } }, \"parent_id\" : { \"title\" : \"Position Id\" } } } )","title":"field_config"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.indexing_func","text":"Trades . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on Trades .","title":"indexing_func()"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.long","text":"Records filtered by direction == 0 .","title":"long"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.losing","text":"Losing trades.","title":"losing"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.losing_streak","text":"Losing streak at each trade in the current column. See trade_losing_streak_nb() .","title":"losing_streak"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.metrics","text":"Metrics supported by Trades . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function Trades.<lambda> at 0x7fac99068378>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function Trades.<lambda> at 0x7fac99068400>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function Trades.<lambda> at 0x7fac99068488>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"first_trade_start\" : { \"title\" : \"First Trade Start\" , \"calc_func\" : \"entry_idx.nth\" , \"n\" : 0 , \"wrap_kwargs\" : { \"to_index\" : true }, \"tags\" : [ \"trades\" , \"index\" ] }, \"last_trade_end\" : { \"title\" : \"Last Trade End\" , \"calc_func\" : \"exit_idx.nth\" , \"n\" : -1 , \"wrap_kwargs\" : { \"to_index\" : true }, \"tags\" : [ \"trades\" , \"index\" ] }, \"coverage\" : { \"title\" : \"Coverage\" , \"calc_func\" : \"coverage\" , \"overlapping\" : false , \"normalize\" : false , \"apply_to_timedelta\" : true , \"tags\" : [ \"ranges\" , \"coverage\" ] }, \"overlap_coverage\" : { \"title\" : \"Overlap Coverage\" , \"calc_func\" : \"coverage\" , \"overlapping\" : true , \"normalize\" : false , \"apply_to_timedelta\" : true , \"tags\" : [ \"ranges\" , \"coverage\" ] }, \"total_records\" : { \"title\" : \"Total Records\" , \"calc_func\" : \"count\" , \"tags\" : \"records\" }, \"total_long_trades\" : { \"title\" : \"Total Long Trades\" , \"calc_func\" : \"long.count\" , \"tags\" : [ \"trades\" , \"long\" ] }, \"total_short_trades\" : { \"title\" : \"Total Short Trades\" , \"calc_func\" : \"short.count\" , \"tags\" : [ \"trades\" , \"short\" ] }, \"total_closed_trades\" : { \"title\" : \"Total Closed Trades\" , \"calc_func\" : \"closed.count\" , \"tags\" : [ \"trades\" , \"closed\" ] }, \"total_open_trades\" : { \"title\" : \"Total Open Trades\" , \"calc_func\" : \"open.count\" , \"tags\" : [ \"trades\" , \"open\" ] }, \"open_trade_pnl\" : { \"title\" : \"Open Trade PnL\" , \"calc_func\" : \"open.pnl.sum\" , \"tags\" : [ \"trades\" , \"open\" ] }, \"win_rate\" : { \"title\" : \"Win Rate [%]\" , \"calc_func\" : \"closed.win_rate\" , \"post_calc_func\" : \"<function Trades.<lambda> at 0x7fac99068510>\" , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags]\\\", mapping={})\" }, \"winning_streak\" : { \"title\" : \"Max Win Streak\" , \"calc_func\" : \"RepEval(expression=\\\"'winning_streak.max' if incl_open else 'closed.winning_streak.max'\\\", mapping={})\" , \"wrap_kwargs\" : { \"dtype\" : \"Int64\" }, \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags, 'streak']\\\", mapping={})\" }, \"losing_streak\" : { \"title\" : \"Max Loss Streak\" , \"calc_func\" : \"RepEval(expression=\\\"'losing_streak.max' if incl_open else 'closed.losing_streak.max'\\\", mapping={})\" , \"wrap_kwargs\" : { \"dtype\" : \"Int64\" }, \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags, 'streak']\\\", mapping={})\" }, \"best_trade\" : { \"title\" : \"Best Trade [%]\" , \"calc_func\" : \"RepEval(expression=\\\"'returns.max' if incl_open else 'closed.returns.max'\\\", mapping={})\" , \"post_calc_func\" : \"<function Trades.<lambda> at 0x7fac99068598>\" , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags]\\\", mapping={})\" }, \"worst_trade\" : { \"title\" : \"Worst Trade [%]\" , \"calc_func\" : \"RepEval(expression=\\\"'returns.min' if incl_open else 'closed.returns.min'\\\", mapping={})\" , \"post_calc_func\" : \"<function Trades.<lambda> at 0x7fac99068620>\" , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags]\\\", mapping={})\" }, \"avg_winning_trade\" : { \"title\" : \"Avg Winning Trade [%]\" , \"calc_func\" : \"RepEval(expression=\\\"'winning.returns.mean' if incl_open else 'closed.winning.returns.mean'\\\", mapping={})\" , \"post_calc_func\" : \"<function Trades.<lambda> at 0x7fac990686a8>\" , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags, 'winning']\\\", mapping={})\" }, \"avg_losing_trade\" : { \"title\" : \"Avg Losing Trade [%]\" , \"calc_func\" : \"RepEval(expression=\\\"'losing.returns.mean' if incl_open else 'closed.losing.returns.mean'\\\", mapping={})\" , \"post_calc_func\" : \"<function Trades.<lambda> at 0x7fac99068730>\" , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags, 'losing']\\\", mapping={})\" }, \"avg_winning_trade_duration\" : { \"title\" : \"Avg Winning Trade Duration\" , \"calc_func\" : \"RepEval(expression=\\\"'winning.avg_duration' if incl_open else 'closed.winning.avg_duration'\\\", mapping={})\" , \"fill_wrap_kwargs\" : true , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags, 'winning', 'duration']\\\", mapping={})\" }, \"avg_losing_trade_duration\" : { \"title\" : \"Avg Losing Trade Duration\" , \"calc_func\" : \"RepEval(expression=\\\"'losing.avg_duration' if incl_open else 'closed.losing.avg_duration'\\\", mapping={})\" , \"fill_wrap_kwargs\" : true , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags, 'losing', 'duration']\\\", mapping={})\" }, \"profit_factor\" : { \"title\" : \"Profit Factor\" , \"calc_func\" : \"RepEval(expression=\\\"'profit_factor' if incl_open else 'closed.profit_factor'\\\", mapping={})\" , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags]\\\", mapping={})\" }, \"expectancy\" : { \"title\" : \"Expectancy\" , \"calc_func\" : \"RepEval(expression=\\\"'expectancy' if incl_open else 'closed.expectancy'\\\", mapping={})\" , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags]\\\", mapping={})\" }, \"sqn\" : { \"title\" : \"SQN\" , \"calc_func\" : \"RepEval(expression=\\\"'sqn' if incl_open else 'closed.sqn'\\\", mapping={})\" , \"tags\" : \"RepEval(expression=\\\"['trades', *incl_open_tags]\\\", mapping={})\" } } ) Returns Trades._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable Trades._metrics .","title":"metrics"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.parent_id","text":"Mapped array of the field parent_id .","title":"parent_id"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.plot","text":"Trades . plot ( column = None , plot_zones = True , close_trace_kwargs = None , entry_trace_kwargs = None , exit_trace_kwargs = None , exit_profit_trace_kwargs = None , exit_loss_trace_kwargs = None , active_trace_kwargs = None , profit_shape_kwargs = None , loss_shape_kwargs = None , add_trace_kwargs = None , xref = 'x' , yref = 'y' , fig = None , ** layout_kwargs ) Plot orders. Args column :\u2002 str Name of the column to plot. plot_zones :\u2002 bool Whether to plot zones. Set to False if there are many trades within one position. close_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for Trades.close . entry_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Entry\" markers. exit_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Exit\" markers. exit_profit_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Exit - Profit\" markers. exit_loss_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Exit - Loss\" markers. active_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Active\" markers. profit_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for profit zones. loss_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for loss zones. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import pandas as pd >>> from datetime import datetime , timedelta >>> import vectorbt as vbt >>> price = pd . Series ([ 1. , 2. , 3. , 4. , 3. , 2. , 1. ], name = 'Price' ) >>> price . index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( len ( price ))] >>> orders = pd . Series ([ 1. , - 0.5 , - 0.5 , 2. , - 0.5 , - 0.5 , - 0.5 ]) >>> pf = vbt . Portfolio . from_orders ( price , orders ) >>> pf . trades . plot ()","title":"plot()"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.plot_pnl","text":"Trades . plot_pnl ( column = None , pct_scale = True , marker_size_range = ( 7 , 14 ), opacity_range = ( 0.75 , 0.9 ), closed_profit_trace_kwargs = None , closed_loss_trace_kwargs = None , open_trace_kwargs = None , hline_shape_kwargs = None , add_trace_kwargs = None , xref = 'x' , yref = 'y' , fig = None , ** layout_kwargs ) Plot trade PnL and returns. Args column :\u2002 str Name of the column to plot. pct_scale :\u2002 bool Whether to set y-axis to Trades.returns , otherwise to Trades.pnl . marker_size_range :\u2002 tuple Range of marker size. opacity_range :\u2002 tuple Range of marker opacity. closed_profit_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Closed - Profit\" markers. closed_loss_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Closed - Loss\" markers. open_trace_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Scatter for \"Open\" markers. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for zeroline. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import pandas as pd >>> from datetime import datetime , timedelta >>> import vectorbt as vbt >>> price = pd . Series ([ 1. , 2. , 3. , 4. , 3. , 2. , 1. ]) >>> price . index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( len ( price ))] >>> orders = pd . Series ([ 1. , - 0.5 , - 0.5 , 2. , - 0.5 , - 0.5 , - 0.5 ]) >>> pf = vbt . Portfolio . from_orders ( price , orders ) >>> pf . trades . plot_pnl ()","title":"plot_pnl()"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.plots_defaults","text":"Defaults for PlotsBuilderMixin.plots() . Merges Ranges.plots_defaults and trades.plots from settings .","title":"plots_defaults"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.pnl","text":"Mapped array of the field pnl .","title":"pnl"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.profit_factor","text":"Trades . profit_factor ( group_by = None , wrap_kwargs = None ) Profit factor.","title":"profit_factor()"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.returns","text":"Mapped array of the field return .","title":"returns"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.short","text":"Records filtered by direction == 1 .","title":"short"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.size","text":"Mapped array of the field size .","title":"size"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.sqn","text":"Trades . sqn ( group_by = None , wrap_kwargs = None ) System Quality Number (SQN).","title":"sqn()"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.stats_defaults","text":"Defaults for StatsBuilderMixin.stats() . Merges Ranges.stats_defaults and trades.stats from settings .","title":"stats_defaults"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.subplots","text":"Subplots supported by Trades . Co nf ig( { \"plot\" : { \"title\" : \"Trades\" , \"yaxis_kwargs\" : { \"title\" : \"Price\" }, \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"tags\" : \"trades\" }, \"plot_pnl\" : { \"title\" : \"Trade PnL\" , \"yaxis_kwargs\" : { \"title\" : \"Trade PnL\" }, \"check_is_not_grouped\" : true , \"plot_func\" : \"plot_pnl\" , \"tags\" : \"trades\" } } ) Returns Trades._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable Trades._subplots .","title":"subplots"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.win_rate","text":"Trades . win_rate ( group_by = None , wrap_kwargs = None ) Rate of winning trades.","title":"win_rate()"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.winning","text":"Winning trades.","title":"winning"},{"location":"api/portfolio/trades/#vectorbt.portfolio.trades.Trades.winning_streak","text":"Winning streak at each trade in the current column. See trade_winning_streak_nb() .","title":"winning_streak"},{"location":"api/records/","text":"records package \u00b6 Modules for working with records. Records are the second form of data representation in vectorbt. They allow storing sparse event data such as drawdowns, orders, trades, and positions, without converting them back to the matrix form and occupying the user's memory. Sub-modules \u00b6 vectorbt.records.base vectorbt.records.col_mapper vectorbt.records.decorators vectorbt.records.mapped_array vectorbt.records.nb","title":"records"},{"location":"api/records/#vectorbt.records","text":"Modules for working with records. Records are the second form of data representation in vectorbt. They allow storing sparse event data such as drawdowns, orders, trades, and positions, without converting them back to the matrix form and occupying the user's memory.","title":"vectorbt.records"},{"location":"api/records/#sub-modules","text":"vectorbt.records.base vectorbt.records.col_mapper vectorbt.records.decorators vectorbt.records.mapped_array vectorbt.records.nb","title":"Sub-modules"},{"location":"api/records/base/","text":"base module \u00b6 Base class for working with records. vectorbt works with two different representations of data: matrices and records. A matrix, in this context, is just an array of one-dimensional arrays, each corresponding to a separate feature. The matrix itself holds only one kind of information (one attribute). For example, one can create a matrix for entry signals, with columns being different strategy configurations. But what if the matrix is huge and sparse? What if there is more information we would like to represent by each element? Creating multiple matrices would be a waste of memory. Records make possible representing complex, sparse information in a dense format. They are just an array of one-dimensional arrays of fixed schema. You can imagine records being a DataFrame, where each row represents a record and each column represents a specific attribute. a b 0 1.0 5.0 attr1 = 1 2.0 NaN 2 NaN 7.0 3 4.0 8.0 a b 0 9.0 13.0 attr2 = 1 10.0 NaN 2 NaN 15.0 3 12.0 16.0 | v id col idx attr1 attr2 0 0 0 0 1 9 1 1 0 1 2 10 2 2 0 3 4 12 3 3 1 0 5 13 4 4 1 1 7 15 5 5 1 3 8 16 Another advantage of records is that they are not constrained by size. Multiple records can map to a single element in a matrix. For example, one can define multiple orders at the same time step, which is impossible to represent in a matrix form without using complex data types. Consider the following example: >>> import numpy as np >>> import pandas as pd >>> from numba import njit >>> from collections import namedtuple >>> import vectorbt as vbt >>> example_dt = np . dtype ([ ... ( 'id' , np . int_ ), ... ( 'col' , np . int_ ), ... ( 'idx' , np . int_ ), ... ( 'some_field' , np . float_ ) ... ]) >>> records_arr = np . array ([ ... ( 0 , 0 , 0 , 10. ), ... ( 1 , 0 , 1 , 11. ), ... ( 2 , 0 , 2 , 12. ), ... ( 3 , 1 , 0 , 13. ), ... ( 4 , 1 , 1 , 14. ), ... ( 5 , 1 , 2 , 15. ), ... ( 6 , 2 , 0 , 16. ), ... ( 7 , 2 , 1 , 17. ), ... ( 8 , 2 , 2 , 18. ) ... ], dtype = example_dt ) >>> wrapper = vbt . ArrayWrapper ( index = [ 'x' , 'y' , 'z' ], ... columns = [ 'a' , 'b' , 'c' ], ndim = 2 , freq = '1 day' ) >>> records = vbt . Records ( wrapper , records_arr ) Printing \u00b6 There are two ways to print records: Raw dataframe that preserves field names and data types: >>> records . records id col idx some_field 0 0 0 0 10.0 1 1 0 1 11.0 2 2 0 2 12.0 3 3 1 0 13.0 4 4 1 1 14.0 5 5 1 2 15.0 6 6 2 0 16.0 7 7 2 1 17.0 8 8 2 2 18.0 Readable dataframe that takes into consideration Records.field_config : >>> records . records_readable Id Column Timestamp some_field 0 0 a x 10.0 1 1 a y 11.0 2 2 a z 12.0 3 3 b x 13.0 4 4 b y 14.0 5 5 b z 15.0 6 6 c x 16.0 7 7 c y 17.0 8 8 c z 18.0 Mapping \u00b6 Records are just structured arrays with a bunch of methods and properties for processing them. Their main feature is to map the records array and to reduce it by column (similar to the MapReduce paradigm). The main advantage is that it all happens without conversion to the matrix form and wasting memory resources. Records can be mapped to MappedArray in several ways: Use Records.map_field() to map a record field: >>> records . map_field ( 'some_field' ) <vectorbt.records.mapped_array.MappedArray at 0x7ff49bd31a58> >>> records . map_field ( 'some_field' ) . values array([10., 11., 12., 13., 14., 15., 16., 17., 18.]) Use Records.map() to map records using a custom function. >>> @njit ... def power_map_nb ( record , pow ): ... return record . some_field ** pow >>> records . map ( power_map_nb , 2 ) <vectorbt.records.mapped_array.MappedArray at 0x7ff49c990cf8> >>> records . map ( power_map_nb , 2 ) . values array([100., 121., 144., 169., 196., 225., 256., 289., 324.]) Use Records.map_array() to convert an array to MappedArray . >>> records . map_array ( records_arr [ 'some_field' ] ** 2 ) <vectorbt.records.mapped_array.MappedArray object at 0x7fe9bccf2978> >>> records . map_array ( records_arr [ 'some_field' ] ** 2 ) . values array([100., 121., 144., 169., 196., 225., 256., 289., 324.]) Use Records.apply() to apply a function on each column/group: >>> @njit ... def cumsum_apply_nb ( records ): ... return np . cumsum ( records . some_field ) >>> records . apply ( cumsum_apply_nb ) <vectorbt.records.mapped_array.MappedArray at 0x7ff49c990cf8> >>> records . apply ( cumsum_apply_nb ) . values array([10., 21., 33., 13., 27., 42., 16., 33., 51.]) >>> group_by = np . array ([ 'first' , 'first' , 'second' ]) >>> records . apply ( cumsum_apply_nb , group_by = group_by , apply_per_group = True ) . values array([10., 21., 33., 46., 60., 75., 16., 33., 51.]) Notice how cumsum resets at each column in the first example and at each group in the second example. Filtering \u00b6 Use Records.apply_mask() to filter elements per column/group: >>> mask = [ True , False , True , False , True , False , True , False , True ] >>> filtered_records = records . apply_mask ( mask ) >>> filtered_records . count () a 2 b 1 c 2 dtype: int64 >>> filtered_records . values [ 'id' ] array([0, 2, 4, 6, 8]) Grouping \u00b6 One of the key features of Records is that you can perform reducing operations on a group of columns as if they were a single column. Groups can be specified by group_by , which can be anything from positions or names of column levels, to a NumPy array with actual groups. There are multiple ways of define grouping: When creating Records , pass group_by to ArrayWrapper : >>> group_by = np . array ([ 'first' , 'first' , 'second' ]) >>> grouped_wrapper = wrapper . replace ( group_by = group_by ) >>> grouped_records = vbt . Records ( grouped_wrapper , records_arr ) >>> grouped_records . map_field ( 'some_field' ) . mean () first 12.5 second 17.0 dtype: float64 Regroup an existing Records : >>> records . regroup ( group_by ) . map_field ( 'some_field' ) . mean () first 12.5 second 17.0 dtype: float64 Pass group_by directly to the mapping method: >>> records . map_field ( 'some_field' , group_by = group_by ) . mean () first 12.5 second 17.0 dtype: float64 Pass group_by directly to the reducing method: >>> records . map_field ( 'some_field' ) . mean ( group_by = group_by ) a 11.0 b 14.0 c 17.0 dtype: float64 Note Grouping applies only to reducing operations, there is no change to the arrays. Indexing \u00b6 Like any other class subclassing Wrapping , we can do pandas indexing on a Records instance, which forwards indexing operation to each object with columns: >>> records [ 'a' ] . records id col idx some_field 0 0 0 0 10.0 1 1 0 1 11.0 2 2 0 2 12.0 >>> grouped_records [ 'first' ] . records id col idx some_field 0 0 0 0 10.0 1 1 0 1 11.0 2 2 0 2 12.0 3 3 1 0 13.0 4 4 1 1 14.0 5 5 1 2 15.0 Note Changing index (time axis) is not supported. The object should be treated as a Series rather than a DataFrame; for example, use some_field.iloc[0] instead of some_field.iloc[:, 0] . Indexing behavior depends solely upon ArrayWrapper . For example, if group_select is enabled indexing will be performed on groups, otherwise on single columns. Caching \u00b6 Records supports caching. If a method or a property requires heavy computation, it's wrapped with cached_method() and cached_property respectively. Caching can be disabled globally via caching in settings . Note Because of caching, class is meant to be immutable and all properties are read-only. To change any attribute, use the copy method and pass the attribute as keyword argument. Saving and loading \u00b6 Like any other class subclassing Pickleable , we can save a Records instance to the disk with Pickleable.save() and load it with Pickleable.load() . Stats \u00b6 Hint See StatsBuilderMixin.stats() and Records.metrics . >>> records . stats ( column = 'a' ) Start x End z Period 3 days 00:00:00 Total Records 3 Name: a, dtype: object StatsBuilderMixin.stats() also supports (re-)grouping: >>> grouped_records . stats ( column = 'first' ) Start x End z Period 3 days 00:00:00 Total Records 6 Name: first, dtype: object Plots \u00b6 Hint See PlotsBuilderMixin.plots() and Records.subplots . This class is too generic to have any subplots, but feel free to add custom subplots to your subclass. Extending \u00b6 Records class can be extended by subclassing. In case some of our fields have the same meaning but different naming (such as the base field idx ) or other properties, we can override field_config using override_field_config() . It will look for configs of all base classes and merge our config on top of them. This preserves any base class property that is not explicitly listed in our config. >>> from vectorbt.records.decorators import override_field_config >>> my_dt = np . dtype ([ ... ( 'my_id' , np . int_ ), ... ( 'my_col' , np . int_ ), ... ( 'my_idx' , np . int_ ) ... ]) >>> my_fields_config = dict ( ... dtype = my_dt , ... settings = dict ( ... id = dict ( name = 'my_id' ), ... col = dict ( name = 'my_col' ), ... idx = dict ( name = 'my_idx' ) ... ) ... ) >>> @override_field_config ( my_fields_config ) ... class MyRecords ( vbt . Records ): ... pass >>> records_arr = np . array ([ ... ( 0 , 0 , 0 ), ... ( 1 , 0 , 1 ), ... ( 2 , 1 , 0 ), ... ( 3 , 1 , 1 ) ... ], dtype = my_dt ) >>> wrapper = vbt . ArrayWrapper ( index = [ 'x' , 'y' ], ... columns = [ 'a' , 'b' ], ndim = 2 , freq = '1 day' ) >>> my_records = MyRecords ( wrapper , records_arr ) >>> my_records . id_arr array([0, 1, 2, 3]) >>> my_records . col_arr array([0, 0, 1, 1]) >>> my_records . idx_arr array([0, 1, 0, 1]) Alternatively, we can override the _field_config class attribute. >>> @override_field_config ... class MyRecords ( vbt . Records ): ... _field_config = dict ( ... dtype = my_dt , ... settings = dict ( ... id = dict ( name = 'my_id' ), ... idx = dict ( name = 'my_idx' ), ... col = dict ( name = 'my_col' ) ... ) ... ) Note Don't forget to decorate the class with @override_field_config to inherit configs from base classes. You can stop inheritance by not decorating or passing merge_configs=False to the decorator. MetaFields class \u00b6 Meta class that exposes a read-only class property MetaFields.field_config . Superclasses builtins.type Subclasses MetaRecords field_config property \u00b6 Field config. MetaRecords class \u00b6 Meta class that exposes a read-only class property StatsBuilderMixin.metrics . Superclasses MetaFields MetaPlotsBuilderMixin MetaStatsBuilderMixin builtins.type Inherited members MetaFields.field_config MetaPlotsBuilderMixin.subplots MetaStatsBuilderMixin.metrics Records class \u00b6 Wraps the actual records array (such as trades) and exposes methods for mapping it to some array of values (such as PnL of each trade). Args wrapper :\u2002 ArrayWrapper Array wrapper. See ArrayWrapper . records_arr :\u2002 array_like A structured NumPy array of records. Must have the fields id (record index) and col (column index). col_mapper :\u2002 ColumnMapper Column mapper if already known. Note It depends on records_arr , so make sure to invalidate col_mapper upon creating a Records instance with a modified records_arr . Records.replace() does it automatically. **kwargs Custom keyword arguments passed to the config. Useful if any subclass wants to extend the config. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin RecordsWithFields StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.config Wrapping.iloc Wrapping.indexing_kwargs Wrapping.loc Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Wrapping.self_aliases Wrapping.wrapper Wrapping.writeable_attrs Subclasses Logs Orders Ranges apply method \u00b6 Records . apply ( apply_func_nb , * args , group_by = None , apply_per_group = False , dtype = None , ** kwargs ) Apply function on records per column/group. Returns mapped array. Applies per group if apply_per_group is True. See apply_on_records_nb() . **kwargs are passed to Records.map_array() . apply_mask method \u00b6 Records . apply_mask ( mask , group_by = None , ** kwargs ) Return a new class instance, filtered by mask. build_field_config_doc class method \u00b6 Records . build_field_config_doc ( source_cls = None ) Build field config documentation. col_arr property \u00b6 Get column array. col_mapper property \u00b6 Column mapper. See ColumnMapper . count method \u00b6 Records . count ( group_by = None , wrap_kwargs = None ) Return count by column. field_config class variable \u00b6 Field config of Records . Co nf ig( { \"dtype\" : null , \"settings\" : { \"id\" : { \"name\" : \"id\" , \"title\" : \"Id\" }, \"col\" : { \"name\" : \"col\" , \"title\" : \"Column\" , \"mapping\" : \"columns\" }, \"idx\" : { \"name\" : \"idx\" , \"title\" : \"Timestamp\" , \"mapping\" : \"index\" } } } ) get_apply_mapping_arr method \u00b6 Records . get_apply_mapping_arr ( field , ** kwargs ) Resolve the mapped array on the field, with mapping applied. Uses Records.field_config . get_by_col_idxs method \u00b6 Records . get_by_col_idxs ( col_idxs ) Get records corresponding to column indices. Returns new records array. get_field_arr method \u00b6 Records . get_field_arr ( field ) Resolve the array of the field. Uses Records.field_config . get_field_mapping method \u00b6 Records . get_field_mapping ( field ) Resolve the mapping of the field. Uses Records.field_config . get_field_name method \u00b6 Records . get_field_name ( field ) Resolve the name of the field. Uses Records.field_config .. get_field_setting method \u00b6 Records . get_field_setting ( field , setting , default = None ) Resolve any setting of the field. Uses Records.field_config . get_field_title method \u00b6 Records . get_field_title ( field ) Resolve the title of the field. Uses Records.field_config . get_map_field method \u00b6 Records . get_map_field ( field , ** kwargs ) Resolve the mapped array of the field. Uses Records.field_config . get_map_field_to_index method \u00b6 Records . get_map_field_to_index ( field , ** kwargs ) Resolve the mapped array on the field, with index applied. Uses Records.field_config . id_arr property \u00b6 Get id array. idx_arr property \u00b6 Get index array. indexing_func method \u00b6 Records . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on Records . indexing_func_meta method \u00b6 Records . indexing_func_meta ( pd_indexing_func , ** kwargs ) Perform indexing on Records and return metadata. is_sorted method \u00b6 Records . is_sorted ( incl_id = False ) Check whether records are sorted. map method \u00b6 Records . map ( map_func_nb , * args , dtype = None , ** kwargs ) Map each record to a scalar value. Returns mapped array. See map_records_nb() . **kwargs are passed to Records.map_array() . map_array method \u00b6 Records . map_array ( a , idx_arr = None , mapping = None , group_by = None , ** kwargs ) Convert array to mapped array. The length of the array should match that of the records. map_field method \u00b6 Records . map_field ( field , ** kwargs ) Convert field to mapped array. **kwargs are passed to Records.map_array() . metrics class variable \u00b6 Metrics supported by Records . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function Records.<lambda> at 0x7fac881edf28>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function Records.<lambda> at 0x7fac881e9048>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function Records.<lambda> at 0x7fac881e90d0>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"count\" : { \"title\" : \"Count\" , \"calc_func\" : \"count\" , \"tags\" : \"records\" } } ) Returns Records._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable Records._metrics . override_field_config_doc class method \u00b6 Records . override_field_config_doc ( __pdoc__ , source_cls = None ) Call this method on each subclass that overrides field_config . plots_defaults property \u00b6 Defaults for PlotsBuilderMixin.plots() . Merges PlotsBuilderMixin.plots_defaults and records.plots from settings . recarray property \u00b6 records property \u00b6 Records. records_arr property \u00b6 Records array. records_readable property \u00b6 Records in readable format. replace method \u00b6 Records . replace ( ** kwargs ) See Configured.replace() . Also, makes sure that Records.col_mapper is not passed to the new instance. sort method \u00b6 Records . sort ( incl_id = False , group_by = None , ** kwargs ) Sort records by columns (primary) and ids (secondary, optional). Note Sorting is expensive. A better approach is to append records already in the correct order. stats_defaults property \u00b6 Defaults for StatsBuilderMixin.stats() . Merges StatsBuilderMixin.stats_defaults and records.stats from settings . subplots class variable \u00b6 Subplots supported by Records . Co nf ig( {} ) Returns Records._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable Records._subplots . values property \u00b6 Records array. RecordsWithFields class \u00b6 Class exposes a read-only class property RecordsWithFields.field_config . Subclasses Records field_config function \u00b6 Field config of ${cls_name} . $ { f ield_co nf ig }","title":"base"},{"location":"api/records/base/#vectorbt.records.base","text":"Base class for working with records. vectorbt works with two different representations of data: matrices and records. A matrix, in this context, is just an array of one-dimensional arrays, each corresponding to a separate feature. The matrix itself holds only one kind of information (one attribute). For example, one can create a matrix for entry signals, with columns being different strategy configurations. But what if the matrix is huge and sparse? What if there is more information we would like to represent by each element? Creating multiple matrices would be a waste of memory. Records make possible representing complex, sparse information in a dense format. They are just an array of one-dimensional arrays of fixed schema. You can imagine records being a DataFrame, where each row represents a record and each column represents a specific attribute. a b 0 1.0 5.0 attr1 = 1 2.0 NaN 2 NaN 7.0 3 4.0 8.0 a b 0 9.0 13.0 attr2 = 1 10.0 NaN 2 NaN 15.0 3 12.0 16.0 | v id col idx attr1 attr2 0 0 0 0 1 9 1 1 0 1 2 10 2 2 0 3 4 12 3 3 1 0 5 13 4 4 1 1 7 15 5 5 1 3 8 16 Another advantage of records is that they are not constrained by size. Multiple records can map to a single element in a matrix. For example, one can define multiple orders at the same time step, which is impossible to represent in a matrix form without using complex data types. Consider the following example: >>> import numpy as np >>> import pandas as pd >>> from numba import njit >>> from collections import namedtuple >>> import vectorbt as vbt >>> example_dt = np . dtype ([ ... ( 'id' , np . int_ ), ... ( 'col' , np . int_ ), ... ( 'idx' , np . int_ ), ... ( 'some_field' , np . float_ ) ... ]) >>> records_arr = np . array ([ ... ( 0 , 0 , 0 , 10. ), ... ( 1 , 0 , 1 , 11. ), ... ( 2 , 0 , 2 , 12. ), ... ( 3 , 1 , 0 , 13. ), ... ( 4 , 1 , 1 , 14. ), ... ( 5 , 1 , 2 , 15. ), ... ( 6 , 2 , 0 , 16. ), ... ( 7 , 2 , 1 , 17. ), ... ( 8 , 2 , 2 , 18. ) ... ], dtype = example_dt ) >>> wrapper = vbt . ArrayWrapper ( index = [ 'x' , 'y' , 'z' ], ... columns = [ 'a' , 'b' , 'c' ], ndim = 2 , freq = '1 day' ) >>> records = vbt . Records ( wrapper , records_arr )","title":"vectorbt.records.base"},{"location":"api/records/base/#printing","text":"There are two ways to print records: Raw dataframe that preserves field names and data types: >>> records . records id col idx some_field 0 0 0 0 10.0 1 1 0 1 11.0 2 2 0 2 12.0 3 3 1 0 13.0 4 4 1 1 14.0 5 5 1 2 15.0 6 6 2 0 16.0 7 7 2 1 17.0 8 8 2 2 18.0 Readable dataframe that takes into consideration Records.field_config : >>> records . records_readable Id Column Timestamp some_field 0 0 a x 10.0 1 1 a y 11.0 2 2 a z 12.0 3 3 b x 13.0 4 4 b y 14.0 5 5 b z 15.0 6 6 c x 16.0 7 7 c y 17.0 8 8 c z 18.0","title":"Printing"},{"location":"api/records/base/#mapping","text":"Records are just structured arrays with a bunch of methods and properties for processing them. Their main feature is to map the records array and to reduce it by column (similar to the MapReduce paradigm). The main advantage is that it all happens without conversion to the matrix form and wasting memory resources. Records can be mapped to MappedArray in several ways: Use Records.map_field() to map a record field: >>> records . map_field ( 'some_field' ) <vectorbt.records.mapped_array.MappedArray at 0x7ff49bd31a58> >>> records . map_field ( 'some_field' ) . values array([10., 11., 12., 13., 14., 15., 16., 17., 18.]) Use Records.map() to map records using a custom function. >>> @njit ... def power_map_nb ( record , pow ): ... return record . some_field ** pow >>> records . map ( power_map_nb , 2 ) <vectorbt.records.mapped_array.MappedArray at 0x7ff49c990cf8> >>> records . map ( power_map_nb , 2 ) . values array([100., 121., 144., 169., 196., 225., 256., 289., 324.]) Use Records.map_array() to convert an array to MappedArray . >>> records . map_array ( records_arr [ 'some_field' ] ** 2 ) <vectorbt.records.mapped_array.MappedArray object at 0x7fe9bccf2978> >>> records . map_array ( records_arr [ 'some_field' ] ** 2 ) . values array([100., 121., 144., 169., 196., 225., 256., 289., 324.]) Use Records.apply() to apply a function on each column/group: >>> @njit ... def cumsum_apply_nb ( records ): ... return np . cumsum ( records . some_field ) >>> records . apply ( cumsum_apply_nb ) <vectorbt.records.mapped_array.MappedArray at 0x7ff49c990cf8> >>> records . apply ( cumsum_apply_nb ) . values array([10., 21., 33., 13., 27., 42., 16., 33., 51.]) >>> group_by = np . array ([ 'first' , 'first' , 'second' ]) >>> records . apply ( cumsum_apply_nb , group_by = group_by , apply_per_group = True ) . values array([10., 21., 33., 46., 60., 75., 16., 33., 51.]) Notice how cumsum resets at each column in the first example and at each group in the second example.","title":"Mapping"},{"location":"api/records/base/#filtering","text":"Use Records.apply_mask() to filter elements per column/group: >>> mask = [ True , False , True , False , True , False , True , False , True ] >>> filtered_records = records . apply_mask ( mask ) >>> filtered_records . count () a 2 b 1 c 2 dtype: int64 >>> filtered_records . values [ 'id' ] array([0, 2, 4, 6, 8])","title":"Filtering"},{"location":"api/records/base/#grouping","text":"One of the key features of Records is that you can perform reducing operations on a group of columns as if they were a single column. Groups can be specified by group_by , which can be anything from positions or names of column levels, to a NumPy array with actual groups. There are multiple ways of define grouping: When creating Records , pass group_by to ArrayWrapper : >>> group_by = np . array ([ 'first' , 'first' , 'second' ]) >>> grouped_wrapper = wrapper . replace ( group_by = group_by ) >>> grouped_records = vbt . Records ( grouped_wrapper , records_arr ) >>> grouped_records . map_field ( 'some_field' ) . mean () first 12.5 second 17.0 dtype: float64 Regroup an existing Records : >>> records . regroup ( group_by ) . map_field ( 'some_field' ) . mean () first 12.5 second 17.0 dtype: float64 Pass group_by directly to the mapping method: >>> records . map_field ( 'some_field' , group_by = group_by ) . mean () first 12.5 second 17.0 dtype: float64 Pass group_by directly to the reducing method: >>> records . map_field ( 'some_field' ) . mean ( group_by = group_by ) a 11.0 b 14.0 c 17.0 dtype: float64 Note Grouping applies only to reducing operations, there is no change to the arrays.","title":"Grouping"},{"location":"api/records/base/#indexing","text":"Like any other class subclassing Wrapping , we can do pandas indexing on a Records instance, which forwards indexing operation to each object with columns: >>> records [ 'a' ] . records id col idx some_field 0 0 0 0 10.0 1 1 0 1 11.0 2 2 0 2 12.0 >>> grouped_records [ 'first' ] . records id col idx some_field 0 0 0 0 10.0 1 1 0 1 11.0 2 2 0 2 12.0 3 3 1 0 13.0 4 4 1 1 14.0 5 5 1 2 15.0 Note Changing index (time axis) is not supported. The object should be treated as a Series rather than a DataFrame; for example, use some_field.iloc[0] instead of some_field.iloc[:, 0] . Indexing behavior depends solely upon ArrayWrapper . For example, if group_select is enabled indexing will be performed on groups, otherwise on single columns.","title":"Indexing"},{"location":"api/records/base/#caching","text":"Records supports caching. If a method or a property requires heavy computation, it's wrapped with cached_method() and cached_property respectively. Caching can be disabled globally via caching in settings . Note Because of caching, class is meant to be immutable and all properties are read-only. To change any attribute, use the copy method and pass the attribute as keyword argument.","title":"Caching"},{"location":"api/records/base/#saving-and-loading","text":"Like any other class subclassing Pickleable , we can save a Records instance to the disk with Pickleable.save() and load it with Pickleable.load() .","title":"Saving and loading"},{"location":"api/records/base/#stats","text":"Hint See StatsBuilderMixin.stats() and Records.metrics . >>> records . stats ( column = 'a' ) Start x End z Period 3 days 00:00:00 Total Records 3 Name: a, dtype: object StatsBuilderMixin.stats() also supports (re-)grouping: >>> grouped_records . stats ( column = 'first' ) Start x End z Period 3 days 00:00:00 Total Records 6 Name: first, dtype: object","title":"Stats"},{"location":"api/records/base/#plots","text":"Hint See PlotsBuilderMixin.plots() and Records.subplots . This class is too generic to have any subplots, but feel free to add custom subplots to your subclass.","title":"Plots"},{"location":"api/records/base/#extending","text":"Records class can be extended by subclassing. In case some of our fields have the same meaning but different naming (such as the base field idx ) or other properties, we can override field_config using override_field_config() . It will look for configs of all base classes and merge our config on top of them. This preserves any base class property that is not explicitly listed in our config. >>> from vectorbt.records.decorators import override_field_config >>> my_dt = np . dtype ([ ... ( 'my_id' , np . int_ ), ... ( 'my_col' , np . int_ ), ... ( 'my_idx' , np . int_ ) ... ]) >>> my_fields_config = dict ( ... dtype = my_dt , ... settings = dict ( ... id = dict ( name = 'my_id' ), ... col = dict ( name = 'my_col' ), ... idx = dict ( name = 'my_idx' ) ... ) ... ) >>> @override_field_config ( my_fields_config ) ... class MyRecords ( vbt . Records ): ... pass >>> records_arr = np . array ([ ... ( 0 , 0 , 0 ), ... ( 1 , 0 , 1 ), ... ( 2 , 1 , 0 ), ... ( 3 , 1 , 1 ) ... ], dtype = my_dt ) >>> wrapper = vbt . ArrayWrapper ( index = [ 'x' , 'y' ], ... columns = [ 'a' , 'b' ], ndim = 2 , freq = '1 day' ) >>> my_records = MyRecords ( wrapper , records_arr ) >>> my_records . id_arr array([0, 1, 2, 3]) >>> my_records . col_arr array([0, 0, 1, 1]) >>> my_records . idx_arr array([0, 1, 0, 1]) Alternatively, we can override the _field_config class attribute. >>> @override_field_config ... class MyRecords ( vbt . Records ): ... _field_config = dict ( ... dtype = my_dt , ... settings = dict ( ... id = dict ( name = 'my_id' ), ... idx = dict ( name = 'my_idx' ), ... col = dict ( name = 'my_col' ) ... ) ... ) Note Don't forget to decorate the class with @override_field_config to inherit configs from base classes. You can stop inheritance by not decorating or passing merge_configs=False to the decorator.","title":"Extending"},{"location":"api/records/base/#vectorbt.records.base.MetaFields","text":"Meta class that exposes a read-only class property MetaFields.field_config . Superclasses builtins.type Subclasses MetaRecords","title":"MetaFields"},{"location":"api/records/base/#vectorbt.records.base.MetaFields.field_config","text":"Field config.","title":"field_config"},{"location":"api/records/base/#vectorbt.records.base.MetaRecords","text":"Meta class that exposes a read-only class property StatsBuilderMixin.metrics . Superclasses MetaFields MetaPlotsBuilderMixin MetaStatsBuilderMixin builtins.type Inherited members MetaFields.field_config MetaPlotsBuilderMixin.subplots MetaStatsBuilderMixin.metrics","title":"MetaRecords"},{"location":"api/records/base/#vectorbt.records.base.Records","text":"Wraps the actual records array (such as trades) and exposes methods for mapping it to some array of values (such as PnL of each trade). Args wrapper :\u2002 ArrayWrapper Array wrapper. See ArrayWrapper . records_arr :\u2002 array_like A structured NumPy array of records. Must have the fields id (record index) and col (column index). col_mapper :\u2002 ColumnMapper Column mapper if already known. Note It depends on records_arr , so make sure to invalidate col_mapper upon creating a Records instance with a modified records_arr . Records.replace() does it automatically. **kwargs Custom keyword arguments passed to the config. Useful if any subclass wants to extend the config. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin RecordsWithFields StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.config Wrapping.iloc Wrapping.indexing_kwargs Wrapping.loc Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Wrapping.self_aliases Wrapping.wrapper Wrapping.writeable_attrs Subclasses Logs Orders Ranges","title":"Records"},{"location":"api/records/base/#vectorbt.records.base.Records.apply","text":"Records . apply ( apply_func_nb , * args , group_by = None , apply_per_group = False , dtype = None , ** kwargs ) Apply function on records per column/group. Returns mapped array. Applies per group if apply_per_group is True. See apply_on_records_nb() . **kwargs are passed to Records.map_array() .","title":"apply()"},{"location":"api/records/base/#vectorbt.records.base.Records.apply_mask","text":"Records . apply_mask ( mask , group_by = None , ** kwargs ) Return a new class instance, filtered by mask.","title":"apply_mask()"},{"location":"api/records/base/#vectorbt.records.base.Records.build_field_config_doc","text":"Records . build_field_config_doc ( source_cls = None ) Build field config documentation.","title":"build_field_config_doc()"},{"location":"api/records/base/#vectorbt.records.base.Records.col_arr","text":"Get column array.","title":"col_arr"},{"location":"api/records/base/#vectorbt.records.base.Records.col_mapper","text":"Column mapper. See ColumnMapper .","title":"col_mapper"},{"location":"api/records/base/#vectorbt.records.base.Records.count","text":"Records . count ( group_by = None , wrap_kwargs = None ) Return count by column.","title":"count()"},{"location":"api/records/base/#vectorbt.records.base.Records.field_config","text":"Field config of Records . Co nf ig( { \"dtype\" : null , \"settings\" : { \"id\" : { \"name\" : \"id\" , \"title\" : \"Id\" }, \"col\" : { \"name\" : \"col\" , \"title\" : \"Column\" , \"mapping\" : \"columns\" }, \"idx\" : { \"name\" : \"idx\" , \"title\" : \"Timestamp\" , \"mapping\" : \"index\" } } } )","title":"field_config"},{"location":"api/records/base/#vectorbt.records.base.Records.get_apply_mapping_arr","text":"Records . get_apply_mapping_arr ( field , ** kwargs ) Resolve the mapped array on the field, with mapping applied. Uses Records.field_config .","title":"get_apply_mapping_arr()"},{"location":"api/records/base/#vectorbt.records.base.Records.get_by_col_idxs","text":"Records . get_by_col_idxs ( col_idxs ) Get records corresponding to column indices. Returns new records array.","title":"get_by_col_idxs()"},{"location":"api/records/base/#vectorbt.records.base.Records.get_field_arr","text":"Records . get_field_arr ( field ) Resolve the array of the field. Uses Records.field_config .","title":"get_field_arr()"},{"location":"api/records/base/#vectorbt.records.base.Records.get_field_mapping","text":"Records . get_field_mapping ( field ) Resolve the mapping of the field. Uses Records.field_config .","title":"get_field_mapping()"},{"location":"api/records/base/#vectorbt.records.base.Records.get_field_name","text":"Records . get_field_name ( field ) Resolve the name of the field. Uses Records.field_config ..","title":"get_field_name()"},{"location":"api/records/base/#vectorbt.records.base.Records.get_field_setting","text":"Records . get_field_setting ( field , setting , default = None ) Resolve any setting of the field. Uses Records.field_config .","title":"get_field_setting()"},{"location":"api/records/base/#vectorbt.records.base.Records.get_field_title","text":"Records . get_field_title ( field ) Resolve the title of the field. Uses Records.field_config .","title":"get_field_title()"},{"location":"api/records/base/#vectorbt.records.base.Records.get_map_field","text":"Records . get_map_field ( field , ** kwargs ) Resolve the mapped array of the field. Uses Records.field_config .","title":"get_map_field()"},{"location":"api/records/base/#vectorbt.records.base.Records.get_map_field_to_index","text":"Records . get_map_field_to_index ( field , ** kwargs ) Resolve the mapped array on the field, with index applied. Uses Records.field_config .","title":"get_map_field_to_index()"},{"location":"api/records/base/#vectorbt.records.base.Records.id_arr","text":"Get id array.","title":"id_arr"},{"location":"api/records/base/#vectorbt.records.base.Records.idx_arr","text":"Get index array.","title":"idx_arr"},{"location":"api/records/base/#vectorbt.records.base.Records.indexing_func","text":"Records . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on Records .","title":"indexing_func()"},{"location":"api/records/base/#vectorbt.records.base.Records.indexing_func_meta","text":"Records . indexing_func_meta ( pd_indexing_func , ** kwargs ) Perform indexing on Records and return metadata.","title":"indexing_func_meta()"},{"location":"api/records/base/#vectorbt.records.base.Records.is_sorted","text":"Records . is_sorted ( incl_id = False ) Check whether records are sorted.","title":"is_sorted()"},{"location":"api/records/base/#vectorbt.records.base.Records.map","text":"Records . map ( map_func_nb , * args , dtype = None , ** kwargs ) Map each record to a scalar value. Returns mapped array. See map_records_nb() . **kwargs are passed to Records.map_array() .","title":"map()"},{"location":"api/records/base/#vectorbt.records.base.Records.map_array","text":"Records . map_array ( a , idx_arr = None , mapping = None , group_by = None , ** kwargs ) Convert array to mapped array. The length of the array should match that of the records.","title":"map_array()"},{"location":"api/records/base/#vectorbt.records.base.Records.map_field","text":"Records . map_field ( field , ** kwargs ) Convert field to mapped array. **kwargs are passed to Records.map_array() .","title":"map_field()"},{"location":"api/records/base/#vectorbt.records.base.Records.metrics","text":"Metrics supported by Records . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function Records.<lambda> at 0x7fac881edf28>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function Records.<lambda> at 0x7fac881e9048>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function Records.<lambda> at 0x7fac881e90d0>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"count\" : { \"title\" : \"Count\" , \"calc_func\" : \"count\" , \"tags\" : \"records\" } } ) Returns Records._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable Records._metrics .","title":"metrics"},{"location":"api/records/base/#vectorbt.records.base.Records.override_field_config_doc","text":"Records . override_field_config_doc ( __pdoc__ , source_cls = None ) Call this method on each subclass that overrides field_config .","title":"override_field_config_doc()"},{"location":"api/records/base/#vectorbt.records.base.Records.plots_defaults","text":"Defaults for PlotsBuilderMixin.plots() . Merges PlotsBuilderMixin.plots_defaults and records.plots from settings .","title":"plots_defaults"},{"location":"api/records/base/#vectorbt.records.base.Records.recarray","text":"","title":"recarray"},{"location":"api/records/base/#vectorbt.records.base.Records.records","text":"Records.","title":"records"},{"location":"api/records/base/#vectorbt.records.base.Records.records_arr","text":"Records array.","title":"records_arr"},{"location":"api/records/base/#vectorbt.records.base.Records.records_readable","text":"Records in readable format.","title":"records_readable"},{"location":"api/records/base/#vectorbt.records.base.Records.replace","text":"Records . replace ( ** kwargs ) See Configured.replace() . Also, makes sure that Records.col_mapper is not passed to the new instance.","title":"replace()"},{"location":"api/records/base/#vectorbt.records.base.Records.sort","text":"Records . sort ( incl_id = False , group_by = None , ** kwargs ) Sort records by columns (primary) and ids (secondary, optional). Note Sorting is expensive. A better approach is to append records already in the correct order.","title":"sort()"},{"location":"api/records/base/#vectorbt.records.base.Records.stats_defaults","text":"Defaults for StatsBuilderMixin.stats() . Merges StatsBuilderMixin.stats_defaults and records.stats from settings .","title":"stats_defaults"},{"location":"api/records/base/#vectorbt.records.base.Records.subplots","text":"Subplots supported by Records . Co nf ig( {} ) Returns Records._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable Records._subplots .","title":"subplots"},{"location":"api/records/base/#vectorbt.records.base.Records.values","text":"Records array.","title":"values"},{"location":"api/records/base/#vectorbt.records.base.RecordsWithFields","text":"Class exposes a read-only class property RecordsWithFields.field_config . Subclasses Records","title":"RecordsWithFields"},{"location":"api/records/base/#vectorbt.records.base.RecordsWithFields.field_config","text":"Field config of ${cls_name} . $ { f ield_co nf ig }","title":"field_config"},{"location":"api/records/col_mapper/","text":"col_mapper module \u00b6 Class for mapping column arrays. ColumnMapper class \u00b6 Used by Records and MappedArray classes to make use of column and group metadata. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() Wrapping.config Wrapping.iloc Wrapping.indexing_func() Wrapping.indexing_kwargs Wrapping.loc Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Wrapping.self_aliases Wrapping.wrapper Wrapping.writeable_attrs col_arr property \u00b6 Column array. col_map method \u00b6 Column map. More flexible than ColumnMapper.col_range . More suited for mapped arrays. col_range method \u00b6 Column index. Faster than ColumnMapper.col_map but only compatible with sorted columns. More suited for records. get_col_arr method \u00b6 ColumnMapper . get_col_arr ( group_by = None ) Get group-aware column array. get_col_map method \u00b6 ColumnMapper . get_col_map ( group_by = None ) Get group-aware column map. get_col_range method \u00b6 ColumnMapper . get_col_range ( group_by = None ) Get group-aware column range. is_sorted method \u00b6 ColumnMapper . is_sorted () Check whether column array is sorted.","title":"col_mapper"},{"location":"api/records/col_mapper/#vectorbt.records.col_mapper","text":"Class for mapping column arrays.","title":"vectorbt.records.col_mapper"},{"location":"api/records/col_mapper/#vectorbt.records.col_mapper.ColumnMapper","text":"Used by Records and MappedArray classes to make use of column and group metadata. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() Wrapping.config Wrapping.iloc Wrapping.indexing_func() Wrapping.indexing_kwargs Wrapping.loc Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Wrapping.self_aliases Wrapping.wrapper Wrapping.writeable_attrs","title":"ColumnMapper"},{"location":"api/records/col_mapper/#vectorbt.records.col_mapper.ColumnMapper.col_arr","text":"Column array.","title":"col_arr"},{"location":"api/records/col_mapper/#vectorbt.records.col_mapper.ColumnMapper.col_map","text":"Column map. More flexible than ColumnMapper.col_range . More suited for mapped arrays.","title":"col_map"},{"location":"api/records/col_mapper/#vectorbt.records.col_mapper.ColumnMapper.col_range","text":"Column index. Faster than ColumnMapper.col_map but only compatible with sorted columns. More suited for records.","title":"col_range"},{"location":"api/records/col_mapper/#vectorbt.records.col_mapper.ColumnMapper.get_col_arr","text":"ColumnMapper . get_col_arr ( group_by = None ) Get group-aware column array.","title":"get_col_arr()"},{"location":"api/records/col_mapper/#vectorbt.records.col_mapper.ColumnMapper.get_col_map","text":"ColumnMapper . get_col_map ( group_by = None ) Get group-aware column map.","title":"get_col_map()"},{"location":"api/records/col_mapper/#vectorbt.records.col_mapper.ColumnMapper.get_col_range","text":"ColumnMapper . get_col_range ( group_by = None ) Get group-aware column range.","title":"get_col_range()"},{"location":"api/records/col_mapper/#vectorbt.records.col_mapper.ColumnMapper.is_sorted","text":"ColumnMapper . is_sorted () Check whether column array is sorted.","title":"is_sorted()"},{"location":"api/records/decorators/","text":"decorators module \u00b6 Class and function decorators. attach_fields function \u00b6 attach_fields ( * args , on_conflict = 'raise' ) Class decorator to attach field properties in a Records class. Will extract dtype and other relevant information from Records.field_config and map its fields as properties. This behavior can be changed by using config . Note Make sure to run attach_fields() after override_field_config() . config should contain fields (keys) and dictionaries (values) with the following keys: attach : Whether to attach the field property. Can be provided as a string to be used as a target attribute name. Defaults to True. defaults : Dictionary with default keyword arguments for Records.map_field() . Defaults to an empty dict. attach_filters : Whether to attach filters based on the field's values. Can be provided as a dict to be used instead of the mapping (filter value -> target filter name). Defaults to False. If True, defaults to mapping in Records.field_config . filter_defaults : Dictionary with default keyword arguments for Records.apply_mask() . Can be provided by target filter name. Defaults to an empty dict. on_conflict : Overrides global on_conflict for both field and filter properties. Any potential attribute name is prepared by placing underscores between capital letters and converting to the lower case. If an attribute with the same name already exists in the class but the name is not listed in the field config: it will be overridden if on_conflict is 'override' it will be ignored if on_conflict is 'ignore' an error will be raised if on_conflict is 'raise' override_field_config function \u00b6 override_field_config ( * args , merge_configs = True ) Class decorator to override field configs of all base classes in MRO that subclass Records . Instead of overriding _field_config class attribute, you can pass config directly to this decorator. Disable merge_configs to not merge, which will effectively disable field inheritance.","title":"decorators"},{"location":"api/records/decorators/#vectorbt.records.decorators","text":"Class and function decorators.","title":"vectorbt.records.decorators"},{"location":"api/records/decorators/#vectorbt.records.decorators.attach_fields","text":"attach_fields ( * args , on_conflict = 'raise' ) Class decorator to attach field properties in a Records class. Will extract dtype and other relevant information from Records.field_config and map its fields as properties. This behavior can be changed by using config . Note Make sure to run attach_fields() after override_field_config() . config should contain fields (keys) and dictionaries (values) with the following keys: attach : Whether to attach the field property. Can be provided as a string to be used as a target attribute name. Defaults to True. defaults : Dictionary with default keyword arguments for Records.map_field() . Defaults to an empty dict. attach_filters : Whether to attach filters based on the field's values. Can be provided as a dict to be used instead of the mapping (filter value -> target filter name). Defaults to False. If True, defaults to mapping in Records.field_config . filter_defaults : Dictionary with default keyword arguments for Records.apply_mask() . Can be provided by target filter name. Defaults to an empty dict. on_conflict : Overrides global on_conflict for both field and filter properties. Any potential attribute name is prepared by placing underscores between capital letters and converting to the lower case. If an attribute with the same name already exists in the class but the name is not listed in the field config: it will be overridden if on_conflict is 'override' it will be ignored if on_conflict is 'ignore' an error will be raised if on_conflict is 'raise'","title":"attach_fields()"},{"location":"api/records/decorators/#vectorbt.records.decorators.override_field_config","text":"override_field_config ( * args , merge_configs = True ) Class decorator to override field configs of all base classes in MRO that subclass Records . Instead of overriding _field_config class attribute, you can pass config directly to this decorator. Disable merge_configs to not merge, which will effectively disable field inheritance.","title":"override_field_config()"},{"location":"api/records/mapped_array/","text":"mapped_array module \u00b6 Base class for working with mapped arrays. This class takes the mapped array and the corresponding column and (optionally) index arrays, and offers features to directly process the mapped array without converting it to pandas; for example, to compute various statistics by column, such as standard deviation. Consider the following example: >>> import numpy as np >>> import pandas as pd >>> from numba import njit >>> import vectorbt as vbt >>> a = np . array ([ 10. , 11. , 12. , 13. , 14. , 15. , 16. , 17. , 18. ]) >>> col_arr = np . array ([ 0 , 0 , 0 , 1 , 1 , 1 , 2 , 2 , 2 ]) >>> idx_arr = np . array ([ 0 , 1 , 2 , 0 , 1 , 2 , 0 , 1 , 2 ]) >>> wrapper = vbt . ArrayWrapper ( index = [ 'x' , 'y' , 'z' ], ... columns = [ 'a' , 'b' , 'c' ], ndim = 2 , freq = '1 day' ) >>> ma = vbt . MappedArray ( wrapper , a , col_arr , idx_arr = idx_arr ) Reducing \u00b6 Using MappedArray , you can then reduce by column as follows: Use already provided reducers such as MappedArray.mean() : >>> ma . mean () a 11.0 b 14.0 c 17.0 dtype: float64 Use MappedArray.to_pd() to map to pandas and then reduce manually (expensive): >>> ma . to_pd () . mean () a 11.0 b 14.0 c 17.0 dtype: float64 Use MappedArray.reduce() to reduce using a custom function: >>> @njit ... def pow_mean_reduce_nb ( col , a , pow ): ... return np . mean ( a ** pow ) >>> ma . reduce ( pow_mean_reduce_nb , 2 ) a 121.666667 b 196.666667 c 289.666667 dtype: float64 >>> @njit ... def min_max_reduce_nb ( col , a ): ... return np . array ([ np . min ( a ), np . max ( a )]) >>> ma . reduce ( min_max_reduce_nb , returns_array = True , index = [ 'min' , 'max' ]) a b c min 10.0 13.0 16.0 max 12.0 15.0 18.0 >>> @njit ... def idxmin_idxmax_reduce_nb ( col , a ): ... return np . array ([ np . argmin ( a ), np . argmax ( a )]) >>> ma . reduce ( idxmin_idxmax_reduce_nb , returns_array = True , ... returns_idx = True , index = [ 'idxmin' , 'idxmax' ]) a b c idxmin x x x idxmax z z z Mapping \u00b6 Use MappedArray.apply() to apply a function on each column/group: >>> @njit ... def cumsum_apply_nb ( idxs , col , a ): ... return np . cumsum ( a ) >>> ma . apply ( cumsum_apply_nb ) <vectorbt.records.mapped_array.MappedArray at 0x7ff061382198> >>> ma . apply ( cumsum_apply_nb ) . values array([10., 21., 33., 13., 27., 42., 16., 33., 51.]) >>> group_by = np . array ([ 'first' , 'first' , 'second' ]) >>> ma . apply ( cumsum_apply_nb , group_by = group_by , apply_per_group = True ) . values array([10., 21., 33., 46., 60., 75., 16., 33., 51.]) Notice how cumsum resets at each column in the first example and at each group in the second example. ## Conversion You can expand any `MappedArray` instance to pandas: * Given `idx_arr` was provided: ```pycon >>> ma . to_pd () a b c x 10.0 13.0 16.0 y 11.0 14.0 17.0 z 12.0 15.0 18.0 Note Will raise an error if there are multiple values pointing to the same position. In case group_by was provided, index can be ignored, or there are position conflicts: >>> ma . to_pd ( group_by = np . array ([ 'first' , 'first' , 'second' ]), ignore_index = True ) first second 0 10.0 16.0 1 11.0 17.0 2 12.0 18.0 3 13.0 NaN 4 14.0 NaN 5 15.0 NaN Filtering \u00b6 Use MappedArray.apply_mask() to filter elements per column/group: >>> mask = [ True , False , True , False , True , False , True , False , True ] >>> filtered_ma = ma . apply_mask ( mask ) >>> filtered_ma . count () a 2 b 1 c 2 dtype: int64 >>> filtered_ma . id_arr array([0, 2, 4, 6, 8]) Plotting \u00b6 You can build histograms and boxplots of MappedArray directly: >>> ma . boxplot () To use scatterplots or any other plots that require index, convert to pandas first: >>> ma . to_pd () . vbt . plot () Grouping \u00b6 One of the key features of MappedArray is that you can perform reducing operations on a group of columns as if they were a single column. Groups can be specified by group_by , which can be anything from positions or names of column levels, to a NumPy array with actual groups. There are multiple ways of define grouping: When creating MappedArray , pass group_by to ArrayWrapper : >>> group_by = np . array ([ 'first' , 'first' , 'second' ]) >>> grouped_wrapper = wrapper . replace ( group_by = group_by ) >>> grouped_ma = vbt . MappedArray ( grouped_wrapper , a , col_arr , idx_arr = idx_arr ) >>> grouped_ma . mean () first 12.5 second 17.0 dtype: float64 Regroup an existing MappedArray : >>> ma . regroup ( group_by ) . mean () first 12.5 second 17.0 dtype: float64 Pass group_by directly to the reducing method: >>> ma . mean ( group_by = group_by ) first 12.5 second 17.0 dtype: float64 By the same way you can disable or modify any existing grouping: >>> grouped_ma . mean ( group_by = False ) a 11.0 b 14.0 c 17.0 dtype: float64 Note Grouping applies only to reducing operations, there is no change to the arrays. Operators \u00b6 MappedArray implements arithmetic, comparison and logical operators. You can perform basic operations (such as addition) on mapped arrays as if they were NumPy arrays. >>> ma ** 2 <vectorbt.records.mapped_array.MappedArray at 0x7f97bfc49358> >>> ma * np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) <vectorbt.records.mapped_array.MappedArray at 0x7f97bfc65e80> >>> ma + ma <vectorbt.records.mapped_array.MappedArray at 0x7fd638004d30> Note You should ensure that your MappedArray operand is on the left if the other operand is an array. If two MappedArray operands have different metadata, will copy metadata from the first one, but at least their id_arr and col_arr must match. Indexing \u00b6 Like any other class subclassing Wrapping , we can do pandas indexing on a MappedArray instance, which forwards indexing operation to each object with columns: >>> ma [ 'a' ] . values array([10., 11., 12.]) >>> grouped_ma [ 'first' ] . values array([10., 11., 12., 13., 14., 15.]) Note Changing index (time axis) is not supported. The object should be treated as a Series rather than a DataFrame; for example, use some_field.iloc[0] instead of some_field.iloc[:, 0] . Indexing behavior depends solely upon ArrayWrapper . For example, if group_select is enabled indexing will be performed on groups, otherwise on single columns. Caching \u00b6 MappedArray supports caching. If a method or a property requires heavy computation, it's wrapped with cached_method() and cached_property respectively. Caching can be disabled globally via caching in settings . Note Because of caching, class is meant to be immutable and all properties are read-only. To change any attribute, use the copy method and pass the attribute as keyword argument. Saving and loading \u00b6 Like any other class subclassing Pickleable , we can save a MappedArray instance to the disk with Pickleable.save() and load it with Pickleable.load() . Stats \u00b6 Hint See StatsBuilderMixin.stats() and MappedArray.metrics . Metric for mapped arrays are similar to that for GenericAccessor . >>> ma . stats ( column = 'a' ) Start x End z Period 3 days 00:00:00 Count 3 Mean 11.0 Std 1.0 Min 10.0 Median 11.0 Max 12.0 Min Index x Max Index z Name: a, dtype: object The main difference unfolds once the mapped array has a mapping: values are then considered as categorical and usual statistics are meaningless to compute. For this case, StatsBuilderMixin.stats() returns the value counts: >>> mapping = { v : \"test_\" + str ( v ) for v in np . unique ( ma . values )} >>> ma . stats ( column = 'a' , settings = dict ( mapping = mapping )) Start x End z Period 3 days 00:00:00 Count 3 Value Counts: test_10.0 1 Value Counts: test_11.0 1 Value Counts: test_12.0 1 Value Counts: test_13.0 0 Value Counts: test_14.0 0 Value Counts: test_15.0 0 Value Counts: test_16.0 0 Value Counts: test_17.0 0 Value Counts: test_18.0 0 Name: a, dtype: object `MappedArray.stats` also supports (re-)grouping: ```pycon >>> grouped_ma . stats ( column = 'first' ) Start x End z Period 3 days 00:00:00 Count 6 Mean 12.5 Std 1.870829 Min 10.0 Median 12.5 Max 15.0 Min Index x Max Index z Name: first, dtype: object Plots \u00b6 Hint See PlotsBuilderMixin.plots() and MappedArray.subplots . MappedArray class has a single subplot based on MappedArray.to_pd() and GenericAccessor.plot() : >>> ma . plots () combine_mapped_with_other function \u00b6 combine_mapped_with_other ( other , np_func ) Combine MappedArray with other compatible object. If other object is also MappedArray , their id_arr and col_arr must match. MappedArray class \u00b6 Exposes methods for reducing, converting, and plotting arrays mapped by Records class. Args wrapper :\u2002 ArrayWrapper Array wrapper. See ArrayWrapper . mapped_arr :\u2002 array_like A one-dimensional array of mapped record values. col_arr :\u2002 array_like A one-dimensional column array. Must be of the same size as mapped_arr . id_arr :\u2002 array_like A one-dimensional id array. Defaults to simple range. Must be of the same size as mapped_arr . idx_arr :\u2002 array_like A one-dimensional index array. Optional. Must be of the same size as mapped_arr . mapping :\u2002 namedtuple , dict or callable Mapping. col_mapper :\u2002 ColumnMapper Column mapper if already known. Note It depends upon wrapper and col_arr , so make sure to invalidate col_mapper upon creating a MappedArray instance with a modified wrapper or `col_arr. MappedArray.replace() does it automatically. **kwargs Custom keyword arguments passed to the config. Useful if any subclass wants to extend the config. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.config Wrapping.iloc Wrapping.indexing_kwargs Wrapping.loc Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Wrapping.self_aliases Wrapping.wrapper Wrapping.writeable_attrs apply method \u00b6 MappedArray . apply ( apply_func_nb , * args , group_by = None , apply_per_group = False , dtype = None , ** kwargs ) Apply function on mapped array per column/group. Returns mapped array. Applies per group if apply_per_group is True. See apply_on_mapped_nb() . **kwargs are passed to MappedArray.replace() . apply_mapping method \u00b6 MappedArray . apply_mapping ( mapping = None , ** kwargs ) Apply mapping on each element. apply_mask method \u00b6 MappedArray . apply_mask ( mask , idx_arr = None , group_by = None , ** kwargs ) Return a new class instance, filtered by mask. **kwargs are passed to MappedArray.replace() . bottom_n method \u00b6 MappedArray . bottom_n ( n , ** kwargs ) Filter bottom N elements from each column/group. bottom_n_mask method \u00b6 MappedArray . bottom_n_mask ( n , ** kwargs ) Return mask of bottom N elements in each column/group. boxplot method \u00b6 MappedArray . boxplot ( group_by = None , ** kwargs ) Plot box plot by column/group. col_arr property \u00b6 Column array. col_mapper property \u00b6 Column mapper. See ColumnMapper . count method \u00b6 MappedArray . count ( group_by = None , wrap_kwargs = None ) Return number of values by column/group. describe method \u00b6 MappedArray . describe ( percentiles = None , ddof = 1 , group_by = None , wrap_kwargs = None , ** kwargs ) Return statistics by column/group. histplot method \u00b6 MappedArray . histplot ( group_by = None , ** kwargs ) Plot histogram by column/group. id_arr property \u00b6 Id array. idx_arr property \u00b6 Index array. idxmax method \u00b6 MappedArray . idxmax ( group_by = None , wrap_kwargs = None , ** kwargs ) Return index of max by column/group. idxmin method \u00b6 MappedArray . idxmin ( group_by = None , wrap_kwargs = None , ** kwargs ) Return index of min by column/group. indexing_func method \u00b6 MappedArray . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on MappedArray . indexing_func_meta method \u00b6 MappedArray . indexing_func_meta ( pd_indexing_func , ** kwargs ) Perform indexing on MappedArray and return metadata. is_expandable method \u00b6 MappedArray . is_expandable ( idx_arr = None , group_by = None ) See is_mapped_expandable_nb() . is_sorted method \u00b6 MappedArray . is_sorted ( incl_id = False ) Check whether mapped array is sorted. map_to_mask method \u00b6 MappedArray . map_to_mask ( inout_map_func_nb , * args , group_by = None ) Map mapped array to a mask. See mapped_to_mask_nb() . mapped_arr property \u00b6 Mapped array. mapping property \u00b6 Mapping. max method \u00b6 MappedArray . max ( group_by = None , wrap_kwargs = None , ** kwargs ) Return max by column/group. mean method \u00b6 MappedArray . mean ( group_by = None , wrap_kwargs = None , ** kwargs ) Return mean by column/group. median method \u00b6 MappedArray . median ( group_by = None , wrap_kwargs = None , ** kwargs ) Return median by column/group. metrics class variable \u00b6 Metrics supported by MappedArray . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function MappedArray.<lambda> at 0x7fac881fb378>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function MappedArray.<lambda> at 0x7fac881fb400>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function MappedArray.<lambda> at 0x7fac881fb488>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"count\" : { \"title\" : \"Count\" , \"calc_func\" : \"count\" , \"tags\" : \"mapped_array\" }, \"mean\" : { \"title\" : \"Mean\" , \"calc_func\" : \"mean\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"mapped_array\" , \"describe\" ] }, \"std\" : { \"title\" : \"Std\" , \"calc_func\" : \"std\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"mapped_array\" , \"describe\" ] }, \"min\" : { \"title\" : \"Min\" , \"calc_func\" : \"min\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"mapped_array\" , \"describe\" ] }, \"median\" : { \"title\" : \"Median\" , \"calc_func\" : \"median\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"mapped_array\" , \"describe\" ] }, \"max\" : { \"title\" : \"Max\" , \"calc_func\" : \"max\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"mapped_array\" , \"describe\" ] }, \"idx_min\" : { \"title\" : \"Min Index\" , \"calc_func\" : \"idxmin\" , \"inv_check_has_mapping\" : true , \"agg_func\" : null , \"tags\" : [ \"mapped_array\" , \"index\" ] }, \"idx_max\" : { \"title\" : \"Max Index\" , \"calc_func\" : \"idxmax\" , \"inv_check_has_mapping\" : true , \"agg_func\" : null , \"tags\" : [ \"mapped_array\" , \"index\" ] }, \"value_counts\" : { \"title\" : \"Value Counts\" , \"calc_func\" : \"<function MappedArray.<lambda> at 0x7fac881fb510>\" , \"resolve_value_counts\" : true , \"check_has_mapping\" : true , \"tags\" : [ \"mapped_array\" , \"value_counts\" ] } } ) Returns MappedArray._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable MappedArray._metrics . min method \u00b6 MappedArray . min ( group_by = None , wrap_kwargs = None , ** kwargs ) Return min by column/group. nth method \u00b6 MappedArray . nth ( n , group_by = None , wrap_kwargs = None , ** kwargs ) Return n-th element of each column/group. nth_index method \u00b6 MappedArray . nth_index ( n , group_by = None , wrap_kwargs = None , ** kwargs ) Return index of n-th element of each column/group. plots_defaults property \u00b6 Defaults for PlotsBuilderMixin.plots() . Merges PlotsBuilderMixin.plots_defaults and mapped_array.plots from settings . reduce method \u00b6 MappedArray . reduce ( reduce_func_nb , * args , idx_arr = None , returns_array = False , returns_idx = False , to_index = True , fill_value = nan , group_by = None , wrap_kwargs = None ) Reduce mapped array by column/group. If returns_array is False and returns_idx is False, see reduce_mapped_nb() . If returns_array is False and returns_idx is True, see reduce_mapped_to_idx_nb() . If returns_array is True and returns_idx is False, see reduce_mapped_to_array_nb() . If returns_array is True and returns_idx is True, see reduce_mapped_to_idx_array_nb() . If returns_idx is True, must pass idx_arr . Set to_index to False to return raw positions instead of labels. Use fill_value to set the default value. Set group_by to False to disable grouping. replace method \u00b6 MappedArray . replace ( ** kwargs ) See Configured.replace() . Also, makes sure that MappedArray.col_mapper is not passed to the new instance. sort method \u00b6 MappedArray . sort ( incl_id = False , idx_arr = None , group_by = None , ** kwargs ) Sort mapped array by column array (primary) and id array (secondary, optional). **kwargs are passed to MappedArray.replace() . stats_defaults property \u00b6 Defaults for StatsBuilderMixin.stats() . Merges StatsBuilderMixin.stats_defaults and mapped_array.stats from settings . std method \u00b6 MappedArray . std ( ddof = 1 , group_by = None , wrap_kwargs = None , ** kwargs ) Return std by column/group. subplots class variable \u00b6 Subplots supported by MappedArray . Co nf ig( { \"to_pd_plot\" : { \"check_is_not_grouped\" : true , \"plot_func\" : \"to_pd.vbt.plot\" , \"pass_trace_names\" : false , \"tags\" : \"mapped_array\" } } ) Returns MappedArray._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable MappedArray._subplots . sum method \u00b6 MappedArray . sum ( fill_value = 0.0 , group_by = None , wrap_kwargs = None , ** kwargs ) Return sum by column/group. to_index method \u00b6 MappedArray . to_index () Convert to index. to_pd method \u00b6 MappedArray . to_pd ( idx_arr = None , ignore_index = False , fill_value = nan , group_by = None , wrap_kwargs = None ) Expand mapped array to a Series/DataFrame. If ignore_index , will ignore the index and stack data points on top of each other in every column/group (see stack_expand_mapped_nb() ). Otherwise, see expand_mapped_nb() . Note Will raise an error if there are multiple values pointing to the same position. Set ignore_index to True in this case. Warning Mapped arrays represent information in the most memory-friendly format. Mapping back to pandas may occupy lots of memory if records are sparse. top_n method \u00b6 MappedArray . top_n ( n , ** kwargs ) Filter top N elements from each column/group. top_n_mask method \u00b6 MappedArray . top_n_mask ( n , ** kwargs ) Return mask of top N elements in each column/group. value_counts method \u00b6 MappedArray . value_counts ( normalize = False , sort_uniques = True , sort = False , ascending = False , dropna = False , group_by = None , mapping = None , incl_all_keys = False , wrap_kwargs = None , ** kwargs ) See GenericAccessor.value_counts() . Note Does not take into account missing values. values property \u00b6 Mapped array. MetaMappedArray class \u00b6 Meta class that exposes a read-only class property StatsBuilderMixin.metrics . Superclasses MetaPlotsBuilderMixin MetaStatsBuilderMixin builtins.type Inherited members MetaPlotsBuilderMixin.subplots MetaStatsBuilderMixin.metrics","title":"mapped_array"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array","text":"Base class for working with mapped arrays. This class takes the mapped array and the corresponding column and (optionally) index arrays, and offers features to directly process the mapped array without converting it to pandas; for example, to compute various statistics by column, such as standard deviation. Consider the following example: >>> import numpy as np >>> import pandas as pd >>> from numba import njit >>> import vectorbt as vbt >>> a = np . array ([ 10. , 11. , 12. , 13. , 14. , 15. , 16. , 17. , 18. ]) >>> col_arr = np . array ([ 0 , 0 , 0 , 1 , 1 , 1 , 2 , 2 , 2 ]) >>> idx_arr = np . array ([ 0 , 1 , 2 , 0 , 1 , 2 , 0 , 1 , 2 ]) >>> wrapper = vbt . ArrayWrapper ( index = [ 'x' , 'y' , 'z' ], ... columns = [ 'a' , 'b' , 'c' ], ndim = 2 , freq = '1 day' ) >>> ma = vbt . MappedArray ( wrapper , a , col_arr , idx_arr = idx_arr )","title":"vectorbt.records.mapped_array"},{"location":"api/records/mapped_array/#reducing","text":"Using MappedArray , you can then reduce by column as follows: Use already provided reducers such as MappedArray.mean() : >>> ma . mean () a 11.0 b 14.0 c 17.0 dtype: float64 Use MappedArray.to_pd() to map to pandas and then reduce manually (expensive): >>> ma . to_pd () . mean () a 11.0 b 14.0 c 17.0 dtype: float64 Use MappedArray.reduce() to reduce using a custom function: >>> @njit ... def pow_mean_reduce_nb ( col , a , pow ): ... return np . mean ( a ** pow ) >>> ma . reduce ( pow_mean_reduce_nb , 2 ) a 121.666667 b 196.666667 c 289.666667 dtype: float64 >>> @njit ... def min_max_reduce_nb ( col , a ): ... return np . array ([ np . min ( a ), np . max ( a )]) >>> ma . reduce ( min_max_reduce_nb , returns_array = True , index = [ 'min' , 'max' ]) a b c min 10.0 13.0 16.0 max 12.0 15.0 18.0 >>> @njit ... def idxmin_idxmax_reduce_nb ( col , a ): ... return np . array ([ np . argmin ( a ), np . argmax ( a )]) >>> ma . reduce ( idxmin_idxmax_reduce_nb , returns_array = True , ... returns_idx = True , index = [ 'idxmin' , 'idxmax' ]) a b c idxmin x x x idxmax z z z","title":"Reducing"},{"location":"api/records/mapped_array/#mapping","text":"Use MappedArray.apply() to apply a function on each column/group: >>> @njit ... def cumsum_apply_nb ( idxs , col , a ): ... return np . cumsum ( a ) >>> ma . apply ( cumsum_apply_nb ) <vectorbt.records.mapped_array.MappedArray at 0x7ff061382198> >>> ma . apply ( cumsum_apply_nb ) . values array([10., 21., 33., 13., 27., 42., 16., 33., 51.]) >>> group_by = np . array ([ 'first' , 'first' , 'second' ]) >>> ma . apply ( cumsum_apply_nb , group_by = group_by , apply_per_group = True ) . values array([10., 21., 33., 46., 60., 75., 16., 33., 51.]) Notice how cumsum resets at each column in the first example and at each group in the second example. ## Conversion You can expand any `MappedArray` instance to pandas: * Given `idx_arr` was provided: ```pycon >>> ma . to_pd () a b c x 10.0 13.0 16.0 y 11.0 14.0 17.0 z 12.0 15.0 18.0 Note Will raise an error if there are multiple values pointing to the same position. In case group_by was provided, index can be ignored, or there are position conflicts: >>> ma . to_pd ( group_by = np . array ([ 'first' , 'first' , 'second' ]), ignore_index = True ) first second 0 10.0 16.0 1 11.0 17.0 2 12.0 18.0 3 13.0 NaN 4 14.0 NaN 5 15.0 NaN","title":"Mapping"},{"location":"api/records/mapped_array/#filtering","text":"Use MappedArray.apply_mask() to filter elements per column/group: >>> mask = [ True , False , True , False , True , False , True , False , True ] >>> filtered_ma = ma . apply_mask ( mask ) >>> filtered_ma . count () a 2 b 1 c 2 dtype: int64 >>> filtered_ma . id_arr array([0, 2, 4, 6, 8])","title":"Filtering"},{"location":"api/records/mapped_array/#plotting","text":"You can build histograms and boxplots of MappedArray directly: >>> ma . boxplot () To use scatterplots or any other plots that require index, convert to pandas first: >>> ma . to_pd () . vbt . plot ()","title":"Plotting"},{"location":"api/records/mapped_array/#grouping","text":"One of the key features of MappedArray is that you can perform reducing operations on a group of columns as if they were a single column. Groups can be specified by group_by , which can be anything from positions or names of column levels, to a NumPy array with actual groups. There are multiple ways of define grouping: When creating MappedArray , pass group_by to ArrayWrapper : >>> group_by = np . array ([ 'first' , 'first' , 'second' ]) >>> grouped_wrapper = wrapper . replace ( group_by = group_by ) >>> grouped_ma = vbt . MappedArray ( grouped_wrapper , a , col_arr , idx_arr = idx_arr ) >>> grouped_ma . mean () first 12.5 second 17.0 dtype: float64 Regroup an existing MappedArray : >>> ma . regroup ( group_by ) . mean () first 12.5 second 17.0 dtype: float64 Pass group_by directly to the reducing method: >>> ma . mean ( group_by = group_by ) first 12.5 second 17.0 dtype: float64 By the same way you can disable or modify any existing grouping: >>> grouped_ma . mean ( group_by = False ) a 11.0 b 14.0 c 17.0 dtype: float64 Note Grouping applies only to reducing operations, there is no change to the arrays.","title":"Grouping"},{"location":"api/records/mapped_array/#operators","text":"MappedArray implements arithmetic, comparison and logical operators. You can perform basic operations (such as addition) on mapped arrays as if they were NumPy arrays. >>> ma ** 2 <vectorbt.records.mapped_array.MappedArray at 0x7f97bfc49358> >>> ma * np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) <vectorbt.records.mapped_array.MappedArray at 0x7f97bfc65e80> >>> ma + ma <vectorbt.records.mapped_array.MappedArray at 0x7fd638004d30> Note You should ensure that your MappedArray operand is on the left if the other operand is an array. If two MappedArray operands have different metadata, will copy metadata from the first one, but at least their id_arr and col_arr must match.","title":"Operators"},{"location":"api/records/mapped_array/#indexing","text":"Like any other class subclassing Wrapping , we can do pandas indexing on a MappedArray instance, which forwards indexing operation to each object with columns: >>> ma [ 'a' ] . values array([10., 11., 12.]) >>> grouped_ma [ 'first' ] . values array([10., 11., 12., 13., 14., 15.]) Note Changing index (time axis) is not supported. The object should be treated as a Series rather than a DataFrame; for example, use some_field.iloc[0] instead of some_field.iloc[:, 0] . Indexing behavior depends solely upon ArrayWrapper . For example, if group_select is enabled indexing will be performed on groups, otherwise on single columns.","title":"Indexing"},{"location":"api/records/mapped_array/#caching","text":"MappedArray supports caching. If a method or a property requires heavy computation, it's wrapped with cached_method() and cached_property respectively. Caching can be disabled globally via caching in settings . Note Because of caching, class is meant to be immutable and all properties are read-only. To change any attribute, use the copy method and pass the attribute as keyword argument.","title":"Caching"},{"location":"api/records/mapped_array/#saving-and-loading","text":"Like any other class subclassing Pickleable , we can save a MappedArray instance to the disk with Pickleable.save() and load it with Pickleable.load() .","title":"Saving and loading"},{"location":"api/records/mapped_array/#stats","text":"Hint See StatsBuilderMixin.stats() and MappedArray.metrics . Metric for mapped arrays are similar to that for GenericAccessor . >>> ma . stats ( column = 'a' ) Start x End z Period 3 days 00:00:00 Count 3 Mean 11.0 Std 1.0 Min 10.0 Median 11.0 Max 12.0 Min Index x Max Index z Name: a, dtype: object The main difference unfolds once the mapped array has a mapping: values are then considered as categorical and usual statistics are meaningless to compute. For this case, StatsBuilderMixin.stats() returns the value counts: >>> mapping = { v : \"test_\" + str ( v ) for v in np . unique ( ma . values )} >>> ma . stats ( column = 'a' , settings = dict ( mapping = mapping )) Start x End z Period 3 days 00:00:00 Count 3 Value Counts: test_10.0 1 Value Counts: test_11.0 1 Value Counts: test_12.0 1 Value Counts: test_13.0 0 Value Counts: test_14.0 0 Value Counts: test_15.0 0 Value Counts: test_16.0 0 Value Counts: test_17.0 0 Value Counts: test_18.0 0 Name: a, dtype: object `MappedArray.stats` also supports (re-)grouping: ```pycon >>> grouped_ma . stats ( column = 'first' ) Start x End z Period 3 days 00:00:00 Count 6 Mean 12.5 Std 1.870829 Min 10.0 Median 12.5 Max 15.0 Min Index x Max Index z Name: first, dtype: object","title":"Stats"},{"location":"api/records/mapped_array/#plots","text":"Hint See PlotsBuilderMixin.plots() and MappedArray.subplots . MappedArray class has a single subplot based on MappedArray.to_pd() and GenericAccessor.plot() : >>> ma . plots ()","title":"Plots"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.combine_mapped_with_other","text":"combine_mapped_with_other ( other , np_func ) Combine MappedArray with other compatible object. If other object is also MappedArray , their id_arr and col_arr must match.","title":"combine_mapped_with_other()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray","text":"Exposes methods for reducing, converting, and plotting arrays mapped by Records class. Args wrapper :\u2002 ArrayWrapper Array wrapper. See ArrayWrapper . mapped_arr :\u2002 array_like A one-dimensional array of mapped record values. col_arr :\u2002 array_like A one-dimensional column array. Must be of the same size as mapped_arr . id_arr :\u2002 array_like A one-dimensional id array. Defaults to simple range. Must be of the same size as mapped_arr . idx_arr :\u2002 array_like A one-dimensional index array. Optional. Must be of the same size as mapped_arr . mapping :\u2002 namedtuple , dict or callable Mapping. col_mapper :\u2002 ColumnMapper Column mapper if already known. Note It depends upon wrapper and col_arr , so make sure to invalidate col_mapper upon creating a MappedArray instance with a modified wrapper or `col_arr. MappedArray.replace() does it automatically. **kwargs Custom keyword arguments passed to the config. Useful if any subclass wants to extend the config. Superclasses AttrResolver Configured Documented IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.to_doc() Configured.update_config() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.config Wrapping.iloc Wrapping.indexing_kwargs Wrapping.loc Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Wrapping.self_aliases Wrapping.wrapper Wrapping.writeable_attrs","title":"MappedArray"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.apply","text":"MappedArray . apply ( apply_func_nb , * args , group_by = None , apply_per_group = False , dtype = None , ** kwargs ) Apply function on mapped array per column/group. Returns mapped array. Applies per group if apply_per_group is True. See apply_on_mapped_nb() . **kwargs are passed to MappedArray.replace() .","title":"apply()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.apply_mapping","text":"MappedArray . apply_mapping ( mapping = None , ** kwargs ) Apply mapping on each element.","title":"apply_mapping()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.apply_mask","text":"MappedArray . apply_mask ( mask , idx_arr = None , group_by = None , ** kwargs ) Return a new class instance, filtered by mask. **kwargs are passed to MappedArray.replace() .","title":"apply_mask()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.bottom_n","text":"MappedArray . bottom_n ( n , ** kwargs ) Filter bottom N elements from each column/group.","title":"bottom_n()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.bottom_n_mask","text":"MappedArray . bottom_n_mask ( n , ** kwargs ) Return mask of bottom N elements in each column/group.","title":"bottom_n_mask()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.boxplot","text":"MappedArray . boxplot ( group_by = None , ** kwargs ) Plot box plot by column/group.","title":"boxplot()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.col_arr","text":"Column array.","title":"col_arr"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.col_mapper","text":"Column mapper. See ColumnMapper .","title":"col_mapper"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.count","text":"MappedArray . count ( group_by = None , wrap_kwargs = None ) Return number of values by column/group.","title":"count()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.describe","text":"MappedArray . describe ( percentiles = None , ddof = 1 , group_by = None , wrap_kwargs = None , ** kwargs ) Return statistics by column/group.","title":"describe()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.histplot","text":"MappedArray . histplot ( group_by = None , ** kwargs ) Plot histogram by column/group.","title":"histplot()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.id_arr","text":"Id array.","title":"id_arr"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.idx_arr","text":"Index array.","title":"idx_arr"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.idxmax","text":"MappedArray . idxmax ( group_by = None , wrap_kwargs = None , ** kwargs ) Return index of max by column/group.","title":"idxmax()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.idxmin","text":"MappedArray . idxmin ( group_by = None , wrap_kwargs = None , ** kwargs ) Return index of min by column/group.","title":"idxmin()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.indexing_func","text":"MappedArray . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on MappedArray .","title":"indexing_func()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.indexing_func_meta","text":"MappedArray . indexing_func_meta ( pd_indexing_func , ** kwargs ) Perform indexing on MappedArray and return metadata.","title":"indexing_func_meta()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.is_expandable","text":"MappedArray . is_expandable ( idx_arr = None , group_by = None ) See is_mapped_expandable_nb() .","title":"is_expandable()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.is_sorted","text":"MappedArray . is_sorted ( incl_id = False ) Check whether mapped array is sorted.","title":"is_sorted()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.map_to_mask","text":"MappedArray . map_to_mask ( inout_map_func_nb , * args , group_by = None ) Map mapped array to a mask. See mapped_to_mask_nb() .","title":"map_to_mask()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.mapped_arr","text":"Mapped array.","title":"mapped_arr"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.mapping","text":"Mapping.","title":"mapping"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.max","text":"MappedArray . max ( group_by = None , wrap_kwargs = None , ** kwargs ) Return max by column/group.","title":"max()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.mean","text":"MappedArray . mean ( group_by = None , wrap_kwargs = None , ** kwargs ) Return mean by column/group.","title":"mean()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.median","text":"MappedArray . median ( group_by = None , wrap_kwargs = None , ** kwargs ) Return median by column/group.","title":"median()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.metrics","text":"Metrics supported by MappedArray . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function MappedArray.<lambda> at 0x7fac881fb378>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function MappedArray.<lambda> at 0x7fac881fb400>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function MappedArray.<lambda> at 0x7fac881fb488>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"count\" : { \"title\" : \"Count\" , \"calc_func\" : \"count\" , \"tags\" : \"mapped_array\" }, \"mean\" : { \"title\" : \"Mean\" , \"calc_func\" : \"mean\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"mapped_array\" , \"describe\" ] }, \"std\" : { \"title\" : \"Std\" , \"calc_func\" : \"std\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"mapped_array\" , \"describe\" ] }, \"min\" : { \"title\" : \"Min\" , \"calc_func\" : \"min\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"mapped_array\" , \"describe\" ] }, \"median\" : { \"title\" : \"Median\" , \"calc_func\" : \"median\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"mapped_array\" , \"describe\" ] }, \"max\" : { \"title\" : \"Max\" , \"calc_func\" : \"max\" , \"inv_check_has_mapping\" : true , \"tags\" : [ \"mapped_array\" , \"describe\" ] }, \"idx_min\" : { \"title\" : \"Min Index\" , \"calc_func\" : \"idxmin\" , \"inv_check_has_mapping\" : true , \"agg_func\" : null , \"tags\" : [ \"mapped_array\" , \"index\" ] }, \"idx_max\" : { \"title\" : \"Max Index\" , \"calc_func\" : \"idxmax\" , \"inv_check_has_mapping\" : true , \"agg_func\" : null , \"tags\" : [ \"mapped_array\" , \"index\" ] }, \"value_counts\" : { \"title\" : \"Value Counts\" , \"calc_func\" : \"<function MappedArray.<lambda> at 0x7fac881fb510>\" , \"resolve_value_counts\" : true , \"check_has_mapping\" : true , \"tags\" : [ \"mapped_array\" , \"value_counts\" ] } } ) Returns MappedArray._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable MappedArray._metrics .","title":"metrics"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.min","text":"MappedArray . min ( group_by = None , wrap_kwargs = None , ** kwargs ) Return min by column/group.","title":"min()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.nth","text":"MappedArray . nth ( n , group_by = None , wrap_kwargs = None , ** kwargs ) Return n-th element of each column/group.","title":"nth()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.nth_index","text":"MappedArray . nth_index ( n , group_by = None , wrap_kwargs = None , ** kwargs ) Return index of n-th element of each column/group.","title":"nth_index()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.plots_defaults","text":"Defaults for PlotsBuilderMixin.plots() . Merges PlotsBuilderMixin.plots_defaults and mapped_array.plots from settings .","title":"plots_defaults"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.reduce","text":"MappedArray . reduce ( reduce_func_nb , * args , idx_arr = None , returns_array = False , returns_idx = False , to_index = True , fill_value = nan , group_by = None , wrap_kwargs = None ) Reduce mapped array by column/group. If returns_array is False and returns_idx is False, see reduce_mapped_nb() . If returns_array is False and returns_idx is True, see reduce_mapped_to_idx_nb() . If returns_array is True and returns_idx is False, see reduce_mapped_to_array_nb() . If returns_array is True and returns_idx is True, see reduce_mapped_to_idx_array_nb() . If returns_idx is True, must pass idx_arr . Set to_index to False to return raw positions instead of labels. Use fill_value to set the default value. Set group_by to False to disable grouping.","title":"reduce()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.replace","text":"MappedArray . replace ( ** kwargs ) See Configured.replace() . Also, makes sure that MappedArray.col_mapper is not passed to the new instance.","title":"replace()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.sort","text":"MappedArray . sort ( incl_id = False , idx_arr = None , group_by = None , ** kwargs ) Sort mapped array by column array (primary) and id array (secondary, optional). **kwargs are passed to MappedArray.replace() .","title":"sort()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.stats_defaults","text":"Defaults for StatsBuilderMixin.stats() . Merges StatsBuilderMixin.stats_defaults and mapped_array.stats from settings .","title":"stats_defaults"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.std","text":"MappedArray . std ( ddof = 1 , group_by = None , wrap_kwargs = None , ** kwargs ) Return std by column/group.","title":"std()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.subplots","text":"Subplots supported by MappedArray . Co nf ig( { \"to_pd_plot\" : { \"check_is_not_grouped\" : true , \"plot_func\" : \"to_pd.vbt.plot\" , \"pass_trace_names\" : false , \"tags\" : \"mapped_array\" } } ) Returns MappedArray._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable MappedArray._subplots .","title":"subplots"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.sum","text":"MappedArray . sum ( fill_value = 0.0 , group_by = None , wrap_kwargs = None , ** kwargs ) Return sum by column/group.","title":"sum()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.to_index","text":"MappedArray . to_index () Convert to index.","title":"to_index()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.to_pd","text":"MappedArray . to_pd ( idx_arr = None , ignore_index = False , fill_value = nan , group_by = None , wrap_kwargs = None ) Expand mapped array to a Series/DataFrame. If ignore_index , will ignore the index and stack data points on top of each other in every column/group (see stack_expand_mapped_nb() ). Otherwise, see expand_mapped_nb() . Note Will raise an error if there are multiple values pointing to the same position. Set ignore_index to True in this case. Warning Mapped arrays represent information in the most memory-friendly format. Mapping back to pandas may occupy lots of memory if records are sparse.","title":"to_pd()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.top_n","text":"MappedArray . top_n ( n , ** kwargs ) Filter top N elements from each column/group.","title":"top_n()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.top_n_mask","text":"MappedArray . top_n_mask ( n , ** kwargs ) Return mask of top N elements in each column/group.","title":"top_n_mask()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.value_counts","text":"MappedArray . value_counts ( normalize = False , sort_uniques = True , sort = False , ascending = False , dropna = False , group_by = None , mapping = None , incl_all_keys = False , wrap_kwargs = None , ** kwargs ) See GenericAccessor.value_counts() . Note Does not take into account missing values.","title":"value_counts()"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MappedArray.values","text":"Mapped array.","title":"values"},{"location":"api/records/mapped_array/#vectorbt.records.mapped_array.MetaMappedArray","text":"Meta class that exposes a read-only class property StatsBuilderMixin.metrics . Superclasses MetaPlotsBuilderMixin MetaStatsBuilderMixin builtins.type Inherited members MetaPlotsBuilderMixin.subplots MetaStatsBuilderMixin.metrics","title":"MetaMappedArray"},{"location":"api/records/nb/","text":"nb module \u00b6 Numba-compiled functions. Provides an arsenal of Numba-compiled functions for records and mapped arrays. These only accept NumPy arrays and other Numba-compatible types. Note vectorbt treats matrices as first-class citizens and expects input arrays to be 2-dim, unless function has suffix _1d or is meant to be input to another function. Data is processed along index (axis 0). All functions passed as argument should be Numba-compiled. Records should retain the order they were created in. apply_on_mapped_nb function \u00b6 apply_on_mapped_nb ( mapped_arr , col_map , apply_func_nb , * args ) Apply function on mapped array per column. Returns the same shape as mapped_arr . apply_func_nb should accept the indices of values, index of the column, values of the column, and *args , and return an array. apply_on_records_nb function \u00b6 apply_on_records_nb ( records , col_map , apply_func_nb , * args ) Apply function on records per column. Returns the same shape as records . apply_func_nb should accept the records of the column and *args , and return an array. bottom_n_inout_map_nb function \u00b6 bottom_n_inout_map_nb ( inout , idxs , col , mapped_arr , n ) inout_map_func_nb that returns indices of bottom N elements. col_map_nb function \u00b6 col_map_nb ( col_arr , n_cols ) Build a map between columns and their indices. Returns an array with indices segmented by column, and an array with count per segment. Works well for unsorted column arrays. col_map_select_nb function \u00b6 col_map_select_nb ( col_map , new_cols ) Same as mapped_col_range_select_nb but using column map col_map . col_range_nb function \u00b6 col_range_nb ( col_arr , n_cols ) Build column range for sorted column array. Creates a 2-dim array with first column being start indices (inclusive) and second column being end indices (exclusive). Note Requires col_arr to be in ascending order. This can be done by sorting. col_range_select_nb function \u00b6 col_range_select_nb ( col_range , new_cols ) Perform indexing on a sorted array using column range col_range . Returns indices of elements corresponding to columns in new_cols and a new column array. expand_mapped_nb function \u00b6 expand_mapped_nb ( mapped_arr , col_arr , idx_arr , target_shape , fill_value ) Set each element to a value by boolean mask. is_col_idx_sorted_nb function \u00b6 is_col_idx_sorted_nb ( col_arr , id_arr ) Check whether the column and index arrays are sorted. is_col_sorted_nb function \u00b6 is_col_sorted_nb ( col_arr ) Check whether the column array is sorted. is_mapped_expandable_nb function \u00b6 is_mapped_expandable_nb ( col_arr , idx_arr , target_shape ) Check whether mapped array can be expanded without positional conflicts. map_records_nb function \u00b6 map_records_nb ( records , map_func_nb , * args ) Map each record to a single value. map_func_nb should accept a single record and *args , and return a single value. mapped_to_mask_nb function \u00b6 mapped_to_mask_nb ( mapped_arr , col_map , inout_map_func_nb , * args ) Map mapped array to a mask per column. Returns the same shape as mapped_arr . inout_map_func_nb should accept the boolean array that should be written, indices of values, index of the column, values of the column, and *args , and return nothing. mapped_value_counts_nb function \u00b6 mapped_value_counts_nb ( codes , n_uniques , col_map ) Get value counts of an already factorized mapped array. record_col_map_select_nb function \u00b6 record_col_map_select_nb ( records , col_map , new_cols ) Same as record_col_range_select_nb() but using column map col_map . record_col_range_select_nb function \u00b6 record_col_range_select_nb ( records , col_range , new_cols ) Perform indexing on sorted records using column range col_range . Returns new records. reduce_mapped_nb function \u00b6 reduce_mapped_nb ( mapped_arr , col_map , fill_value , reduce_func_nb , * args ) Reduce mapped array by column to a single value. Faster than expand_mapped_nb() and vbt.* used together, and also requires less memory. But does not take advantage of caching. reduce_func_nb should accept index of the column, mapped array and *args , and return a single value. reduce_mapped_to_array_nb function \u00b6 reduce_mapped_to_array_nb ( mapped_arr , col_map , fill_value , reduce_func_nb , * args ) Reduce mapped array by column to an array. reduce_func_nb same as for reduce_mapped_nb() but should return an array. reduce_mapped_to_idx_array_nb function \u00b6 reduce_mapped_to_idx_array_nb ( mapped_arr , col_map , idx_arr , fill_value , reduce_func_nb , * args ) Reduce mapped array by column to an index array. Same as reduce_mapped_to_array_nb() except idx_arr should be passed. Note Must return integers or raise an exception. reduce_mapped_to_idx_nb function \u00b6 reduce_mapped_to_idx_nb ( mapped_arr , col_map , idx_arr , fill_value , reduce_func_nb , * args ) Reduce mapped array by column to an index. Same as reduce_mapped_nb() except idx_arr should be passed. Note Must return integers or raise an exception. stack_expand_mapped_nb function \u00b6 stack_expand_mapped_nb ( mapped_arr , col_map , fill_value ) Expand mapped array by stacking without using index data. top_n_inout_map_nb function \u00b6 top_n_inout_map_nb ( inout , idxs , col , mapped_arr , n ) inout_map_func_nb that returns indices of top N elements.","title":"nb"},{"location":"api/records/nb/#vectorbt.records.nb","text":"Numba-compiled functions. Provides an arsenal of Numba-compiled functions for records and mapped arrays. These only accept NumPy arrays and other Numba-compatible types. Note vectorbt treats matrices as first-class citizens and expects input arrays to be 2-dim, unless function has suffix _1d or is meant to be input to another function. Data is processed along index (axis 0). All functions passed as argument should be Numba-compiled. Records should retain the order they were created in.","title":"vectorbt.records.nb"},{"location":"api/records/nb/#vectorbt.records.nb.apply_on_mapped_nb","text":"apply_on_mapped_nb ( mapped_arr , col_map , apply_func_nb , * args ) Apply function on mapped array per column. Returns the same shape as mapped_arr . apply_func_nb should accept the indices of values, index of the column, values of the column, and *args , and return an array.","title":"apply_on_mapped_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.apply_on_records_nb","text":"apply_on_records_nb ( records , col_map , apply_func_nb , * args ) Apply function on records per column. Returns the same shape as records . apply_func_nb should accept the records of the column and *args , and return an array.","title":"apply_on_records_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.bottom_n_inout_map_nb","text":"bottom_n_inout_map_nb ( inout , idxs , col , mapped_arr , n ) inout_map_func_nb that returns indices of bottom N elements.","title":"bottom_n_inout_map_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.col_map_nb","text":"col_map_nb ( col_arr , n_cols ) Build a map between columns and their indices. Returns an array with indices segmented by column, and an array with count per segment. Works well for unsorted column arrays.","title":"col_map_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.col_map_select_nb","text":"col_map_select_nb ( col_map , new_cols ) Same as mapped_col_range_select_nb but using column map col_map .","title":"col_map_select_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.col_range_nb","text":"col_range_nb ( col_arr , n_cols ) Build column range for sorted column array. Creates a 2-dim array with first column being start indices (inclusive) and second column being end indices (exclusive). Note Requires col_arr to be in ascending order. This can be done by sorting.","title":"col_range_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.col_range_select_nb","text":"col_range_select_nb ( col_range , new_cols ) Perform indexing on a sorted array using column range col_range . Returns indices of elements corresponding to columns in new_cols and a new column array.","title":"col_range_select_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.expand_mapped_nb","text":"expand_mapped_nb ( mapped_arr , col_arr , idx_arr , target_shape , fill_value ) Set each element to a value by boolean mask.","title":"expand_mapped_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.is_col_idx_sorted_nb","text":"is_col_idx_sorted_nb ( col_arr , id_arr ) Check whether the column and index arrays are sorted.","title":"is_col_idx_sorted_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.is_col_sorted_nb","text":"is_col_sorted_nb ( col_arr ) Check whether the column array is sorted.","title":"is_col_sorted_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.is_mapped_expandable_nb","text":"is_mapped_expandable_nb ( col_arr , idx_arr , target_shape ) Check whether mapped array can be expanded without positional conflicts.","title":"is_mapped_expandable_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.map_records_nb","text":"map_records_nb ( records , map_func_nb , * args ) Map each record to a single value. map_func_nb should accept a single record and *args , and return a single value.","title":"map_records_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.mapped_to_mask_nb","text":"mapped_to_mask_nb ( mapped_arr , col_map , inout_map_func_nb , * args ) Map mapped array to a mask per column. Returns the same shape as mapped_arr . inout_map_func_nb should accept the boolean array that should be written, indices of values, index of the column, values of the column, and *args , and return nothing.","title":"mapped_to_mask_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.mapped_value_counts_nb","text":"mapped_value_counts_nb ( codes , n_uniques , col_map ) Get value counts of an already factorized mapped array.","title":"mapped_value_counts_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.record_col_map_select_nb","text":"record_col_map_select_nb ( records , col_map , new_cols ) Same as record_col_range_select_nb() but using column map col_map .","title":"record_col_map_select_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.record_col_range_select_nb","text":"record_col_range_select_nb ( records , col_range , new_cols ) Perform indexing on sorted records using column range col_range . Returns new records.","title":"record_col_range_select_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.reduce_mapped_nb","text":"reduce_mapped_nb ( mapped_arr , col_map , fill_value , reduce_func_nb , * args ) Reduce mapped array by column to a single value. Faster than expand_mapped_nb() and vbt.* used together, and also requires less memory. But does not take advantage of caching. reduce_func_nb should accept index of the column, mapped array and *args , and return a single value.","title":"reduce_mapped_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.reduce_mapped_to_array_nb","text":"reduce_mapped_to_array_nb ( mapped_arr , col_map , fill_value , reduce_func_nb , * args ) Reduce mapped array by column to an array. reduce_func_nb same as for reduce_mapped_nb() but should return an array.","title":"reduce_mapped_to_array_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.reduce_mapped_to_idx_array_nb","text":"reduce_mapped_to_idx_array_nb ( mapped_arr , col_map , idx_arr , fill_value , reduce_func_nb , * args ) Reduce mapped array by column to an index array. Same as reduce_mapped_to_array_nb() except idx_arr should be passed. Note Must return integers or raise an exception.","title":"reduce_mapped_to_idx_array_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.reduce_mapped_to_idx_nb","text":"reduce_mapped_to_idx_nb ( mapped_arr , col_map , idx_arr , fill_value , reduce_func_nb , * args ) Reduce mapped array by column to an index. Same as reduce_mapped_nb() except idx_arr should be passed. Note Must return integers or raise an exception.","title":"reduce_mapped_to_idx_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.stack_expand_mapped_nb","text":"stack_expand_mapped_nb ( mapped_arr , col_map , fill_value ) Expand mapped array by stacking without using index data.","title":"stack_expand_mapped_nb()"},{"location":"api/records/nb/#vectorbt.records.nb.top_n_inout_map_nb","text":"top_n_inout_map_nb ( inout , idxs , col , mapped_arr , n ) inout_map_func_nb that returns indices of top N elements.","title":"top_n_inout_map_nb()"},{"location":"api/returns/","text":"returns package \u00b6 Modules for working with returns. Offers common financial risk and performance metrics as found in empyrical , an adapter for quantstats, and other features based on returns. Sub-modules \u00b6 vectorbt.returns.accessors vectorbt.returns.metrics vectorbt.returns.nb vectorbt.returns.qs_adapter","title":"returns"},{"location":"api/returns/#vectorbt.returns","text":"Modules for working with returns. Offers common financial risk and performance metrics as found in empyrical , an adapter for quantstats, and other features based on returns.","title":"vectorbt.returns"},{"location":"api/returns/#sub-modules","text":"vectorbt.returns.accessors vectorbt.returns.metrics vectorbt.returns.nb vectorbt.returns.qs_adapter","title":"Sub-modules"},{"location":"api/returns/accessors/","text":"accessors module \u00b6 Custom pandas accessors for returns data. Methods can be accessed as follows: ReturnsSRAccessor -> pd.Series.vbt.returns.* ReturnsDFAccessor -> pd.DataFrame.vbt.returns.* Note The underlying Series/DataFrame must already be a return series. To convert price to returns, use ReturnsAccessor.from_value() . Grouping is only supported by the methods that accept the group_by argument. Accessors do not utilize caching. There are three options to compute returns and get the accessor: >>> import numpy as np >>> import pandas as pd >>> import vectorbt as vbt >>> price = pd . Series ([ 1.1 , 1.2 , 1.3 , 1.2 , 1.1 ]) >>> # 1. pd.Series.pct_change >>> rets = price . pct_change () >>> ret_acc = rets . vbt . returns ( freq = 'd' ) >>> # 2. vectorbt.generic.accessors.GenericAccessor.to_returns >>> rets = price . vbt . to_returns () >>> ret_acc = rets . vbt . returns ( freq = 'd' ) >>> # 3. vectorbt.returns.accessors.ReturnsAccessor.from_value >>> ret_acc = pd . Series . vbt . returns . from_value ( price , freq = 'd' ) >>> # vectorbt.returns.accessors.ReturnsAccessor.total >>> ret_acc . total () 0.0 The accessors extend vectorbt.generic.accessors . >>> # inherited from GenericAccessor >>> ret_acc . max () 0.09090909090909083 Defaults \u00b6 ReturnsAccessor accepts defaults dictionary where you can pass defaults for arguments used throughout the accessor, such as start_value : The starting value. window : Window length. minp : Minimum number of observations in a window required to have a value. ddof : Delta Degrees of Freedom. risk_free : Constant risk-free return throughout the period. levy_alpha : Scaling relation (Levy stability exponent). required_return : Minimum acceptance return of the investor. cutoff : Decimal representing the percentage cutoff for the bottom percentile of returns. Stats \u00b6 Hint See StatsBuilderMixin.stats() and ReturnsAccessor.metrics . >>> ret_acc . stats () UserWarning: Metric 'benchmark_return' requires benchmark_rets to be set UserWarning: Metric 'alpha' requires benchmark_rets to be set UserWarning: Metric 'beta' requires benchmark_rets to be set Start 0 End 4 Duration 5 days 00:00:00 Total Return [%] 0 Annualized Return [%] 0 Annualized Volatility [%] 184.643 Sharpe Ratio 0.691185 Calmar Ratio 0 Max Drawdown [%] 15.3846 Omega Ratio 1.08727 Sortino Ratio 1.17805 Skew 0.00151002 Kurtosis -5.94737 Tail Ratio 1.08985 Common Sense Ratio 1.08985 Value at Risk -0.0823718 dtype: object The missing benchmark_rets can be either passed to the contrustor of the accessor or as a setting to StatsBuilderMixin.stats() : >>> benchmark = pd . Series ([ 1.05 , 1.1 , 1.15 , 1.1 , 1.05 ]) >>> benchmark_rets = benchmark . vbt . to_returns () >>> ret_acc . stats ( settings = dict ( benchmark_rets = benchmark_rets )) Start 0 End 4 Duration 5 days 00:00:00 Total Return [%] 0 Benchmark Return [%] 0 Annualized Return [%] 0 Annualized Volatility [%] 184.643 Sharpe Ratio 0.691185 Calmar Ratio 0 Max Drawdown [%] 15.3846 Omega Ratio 1.08727 Sortino Ratio 1.17805 Skew 0.00151002 Kurtosis -5.94737 Tail Ratio 1.08985 Common Sense Ratio 1.08985 Value at Risk -0.0823718 Alpha 0.78789 Beta 1.83864 dtype: object Note StatsBuilderMixin.stats() does not support grouping. Plots \u00b6 Hint See PlotsBuilderMixin.plots() and ReturnsAccessor.subplots . This class inherits subplots from GenericAccessor . ReturnsAccessor class \u00b6 Accessor on top of return series. For both, Series and DataFrames. Accessible through pd.Series.vbt.returns and pd.DataFrame.vbt.returns . Args obj :\u2002 pd.Series or pd.DataFrame Pandas object representing returns. benchmark_rets :\u2002 array_like Pandas object representing benchmark returns. year_freq :\u2002 any Year frequency for annualization purposes. defaults :\u2002 dict Defaults that override returns.defaults in settings . **kwargs Keyword arguments that are passed down to GenericAccessor . Superclasses AttrResolver BaseAccessor Configured Documented GenericAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.bshift() GenericAccessor.config GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.df_accessor_cls GenericAccessor.diff() GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.fshift() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.iloc GenericAccessor.indexing_kwargs GenericAccessor.lineplot() GenericAccessor.loc GenericAccessor.mapping GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.obj GenericAccessor.pct_change() GenericAccessor.plot() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.ranges GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.self_aliases GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.sr_accessor_cls GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.wrapper GenericAccessor.writeable_attrs GenericAccessor.zscore() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses ReturnsDFAccessor ReturnsSRAccessor alpha method \u00b6 ReturnsAccessor . alpha ( benchmark_rets = None , risk_free = None , wrap_kwargs = None ) See alpha_nb() . ann_factor property \u00b6 Get annualization factor. annual method \u00b6 ReturnsAccessor . annual ( ** kwargs ) Annual returns. annualized method \u00b6 ReturnsAccessor . annualized ( wrap_kwargs = None ) See annualized_return_nb() . annualized_volatility method \u00b6 ReturnsAccessor . annualized_volatility ( levy_alpha = None , ddof = None , wrap_kwargs = None ) See annualized_volatility_nb() . benchmark_rets property \u00b6 Benchmark returns. beta method \u00b6 ReturnsAccessor . beta ( benchmark_rets = None , wrap_kwargs = None ) See beta_nb() . calmar_ratio method \u00b6 ReturnsAccessor . calmar_ratio ( wrap_kwargs = None ) See calmar_ratio_nb() . capture method \u00b6 ReturnsAccessor . capture ( benchmark_rets = None , wrap_kwargs = None ) See capture_nb() . common_sense_ratio method \u00b6 ReturnsAccessor . common_sense_ratio ( wrap_kwargs = None ) Common Sense Ratio. cond_value_at_risk method \u00b6 ReturnsAccessor . cond_value_at_risk ( cutoff = None , wrap_kwargs = None ) See cond_value_at_risk_nb() . cumulative method \u00b6 ReturnsAccessor . cumulative ( start_value = None , wrap_kwargs = None ) See cum_returns_nb() . daily method \u00b6 ReturnsAccessor . daily ( ** kwargs ) Daily returns. defaults property \u00b6 Defaults for ReturnsAccessor . Merges returns.defaults from settings with defaults from ReturnsAccessor . deflated_sharpe_ratio method \u00b6 ReturnsAccessor . deflated_sharpe_ratio ( risk_free = None , ddof = None , var_sharpe = None , nb_trials = None , bias = True , wrap_kwargs = None ) Deflated Sharpe Ratio (DSR). Expresses the chance that the advertised strategy has a positive Sharpe ratio. If var_sharpe is None, is calculated based on all columns. If nb_trials is None, is set to the number of columns. down_capture method \u00b6 ReturnsAccessor . down_capture ( benchmark_rets = None , wrap_kwargs = None ) See down_capture_nb() . downside_risk method \u00b6 ReturnsAccessor . downside_risk ( required_return = None , wrap_kwargs = None ) See downside_risk_nb() . drawdown method \u00b6 ReturnsAccessor . drawdown ( wrap_kwargs = None ) Relative decline from a peak. drawdowns property \u00b6 ReturnsAccessor.get_drawdowns() with default arguments. from_value class method \u00b6 ReturnsAccessor . from_value ( value , init_value = nan , broadcast_kwargs = None , wrap_kwargs = None , ** kwargs ) Returns a new ReturnsAccessor instance with returns calculated from value . get_drawdowns method \u00b6 ReturnsAccessor . get_drawdowns ( wrapper_kwargs = None , ** kwargs ) Generate drawdown records of cumulative returns. See Drawdowns . indexing_func method \u00b6 ReturnsAccessor . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on ReturnsAccessor . information_ratio method \u00b6 ReturnsAccessor . information_ratio ( benchmark_rets = None , ddof = None , wrap_kwargs = None ) See information_ratio_nb() . max_drawdown method \u00b6 ReturnsAccessor . max_drawdown ( wrap_kwargs = None ) See max_drawdown_nb() . Yields the same result as max_drawdown of ReturnsAccessor.drawdowns . metrics class variable \u00b6 Metrics supported by ReturnsAccessor . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function ReturnsAccessor.<lambda> at 0x7fac9909d730>\" , \"agg_func\" : null , \"check_is_not_grouped\" : false , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function ReturnsAccessor.<lambda> at 0x7fac9909d7b8>\" , \"agg_func\" : null , \"check_is_not_grouped\" : false , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function ReturnsAccessor.<lambda> at 0x7fac9909d840>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"check_is_not_grouped\" : false , \"tags\" : \"wrapper\" }, \"total_return\" : { \"title\" : \"Total Return [%]\" , \"calc_func\" : \"total\" , \"post_calc_func\" : \"<function ReturnsAccessor.<lambda> at 0x7fac9909d8c8>\" , \"tags\" : \"returns\" }, \"benchmark_return\" : { \"title\" : \"Benchmark Return [%]\" , \"calc_func\" : \"benchmark_rets.vbt.returns.total\" , \"post_calc_func\" : \"<function ReturnsAccessor.<lambda> at 0x7fac9909d950>\" , \"check_has_benchmark_rets\" : true , \"tags\" : \"returns\" }, \"ann_return\" : { \"title\" : \"Annualized Return [%]\" , \"calc_func\" : \"annualized\" , \"post_calc_func\" : \"<function ReturnsAccessor.<lambda> at 0x7fac9909d9d8>\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : \"returns\" }, \"ann_volatility\" : { \"title\" : \"Annualized Volatility [%]\" , \"calc_func\" : \"annualized_volatility\" , \"post_calc_func\" : \"<function ReturnsAccessor.<lambda> at 0x7fac9909da60>\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : \"returns\" }, \"max_dd\" : { \"title\" : \"Max Drawdown [%]\" , \"calc_func\" : \"drawdowns.max_drawdown\" , \"post_calc_func\" : \"<function ReturnsAccessor.<lambda> at 0x7fac9909dae8>\" , \"tags\" : [ \"returns\" , \"drawdowns\" ] }, \"max_dd_duration\" : { \"title\" : \"Max Drawdown Duration\" , \"calc_func\" : \"drawdowns.max_duration\" , \"fill_wrap_kwargs\" : true , \"tags\" : [ \"returns\" , \"drawdowns\" , \"duration\" ] }, \"sharpe_ratio\" : { \"title\" : \"Sharpe Ratio\" , \"calc_func\" : \"sharpe_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : \"returns\" }, \"calmar_ratio\" : { \"title\" : \"Calmar Ratio\" , \"calc_func\" : \"calmar_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : \"returns\" }, \"omega_ratio\" : { \"title\" : \"Omega Ratio\" , \"calc_func\" : \"omega_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : \"returns\" }, \"sortino_ratio\" : { \"title\" : \"Sortino Ratio\" , \"calc_func\" : \"sortino_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : \"returns\" }, \"skew\" : { \"title\" : \"Skew\" , \"calc_func\" : \"obj.skew\" , \"tags\" : \"returns\" }, \"kurtosis\" : { \"title\" : \"Kurtosis\" , \"calc_func\" : \"obj.kurtosis\" , \"tags\" : \"returns\" }, \"tail_ratio\" : { \"title\" : \"Tail Ratio\" , \"calc_func\" : \"tail_ratio\" , \"tags\" : \"returns\" }, \"common_sense_ratio\" : { \"title\" : \"Common Sense Ratio\" , \"calc_func\" : \"common_sense_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : \"returns\" }, \"value_at_risk\" : { \"title\" : \"Value at Risk\" , \"calc_func\" : \"value_at_risk\" , \"tags\" : \"returns\" }, \"alpha\" : { \"title\" : \"Alpha\" , \"calc_func\" : \"alpha\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"check_has_benchmark_rets\" : true , \"tags\" : \"returns\" }, \"beta\" : { \"title\" : \"Beta\" , \"calc_func\" : \"beta\" , \"check_has_benchmark_rets\" : true , \"tags\" : \"returns\" } } ) Returns ReturnsAccessor._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable ReturnsAccessor._metrics . omega_ratio method \u00b6 ReturnsAccessor . omega_ratio ( risk_free = None , required_return = None , wrap_kwargs = None ) See omega_ratio_nb() . plots_defaults property \u00b6 Defaults for PlotsBuilderMixin.plots() . Merges GenericAccessor.plots_defaults , defaults from ReturnsAccessor.defaults (acting as settings ), and returns.plots from settings qs property \u00b6 Quantstats adapter. resolve_self method \u00b6 ReturnsAccessor . resolve_self ( cond_kwargs = None , custom_arg_names = None , impacts_caching = True , silence_warnings = False ) Resolve self. See Wrapping.resolve_self() . Creates a copy of this instance year_freq is different in cond_kwargs . rolling_alpha method \u00b6 ReturnsAccessor . rolling_alpha ( benchmark_rets = None , window = None , minp = None , risk_free = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.alpha() . rolling_annualized method \u00b6 ReturnsAccessor . rolling_annualized ( window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.annualized() . rolling_annualized_volatility method \u00b6 ReturnsAccessor . rolling_annualized_volatility ( window = None , minp = None , levy_alpha = None , ddof = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.annualized_volatility() . rolling_beta method \u00b6 ReturnsAccessor . rolling_beta ( benchmark_rets = None , window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.beta() . rolling_calmar_ratio method \u00b6 ReturnsAccessor . rolling_calmar_ratio ( window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.calmar_ratio() . rolling_capture method \u00b6 ReturnsAccessor . rolling_capture ( benchmark_rets = None , window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.capture() . rolling_common_sense_ratio method \u00b6 ReturnsAccessor . rolling_common_sense_ratio ( window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.common_sense_ratio() . rolling_cond_value_at_risk method \u00b6 ReturnsAccessor . rolling_cond_value_at_risk ( window = None , minp = None , cutoff = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.cond_value_at_risk() . rolling_down_capture method \u00b6 ReturnsAccessor . rolling_down_capture ( benchmark_rets = None , window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.down_capture() . rolling_downside_risk method \u00b6 ReturnsAccessor . rolling_downside_risk ( window = None , minp = None , required_return = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.downside_risk() . rolling_information_ratio method \u00b6 ReturnsAccessor . rolling_information_ratio ( benchmark_rets = None , window = None , minp = None , ddof = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.information_ratio() . rolling_max_drawdown method \u00b6 ReturnsAccessor . rolling_max_drawdown ( window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.max_drawdown() . rolling_omega_ratio method \u00b6 ReturnsAccessor . rolling_omega_ratio ( window = None , minp = None , risk_free = None , required_return = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.omega_ratio() . rolling_sharpe_ratio method \u00b6 ReturnsAccessor . rolling_sharpe_ratio ( window = None , minp = None , risk_free = None , ddof = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.sharpe_ratio() . rolling_sortino_ratio method \u00b6 ReturnsAccessor . rolling_sortino_ratio ( window = None , minp = None , required_return = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.sortino_ratio() . rolling_tail_ratio method \u00b6 ReturnsAccessor . rolling_tail_ratio ( window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.tail_ratio() . rolling_total method \u00b6 ReturnsAccessor . rolling_total ( window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.total() . rolling_up_capture method \u00b6 ReturnsAccessor . rolling_up_capture ( benchmark_rets = None , window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.up_capture() . rolling_value_at_risk method \u00b6 ReturnsAccessor . rolling_value_at_risk ( window = None , minp = None , cutoff = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.value_at_risk() . sharpe_ratio method \u00b6 ReturnsAccessor . sharpe_ratio ( risk_free = None , ddof = None , wrap_kwargs = None ) See sharpe_ratio_nb() . sortino_ratio method \u00b6 ReturnsAccessor . sortino_ratio ( required_return = None , wrap_kwargs = None ) See sortino_ratio_nb() . stats_defaults property \u00b6 Defaults for StatsBuilderMixin.stats() . Merges GenericAccessor.stats_defaults , defaults from ReturnsAccessor.defaults (acting as settings ), and returns.stats from settings subplots class variable \u00b6 Subplots supported by ReturnsAccessor . Co nf ig( { \"plot\" : { \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"pass_trace_names\" : false , \"tags\" : \"generic\" } } ) Returns ReturnsAccessor._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable ReturnsAccessor._subplots . tail_ratio method \u00b6 ReturnsAccessor . tail_ratio ( wrap_kwargs = None ) See tail_ratio_nb() . total method \u00b6 ReturnsAccessor . total ( wrap_kwargs = None ) See cum_returns_final_nb() . up_capture method \u00b6 ReturnsAccessor . up_capture ( benchmark_rets = None , wrap_kwargs = None ) See up_capture_nb() . value_at_risk method \u00b6 ReturnsAccessor . value_at_risk ( cutoff = None , wrap_kwargs = None ) See value_at_risk_nb() . year_freq property \u00b6 Year frequency for annualization purposes. ReturnsDFAccessor class \u00b6 Accessor on top of return series. For DataFrames only. Accessible through pd.DataFrame.vbt.returns . Superclasses AttrResolver BaseAccessor BaseDFAccessor Configured Documented GenericAccessor GenericDFAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin ReturnsAccessor StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.bshift() GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.diff() GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.fshift() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.lineplot() GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.pct_change() GenericAccessor.plot() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.zscore() GenericDFAccessor.flatten_grouped() GenericDFAccessor.heatmap() GenericDFAccessor.squeeze_grouped() GenericDFAccessor.ts_heatmap() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() ReturnsAccessor.alpha() ReturnsAccessor.ann_factor ReturnsAccessor.annual() ReturnsAccessor.annualized() ReturnsAccessor.annualized_volatility() ReturnsAccessor.benchmark_rets ReturnsAccessor.beta() ReturnsAccessor.calmar_ratio() ReturnsAccessor.capture() ReturnsAccessor.common_sense_ratio() ReturnsAccessor.cond_value_at_risk() ReturnsAccessor.config ReturnsAccessor.cumulative() ReturnsAccessor.daily() ReturnsAccessor.defaults ReturnsAccessor.deflated_sharpe_ratio() ReturnsAccessor.df_accessor_cls ReturnsAccessor.down_capture() ReturnsAccessor.downside_risk() ReturnsAccessor.drawdown() ReturnsAccessor.drawdowns ReturnsAccessor.from_value() ReturnsAccessor.get_drawdowns() ReturnsAccessor.iloc ReturnsAccessor.indexing_func() ReturnsAccessor.indexing_kwargs ReturnsAccessor.information_ratio() ReturnsAccessor.loc ReturnsAccessor.mapping ReturnsAccessor.max_drawdown() ReturnsAccessor.obj ReturnsAccessor.omega_ratio() ReturnsAccessor.plots_defaults ReturnsAccessor.qs ReturnsAccessor.ranges ReturnsAccessor.resolve_self() ReturnsAccessor.rolling_alpha() ReturnsAccessor.rolling_annualized() ReturnsAccessor.rolling_annualized_volatility() ReturnsAccessor.rolling_beta() ReturnsAccessor.rolling_calmar_ratio() ReturnsAccessor.rolling_capture() ReturnsAccessor.rolling_common_sense_ratio() ReturnsAccessor.rolling_cond_value_at_risk() ReturnsAccessor.rolling_down_capture() ReturnsAccessor.rolling_downside_risk() ReturnsAccessor.rolling_information_ratio() ReturnsAccessor.rolling_max_drawdown() ReturnsAccessor.rolling_omega_ratio() ReturnsAccessor.rolling_sharpe_ratio() ReturnsAccessor.rolling_sortino_ratio() ReturnsAccessor.rolling_tail_ratio() ReturnsAccessor.rolling_total() ReturnsAccessor.rolling_up_capture() ReturnsAccessor.rolling_value_at_risk() ReturnsAccessor.self_aliases ReturnsAccessor.sharpe_ratio() ReturnsAccessor.sortino_ratio() ReturnsAccessor.sr_accessor_cls ReturnsAccessor.stats_defaults ReturnsAccessor.tail_ratio() ReturnsAccessor.total() ReturnsAccessor.up_capture() ReturnsAccessor.value_at_risk() ReturnsAccessor.wrapper ReturnsAccessor.writeable_attrs ReturnsAccessor.year_freq StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj() ReturnsSRAccessor class \u00b6 Accessor on top of return series. For Series only. Accessible through pd.Series.vbt.returns . Superclasses AttrResolver BaseAccessor BaseSRAccessor Configured Documented GenericAccessor GenericSRAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin ReturnsAccessor StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.bshift() GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.diff() GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.fshift() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.lineplot() GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.pct_change() GenericAccessor.plot() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.zscore() GenericSRAccessor.flatten_grouped() GenericSRAccessor.heatmap() GenericSRAccessor.overlay_with_heatmap() GenericSRAccessor.plot_against() GenericSRAccessor.qqplot() GenericSRAccessor.squeeze_grouped() GenericSRAccessor.ts_heatmap() GenericSRAccessor.volume() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() ReturnsAccessor.alpha() ReturnsAccessor.ann_factor ReturnsAccessor.annual() ReturnsAccessor.annualized() ReturnsAccessor.annualized_volatility() ReturnsAccessor.benchmark_rets ReturnsAccessor.beta() ReturnsAccessor.calmar_ratio() ReturnsAccessor.capture() ReturnsAccessor.common_sense_ratio() ReturnsAccessor.cond_value_at_risk() ReturnsAccessor.config ReturnsAccessor.cumulative() ReturnsAccessor.daily() ReturnsAccessor.defaults ReturnsAccessor.deflated_sharpe_ratio() ReturnsAccessor.df_accessor_cls ReturnsAccessor.down_capture() ReturnsAccessor.downside_risk() ReturnsAccessor.drawdown() ReturnsAccessor.drawdowns ReturnsAccessor.from_value() ReturnsAccessor.get_drawdowns() ReturnsAccessor.iloc ReturnsAccessor.indexing_func() ReturnsAccessor.indexing_kwargs ReturnsAccessor.information_ratio() ReturnsAccessor.loc ReturnsAccessor.mapping ReturnsAccessor.max_drawdown() ReturnsAccessor.obj ReturnsAccessor.omega_ratio() ReturnsAccessor.plots_defaults ReturnsAccessor.qs ReturnsAccessor.ranges ReturnsAccessor.resolve_self() ReturnsAccessor.rolling_alpha() ReturnsAccessor.rolling_annualized() ReturnsAccessor.rolling_annualized_volatility() ReturnsAccessor.rolling_beta() ReturnsAccessor.rolling_calmar_ratio() ReturnsAccessor.rolling_capture() ReturnsAccessor.rolling_common_sense_ratio() ReturnsAccessor.rolling_cond_value_at_risk() ReturnsAccessor.rolling_down_capture() ReturnsAccessor.rolling_downside_risk() ReturnsAccessor.rolling_information_ratio() ReturnsAccessor.rolling_max_drawdown() ReturnsAccessor.rolling_omega_ratio() ReturnsAccessor.rolling_sharpe_ratio() ReturnsAccessor.rolling_sortino_ratio() ReturnsAccessor.rolling_tail_ratio() ReturnsAccessor.rolling_total() ReturnsAccessor.rolling_up_capture() ReturnsAccessor.rolling_value_at_risk() ReturnsAccessor.self_aliases ReturnsAccessor.sharpe_ratio() ReturnsAccessor.sortino_ratio() ReturnsAccessor.sr_accessor_cls ReturnsAccessor.stats_defaults ReturnsAccessor.tail_ratio() ReturnsAccessor.total() ReturnsAccessor.up_capture() ReturnsAccessor.value_at_risk() ReturnsAccessor.wrapper ReturnsAccessor.writeable_attrs ReturnsAccessor.year_freq StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj() plot_cumulative method \u00b6 ReturnsSRAccessor . plot_cumulative ( benchmark_rets = None , start_value = 1 , fill_to_benchmark = False , main_kwargs = None , benchmark_kwargs = None , hline_shape_kwargs = None , add_trace_kwargs = None , xref = 'x' , yref = 'y' , fig = None , ** layout_kwargs ) Plot cumulative returns. Args benchmark_rets :\u2002 array_like Benchmark return to compare returns against. Will broadcast per element. start_value :\u2002 float The starting returns. fill_to_benchmark :\u2002 bool Whether to fill between main and benchmark, or between main and start_value . main_kwargs :\u2002 dict Keyword arguments passed to GenericAccessor.plot() for main. benchmark_kwargs :\u2002 dict Keyword arguments passed to GenericAccessor.plot() for benchmark. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for start_value line. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import pandas as pd >>> import numpy as np >>> np . random . seed ( 0 ) >>> rets = pd . Series ( np . random . uniform ( - 0.05 , 0.05 , size = 100 )) >>> benchmark_rets = pd . Series ( np . random . uniform ( - 0.05 , 0.05 , size = 100 )) >>> rets . vbt . returns . plot_cumulative ( benchmark_rets = benchmark_rets )","title":"accessors"},{"location":"api/returns/accessors/#vectorbt.returns.accessors","text":"Custom pandas accessors for returns data. Methods can be accessed as follows: ReturnsSRAccessor -> pd.Series.vbt.returns.* ReturnsDFAccessor -> pd.DataFrame.vbt.returns.* Note The underlying Series/DataFrame must already be a return series. To convert price to returns, use ReturnsAccessor.from_value() . Grouping is only supported by the methods that accept the group_by argument. Accessors do not utilize caching. There are three options to compute returns and get the accessor: >>> import numpy as np >>> import pandas as pd >>> import vectorbt as vbt >>> price = pd . Series ([ 1.1 , 1.2 , 1.3 , 1.2 , 1.1 ]) >>> # 1. pd.Series.pct_change >>> rets = price . pct_change () >>> ret_acc = rets . vbt . returns ( freq = 'd' ) >>> # 2. vectorbt.generic.accessors.GenericAccessor.to_returns >>> rets = price . vbt . to_returns () >>> ret_acc = rets . vbt . returns ( freq = 'd' ) >>> # 3. vectorbt.returns.accessors.ReturnsAccessor.from_value >>> ret_acc = pd . Series . vbt . returns . from_value ( price , freq = 'd' ) >>> # vectorbt.returns.accessors.ReturnsAccessor.total >>> ret_acc . total () 0.0 The accessors extend vectorbt.generic.accessors . >>> # inherited from GenericAccessor >>> ret_acc . max () 0.09090909090909083","title":"vectorbt.returns.accessors"},{"location":"api/returns/accessors/#defaults","text":"ReturnsAccessor accepts defaults dictionary where you can pass defaults for arguments used throughout the accessor, such as start_value : The starting value. window : Window length. minp : Minimum number of observations in a window required to have a value. ddof : Delta Degrees of Freedom. risk_free : Constant risk-free return throughout the period. levy_alpha : Scaling relation (Levy stability exponent). required_return : Minimum acceptance return of the investor. cutoff : Decimal representing the percentage cutoff for the bottom percentile of returns.","title":"Defaults"},{"location":"api/returns/accessors/#stats","text":"Hint See StatsBuilderMixin.stats() and ReturnsAccessor.metrics . >>> ret_acc . stats () UserWarning: Metric 'benchmark_return' requires benchmark_rets to be set UserWarning: Metric 'alpha' requires benchmark_rets to be set UserWarning: Metric 'beta' requires benchmark_rets to be set Start 0 End 4 Duration 5 days 00:00:00 Total Return [%] 0 Annualized Return [%] 0 Annualized Volatility [%] 184.643 Sharpe Ratio 0.691185 Calmar Ratio 0 Max Drawdown [%] 15.3846 Omega Ratio 1.08727 Sortino Ratio 1.17805 Skew 0.00151002 Kurtosis -5.94737 Tail Ratio 1.08985 Common Sense Ratio 1.08985 Value at Risk -0.0823718 dtype: object The missing benchmark_rets can be either passed to the contrustor of the accessor or as a setting to StatsBuilderMixin.stats() : >>> benchmark = pd . Series ([ 1.05 , 1.1 , 1.15 , 1.1 , 1.05 ]) >>> benchmark_rets = benchmark . vbt . to_returns () >>> ret_acc . stats ( settings = dict ( benchmark_rets = benchmark_rets )) Start 0 End 4 Duration 5 days 00:00:00 Total Return [%] 0 Benchmark Return [%] 0 Annualized Return [%] 0 Annualized Volatility [%] 184.643 Sharpe Ratio 0.691185 Calmar Ratio 0 Max Drawdown [%] 15.3846 Omega Ratio 1.08727 Sortino Ratio 1.17805 Skew 0.00151002 Kurtosis -5.94737 Tail Ratio 1.08985 Common Sense Ratio 1.08985 Value at Risk -0.0823718 Alpha 0.78789 Beta 1.83864 dtype: object Note StatsBuilderMixin.stats() does not support grouping.","title":"Stats"},{"location":"api/returns/accessors/#plots","text":"Hint See PlotsBuilderMixin.plots() and ReturnsAccessor.subplots . This class inherits subplots from GenericAccessor .","title":"Plots"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor","text":"Accessor on top of return series. For both, Series and DataFrames. Accessible through pd.Series.vbt.returns and pd.DataFrame.vbt.returns . Args obj :\u2002 pd.Series or pd.DataFrame Pandas object representing returns. benchmark_rets :\u2002 array_like Pandas object representing benchmark returns. year_freq :\u2002 any Year frequency for annualization purposes. defaults :\u2002 dict Defaults that override returns.defaults in settings . **kwargs Keyword arguments that are passed down to GenericAccessor . Superclasses AttrResolver BaseAccessor Configured Documented GenericAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.bshift() GenericAccessor.config GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.df_accessor_cls GenericAccessor.diff() GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.fshift() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.iloc GenericAccessor.indexing_kwargs GenericAccessor.lineplot() GenericAccessor.loc GenericAccessor.mapping GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.obj GenericAccessor.pct_change() GenericAccessor.plot() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.ranges GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.self_aliases GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.sr_accessor_cls GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.wrapper GenericAccessor.writeable_attrs GenericAccessor.zscore() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses ReturnsDFAccessor ReturnsSRAccessor","title":"ReturnsAccessor"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.alpha","text":"ReturnsAccessor . alpha ( benchmark_rets = None , risk_free = None , wrap_kwargs = None ) See alpha_nb() .","title":"alpha()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.ann_factor","text":"Get annualization factor.","title":"ann_factor"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.annual","text":"ReturnsAccessor . annual ( ** kwargs ) Annual returns.","title":"annual()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.annualized","text":"ReturnsAccessor . annualized ( wrap_kwargs = None ) See annualized_return_nb() .","title":"annualized()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.annualized_volatility","text":"ReturnsAccessor . annualized_volatility ( levy_alpha = None , ddof = None , wrap_kwargs = None ) See annualized_volatility_nb() .","title":"annualized_volatility()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.benchmark_rets","text":"Benchmark returns.","title":"benchmark_rets"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.beta","text":"ReturnsAccessor . beta ( benchmark_rets = None , wrap_kwargs = None ) See beta_nb() .","title":"beta()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.calmar_ratio","text":"ReturnsAccessor . calmar_ratio ( wrap_kwargs = None ) See calmar_ratio_nb() .","title":"calmar_ratio()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.capture","text":"ReturnsAccessor . capture ( benchmark_rets = None , wrap_kwargs = None ) See capture_nb() .","title":"capture()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.common_sense_ratio","text":"ReturnsAccessor . common_sense_ratio ( wrap_kwargs = None ) Common Sense Ratio.","title":"common_sense_ratio()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.cond_value_at_risk","text":"ReturnsAccessor . cond_value_at_risk ( cutoff = None , wrap_kwargs = None ) See cond_value_at_risk_nb() .","title":"cond_value_at_risk()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.cumulative","text":"ReturnsAccessor . cumulative ( start_value = None , wrap_kwargs = None ) See cum_returns_nb() .","title":"cumulative()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.daily","text":"ReturnsAccessor . daily ( ** kwargs ) Daily returns.","title":"daily()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.defaults","text":"Defaults for ReturnsAccessor . Merges returns.defaults from settings with defaults from ReturnsAccessor .","title":"defaults"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.deflated_sharpe_ratio","text":"ReturnsAccessor . deflated_sharpe_ratio ( risk_free = None , ddof = None , var_sharpe = None , nb_trials = None , bias = True , wrap_kwargs = None ) Deflated Sharpe Ratio (DSR). Expresses the chance that the advertised strategy has a positive Sharpe ratio. If var_sharpe is None, is calculated based on all columns. If nb_trials is None, is set to the number of columns.","title":"deflated_sharpe_ratio()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.down_capture","text":"ReturnsAccessor . down_capture ( benchmark_rets = None , wrap_kwargs = None ) See down_capture_nb() .","title":"down_capture()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.downside_risk","text":"ReturnsAccessor . downside_risk ( required_return = None , wrap_kwargs = None ) See downside_risk_nb() .","title":"downside_risk()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.drawdown","text":"ReturnsAccessor . drawdown ( wrap_kwargs = None ) Relative decline from a peak.","title":"drawdown()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.drawdowns","text":"ReturnsAccessor.get_drawdowns() with default arguments.","title":"drawdowns"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.from_value","text":"ReturnsAccessor . from_value ( value , init_value = nan , broadcast_kwargs = None , wrap_kwargs = None , ** kwargs ) Returns a new ReturnsAccessor instance with returns calculated from value .","title":"from_value()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.get_drawdowns","text":"ReturnsAccessor . get_drawdowns ( wrapper_kwargs = None , ** kwargs ) Generate drawdown records of cumulative returns. See Drawdowns .","title":"get_drawdowns()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.indexing_func","text":"ReturnsAccessor . indexing_func ( pd_indexing_func , ** kwargs ) Perform indexing on ReturnsAccessor .","title":"indexing_func()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.information_ratio","text":"ReturnsAccessor . information_ratio ( benchmark_rets = None , ddof = None , wrap_kwargs = None ) See information_ratio_nb() .","title":"information_ratio()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.max_drawdown","text":"ReturnsAccessor . max_drawdown ( wrap_kwargs = None ) See max_drawdown_nb() . Yields the same result as max_drawdown of ReturnsAccessor.drawdowns .","title":"max_drawdown()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.metrics","text":"Metrics supported by ReturnsAccessor . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function ReturnsAccessor.<lambda> at 0x7fac9909d730>\" , \"agg_func\" : null , \"check_is_not_grouped\" : false , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function ReturnsAccessor.<lambda> at 0x7fac9909d7b8>\" , \"agg_func\" : null , \"check_is_not_grouped\" : false , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function ReturnsAccessor.<lambda> at 0x7fac9909d840>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"check_is_not_grouped\" : false , \"tags\" : \"wrapper\" }, \"total_return\" : { \"title\" : \"Total Return [%]\" , \"calc_func\" : \"total\" , \"post_calc_func\" : \"<function ReturnsAccessor.<lambda> at 0x7fac9909d8c8>\" , \"tags\" : \"returns\" }, \"benchmark_return\" : { \"title\" : \"Benchmark Return [%]\" , \"calc_func\" : \"benchmark_rets.vbt.returns.total\" , \"post_calc_func\" : \"<function ReturnsAccessor.<lambda> at 0x7fac9909d950>\" , \"check_has_benchmark_rets\" : true , \"tags\" : \"returns\" }, \"ann_return\" : { \"title\" : \"Annualized Return [%]\" , \"calc_func\" : \"annualized\" , \"post_calc_func\" : \"<function ReturnsAccessor.<lambda> at 0x7fac9909d9d8>\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : \"returns\" }, \"ann_volatility\" : { \"title\" : \"Annualized Volatility [%]\" , \"calc_func\" : \"annualized_volatility\" , \"post_calc_func\" : \"<function ReturnsAccessor.<lambda> at 0x7fac9909da60>\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : \"returns\" }, \"max_dd\" : { \"title\" : \"Max Drawdown [%]\" , \"calc_func\" : \"drawdowns.max_drawdown\" , \"post_calc_func\" : \"<function ReturnsAccessor.<lambda> at 0x7fac9909dae8>\" , \"tags\" : [ \"returns\" , \"drawdowns\" ] }, \"max_dd_duration\" : { \"title\" : \"Max Drawdown Duration\" , \"calc_func\" : \"drawdowns.max_duration\" , \"fill_wrap_kwargs\" : true , \"tags\" : [ \"returns\" , \"drawdowns\" , \"duration\" ] }, \"sharpe_ratio\" : { \"title\" : \"Sharpe Ratio\" , \"calc_func\" : \"sharpe_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : \"returns\" }, \"calmar_ratio\" : { \"title\" : \"Calmar Ratio\" , \"calc_func\" : \"calmar_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : \"returns\" }, \"omega_ratio\" : { \"title\" : \"Omega Ratio\" , \"calc_func\" : \"omega_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : \"returns\" }, \"sortino_ratio\" : { \"title\" : \"Sortino Ratio\" , \"calc_func\" : \"sortino_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : \"returns\" }, \"skew\" : { \"title\" : \"Skew\" , \"calc_func\" : \"obj.skew\" , \"tags\" : \"returns\" }, \"kurtosis\" : { \"title\" : \"Kurtosis\" , \"calc_func\" : \"obj.kurtosis\" , \"tags\" : \"returns\" }, \"tail_ratio\" : { \"title\" : \"Tail Ratio\" , \"calc_func\" : \"tail_ratio\" , \"tags\" : \"returns\" }, \"common_sense_ratio\" : { \"title\" : \"Common Sense Ratio\" , \"calc_func\" : \"common_sense_ratio\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"tags\" : \"returns\" }, \"value_at_risk\" : { \"title\" : \"Value at Risk\" , \"calc_func\" : \"value_at_risk\" , \"tags\" : \"returns\" }, \"alpha\" : { \"title\" : \"Alpha\" , \"calc_func\" : \"alpha\" , \"check_has_freq\" : true , \"check_has_year_freq\" : true , \"check_has_benchmark_rets\" : true , \"tags\" : \"returns\" }, \"beta\" : { \"title\" : \"Beta\" , \"calc_func\" : \"beta\" , \"check_has_benchmark_rets\" : true , \"tags\" : \"returns\" } } ) Returns ReturnsAccessor._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable ReturnsAccessor._metrics .","title":"metrics"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.omega_ratio","text":"ReturnsAccessor . omega_ratio ( risk_free = None , required_return = None , wrap_kwargs = None ) See omega_ratio_nb() .","title":"omega_ratio()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.plots_defaults","text":"Defaults for PlotsBuilderMixin.plots() . Merges GenericAccessor.plots_defaults , defaults from ReturnsAccessor.defaults (acting as settings ), and returns.plots from settings","title":"plots_defaults"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.qs","text":"Quantstats adapter.","title":"qs"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.resolve_self","text":"ReturnsAccessor . resolve_self ( cond_kwargs = None , custom_arg_names = None , impacts_caching = True , silence_warnings = False ) Resolve self. See Wrapping.resolve_self() . Creates a copy of this instance year_freq is different in cond_kwargs .","title":"resolve_self()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_alpha","text":"ReturnsAccessor . rolling_alpha ( benchmark_rets = None , window = None , minp = None , risk_free = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.alpha() .","title":"rolling_alpha()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_annualized","text":"ReturnsAccessor . rolling_annualized ( window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.annualized() .","title":"rolling_annualized()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_annualized_volatility","text":"ReturnsAccessor . rolling_annualized_volatility ( window = None , minp = None , levy_alpha = None , ddof = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.annualized_volatility() .","title":"rolling_annualized_volatility()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_beta","text":"ReturnsAccessor . rolling_beta ( benchmark_rets = None , window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.beta() .","title":"rolling_beta()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_calmar_ratio","text":"ReturnsAccessor . rolling_calmar_ratio ( window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.calmar_ratio() .","title":"rolling_calmar_ratio()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_capture","text":"ReturnsAccessor . rolling_capture ( benchmark_rets = None , window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.capture() .","title":"rolling_capture()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_common_sense_ratio","text":"ReturnsAccessor . rolling_common_sense_ratio ( window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.common_sense_ratio() .","title":"rolling_common_sense_ratio()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_cond_value_at_risk","text":"ReturnsAccessor . rolling_cond_value_at_risk ( window = None , minp = None , cutoff = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.cond_value_at_risk() .","title":"rolling_cond_value_at_risk()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_down_capture","text":"ReturnsAccessor . rolling_down_capture ( benchmark_rets = None , window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.down_capture() .","title":"rolling_down_capture()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_downside_risk","text":"ReturnsAccessor . rolling_downside_risk ( window = None , minp = None , required_return = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.downside_risk() .","title":"rolling_downside_risk()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_information_ratio","text":"ReturnsAccessor . rolling_information_ratio ( benchmark_rets = None , window = None , minp = None , ddof = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.information_ratio() .","title":"rolling_information_ratio()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_max_drawdown","text":"ReturnsAccessor . rolling_max_drawdown ( window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.max_drawdown() .","title":"rolling_max_drawdown()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_omega_ratio","text":"ReturnsAccessor . rolling_omega_ratio ( window = None , minp = None , risk_free = None , required_return = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.omega_ratio() .","title":"rolling_omega_ratio()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_sharpe_ratio","text":"ReturnsAccessor . rolling_sharpe_ratio ( window = None , minp = None , risk_free = None , ddof = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.sharpe_ratio() .","title":"rolling_sharpe_ratio()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_sortino_ratio","text":"ReturnsAccessor . rolling_sortino_ratio ( window = None , minp = None , required_return = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.sortino_ratio() .","title":"rolling_sortino_ratio()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_tail_ratio","text":"ReturnsAccessor . rolling_tail_ratio ( window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.tail_ratio() .","title":"rolling_tail_ratio()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_total","text":"ReturnsAccessor . rolling_total ( window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.total() .","title":"rolling_total()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_up_capture","text":"ReturnsAccessor . rolling_up_capture ( benchmark_rets = None , window = None , minp = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.up_capture() .","title":"rolling_up_capture()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.rolling_value_at_risk","text":"ReturnsAccessor . rolling_value_at_risk ( window = None , minp = None , cutoff = None , wrap_kwargs = None ) Rolling version of ReturnsAccessor.value_at_risk() .","title":"rolling_value_at_risk()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.sharpe_ratio","text":"ReturnsAccessor . sharpe_ratio ( risk_free = None , ddof = None , wrap_kwargs = None ) See sharpe_ratio_nb() .","title":"sharpe_ratio()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.sortino_ratio","text":"ReturnsAccessor . sortino_ratio ( required_return = None , wrap_kwargs = None ) See sortino_ratio_nb() .","title":"sortino_ratio()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.stats_defaults","text":"Defaults for StatsBuilderMixin.stats() . Merges GenericAccessor.stats_defaults , defaults from ReturnsAccessor.defaults (acting as settings ), and returns.stats from settings","title":"stats_defaults"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.subplots","text":"Subplots supported by ReturnsAccessor . Co nf ig( { \"plot\" : { \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"pass_trace_names\" : false , \"tags\" : \"generic\" } } ) Returns ReturnsAccessor._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable ReturnsAccessor._subplots .","title":"subplots"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.tail_ratio","text":"ReturnsAccessor . tail_ratio ( wrap_kwargs = None ) See tail_ratio_nb() .","title":"tail_ratio()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.total","text":"ReturnsAccessor . total ( wrap_kwargs = None ) See cum_returns_final_nb() .","title":"total()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.up_capture","text":"ReturnsAccessor . up_capture ( benchmark_rets = None , wrap_kwargs = None ) See up_capture_nb() .","title":"up_capture()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.value_at_risk","text":"ReturnsAccessor . value_at_risk ( cutoff = None , wrap_kwargs = None ) See value_at_risk_nb() .","title":"value_at_risk()"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsAccessor.year_freq","text":"Year frequency for annualization purposes.","title":"year_freq"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsDFAccessor","text":"Accessor on top of return series. For DataFrames only. Accessible through pd.DataFrame.vbt.returns . Superclasses AttrResolver BaseAccessor BaseDFAccessor Configured Documented GenericAccessor GenericDFAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin ReturnsAccessor StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.bshift() GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.diff() GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.fshift() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.lineplot() GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.pct_change() GenericAccessor.plot() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.zscore() GenericDFAccessor.flatten_grouped() GenericDFAccessor.heatmap() GenericDFAccessor.squeeze_grouped() GenericDFAccessor.ts_heatmap() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() ReturnsAccessor.alpha() ReturnsAccessor.ann_factor ReturnsAccessor.annual() ReturnsAccessor.annualized() ReturnsAccessor.annualized_volatility() ReturnsAccessor.benchmark_rets ReturnsAccessor.beta() ReturnsAccessor.calmar_ratio() ReturnsAccessor.capture() ReturnsAccessor.common_sense_ratio() ReturnsAccessor.cond_value_at_risk() ReturnsAccessor.config ReturnsAccessor.cumulative() ReturnsAccessor.daily() ReturnsAccessor.defaults ReturnsAccessor.deflated_sharpe_ratio() ReturnsAccessor.df_accessor_cls ReturnsAccessor.down_capture() ReturnsAccessor.downside_risk() ReturnsAccessor.drawdown() ReturnsAccessor.drawdowns ReturnsAccessor.from_value() ReturnsAccessor.get_drawdowns() ReturnsAccessor.iloc ReturnsAccessor.indexing_func() ReturnsAccessor.indexing_kwargs ReturnsAccessor.information_ratio() ReturnsAccessor.loc ReturnsAccessor.mapping ReturnsAccessor.max_drawdown() ReturnsAccessor.obj ReturnsAccessor.omega_ratio() ReturnsAccessor.plots_defaults ReturnsAccessor.qs ReturnsAccessor.ranges ReturnsAccessor.resolve_self() ReturnsAccessor.rolling_alpha() ReturnsAccessor.rolling_annualized() ReturnsAccessor.rolling_annualized_volatility() ReturnsAccessor.rolling_beta() ReturnsAccessor.rolling_calmar_ratio() ReturnsAccessor.rolling_capture() ReturnsAccessor.rolling_common_sense_ratio() ReturnsAccessor.rolling_cond_value_at_risk() ReturnsAccessor.rolling_down_capture() ReturnsAccessor.rolling_downside_risk() ReturnsAccessor.rolling_information_ratio() ReturnsAccessor.rolling_max_drawdown() ReturnsAccessor.rolling_omega_ratio() ReturnsAccessor.rolling_sharpe_ratio() ReturnsAccessor.rolling_sortino_ratio() ReturnsAccessor.rolling_tail_ratio() ReturnsAccessor.rolling_total() ReturnsAccessor.rolling_up_capture() ReturnsAccessor.rolling_value_at_risk() ReturnsAccessor.self_aliases ReturnsAccessor.sharpe_ratio() ReturnsAccessor.sortino_ratio() ReturnsAccessor.sr_accessor_cls ReturnsAccessor.stats_defaults ReturnsAccessor.tail_ratio() ReturnsAccessor.total() ReturnsAccessor.up_capture() ReturnsAccessor.value_at_risk() ReturnsAccessor.wrapper ReturnsAccessor.writeable_attrs ReturnsAccessor.year_freq StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"ReturnsDFAccessor"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsSRAccessor","text":"Accessor on top of return series. For Series only. Accessible through pd.Series.vbt.returns . Superclasses AttrResolver BaseAccessor BaseSRAccessor Configured Documented GenericAccessor GenericSRAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin ReturnsAccessor StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.empty() BaseAccessor.empty_like() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.bshift() GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.diff() GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.fshift() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.lineplot() GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.pct_change() GenericAccessor.plot() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.zscore() GenericSRAccessor.flatten_grouped() GenericSRAccessor.heatmap() GenericSRAccessor.overlay_with_heatmap() GenericSRAccessor.plot_against() GenericSRAccessor.qqplot() GenericSRAccessor.squeeze_grouped() GenericSRAccessor.ts_heatmap() GenericSRAccessor.volume() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() ReturnsAccessor.alpha() ReturnsAccessor.ann_factor ReturnsAccessor.annual() ReturnsAccessor.annualized() ReturnsAccessor.annualized_volatility() ReturnsAccessor.benchmark_rets ReturnsAccessor.beta() ReturnsAccessor.calmar_ratio() ReturnsAccessor.capture() ReturnsAccessor.common_sense_ratio() ReturnsAccessor.cond_value_at_risk() ReturnsAccessor.config ReturnsAccessor.cumulative() ReturnsAccessor.daily() ReturnsAccessor.defaults ReturnsAccessor.deflated_sharpe_ratio() ReturnsAccessor.df_accessor_cls ReturnsAccessor.down_capture() ReturnsAccessor.downside_risk() ReturnsAccessor.drawdown() ReturnsAccessor.drawdowns ReturnsAccessor.from_value() ReturnsAccessor.get_drawdowns() ReturnsAccessor.iloc ReturnsAccessor.indexing_func() ReturnsAccessor.indexing_kwargs ReturnsAccessor.information_ratio() ReturnsAccessor.loc ReturnsAccessor.mapping ReturnsAccessor.max_drawdown() ReturnsAccessor.obj ReturnsAccessor.omega_ratio() ReturnsAccessor.plots_defaults ReturnsAccessor.qs ReturnsAccessor.ranges ReturnsAccessor.resolve_self() ReturnsAccessor.rolling_alpha() ReturnsAccessor.rolling_annualized() ReturnsAccessor.rolling_annualized_volatility() ReturnsAccessor.rolling_beta() ReturnsAccessor.rolling_calmar_ratio() ReturnsAccessor.rolling_capture() ReturnsAccessor.rolling_common_sense_ratio() ReturnsAccessor.rolling_cond_value_at_risk() ReturnsAccessor.rolling_down_capture() ReturnsAccessor.rolling_downside_risk() ReturnsAccessor.rolling_information_ratio() ReturnsAccessor.rolling_max_drawdown() ReturnsAccessor.rolling_omega_ratio() ReturnsAccessor.rolling_sharpe_ratio() ReturnsAccessor.rolling_sortino_ratio() ReturnsAccessor.rolling_tail_ratio() ReturnsAccessor.rolling_total() ReturnsAccessor.rolling_up_capture() ReturnsAccessor.rolling_value_at_risk() ReturnsAccessor.self_aliases ReturnsAccessor.sharpe_ratio() ReturnsAccessor.sortino_ratio() ReturnsAccessor.sr_accessor_cls ReturnsAccessor.stats_defaults ReturnsAccessor.tail_ratio() ReturnsAccessor.total() ReturnsAccessor.up_capture() ReturnsAccessor.value_at_risk() ReturnsAccessor.wrapper ReturnsAccessor.writeable_attrs ReturnsAccessor.year_freq StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"ReturnsSRAccessor"},{"location":"api/returns/accessors/#vectorbt.returns.accessors.ReturnsSRAccessor.plot_cumulative","text":"ReturnsSRAccessor . plot_cumulative ( benchmark_rets = None , start_value = 1 , fill_to_benchmark = False , main_kwargs = None , benchmark_kwargs = None , hline_shape_kwargs = None , add_trace_kwargs = None , xref = 'x' , yref = 'y' , fig = None , ** layout_kwargs ) Plot cumulative returns. Args benchmark_rets :\u2002 array_like Benchmark return to compare returns against. Will broadcast per element. start_value :\u2002 float The starting returns. fill_to_benchmark :\u2002 bool Whether to fill between main and benchmark, or between main and start_value . main_kwargs :\u2002 dict Keyword arguments passed to GenericAccessor.plot() for main. benchmark_kwargs :\u2002 dict Keyword arguments passed to GenericAccessor.plot() for benchmark. hline_shape_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.add_shape for start_value line. add_trace_kwargs :\u2002 dict Keyword arguments passed to add_trace . xref :\u2002 str X coordinate axis. yref :\u2002 str Y coordinate axis. fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> import pandas as pd >>> import numpy as np >>> np . random . seed ( 0 ) >>> rets = pd . Series ( np . random . uniform ( - 0.05 , 0.05 , size = 100 )) >>> benchmark_rets = pd . Series ( np . random . uniform ( - 0.05 , 0.05 , size = 100 )) >>> rets . vbt . returns . plot_cumulative ( benchmark_rets = benchmark_rets )","title":"plot_cumulative()"},{"location":"api/returns/metrics/","text":"metrics module \u00b6 Other metrics that are not compiled with Numba. approx_exp_max_sharpe function \u00b6 approx_exp_max_sharpe ( mean_sharpe , var_sharpe , nb_trials ) Expected Maximum Sharpe Ratio. deflated_sharpe_ratio function \u00b6 deflated_sharpe_ratio ( * , est_sharpe , var_sharpe , nb_trials , backtest_horizon , skew , kurtosis ) Deflated Sharpe Ratio (DSR). See Deflated Sharpe Ratio .","title":"metrics"},{"location":"api/returns/metrics/#vectorbt.returns.metrics","text":"Other metrics that are not compiled with Numba.","title":"vectorbt.returns.metrics"},{"location":"api/returns/metrics/#vectorbt.returns.metrics.approx_exp_max_sharpe","text":"approx_exp_max_sharpe ( mean_sharpe , var_sharpe , nb_trials ) Expected Maximum Sharpe Ratio.","title":"approx_exp_max_sharpe()"},{"location":"api/returns/metrics/#vectorbt.returns.metrics.deflated_sharpe_ratio","text":"deflated_sharpe_ratio ( * , est_sharpe , var_sharpe , nb_trials , backtest_horizon , skew , kurtosis ) Deflated Sharpe Ratio (DSR). See Deflated Sharpe Ratio .","title":"deflated_sharpe_ratio()"},{"location":"api/returns/nb/","text":"nb module \u00b6 Numba-compiled functions. Provides an arsenal of Numba-compiled functions that are used by accessors and for measuring portfolio performance. These only accept NumPy arrays and other Numba-compatible types. >>> import numpy as np >>> import vectorbt as vbt >>> price = np . array ([ 1.1 , 1.2 , 1.3 , 1.2 , 1.1 ]) >>> returns = vbt . generic . nb . pct_change_1d_nb ( price ) >>> # vectorbt.returns.nb.cum_returns_1d_nb >>> vbt . returns . nb . cum_returns_1d_nb ( returns , 0 ) array([0., 0.09090909, 0.18181818, 0.09090909, 0.]) Note vectorbt treats matrices as first-class citizens and expects input arrays to be 2-dim, unless function has suffix _1d or is meant to be input to another function. Data is processed along index (axis 0). All functions passed as argument should be Numba-compiled. alpha_1d_nb function \u00b6 alpha_1d_nb ( returns , benchmark_rets , ann_factor , risk_free = 0.0 ) Annualized alpha. alpha_nb function \u00b6 alpha_nb ( returns , benchmark_rets , ann_factor , risk_free = 0.0 ) 2-dim version of alpha_1d_nb() . annualized_return_1d_nb function \u00b6 annualized_return_1d_nb ( returns , ann_factor ) Mean annual growth rate of returns. This is equivalent to the compound annual growth rate. annualized_return_nb function \u00b6 annualized_return_nb ( returns , ann_factor ) 2-dim version of annualized_return_1d_nb() . annualized_volatility_1d_nb function \u00b6 annualized_volatility_1d_nb ( returns , ann_factor , levy_alpha = 2.0 , ddof = 1 ) Annualized volatility of a strategy. annualized_volatility_nb function \u00b6 annualized_volatility_nb ( returns , ann_factor , levy_alpha = 2.0 , ddof = 1 ) 2-dim version of annualized_volatility_1d_nb() . beta_1d_nb function \u00b6 beta_1d_nb ( returns , benchmark_rets ) Beta. beta_nb function \u00b6 beta_nb ( returns , benchmark_rets ) 2-dim version of beta_1d_nb() . calmar_ratio_1d_nb function \u00b6 calmar_ratio_1d_nb ( returns , ann_factor ) Calmar ratio, or drawdown ratio, of a strategy. calmar_ratio_nb function \u00b6 calmar_ratio_nb ( returns , ann_factor ) 2-dim version of calmar_ratio_1d_nb() . capture_1d_nb function \u00b6 capture_1d_nb ( returns , benchmark_rets , ann_factor ) Capture ratio. capture_nb function \u00b6 capture_nb ( returns , benchmark_rets , ann_factor ) 2-dim version of capture_1d_nb() . cond_value_at_risk_1d_nb function \u00b6 cond_value_at_risk_1d_nb ( returns , cutoff = 0.05 ) Conditional value at risk (CVaR) of a returns stream. cond_value_at_risk_nb function \u00b6 cond_value_at_risk_nb ( returns , cutoff = 0.05 ) 2-dim version of cond_value_at_risk_1d_nb() . cum_returns_1d_nb function \u00b6 cum_returns_1d_nb ( returns , start_value ) Cumulative returns. cum_returns_final_1d_nb function \u00b6 cum_returns_final_1d_nb ( returns , start_value = 0.0 ) Total return. cum_returns_final_nb function \u00b6 cum_returns_final_nb ( returns , start_value = 0.0 ) 2-dim version of cum_returns_final_1d_nb() . cum_returns_nb function \u00b6 cum_returns_nb ( returns , start_value ) 2-dim version of cum_returns_1d_nb() . down_capture_1d_nb function \u00b6 down_capture_1d_nb ( returns , benchmark_rets , ann_factor ) Capture ratio for periods when the benchmark return is negative. down_capture_nb function \u00b6 down_capture_nb ( returns , benchmark_rets , ann_factor ) 2-dim version of down_capture_1d_nb() . downside_risk_1d_nb function \u00b6 downside_risk_1d_nb ( returns , ann_factor , required_return = 0.0 ) Downside deviation below a threshold. downside_risk_nb function \u00b6 downside_risk_nb ( returns , ann_factor , required_return = 0.0 ) 2-dim version of downside_risk_1d_nb() . drawdown_1d_nb function \u00b6 drawdown_1d_nb ( returns ) Drawdown of cumulative returns. drawdown_nb function \u00b6 drawdown_nb ( returns ) 2-dim version of drawdown_1d_nb() . get_return_nb function \u00b6 get_return_nb ( input_value , output_value ) Calculate return from input and output value. information_ratio_1d_nb function \u00b6 information_ratio_1d_nb ( returns , benchmark_rets , ddof = 1 ) Information ratio of a strategy. information_ratio_nb function \u00b6 information_ratio_nb ( returns , benchmark_rets , ddof = 1 ) 2-dim version of information_ratio_1d_nb() . max_drawdown_1d_nb function \u00b6 max_drawdown_1d_nb ( returns ) Total maximum drawdown (MDD). max_drawdown_nb function \u00b6 max_drawdown_nb ( returns ) 2-dim version of max_drawdown_1d_nb() . omega_ratio_1d_nb function \u00b6 omega_ratio_1d_nb ( returns , ann_factor , risk_free = 0.0 , required_return = 0.0 ) Omega ratio of a strategy.. omega_ratio_nb function \u00b6 omega_ratio_nb ( returns , ann_factor , risk_free = 0.0 , required_return = 0.0 ) 2-dim version of omega_ratio_1d_nb() . returns_1d_nb function \u00b6 returns_1d_nb ( value , init_value ) Calculate returns from value. returns_nb function \u00b6 returns_nb ( value , init_value ) 2-dim version of returns_1d_nb() . rolling_alpha_nb function \u00b6 rolling_alpha_nb ( returns , window , minp , benchmark_rets , ann_factor , risk_free = 0.0 ) Rolling version of alpha_nb() . rolling_annualized_return_nb function \u00b6 rolling_annualized_return_nb ( returns , window , minp , ann_factor ) Rolling version of annualized_return_nb() . rolling_annualized_volatility_nb function \u00b6 rolling_annualized_volatility_nb ( returns , window , minp , ann_factor , levy_alpha = 2.0 , ddof = 1 ) Rolling version of annualized_volatility_nb() . rolling_beta_nb function \u00b6 rolling_beta_nb ( returns , window , minp , benchmark_rets ) Rolling version of beta_nb() . rolling_calmar_ratio_nb function \u00b6 rolling_calmar_ratio_nb ( returns , window , minp , ann_factor ) Rolling version of calmar_ratio_nb() . rolling_capture_nb function \u00b6 rolling_capture_nb ( returns , window , minp , benchmark_rets , ann_factor ) Rolling version of capture_nb() . rolling_cond_value_at_risk_nb function \u00b6 rolling_cond_value_at_risk_nb ( returns , window , minp , cutoff = 0.05 ) Rolling version of cond_value_at_risk_nb() . rolling_cum_returns_final_nb function \u00b6 rolling_cum_returns_final_nb ( returns , window , minp , start_value = 0.0 ) Rolling version of cum_returns_final_nb() . rolling_down_capture_nb function \u00b6 rolling_down_capture_nb ( returns , window , minp , benchmark_rets , ann_factor ) Rolling version of down_capture_nb() . rolling_downside_risk_nb function \u00b6 rolling_downside_risk_nb ( returns , window , minp , ann_factor , required_return = 0.0 ) Rolling version of downside_risk_nb() . rolling_information_ratio_nb function \u00b6 rolling_information_ratio_nb ( returns , window , minp , benchmark_rets , ddof = 1 ) Rolling version of information_ratio_nb() . rolling_max_drawdown_nb function \u00b6 rolling_max_drawdown_nb ( returns , window , minp ) Rolling version of max_drawdown_nb() . rolling_omega_ratio_nb function \u00b6 rolling_omega_ratio_nb ( returns , window , minp , ann_factor , risk_free = 0.0 , required_return = 0.0 ) Rolling version of omega_ratio_nb() . rolling_sharpe_ratio_nb function \u00b6 rolling_sharpe_ratio_nb ( returns , window , minp , ann_factor , risk_free = 0.0 , ddof = 1 ) Rolling version of sharpe_ratio_nb() . rolling_sortino_ratio_nb function \u00b6 rolling_sortino_ratio_nb ( returns , window , minp , ann_factor , required_return = 0.0 ) Rolling version of sortino_ratio_nb() . rolling_tail_ratio_nb function \u00b6 rolling_tail_ratio_nb ( returns , window , minp ) Rolling version of tail_ratio_nb() . rolling_up_capture_nb function \u00b6 rolling_up_capture_nb ( returns , window , minp , benchmark_rets , ann_factor ) Rolling version of up_capture_nb() . rolling_value_at_risk_nb function \u00b6 rolling_value_at_risk_nb ( returns , window , minp , cutoff = 0.05 ) Rolling version of value_at_risk_nb() . sharpe_ratio_1d_nb function \u00b6 sharpe_ratio_1d_nb ( returns , ann_factor , risk_free = 0.0 , ddof = 1 ) Sharpe ratio of a strategy. sharpe_ratio_nb function \u00b6 sharpe_ratio_nb ( returns , ann_factor , risk_free = 0.0 , ddof = 1 ) 2-dim version of sharpe_ratio_1d_nb() . sortino_ratio_1d_nb function \u00b6 sortino_ratio_1d_nb ( returns , ann_factor , required_return = 0.0 ) Sortino ratio of a strategy. sortino_ratio_nb function \u00b6 sortino_ratio_nb ( returns , ann_factor , required_return = 0.0 ) 2-dim version of sortino_ratio_1d_nb() . tail_ratio_1d_nb function \u00b6 tail_ratio_1d_nb ( returns ) Ratio between the right (95%) and left tail (5%). tail_ratio_nb function \u00b6 tail_ratio_nb ( returns ) 2-dim version of tail_ratio_1d_nb() . total_return_apply_nb function \u00b6 total_return_apply_nb ( idxs , col , returns ) Calculate total return from returns. up_capture_1d_nb function \u00b6 up_capture_1d_nb ( returns , benchmark_rets , ann_factor ) Capture ratio for periods when the benchmark return is positive. up_capture_nb function \u00b6 up_capture_nb ( returns , benchmark_rets , ann_factor ) 2-dim version of up_capture_1d_nb() . value_at_risk_1d_nb function \u00b6 value_at_risk_1d_nb ( returns , cutoff = 0.05 ) Value at risk (VaR) of a returns stream. value_at_risk_nb function \u00b6 value_at_risk_nb ( returns , cutoff = 0.05 ) 2-dim version of value_at_risk_1d_nb() .","title":"nb"},{"location":"api/returns/nb/#vectorbt.returns.nb","text":"Numba-compiled functions. Provides an arsenal of Numba-compiled functions that are used by accessors and for measuring portfolio performance. These only accept NumPy arrays and other Numba-compatible types. >>> import numpy as np >>> import vectorbt as vbt >>> price = np . array ([ 1.1 , 1.2 , 1.3 , 1.2 , 1.1 ]) >>> returns = vbt . generic . nb . pct_change_1d_nb ( price ) >>> # vectorbt.returns.nb.cum_returns_1d_nb >>> vbt . returns . nb . cum_returns_1d_nb ( returns , 0 ) array([0., 0.09090909, 0.18181818, 0.09090909, 0.]) Note vectorbt treats matrices as first-class citizens and expects input arrays to be 2-dim, unless function has suffix _1d or is meant to be input to another function. Data is processed along index (axis 0). All functions passed as argument should be Numba-compiled.","title":"vectorbt.returns.nb"},{"location":"api/returns/nb/#vectorbt.returns.nb.alpha_1d_nb","text":"alpha_1d_nb ( returns , benchmark_rets , ann_factor , risk_free = 0.0 ) Annualized alpha.","title":"alpha_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.alpha_nb","text":"alpha_nb ( returns , benchmark_rets , ann_factor , risk_free = 0.0 ) 2-dim version of alpha_1d_nb() .","title":"alpha_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.annualized_return_1d_nb","text":"annualized_return_1d_nb ( returns , ann_factor ) Mean annual growth rate of returns. This is equivalent to the compound annual growth rate.","title":"annualized_return_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.annualized_return_nb","text":"annualized_return_nb ( returns , ann_factor ) 2-dim version of annualized_return_1d_nb() .","title":"annualized_return_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.annualized_volatility_1d_nb","text":"annualized_volatility_1d_nb ( returns , ann_factor , levy_alpha = 2.0 , ddof = 1 ) Annualized volatility of a strategy.","title":"annualized_volatility_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.annualized_volatility_nb","text":"annualized_volatility_nb ( returns , ann_factor , levy_alpha = 2.0 , ddof = 1 ) 2-dim version of annualized_volatility_1d_nb() .","title":"annualized_volatility_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.beta_1d_nb","text":"beta_1d_nb ( returns , benchmark_rets ) Beta.","title":"beta_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.beta_nb","text":"beta_nb ( returns , benchmark_rets ) 2-dim version of beta_1d_nb() .","title":"beta_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.calmar_ratio_1d_nb","text":"calmar_ratio_1d_nb ( returns , ann_factor ) Calmar ratio, or drawdown ratio, of a strategy.","title":"calmar_ratio_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.calmar_ratio_nb","text":"calmar_ratio_nb ( returns , ann_factor ) 2-dim version of calmar_ratio_1d_nb() .","title":"calmar_ratio_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.capture_1d_nb","text":"capture_1d_nb ( returns , benchmark_rets , ann_factor ) Capture ratio.","title":"capture_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.capture_nb","text":"capture_nb ( returns , benchmark_rets , ann_factor ) 2-dim version of capture_1d_nb() .","title":"capture_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.cond_value_at_risk_1d_nb","text":"cond_value_at_risk_1d_nb ( returns , cutoff = 0.05 ) Conditional value at risk (CVaR) of a returns stream.","title":"cond_value_at_risk_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.cond_value_at_risk_nb","text":"cond_value_at_risk_nb ( returns , cutoff = 0.05 ) 2-dim version of cond_value_at_risk_1d_nb() .","title":"cond_value_at_risk_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.cum_returns_1d_nb","text":"cum_returns_1d_nb ( returns , start_value ) Cumulative returns.","title":"cum_returns_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.cum_returns_final_1d_nb","text":"cum_returns_final_1d_nb ( returns , start_value = 0.0 ) Total return.","title":"cum_returns_final_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.cum_returns_final_nb","text":"cum_returns_final_nb ( returns , start_value = 0.0 ) 2-dim version of cum_returns_final_1d_nb() .","title":"cum_returns_final_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.cum_returns_nb","text":"cum_returns_nb ( returns , start_value ) 2-dim version of cum_returns_1d_nb() .","title":"cum_returns_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.down_capture_1d_nb","text":"down_capture_1d_nb ( returns , benchmark_rets , ann_factor ) Capture ratio for periods when the benchmark return is negative.","title":"down_capture_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.down_capture_nb","text":"down_capture_nb ( returns , benchmark_rets , ann_factor ) 2-dim version of down_capture_1d_nb() .","title":"down_capture_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.downside_risk_1d_nb","text":"downside_risk_1d_nb ( returns , ann_factor , required_return = 0.0 ) Downside deviation below a threshold.","title":"downside_risk_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.downside_risk_nb","text":"downside_risk_nb ( returns , ann_factor , required_return = 0.0 ) 2-dim version of downside_risk_1d_nb() .","title":"downside_risk_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.drawdown_1d_nb","text":"drawdown_1d_nb ( returns ) Drawdown of cumulative returns.","title":"drawdown_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.drawdown_nb","text":"drawdown_nb ( returns ) 2-dim version of drawdown_1d_nb() .","title":"drawdown_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.get_return_nb","text":"get_return_nb ( input_value , output_value ) Calculate return from input and output value.","title":"get_return_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.information_ratio_1d_nb","text":"information_ratio_1d_nb ( returns , benchmark_rets , ddof = 1 ) Information ratio of a strategy.","title":"information_ratio_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.information_ratio_nb","text":"information_ratio_nb ( returns , benchmark_rets , ddof = 1 ) 2-dim version of information_ratio_1d_nb() .","title":"information_ratio_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.max_drawdown_1d_nb","text":"max_drawdown_1d_nb ( returns ) Total maximum drawdown (MDD).","title":"max_drawdown_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.max_drawdown_nb","text":"max_drawdown_nb ( returns ) 2-dim version of max_drawdown_1d_nb() .","title":"max_drawdown_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.omega_ratio_1d_nb","text":"omega_ratio_1d_nb ( returns , ann_factor , risk_free = 0.0 , required_return = 0.0 ) Omega ratio of a strategy..","title":"omega_ratio_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.omega_ratio_nb","text":"omega_ratio_nb ( returns , ann_factor , risk_free = 0.0 , required_return = 0.0 ) 2-dim version of omega_ratio_1d_nb() .","title":"omega_ratio_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.returns_1d_nb","text":"returns_1d_nb ( value , init_value ) Calculate returns from value.","title":"returns_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.returns_nb","text":"returns_nb ( value , init_value ) 2-dim version of returns_1d_nb() .","title":"returns_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_alpha_nb","text":"rolling_alpha_nb ( returns , window , minp , benchmark_rets , ann_factor , risk_free = 0.0 ) Rolling version of alpha_nb() .","title":"rolling_alpha_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_annualized_return_nb","text":"rolling_annualized_return_nb ( returns , window , minp , ann_factor ) Rolling version of annualized_return_nb() .","title":"rolling_annualized_return_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_annualized_volatility_nb","text":"rolling_annualized_volatility_nb ( returns , window , minp , ann_factor , levy_alpha = 2.0 , ddof = 1 ) Rolling version of annualized_volatility_nb() .","title":"rolling_annualized_volatility_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_beta_nb","text":"rolling_beta_nb ( returns , window , minp , benchmark_rets ) Rolling version of beta_nb() .","title":"rolling_beta_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_calmar_ratio_nb","text":"rolling_calmar_ratio_nb ( returns , window , minp , ann_factor ) Rolling version of calmar_ratio_nb() .","title":"rolling_calmar_ratio_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_capture_nb","text":"rolling_capture_nb ( returns , window , minp , benchmark_rets , ann_factor ) Rolling version of capture_nb() .","title":"rolling_capture_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_cond_value_at_risk_nb","text":"rolling_cond_value_at_risk_nb ( returns , window , minp , cutoff = 0.05 ) Rolling version of cond_value_at_risk_nb() .","title":"rolling_cond_value_at_risk_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_cum_returns_final_nb","text":"rolling_cum_returns_final_nb ( returns , window , minp , start_value = 0.0 ) Rolling version of cum_returns_final_nb() .","title":"rolling_cum_returns_final_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_down_capture_nb","text":"rolling_down_capture_nb ( returns , window , minp , benchmark_rets , ann_factor ) Rolling version of down_capture_nb() .","title":"rolling_down_capture_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_downside_risk_nb","text":"rolling_downside_risk_nb ( returns , window , minp , ann_factor , required_return = 0.0 ) Rolling version of downside_risk_nb() .","title":"rolling_downside_risk_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_information_ratio_nb","text":"rolling_information_ratio_nb ( returns , window , minp , benchmark_rets , ddof = 1 ) Rolling version of information_ratio_nb() .","title":"rolling_information_ratio_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_max_drawdown_nb","text":"rolling_max_drawdown_nb ( returns , window , minp ) Rolling version of max_drawdown_nb() .","title":"rolling_max_drawdown_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_omega_ratio_nb","text":"rolling_omega_ratio_nb ( returns , window , minp , ann_factor , risk_free = 0.0 , required_return = 0.0 ) Rolling version of omega_ratio_nb() .","title":"rolling_omega_ratio_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_sharpe_ratio_nb","text":"rolling_sharpe_ratio_nb ( returns , window , minp , ann_factor , risk_free = 0.0 , ddof = 1 ) Rolling version of sharpe_ratio_nb() .","title":"rolling_sharpe_ratio_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_sortino_ratio_nb","text":"rolling_sortino_ratio_nb ( returns , window , minp , ann_factor , required_return = 0.0 ) Rolling version of sortino_ratio_nb() .","title":"rolling_sortino_ratio_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_tail_ratio_nb","text":"rolling_tail_ratio_nb ( returns , window , minp ) Rolling version of tail_ratio_nb() .","title":"rolling_tail_ratio_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_up_capture_nb","text":"rolling_up_capture_nb ( returns , window , minp , benchmark_rets , ann_factor ) Rolling version of up_capture_nb() .","title":"rolling_up_capture_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.rolling_value_at_risk_nb","text":"rolling_value_at_risk_nb ( returns , window , minp , cutoff = 0.05 ) Rolling version of value_at_risk_nb() .","title":"rolling_value_at_risk_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.sharpe_ratio_1d_nb","text":"sharpe_ratio_1d_nb ( returns , ann_factor , risk_free = 0.0 , ddof = 1 ) Sharpe ratio of a strategy.","title":"sharpe_ratio_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.sharpe_ratio_nb","text":"sharpe_ratio_nb ( returns , ann_factor , risk_free = 0.0 , ddof = 1 ) 2-dim version of sharpe_ratio_1d_nb() .","title":"sharpe_ratio_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.sortino_ratio_1d_nb","text":"sortino_ratio_1d_nb ( returns , ann_factor , required_return = 0.0 ) Sortino ratio of a strategy.","title":"sortino_ratio_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.sortino_ratio_nb","text":"sortino_ratio_nb ( returns , ann_factor , required_return = 0.0 ) 2-dim version of sortino_ratio_1d_nb() .","title":"sortino_ratio_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.tail_ratio_1d_nb","text":"tail_ratio_1d_nb ( returns ) Ratio between the right (95%) and left tail (5%).","title":"tail_ratio_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.tail_ratio_nb","text":"tail_ratio_nb ( returns ) 2-dim version of tail_ratio_1d_nb() .","title":"tail_ratio_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.total_return_apply_nb","text":"total_return_apply_nb ( idxs , col , returns ) Calculate total return from returns.","title":"total_return_apply_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.up_capture_1d_nb","text":"up_capture_1d_nb ( returns , benchmark_rets , ann_factor ) Capture ratio for periods when the benchmark return is positive.","title":"up_capture_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.up_capture_nb","text":"up_capture_nb ( returns , benchmark_rets , ann_factor ) 2-dim version of up_capture_1d_nb() .","title":"up_capture_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.value_at_risk_1d_nb","text":"value_at_risk_1d_nb ( returns , cutoff = 0.05 ) Value at risk (VaR) of a returns stream.","title":"value_at_risk_1d_nb()"},{"location":"api/returns/nb/#vectorbt.returns.nb.value_at_risk_nb","text":"value_at_risk_nb ( returns , cutoff = 0.05 ) 2-dim version of value_at_risk_1d_nb() .","title":"value_at_risk_nb()"},{"location":"api/returns/qs_adapter/","text":"qs_adapter module \u00b6 Adapter class for quantstats. Note Accessors do not utilize caching. We can access the adapter from ReturnsAccessor : >>> import numpy as np >>> import pandas as pd >>> import vectorbt as vbt >>> import quantstats as qs >>> np . random . seed ( 42 ) >>> rets = pd . Series ( np . random . uniform ( - 0.1 , 0.1 , size = ( 100 ,))) >>> benchmark_rets = pd . Series ( np . random . uniform ( - 0.1 , 0.1 , size = ( 100 ,))) >>> rets . vbt . returns . qs . r_squared ( benchmark = benchmark_rets ) 0.0011582111228735541 Which is the same as: >>> qs . stats . r_squared ( rets , benchmark_rets ) So why not just using qs.stats ? First, we can define all parameters such as benchmark returns once and avoid passing them repeatedly to every function. Second, vectorbt automatically translates parameters passed to ReturnsAccessor for the use in quantstats. >>> # Defaults that vectorbt understands >>> ret_acc = rets . vbt . returns ( ... benchmark_rets = benchmark_rets , ... freq = 'd' , ... year_freq = '365d' , ... defaults = dict ( risk_free = 0.001 ) ... ) >>> ret_acc . qs . r_squared () 0.0011582111228735541 >>> ret_acc . qs . sharpe () -1.9158923252075455 >>> # Defaults that only quantstats understands >>> qs_defaults = dict ( ... benchmark = benchmark_rets , ... periods = 365 , ... periods_per_year = 365 , ... rf = 0.001 ... ) >>> ret_acc_qs = rets . vbt . returns . qs ( defaults = qs_defaults ) >>> ret_acc_qs . r_squared () 0.0011582111228735541 >>> ret_acc_qs . sharpe () -1.9158923252075455 The adapter automatically passes the returns to the particular function. It also merges the defaults defined in the settings, the defaults passed to ReturnsAccessor , and the defaults passed to QSAdapter itself, and matches them with the argument names listed in the function's signature. For example, the periods and periods_per_year arguments default to the annualization factor ReturnsAccessor.ann_factor , which itself is based on the freq argument. This makes the results produced by quantstats and vectorbt at least somewhat similar. >>> vbt . settings . array_wrapper [ 'freq' ] = 'h' >>> vbt . settings . returns [ 'year_freq' ] = '365d' >>> rets . vbt . returns . sharpe_ratio () # ReturnsAccessor -9.38160953971508 >>> rets . vbt . returns . qs . sharpe () # quantstats via QSAdapter -9.38160953971508 We can still override any argument by overriding its default or by passing it directly to the function: >>> rets . vbt . returns . qs ( defaults = dict ( periods = 252 )) . sharpe () -1.5912029345745982 >>> rets . vbt . returns . qs . sharpe ( periods = 252 ) -1.5912029345745982 >>> qs . stats . sharpe ( rets ) -1.5912029345745982 attach_qs_methods function \u00b6 attach_qs_methods ( cls , replace_signature = True ) Class decorator to attach quantstats methods. QSAdapter class \u00b6 Adapter class for quantstats. Superclasses Configured Documented Pickleable Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save() adjusted_sortino method \u00b6 QSAdapter . adjusted_sortino ( * , rf = 0 , periods = 252 , annualize = True , smart = False ) See quantstats.stats.adjusted_sortino . aggregate_returns method \u00b6 QSAdapter . aggregate_returns ( * , period = None , compounded = True ) See quantstats.utils.aggregate_returns . autocorr_penalty method \u00b6 QSAdapter . autocorr_penalty ( * , prepare_returns = False ) See quantstats.stats.autocorr_penalty . avg_loss method \u00b6 QSAdapter . avg_loss ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.avg_loss . avg_return method \u00b6 QSAdapter . avg_return ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.avg_return . avg_win method \u00b6 QSAdapter . avg_win ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.avg_win . basic_report method \u00b6 QSAdapter . basic_report ( * , benchmark = None , rf = 0.0 , grayscale = False , figsize = ( 8 , 5 ), display = True , compounded = True , periods_per_year = 252 , match_dates = False ) See quantstats.reports.basic . best method \u00b6 QSAdapter . best ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.best . cagr method \u00b6 QSAdapter . cagr ( * , rf = 0.0 , compounded = True ) See quantstats.stats.cagr . calmar method \u00b6 QSAdapter . calmar ( * , prepare_returns = True ) See quantstats.stats.calmar . common_sense_ratio method \u00b6 QSAdapter . common_sense_ratio ( * , prepare_returns = True ) See quantstats.stats.common_sense_ratio . comp method \u00b6 QSAdapter . comp () See quantstats.stats.comp . compare method \u00b6 QSAdapter . compare ( * , benchmark , aggregate = None , compounded = True , round_vals = None , prepare_returns = True ) See quantstats.stats.compare . compsum method \u00b6 QSAdapter . compsum () See quantstats.stats.compsum . conditional_value_at_risk method \u00b6 QSAdapter . conditional_value_at_risk ( * , sigma = 1 , confidence = 0.95 , prepare_returns = True ) See quantstats.stats.conditional_value_at_risk . consecutive_losses method \u00b6 QSAdapter . consecutive_losses ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.consecutive_losses . consecutive_wins method \u00b6 QSAdapter . consecutive_wins ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.consecutive_wins . cpc_index method \u00b6 QSAdapter . cpc_index ( * , prepare_returns = True ) See quantstats.stats.cpc_index . cvar method \u00b6 QSAdapter . cvar ( * , sigma = 1 , confidence = 0.95 , prepare_returns = True ) See quantstats.stats.cvar . defaults property \u00b6 Defaults for QSAdapter . Merges qs_adapter.defaults from settings , returns_accessor.defaults (with adapted naming), and defaults from QSAdapter . defaults_mapping property \u00b6 Common argument names in quantstats mapped to ReturnsAccessor.defaults . distribution method \u00b6 QSAdapter . distribution ( * , compounded = True , prepare_returns = True ) See quantstats.stats.distribution . expected_return method \u00b6 QSAdapter . expected_return ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.expected_return . expected_shortfall method \u00b6 QSAdapter . expected_shortfall ( * , sigma = 1 , confidence = 0.95 ) See quantstats.stats.expected_shortfall . exponential_stdev method \u00b6 QSAdapter . exponential_stdev ( * , window = 30 , is_halflife = False ) See quantstats.utils.exponential_stdev . exposure method \u00b6 QSAdapter . exposure ( * , prepare_returns = True ) See quantstats.stats.exposure . full_report method \u00b6 QSAdapter . full_report ( * , benchmark = None , rf = 0.0 , grayscale = False , figsize = ( 8 , 5 ), display = True , compounded = True , periods_per_year = 252 , match_dates = False ) See quantstats.reports.full . gain_to_pain_ratio method \u00b6 QSAdapter . gain_to_pain_ratio ( * , rf = 0 , resolution = 'D' ) See quantstats.stats.gain_to_pain_ratio . greeks method \u00b6 QSAdapter . greeks ( * , benchmark , periods = 252.0 , prepare_returns = True ) See quantstats.stats.greeks . group_returns method \u00b6 QSAdapter . group_returns ( * , groupby , compounded = False ) See quantstats.utils.group_returns . html_report method \u00b6 QSAdapter . html_report ( * , benchmark = None , rf = 0.0 , grayscale = False , title = 'Strategy Tearsheet' , output = None , compounded = True , periods_per_year = 252 , download_filename = 'quantstats-tearsheet.html' , figfmt = 'svg' , template_path = None , match_dates = False ) See quantstats.reports.html . implied_volatility method \u00b6 QSAdapter . implied_volatility ( * , periods = 252 , annualize = True ) See quantstats.stats.implied_volatility . information_ratio method \u00b6 QSAdapter . information_ratio ( * , benchmark , prepare_returns = True ) See quantstats.stats.information_ratio . kelly_criterion method \u00b6 QSAdapter . kelly_criterion ( * , prepare_returns = True ) See quantstats.stats.kelly_criterion . kurtosis method \u00b6 QSAdapter . kurtosis ( * , prepare_returns = True ) See quantstats.stats.kurtosis . log_returns method \u00b6 QSAdapter . log_returns ( * , rf = 0.0 , nperiods = None ) See quantstats.utils.log_returns . make_index method \u00b6 QSAdapter . make_index ( * , rebalance = '1M' , period = 'max' , returns = None , match_dates = False ) See quantstats.utils.make_index . make_portfolio method \u00b6 QSAdapter . make_portfolio ( * , start_balance = 100000.0 , mode = 'comp' , round_to = None ) See quantstats.utils.make_portfolio . metrics_report method \u00b6 QSAdapter . metrics_report ( * , benchmark = None , rf = 0.0 , display = True , mode = 'basic' , sep = False , compounded = True , periods_per_year = 252 , prepare_returns = True , match_dates = False , ** kwargs ) See quantstats.reports.metrics . monthly_returns method \u00b6 QSAdapter . monthly_returns ( * , eoy = True , compounded = True , prepare_returns = True ) See quantstats.stats.monthly_returns . omega method \u00b6 QSAdapter . omega ( * , rf = 0.0 , required_return = 0.0 , periods = 252 ) See quantstats.stats.omega . outlier_loss_ratio method \u00b6 QSAdapter . outlier_loss_ratio ( * , quantile = 0.01 , prepare_returns = True ) See quantstats.stats.outlier_loss_ratio . outlier_win_ratio method \u00b6 QSAdapter . outlier_win_ratio ( * , quantile = 0.99 , prepare_returns = True ) See quantstats.stats.outlier_win_ratio . outliers method \u00b6 QSAdapter . outliers ( * , quantile = 0.95 ) See quantstats.stats.outliers . payoff_ratio method \u00b6 QSAdapter . payoff_ratio ( * , prepare_returns = True ) See quantstats.stats.payoff_ratio . plot_daily_returns method \u00b6 QSAdapter . plot_daily_returns ( * , grayscale = False , figsize = ( 10 , 4 ), fontname = 'Arial' , lw = 0.5 , log_scale = False , ylabel = 'Returns' , subtitle = True , savefig = None , show = True , prepare_returns = True ) See quantstats.plots.daily_returns . plot_distribution method \u00b6 QSAdapter . plot_distribution ( * , fontname = 'Arial' , grayscale = False , ylabel = True , figsize = ( 10 , 6 ), subtitle = True , compounded = True , savefig = None , show = True , prepare_returns = True ) See quantstats.plots.distribution . plot_drawdown method \u00b6 QSAdapter . plot_drawdown ( * , grayscale = False , figsize = ( 10 , 5 ), fontname = 'Arial' , lw = 1 , log_scale = False , match_volatility = False , compound = False , ylabel = 'Drawdown' , resample = None , subtitle = True , savefig = None , show = True ) See quantstats.plots.drawdown . plot_drawdowns_periods method \u00b6 QSAdapter . plot_drawdowns_periods ( * , periods = 5 , lw = 1.5 , log_scale = False , fontname = 'Arial' , grayscale = False , figsize = ( 10 , 5 ), ylabel = True , subtitle = True , compounded = True , savefig = None , show = True , prepare_returns = True ) See quantstats.plots.drawdowns_periods . plot_earnings method \u00b6 QSAdapter . plot_earnings ( * , start_balance = 100000.0 , mode = 'comp' , grayscale = False , figsize = ( 10 , 6 ), title = 'Portfolio Earnings' , fontname = 'Arial' , lw = 1.5 , subtitle = True , savefig = None , show = True ) See quantstats.plots.earnings . plot_histogram method \u00b6 QSAdapter . plot_histogram ( * , resample = 'M' , fontname = 'Arial' , grayscale = False , figsize = ( 10 , 5 ), ylabel = True , subtitle = True , compounded = True , savefig = None , show = True , prepare_returns = True ) See quantstats.plots.histogram . plot_log_returns method \u00b6 QSAdapter . plot_log_returns ( * , benchmark = None , grayscale = False , figsize = ( 10 , 5 ), fontname = 'Arial' , lw = 1.5 , match_volatility = False , compound = True , cumulative = True , resample = None , ylabel = 'Cumulative Returns' , subtitle = True , savefig = None , show = True , prepare_returns = True ) See quantstats.plots.log_returns . plot_monthly_heatmap method \u00b6 QSAdapter . plot_monthly_heatmap ( * , annot_size = 10 , figsize = ( 10 , 5 ), cbar = True , square = False , compounded = True , eoy = False , grayscale = False , fontname = 'Arial' , ylabel = True , savefig = None , show = True ) See quantstats.plots.monthly_heatmap . plot_monthly_returns method \u00b6 QSAdapter . plot_monthly_returns ( * , annot_size = 10 , figsize = ( 10 , 5 ), cbar = True , square = False , compounded = True , eoy = False , grayscale = False , fontname = 'Arial' , ylabel = True , savefig = None , show = True ) See quantstats.plots.monthly_returns . plot_returns method \u00b6 QSAdapter . plot_returns ( * , benchmark = None , grayscale = False , figsize = ( 10 , 6 ), fontname = 'Arial' , lw = 1.5 , match_volatility = False , compound = True , cumulative = True , resample = None , ylabel = 'Cumulative Returns' , subtitle = True , savefig = None , show = True , prepare_returns = True ) See quantstats.plots.returns . plot_rolling_beta method \u00b6 QSAdapter . plot_rolling_beta ( * , benchmark , window1 = 126 , window1_label = '6-Months' , window2 = 252 , window2_label = '12-Months' , lw = 1.5 , fontname = 'Arial' , grayscale = False , figsize = ( 10 , 3 ), ylabel = True , subtitle = True , savefig = None , show = True , prepare_returns = True ) See quantstats.plots.rolling_beta . plot_rolling_sharpe method \u00b6 QSAdapter . plot_rolling_sharpe ( * , benchmark = None , rf = 0.0 , period = 126 , period_label = '6-Months' , periods_per_year = 252 , lw = 1.25 , fontname = 'Arial' , grayscale = False , figsize = ( 10 , 3 ), ylabel = 'Sharpe' , subtitle = True , savefig = None , show = True ) See quantstats.plots.rolling_sharpe . plot_rolling_sortino method \u00b6 QSAdapter . plot_rolling_sortino ( * , benchmark = None , rf = 0.0 , period = 126 , period_label = '6-Months' , periods_per_year = 252 , lw = 1.25 , fontname = 'Arial' , grayscale = False , figsize = ( 10 , 3 ), ylabel = 'Sortino' , subtitle = True , savefig = None , show = True ) See quantstats.plots.rolling_sortino . plot_rolling_volatility method \u00b6 QSAdapter . plot_rolling_volatility ( * , benchmark = None , period = 126 , period_label = '6-Months' , periods_per_year = 252 , lw = 1.5 , fontname = 'Arial' , grayscale = False , figsize = ( 10 , 3 ), ylabel = 'Volatility' , subtitle = True , savefig = None , show = True ) See quantstats.plots.rolling_volatility . plot_snapshot method \u00b6 QSAdapter . plot_snapshot ( * , grayscale = False , figsize = ( 10 , 8 ), title = 'Portfolio Summary' , fontname = 'Arial' , lw = 1.5 , mode = 'comp' , subtitle = True , savefig = None , show = True , log_scale = False ) See quantstats.plots.snapshot . plot_yearly_returns method \u00b6 QSAdapter . plot_yearly_returns ( * , benchmark = None , fontname = 'Arial' , grayscale = False , hlw = 1.5 , hlcolor = 'red' , hllabel = '' , match_volatility = False , log_scale = False , figsize = ( 10 , 5 ), ylabel = True , subtitle = True , compounded = True , savefig = None , show = True , prepare_returns = True ) See quantstats.plots.yearly_returns . plots_report method \u00b6 QSAdapter . plots_report ( * , benchmark = None , grayscale = False , figsize = ( 8 , 5 ), mode = 'basic' , compounded = True , periods_per_year = 252 , prepare_returns = True , match_dates = False ) See quantstats.reports.plots . profit_factor method \u00b6 QSAdapter . profit_factor ( * , prepare_returns = True ) See quantstats.stats.profit_factor . profit_ratio method \u00b6 QSAdapter . profit_ratio ( * , prepare_returns = True ) See quantstats.stats.profit_ratio . r2 method \u00b6 QSAdapter . r2 ( * , benchmark ) See quantstats.stats.r2 . r_squared method \u00b6 QSAdapter . r_squared ( * , benchmark , prepare_returns = True ) See quantstats.stats.r_squared . rar method \u00b6 QSAdapter . rar ( * , rf = 0.0 ) See quantstats.stats.rar . recovery_factor method \u00b6 QSAdapter . recovery_factor ( * , prepare_returns = True ) See quantstats.stats.recovery_factor . remove_outliers method \u00b6 QSAdapter . remove_outliers ( * , quantile = 0.95 ) See quantstats.stats.remove_outliers . returns_accessor property \u00b6 Returns accessor. risk_of_ruin method \u00b6 QSAdapter . risk_of_ruin ( * , prepare_returns = True ) See quantstats.stats.risk_of_ruin . risk_return_ratio method \u00b6 QSAdapter . risk_return_ratio ( * , prepare_returns = True ) See quantstats.stats.risk_return_ratio . rolling_greeks method \u00b6 QSAdapter . rolling_greeks ( * , benchmark , periods = 252 , prepare_returns = True ) See quantstats.stats.rolling_greeks . rolling_sharpe method \u00b6 QSAdapter . rolling_sharpe ( * , rf = 0.0 , rolling_period = 126 , annualize = True , periods_per_year = 252 , prepare_returns = True ) See quantstats.stats.rolling_sharpe . rolling_sortino method \u00b6 QSAdapter . rolling_sortino ( * , rf = 0 , rolling_period = 126 , annualize = True , periods_per_year = 252 , ** kwargs ) See quantstats.stats.rolling_sortino . rolling_volatility method \u00b6 QSAdapter . rolling_volatility ( * , rolling_period = 126 , periods_per_year = 252 , prepare_returns = True ) See quantstats.stats.rolling_volatility . ror method \u00b6 QSAdapter . ror () See quantstats.stats.ror . serenity_index method \u00b6 QSAdapter . serenity_index ( * , rf = 0 ) See quantstats.stats.serenity_index . sharpe method \u00b6 QSAdapter . sharpe ( * , rf = 0.0 , periods = 252 , annualize = True , smart = False ) See quantstats.stats.sharpe . skew method \u00b6 QSAdapter . skew ( * , prepare_returns = True ) See quantstats.stats.skew . smart_sharpe method \u00b6 QSAdapter . smart_sharpe ( * , rf = 0.0 , periods = 252 , annualize = True ) See quantstats.stats.smart_sharpe . smart_sortino method \u00b6 QSAdapter . smart_sortino ( * , rf = 0 , periods = 252 , annualize = True ) See quantstats.stats.smart_sortino . sortino method \u00b6 QSAdapter . sortino ( * , rf = 0 , periods = 252 , annualize = True , smart = False ) See quantstats.stats.sortino . tail_ratio method \u00b6 QSAdapter . tail_ratio ( * , cutoff = 0.95 , prepare_returns = True ) See quantstats.stats.tail_ratio . to_drawdown_series method \u00b6 QSAdapter . to_drawdown_series () See quantstats.stats.to_drawdown_series . to_excess_returns method \u00b6 QSAdapter . to_excess_returns ( * , rf , nperiods = None ) See quantstats.utils.to_excess_returns . to_log_returns method \u00b6 QSAdapter . to_log_returns ( * , rf = 0.0 , nperiods = None ) See quantstats.utils.to_log_returns . to_prices method \u00b6 QSAdapter . to_prices ( * , base = 100000.0 ) See quantstats.utils.to_prices . ulcer_index method \u00b6 QSAdapter . ulcer_index () See quantstats.stats.ulcer_index . ulcer_performance_index method \u00b6 QSAdapter . ulcer_performance_index ( * , rf = 0 ) See quantstats.stats.ulcer_performance_index . upi method \u00b6 QSAdapter . upi ( * , rf = 0 ) See quantstats.stats.upi . value_at_risk method \u00b6 QSAdapter . value_at_risk ( * , sigma = 1 , confidence = 0.95 , prepare_returns = True ) See quantstats.stats.value_at_risk . var method \u00b6 QSAdapter . var ( * , sigma = 1 , confidence = 0.95 , prepare_returns = True ) See quantstats.stats.var . volatility method \u00b6 QSAdapter . volatility ( * , periods = 252 , annualize = True , prepare_returns = True ) See quantstats.stats.volatility . win_loss_ratio method \u00b6 QSAdapter . win_loss_ratio ( * , prepare_returns = True ) See quantstats.stats.win_loss_ratio . win_rate method \u00b6 QSAdapter . win_rate ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.win_rate . worst method \u00b6 QSAdapter . worst ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.worst .","title":"qs_adapter"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter","text":"Adapter class for quantstats. Note Accessors do not utilize caching. We can access the adapter from ReturnsAccessor : >>> import numpy as np >>> import pandas as pd >>> import vectorbt as vbt >>> import quantstats as qs >>> np . random . seed ( 42 ) >>> rets = pd . Series ( np . random . uniform ( - 0.1 , 0.1 , size = ( 100 ,))) >>> benchmark_rets = pd . Series ( np . random . uniform ( - 0.1 , 0.1 , size = ( 100 ,))) >>> rets . vbt . returns . qs . r_squared ( benchmark = benchmark_rets ) 0.0011582111228735541 Which is the same as: >>> qs . stats . r_squared ( rets , benchmark_rets ) So why not just using qs.stats ? First, we can define all parameters such as benchmark returns once and avoid passing them repeatedly to every function. Second, vectorbt automatically translates parameters passed to ReturnsAccessor for the use in quantstats. >>> # Defaults that vectorbt understands >>> ret_acc = rets . vbt . returns ( ... benchmark_rets = benchmark_rets , ... freq = 'd' , ... year_freq = '365d' , ... defaults = dict ( risk_free = 0.001 ) ... ) >>> ret_acc . qs . r_squared () 0.0011582111228735541 >>> ret_acc . qs . sharpe () -1.9158923252075455 >>> # Defaults that only quantstats understands >>> qs_defaults = dict ( ... benchmark = benchmark_rets , ... periods = 365 , ... periods_per_year = 365 , ... rf = 0.001 ... ) >>> ret_acc_qs = rets . vbt . returns . qs ( defaults = qs_defaults ) >>> ret_acc_qs . r_squared () 0.0011582111228735541 >>> ret_acc_qs . sharpe () -1.9158923252075455 The adapter automatically passes the returns to the particular function. It also merges the defaults defined in the settings, the defaults passed to ReturnsAccessor , and the defaults passed to QSAdapter itself, and matches them with the argument names listed in the function's signature. For example, the periods and periods_per_year arguments default to the annualization factor ReturnsAccessor.ann_factor , which itself is based on the freq argument. This makes the results produced by quantstats and vectorbt at least somewhat similar. >>> vbt . settings . array_wrapper [ 'freq' ] = 'h' >>> vbt . settings . returns [ 'year_freq' ] = '365d' >>> rets . vbt . returns . sharpe_ratio () # ReturnsAccessor -9.38160953971508 >>> rets . vbt . returns . qs . sharpe () # quantstats via QSAdapter -9.38160953971508 We can still override any argument by overriding its default or by passing it directly to the function: >>> rets . vbt . returns . qs ( defaults = dict ( periods = 252 )) . sharpe () -1.5912029345745982 >>> rets . vbt . returns . qs . sharpe ( periods = 252 ) -1.5912029345745982 >>> qs . stats . sharpe ( rets ) -1.5912029345745982","title":"vectorbt.returns.qs_adapter"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.attach_qs_methods","text":"attach_qs_methods ( cls , replace_signature = True ) Class decorator to attach quantstats methods.","title":"attach_qs_methods()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter","text":"Adapter class for quantstats. Superclasses Configured Documented Pickleable Inherited members Configured.config Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() Configured.writeable_attrs Pickleable.load() Pickleable.save()","title":"QSAdapter"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.adjusted_sortino","text":"QSAdapter . adjusted_sortino ( * , rf = 0 , periods = 252 , annualize = True , smart = False ) See quantstats.stats.adjusted_sortino .","title":"adjusted_sortino()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.aggregate_returns","text":"QSAdapter . aggregate_returns ( * , period = None , compounded = True ) See quantstats.utils.aggregate_returns .","title":"aggregate_returns()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.autocorr_penalty","text":"QSAdapter . autocorr_penalty ( * , prepare_returns = False ) See quantstats.stats.autocorr_penalty .","title":"autocorr_penalty()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.avg_loss","text":"QSAdapter . avg_loss ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.avg_loss .","title":"avg_loss()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.avg_return","text":"QSAdapter . avg_return ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.avg_return .","title":"avg_return()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.avg_win","text":"QSAdapter . avg_win ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.avg_win .","title":"avg_win()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.basic_report","text":"QSAdapter . basic_report ( * , benchmark = None , rf = 0.0 , grayscale = False , figsize = ( 8 , 5 ), display = True , compounded = True , periods_per_year = 252 , match_dates = False ) See quantstats.reports.basic .","title":"basic_report()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.best","text":"QSAdapter . best ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.best .","title":"best()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.cagr","text":"QSAdapter . cagr ( * , rf = 0.0 , compounded = True ) See quantstats.stats.cagr .","title":"cagr()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.calmar","text":"QSAdapter . calmar ( * , prepare_returns = True ) See quantstats.stats.calmar .","title":"calmar()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.common_sense_ratio","text":"QSAdapter . common_sense_ratio ( * , prepare_returns = True ) See quantstats.stats.common_sense_ratio .","title":"common_sense_ratio()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.comp","text":"QSAdapter . comp () See quantstats.stats.comp .","title":"comp()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.compare","text":"QSAdapter . compare ( * , benchmark , aggregate = None , compounded = True , round_vals = None , prepare_returns = True ) See quantstats.stats.compare .","title":"compare()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.compsum","text":"QSAdapter . compsum () See quantstats.stats.compsum .","title":"compsum()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.conditional_value_at_risk","text":"QSAdapter . conditional_value_at_risk ( * , sigma = 1 , confidence = 0.95 , prepare_returns = True ) See quantstats.stats.conditional_value_at_risk .","title":"conditional_value_at_risk()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.consecutive_losses","text":"QSAdapter . consecutive_losses ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.consecutive_losses .","title":"consecutive_losses()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.consecutive_wins","text":"QSAdapter . consecutive_wins ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.consecutive_wins .","title":"consecutive_wins()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.cpc_index","text":"QSAdapter . cpc_index ( * , prepare_returns = True ) See quantstats.stats.cpc_index .","title":"cpc_index()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.cvar","text":"QSAdapter . cvar ( * , sigma = 1 , confidence = 0.95 , prepare_returns = True ) See quantstats.stats.cvar .","title":"cvar()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.defaults","text":"Defaults for QSAdapter . Merges qs_adapter.defaults from settings , returns_accessor.defaults (with adapted naming), and defaults from QSAdapter .","title":"defaults"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.defaults_mapping","text":"Common argument names in quantstats mapped to ReturnsAccessor.defaults .","title":"defaults_mapping"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.distribution","text":"QSAdapter . distribution ( * , compounded = True , prepare_returns = True ) See quantstats.stats.distribution .","title":"distribution()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.expected_return","text":"QSAdapter . expected_return ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.expected_return .","title":"expected_return()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.expected_shortfall","text":"QSAdapter . expected_shortfall ( * , sigma = 1 , confidence = 0.95 ) See quantstats.stats.expected_shortfall .","title":"expected_shortfall()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.exponential_stdev","text":"QSAdapter . exponential_stdev ( * , window = 30 , is_halflife = False ) See quantstats.utils.exponential_stdev .","title":"exponential_stdev()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.exposure","text":"QSAdapter . exposure ( * , prepare_returns = True ) See quantstats.stats.exposure .","title":"exposure()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.full_report","text":"QSAdapter . full_report ( * , benchmark = None , rf = 0.0 , grayscale = False , figsize = ( 8 , 5 ), display = True , compounded = True , periods_per_year = 252 , match_dates = False ) See quantstats.reports.full .","title":"full_report()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.gain_to_pain_ratio","text":"QSAdapter . gain_to_pain_ratio ( * , rf = 0 , resolution = 'D' ) See quantstats.stats.gain_to_pain_ratio .","title":"gain_to_pain_ratio()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.greeks","text":"QSAdapter . greeks ( * , benchmark , periods = 252.0 , prepare_returns = True ) See quantstats.stats.greeks .","title":"greeks()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.group_returns","text":"QSAdapter . group_returns ( * , groupby , compounded = False ) See quantstats.utils.group_returns .","title":"group_returns()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.html_report","text":"QSAdapter . html_report ( * , benchmark = None , rf = 0.0 , grayscale = False , title = 'Strategy Tearsheet' , output = None , compounded = True , periods_per_year = 252 , download_filename = 'quantstats-tearsheet.html' , figfmt = 'svg' , template_path = None , match_dates = False ) See quantstats.reports.html .","title":"html_report()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.implied_volatility","text":"QSAdapter . implied_volatility ( * , periods = 252 , annualize = True ) See quantstats.stats.implied_volatility .","title":"implied_volatility()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.information_ratio","text":"QSAdapter . information_ratio ( * , benchmark , prepare_returns = True ) See quantstats.stats.information_ratio .","title":"information_ratio()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.kelly_criterion","text":"QSAdapter . kelly_criterion ( * , prepare_returns = True ) See quantstats.stats.kelly_criterion .","title":"kelly_criterion()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.kurtosis","text":"QSAdapter . kurtosis ( * , prepare_returns = True ) See quantstats.stats.kurtosis .","title":"kurtosis()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.log_returns","text":"QSAdapter . log_returns ( * , rf = 0.0 , nperiods = None ) See quantstats.utils.log_returns .","title":"log_returns()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.make_index","text":"QSAdapter . make_index ( * , rebalance = '1M' , period = 'max' , returns = None , match_dates = False ) See quantstats.utils.make_index .","title":"make_index()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.make_portfolio","text":"QSAdapter . make_portfolio ( * , start_balance = 100000.0 , mode = 'comp' , round_to = None ) See quantstats.utils.make_portfolio .","title":"make_portfolio()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.metrics_report","text":"QSAdapter . metrics_report ( * , benchmark = None , rf = 0.0 , display = True , mode = 'basic' , sep = False , compounded = True , periods_per_year = 252 , prepare_returns = True , match_dates = False , ** kwargs ) See quantstats.reports.metrics .","title":"metrics_report()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.monthly_returns","text":"QSAdapter . monthly_returns ( * , eoy = True , compounded = True , prepare_returns = True ) See quantstats.stats.monthly_returns .","title":"monthly_returns()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.omega","text":"QSAdapter . omega ( * , rf = 0.0 , required_return = 0.0 , periods = 252 ) See quantstats.stats.omega .","title":"omega()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.outlier_loss_ratio","text":"QSAdapter . outlier_loss_ratio ( * , quantile = 0.01 , prepare_returns = True ) See quantstats.stats.outlier_loss_ratio .","title":"outlier_loss_ratio()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.outlier_win_ratio","text":"QSAdapter . outlier_win_ratio ( * , quantile = 0.99 , prepare_returns = True ) See quantstats.stats.outlier_win_ratio .","title":"outlier_win_ratio()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.outliers","text":"QSAdapter . outliers ( * , quantile = 0.95 ) See quantstats.stats.outliers .","title":"outliers()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.payoff_ratio","text":"QSAdapter . payoff_ratio ( * , prepare_returns = True ) See quantstats.stats.payoff_ratio .","title":"payoff_ratio()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.plot_daily_returns","text":"QSAdapter . plot_daily_returns ( * , grayscale = False , figsize = ( 10 , 4 ), fontname = 'Arial' , lw = 0.5 , log_scale = False , ylabel = 'Returns' , subtitle = True , savefig = None , show = True , prepare_returns = True ) See quantstats.plots.daily_returns .","title":"plot_daily_returns()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.plot_distribution","text":"QSAdapter . plot_distribution ( * , fontname = 'Arial' , grayscale = False , ylabel = True , figsize = ( 10 , 6 ), subtitle = True , compounded = True , savefig = None , show = True , prepare_returns = True ) See quantstats.plots.distribution .","title":"plot_distribution()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.plot_drawdown","text":"QSAdapter . plot_drawdown ( * , grayscale = False , figsize = ( 10 , 5 ), fontname = 'Arial' , lw = 1 , log_scale = False , match_volatility = False , compound = False , ylabel = 'Drawdown' , resample = None , subtitle = True , savefig = None , show = True ) See quantstats.plots.drawdown .","title":"plot_drawdown()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.plot_drawdowns_periods","text":"QSAdapter . plot_drawdowns_periods ( * , periods = 5 , lw = 1.5 , log_scale = False , fontname = 'Arial' , grayscale = False , figsize = ( 10 , 5 ), ylabel = True , subtitle = True , compounded = True , savefig = None , show = True , prepare_returns = True ) See quantstats.plots.drawdowns_periods .","title":"plot_drawdowns_periods()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.plot_earnings","text":"QSAdapter . plot_earnings ( * , start_balance = 100000.0 , mode = 'comp' , grayscale = False , figsize = ( 10 , 6 ), title = 'Portfolio Earnings' , fontname = 'Arial' , lw = 1.5 , subtitle = True , savefig = None , show = True ) See quantstats.plots.earnings .","title":"plot_earnings()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.plot_histogram","text":"QSAdapter . plot_histogram ( * , resample = 'M' , fontname = 'Arial' , grayscale = False , figsize = ( 10 , 5 ), ylabel = True , subtitle = True , compounded = True , savefig = None , show = True , prepare_returns = True ) See quantstats.plots.histogram .","title":"plot_histogram()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.plot_log_returns","text":"QSAdapter . plot_log_returns ( * , benchmark = None , grayscale = False , figsize = ( 10 , 5 ), fontname = 'Arial' , lw = 1.5 , match_volatility = False , compound = True , cumulative = True , resample = None , ylabel = 'Cumulative Returns' , subtitle = True , savefig = None , show = True , prepare_returns = True ) See quantstats.plots.log_returns .","title":"plot_log_returns()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.plot_monthly_heatmap","text":"QSAdapter . plot_monthly_heatmap ( * , annot_size = 10 , figsize = ( 10 , 5 ), cbar = True , square = False , compounded = True , eoy = False , grayscale = False , fontname = 'Arial' , ylabel = True , savefig = None , show = True ) See quantstats.plots.monthly_heatmap .","title":"plot_monthly_heatmap()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.plot_monthly_returns","text":"QSAdapter . plot_monthly_returns ( * , annot_size = 10 , figsize = ( 10 , 5 ), cbar = True , square = False , compounded = True , eoy = False , grayscale = False , fontname = 'Arial' , ylabel = True , savefig = None , show = True ) See quantstats.plots.monthly_returns .","title":"plot_monthly_returns()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.plot_returns","text":"QSAdapter . plot_returns ( * , benchmark = None , grayscale = False , figsize = ( 10 , 6 ), fontname = 'Arial' , lw = 1.5 , match_volatility = False , compound = True , cumulative = True , resample = None , ylabel = 'Cumulative Returns' , subtitle = True , savefig = None , show = True , prepare_returns = True ) See quantstats.plots.returns .","title":"plot_returns()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.plot_rolling_beta","text":"QSAdapter . plot_rolling_beta ( * , benchmark , window1 = 126 , window1_label = '6-Months' , window2 = 252 , window2_label = '12-Months' , lw = 1.5 , fontname = 'Arial' , grayscale = False , figsize = ( 10 , 3 ), ylabel = True , subtitle = True , savefig = None , show = True , prepare_returns = True ) See quantstats.plots.rolling_beta .","title":"plot_rolling_beta()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.plot_rolling_sharpe","text":"QSAdapter . plot_rolling_sharpe ( * , benchmark = None , rf = 0.0 , period = 126 , period_label = '6-Months' , periods_per_year = 252 , lw = 1.25 , fontname = 'Arial' , grayscale = False , figsize = ( 10 , 3 ), ylabel = 'Sharpe' , subtitle = True , savefig = None , show = True ) See quantstats.plots.rolling_sharpe .","title":"plot_rolling_sharpe()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.plot_rolling_sortino","text":"QSAdapter . plot_rolling_sortino ( * , benchmark = None , rf = 0.0 , period = 126 , period_label = '6-Months' , periods_per_year = 252 , lw = 1.25 , fontname = 'Arial' , grayscale = False , figsize = ( 10 , 3 ), ylabel = 'Sortino' , subtitle = True , savefig = None , show = True ) See quantstats.plots.rolling_sortino .","title":"plot_rolling_sortino()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.plot_rolling_volatility","text":"QSAdapter . plot_rolling_volatility ( * , benchmark = None , period = 126 , period_label = '6-Months' , periods_per_year = 252 , lw = 1.5 , fontname = 'Arial' , grayscale = False , figsize = ( 10 , 3 ), ylabel = 'Volatility' , subtitle = True , savefig = None , show = True ) See quantstats.plots.rolling_volatility .","title":"plot_rolling_volatility()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.plot_snapshot","text":"QSAdapter . plot_snapshot ( * , grayscale = False , figsize = ( 10 , 8 ), title = 'Portfolio Summary' , fontname = 'Arial' , lw = 1.5 , mode = 'comp' , subtitle = True , savefig = None , show = True , log_scale = False ) See quantstats.plots.snapshot .","title":"plot_snapshot()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.plot_yearly_returns","text":"QSAdapter . plot_yearly_returns ( * , benchmark = None , fontname = 'Arial' , grayscale = False , hlw = 1.5 , hlcolor = 'red' , hllabel = '' , match_volatility = False , log_scale = False , figsize = ( 10 , 5 ), ylabel = True , subtitle = True , compounded = True , savefig = None , show = True , prepare_returns = True ) See quantstats.plots.yearly_returns .","title":"plot_yearly_returns()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.plots_report","text":"QSAdapter . plots_report ( * , benchmark = None , grayscale = False , figsize = ( 8 , 5 ), mode = 'basic' , compounded = True , periods_per_year = 252 , prepare_returns = True , match_dates = False ) See quantstats.reports.plots .","title":"plots_report()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.profit_factor","text":"QSAdapter . profit_factor ( * , prepare_returns = True ) See quantstats.stats.profit_factor .","title":"profit_factor()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.profit_ratio","text":"QSAdapter . profit_ratio ( * , prepare_returns = True ) See quantstats.stats.profit_ratio .","title":"profit_ratio()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.r2","text":"QSAdapter . r2 ( * , benchmark ) See quantstats.stats.r2 .","title":"r2()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.r_squared","text":"QSAdapter . r_squared ( * , benchmark , prepare_returns = True ) See quantstats.stats.r_squared .","title":"r_squared()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.rar","text":"QSAdapter . rar ( * , rf = 0.0 ) See quantstats.stats.rar .","title":"rar()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.recovery_factor","text":"QSAdapter . recovery_factor ( * , prepare_returns = True ) See quantstats.stats.recovery_factor .","title":"recovery_factor()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.remove_outliers","text":"QSAdapter . remove_outliers ( * , quantile = 0.95 ) See quantstats.stats.remove_outliers .","title":"remove_outliers()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.returns_accessor","text":"Returns accessor.","title":"returns_accessor"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.risk_of_ruin","text":"QSAdapter . risk_of_ruin ( * , prepare_returns = True ) See quantstats.stats.risk_of_ruin .","title":"risk_of_ruin()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.risk_return_ratio","text":"QSAdapter . risk_return_ratio ( * , prepare_returns = True ) See quantstats.stats.risk_return_ratio .","title":"risk_return_ratio()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.rolling_greeks","text":"QSAdapter . rolling_greeks ( * , benchmark , periods = 252 , prepare_returns = True ) See quantstats.stats.rolling_greeks .","title":"rolling_greeks()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.rolling_sharpe","text":"QSAdapter . rolling_sharpe ( * , rf = 0.0 , rolling_period = 126 , annualize = True , periods_per_year = 252 , prepare_returns = True ) See quantstats.stats.rolling_sharpe .","title":"rolling_sharpe()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.rolling_sortino","text":"QSAdapter . rolling_sortino ( * , rf = 0 , rolling_period = 126 , annualize = True , periods_per_year = 252 , ** kwargs ) See quantstats.stats.rolling_sortino .","title":"rolling_sortino()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.rolling_volatility","text":"QSAdapter . rolling_volatility ( * , rolling_period = 126 , periods_per_year = 252 , prepare_returns = True ) See quantstats.stats.rolling_volatility .","title":"rolling_volatility()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.ror","text":"QSAdapter . ror () See quantstats.stats.ror .","title":"ror()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.serenity_index","text":"QSAdapter . serenity_index ( * , rf = 0 ) See quantstats.stats.serenity_index .","title":"serenity_index()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.sharpe","text":"QSAdapter . sharpe ( * , rf = 0.0 , periods = 252 , annualize = True , smart = False ) See quantstats.stats.sharpe .","title":"sharpe()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.skew","text":"QSAdapter . skew ( * , prepare_returns = True ) See quantstats.stats.skew .","title":"skew()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.smart_sharpe","text":"QSAdapter . smart_sharpe ( * , rf = 0.0 , periods = 252 , annualize = True ) See quantstats.stats.smart_sharpe .","title":"smart_sharpe()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.smart_sortino","text":"QSAdapter . smart_sortino ( * , rf = 0 , periods = 252 , annualize = True ) See quantstats.stats.smart_sortino .","title":"smart_sortino()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.sortino","text":"QSAdapter . sortino ( * , rf = 0 , periods = 252 , annualize = True , smart = False ) See quantstats.stats.sortino .","title":"sortino()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.tail_ratio","text":"QSAdapter . tail_ratio ( * , cutoff = 0.95 , prepare_returns = True ) See quantstats.stats.tail_ratio .","title":"tail_ratio()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.to_drawdown_series","text":"QSAdapter . to_drawdown_series () See quantstats.stats.to_drawdown_series .","title":"to_drawdown_series()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.to_excess_returns","text":"QSAdapter . to_excess_returns ( * , rf , nperiods = None ) See quantstats.utils.to_excess_returns .","title":"to_excess_returns()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.to_log_returns","text":"QSAdapter . to_log_returns ( * , rf = 0.0 , nperiods = None ) See quantstats.utils.to_log_returns .","title":"to_log_returns()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.to_prices","text":"QSAdapter . to_prices ( * , base = 100000.0 ) See quantstats.utils.to_prices .","title":"to_prices()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.ulcer_index","text":"QSAdapter . ulcer_index () See quantstats.stats.ulcer_index .","title":"ulcer_index()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.ulcer_performance_index","text":"QSAdapter . ulcer_performance_index ( * , rf = 0 ) See quantstats.stats.ulcer_performance_index .","title":"ulcer_performance_index()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.upi","text":"QSAdapter . upi ( * , rf = 0 ) See quantstats.stats.upi .","title":"upi()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.value_at_risk","text":"QSAdapter . value_at_risk ( * , sigma = 1 , confidence = 0.95 , prepare_returns = True ) See quantstats.stats.value_at_risk .","title":"value_at_risk()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.var","text":"QSAdapter . var ( * , sigma = 1 , confidence = 0.95 , prepare_returns = True ) See quantstats.stats.var .","title":"var()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.volatility","text":"QSAdapter . volatility ( * , periods = 252 , annualize = True , prepare_returns = True ) See quantstats.stats.volatility .","title":"volatility()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.win_loss_ratio","text":"QSAdapter . win_loss_ratio ( * , prepare_returns = True ) See quantstats.stats.win_loss_ratio .","title":"win_loss_ratio()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.win_rate","text":"QSAdapter . win_rate ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.win_rate .","title":"win_rate()"},{"location":"api/returns/qs_adapter/#vectorbt.returns.qs_adapter.QSAdapter.worst","text":"QSAdapter . worst ( * , aggregate = None , compounded = True , prepare_returns = True ) See quantstats.stats.worst .","title":"worst()"},{"location":"api/signals/","text":"signals package \u00b6 Modules for working with signals, such as entry and exit signals. Sub-modules \u00b6 vectorbt.signals.accessors vectorbt.signals.enums vectorbt.signals.factory vectorbt.signals.generators vectorbt.signals.nb","title":"signals"},{"location":"api/signals/#vectorbt.signals","text":"Modules for working with signals, such as entry and exit signals.","title":"vectorbt.signals"},{"location":"api/signals/#sub-modules","text":"vectorbt.signals.accessors vectorbt.signals.enums vectorbt.signals.factory vectorbt.signals.generators vectorbt.signals.nb","title":"Sub-modules"},{"location":"api/signals/accessors/","text":"accessors module \u00b6 Custom pandas accessors for signals data. Methods can be accessed as follows: SignalsSRAccessor -> pd.Series.vbt.signals.* SignalsDFAccessor -> pd.DataFrame.vbt.signals.* >>> import pandas as pd >>> import vectorbt as vbt >>> # vectorbt.signals.accessors.SignalsAccessor.pos_rank >>> pd . Series ([ False , True , True , True , False ]) . vbt . signals . pos_rank () 0 0 1 1 2 2 3 3 4 0 dtype: int64 The accessors extend vectorbt.generic.accessors . Note The underlying Series/DataFrame should already be a signal series. Input arrays should be np.bool_ . Grouping is only supported by the methods that accept the group_by argument. Accessors do not utilize caching. Run for the examples below >>> import vectorbt as vbt >>> import numpy as np >>> import pandas as pd >>> from numba import njit >>> from datetime import datetime >>> mask = pd . DataFrame ({ ... 'a' : [ True , False , False , False , False ], ... 'b' : [ True , False , True , False , True ], ... 'c' : [ True , True , True , False , False ] ... }, index = pd . Index ([ ... datetime ( 2020 , 1 , 1 ), ... datetime ( 2020 , 1 , 2 ), ... datetime ( 2020 , 1 , 3 ), ... datetime ( 2020 , 1 , 4 ), ... datetime ( 2020 , 1 , 5 ) ... ])) >>> mask a b c 2020-01-01 True True True 2020-01-02 False False True 2020-01-03 False True True 2020-01-04 False False False 2020-01-05 False True False Stats \u00b6 Hint See StatsBuilderMixin.stats() and SignalsAccessor.metrics . >>> mask . vbt . signals . stats ( column = 'a' ) Start 2020-01-01 00:00:00 End 2020-01-05 00:00:00 Period 5 days 00:00:00 Total 1 Rate [%] 20 First Index 2020-01-01 00:00:00 Last Index 2020-01-01 00:00:00 Norm Avg Index [-1, 1] -1 Distance: Min NaT Distance: Max NaT Distance: Mean NaT Distance: Std NaT Total Partitions 1 Partition Rate [%] 100 Partition Length: Min 1 days 00:00:00 Partition Length: Max 1 days 00:00:00 Partition Length: Mean 1 days 00:00:00 Partition Length: Std NaT Partition Distance: Min NaT Partition Distance: Max NaT Partition Distance: Mean NaT Partition Distance: Std NaT Name: a, dtype: object We can pass another signal array to compare this array with: >>> mask . vbt . signals . stats ( column = 'a' , settings = dict ( other = mask [ 'b' ])) Start 2020-01-01 00:00:00 End 2020-01-05 00:00:00 Period 5 days 00:00:00 Total 1 Rate [%] 20 Total Overlapping 1 Overlapping Rate [%] 33.3333 First Index 2020-01-01 00:00:00 Last Index 2020-01-01 00:00:00 Norm Avg Index [-1, 1] -1 Distance -> Other: Min 0 days 00:00:00 Distance -> Other: Max 0 days 00:00:00 Distance -> Other: Mean 0 days 00:00:00 Distance -> Other: Std NaT Total Partitions 1 Partition Rate [%] 100 Partition Length: Min 1 days 00:00:00 Partition Length: Max 1 days 00:00:00 Partition Length: Mean 1 days 00:00:00 Partition Length: Std NaT Partition Distance: Min NaT Partition Distance: Max NaT Partition Distance: Mean NaT Partition Distance: Std NaT Name: a, dtype: object We can also return duration as a floating number rather than a timedelta: >>> mask . vbt . signals . stats ( column = 'a' , settings = dict ( to_timedelta = False )) Start 2020-01-01 00:00:00 End 2020-01-05 00:00:00 Period 5 Total 1 Rate [%] 20 First Index 2020-01-01 00:00:00 Last Index 2020-01-01 00:00:00 Norm Avg Index [-1, 1] -1 Distance: Min NaN Distance: Max NaN Distance: Mean NaN Distance: Std NaN Total Partitions 1 Partition Rate [%] 100 Partition Length: Min 1 Partition Length: Max 1 Partition Length: Mean 1 Partition Length: Std NaN Partition Distance: Min NaN Partition Distance: Max NaN Partition Distance: Mean NaN Partition Distance: Std NaN Name: a, dtype: object StatsBuilderMixin.stats() also supports (re-)grouping: >>> mask . vbt . signals . stats ( column = 0 , group_by = [ 0 , 0 , 1 ]) Start 2020-01-01 00:00:00 End 2020-01-05 00:00:00 Period 5 days 00:00:00 Total 4 Rate [%] 40 First Index 2020-01-01 00:00:00 Last Index 2020-01-05 00:00:00 Norm Avg Index [-1, 1] -0.25 Distance: Min 2 days 00:00:00 Distance: Max 2 days 00:00:00 Distance: Mean 2 days 00:00:00 Distance: Std 0 days 00:00:00 Total Partitions 4 Partition Rate [%] 100 Partition Length: Min 1 days 00:00:00 Partition Length: Max 1 days 00:00:00 Partition Length: Mean 1 days 00:00:00 Partition Length: Std 0 days 00:00:00 Partition Distance: Min 2 days 00:00:00 Partition Distance: Max 2 days 00:00:00 Partition Distance: Mean 2 days 00:00:00 Partition Distance: Std 0 days 00:00:00 Name: 0, dtype: object Plots \u00b6 Hint See PlotsBuilderMixin.plots() and SignalsAccessor.subplots . This class inherits subplots from GenericAccessor . SignalsAccessor class \u00b6 Accessor on top of signal series. For both, Series and DataFrames. Accessible through pd.Series.vbt.signals and pd.DataFrame.vbt.signals . Superclasses AttrResolver BaseAccessor Configured Documented GenericAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.config GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.df_accessor_cls GenericAccessor.diff() GenericAccessor.drawdown() GenericAccessor.drawdowns GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.get_drawdowns() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.iloc GenericAccessor.indexing_kwargs GenericAccessor.lineplot() GenericAccessor.loc GenericAccessor.mapping GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.obj GenericAccessor.pct_change() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.ranges GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.resolve_self() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.self_aliases GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.sr_accessor_cls GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.wrapper GenericAccessor.writeable_attrs GenericAccessor.zscore() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses SignalsDFAccessor SignalsSRAccessor AND method \u00b6 SignalsAccessor . AND ( other , ** kwargs ) Combine with other using logical AND. See BaseAccessor.combine() . OR method \u00b6 SignalsAccessor . OR ( other , ** kwargs ) Combine with other using logical OR. See BaseAccessor.combine() . Usage Perform two OR operations and concatenate them: >>> ts = pd . Series ([ 1 , 2 , 3 , 2 , 1 ]) >>> mask . vbt . signals . OR ([ ts > 1 , ts > 2 ], concat = True , keys = [ '>1' , '>2' ]) >1 >2 a b c a b c 2020-01-01 True True True True True True 2020-01-02 True True True False False True 2020-01-03 True True True True True True 2020-01-04 True True True False False False 2020-01-05 False True False False True False XOR method \u00b6 SignalsAccessor . XOR ( other , ** kwargs ) Combine with other using logical XOR. See BaseAccessor.combine() . between_partition_ranges method \u00b6 SignalsAccessor . between_partition_ranges ( group_by = None , attach_ts = True , ** kwargs ) Wrap the result of between_partition_ranges_nb() with Ranges . Usage >>> mask_sr = pd . Series ([ True , False , False , True , False , True , True ]) >>> mask_sr . vbt . signals . between_partition_ranges () . records_readable Range Id Column Start Timestamp End Timestamp Status 0 0 0 0 3 Closed 1 1 0 3 5 Closed between_ranges method \u00b6 SignalsAccessor . between_ranges ( other = None , from_other = False , broadcast_kwargs = None , group_by = None , attach_ts = True , attach_other = False , ** kwargs ) Wrap the result of between_ranges_nb() with Ranges . If other specified, see between_two_ranges_nb() . Both will broadcast using broadcast() and broadcast_kwargs . Usage One array: >>> mask_sr = pd . Series ([ True , False , False , True , False , True , True ]) >>> ranges = mask_sr . vbt . signals . between_ranges () >>> ranges <vectorbt.generic.ranges.Ranges at 0x7ff29ea7c7b8> >>> ranges . records_readable Range Id Column Start Timestamp End Timestamp Status 0 0 0 0 3 Closed 1 1 0 3 5 Closed 2 2 0 5 6 Closed >>> ranges . duration . values array([3, 2, 1]) Two arrays, traversing the signals of the first array: >>> mask_sr = pd . Series ([ True , True , True , False , False ]) >>> mask_sr2 = pd . Series ([ False , False , True , False , True ]) >>> ranges = mask_sr . vbt . signals . between_ranges ( other = mask_sr2 ) >>> ranges <vectorbt.generic.ranges.Ranges at 0x7ff29e3b80f0> >>> ranges . records_readable Range Id Column Start Timestamp End Timestamp Status 0 0 0 0 2 Closed 1 1 0 1 2 Closed 2 2 0 2 2 Closed >>> ranges . duration . values array([2, 1, 0]) Two arrays, traversing the signals of the second array: >>> ranges = mask_sr . vbt . signals . between_ranges ( other = mask_sr2 , from_other = True ) >>> ranges <vectorbt.generic.ranges.Ranges at 0x7ff29eccbd68> >>> ranges . records_readable Range Id Column Start Timestamp End Timestamp Status 0 0 0 2 2 Closed 1 1 0 2 4 Closed >>> ranges . duration . values array([0, 2]) bshift method \u00b6 SignalsAccessor . bshift ( * args , fill_value = False , ** kwargs ) GenericAccessor.bshift() with fill_value=False . clean class method \u00b6 SignalsAccessor . clean ( * args , entry_first = True , broadcast_kwargs = None , wrap_kwargs = None ) Clean signals. If one array passed, see SignalsAccessor.first() . If two arrays passed, entries and exits, see clean_enex_nb() . empty class method \u00b6 SignalsAccessor . empty ( * args , fill_value = False , ** kwargs ) BaseAccessor.empty() with fill_value=False . empty_like class method \u00b6 SignalsAccessor . empty_like ( * args , fill_value = False , ** kwargs ) BaseAccessor.empty_like() with fill_value=False . first method \u00b6 SignalsAccessor . first ( wrap_kwargs = None , ** kwargs ) Select signals that satisfy the condition pos_rank == 0 . from_nth method \u00b6 SignalsAccessor . from_nth ( n , wrap_kwargs = None , ** kwargs ) Select signals that satisfy the condition pos_rank >= n . fshift method \u00b6 SignalsAccessor . fshift ( * args , fill_value = False , ** kwargs ) GenericAccessor.fshift() with fill_value=False . generate class method \u00b6 SignalsAccessor . generate ( shape , choice_func_nb , * args , pick_first = False , ** kwargs ) See generate_nb() . **kwargs will be passed to pandas constructor. Usage Generate random signals manually: >>> @njit ... def choice_func_nb ( from_i , to_i , col ): ... return col + from_i >>> pd . DataFrame . vbt . signals . generate (( 5 , 3 ), ... choice_func_nb , index = mask . index , columns = mask . columns ) a b c 2020-01-01 True False False 2020-01-02 False True False 2020-01-03 False False True 2020-01-04 False False False 2020-01-05 False False False generate_both class method \u00b6 SignalsAccessor . generate_both ( shape , entry_choice_func_nb = None , entry_args = None , exit_choice_func_nb = None , exit_args = None , entry_wait = 1 , exit_wait = 1 , entry_pick_first = True , exit_pick_first = True , ** kwargs ) See generate_enex_nb() . **kwargs will be passed to pandas constructor. Usage Generate entry and exit signals one after another. Each column increment the number of ticks to wait before placing the exit signal. >>> @njit ... def entry_choice_func_nb ( from_i , to_i , col , temp_idx_arr ): ... temp_idx_arr [ 0 ] = from_i ... return temp_idx_arr [: 1 ] # array with one signal >>> @njit ... def exit_choice_func_nb ( from_i , to_i , col , temp_idx_arr ): ... wait = col ... temp_idx_arr [ 0 ] = from_i + wait ... if temp_idx_arr [ 0 ] < to_i : ... return temp_idx_arr [: 1 ] # array with one signal ... return temp_idx_arr [: 0 ] # empty array >>> temp_idx_arr = np . empty (( 1 ,), dtype = np . int_ ) # reuse memory >>> en , ex = pd . DataFrame . vbt . signals . generate_both ( ... ( 5 , 3 ), ... entry_choice_func_nb , ( temp_idx_arr ,), ... exit_choice_func_nb , ( temp_idx_arr ,), ... index = mask . index , columns = mask . columns ) >>> en a b c 2020-01-01 True True True 2020-01-02 False False False 2020-01-03 True False False 2020-01-04 False True False 2020-01-05 True False True >>> ex a b c 2020-01-01 False False False 2020-01-02 True False False 2020-01-03 False True False 2020-01-04 True False True 2020-01-05 False False False generate_exits method \u00b6 SignalsAccessor . generate_exits ( exit_choice_func_nb , * args , wait = 1 , until_next = True , skip_until_exit = False , pick_first = False , wrap_kwargs = None ) See generate_ex_nb() . Usage Fill all space after signals in mask : >>> @njit ... def exit_choice_func_nb ( from_i , to_i , col , temp_range ): ... return temp_range [ from_i : to_i ] >>> temp_range = np . arange ( mask . shape [ 0 ]) # reuse memory >>> mask . vbt . signals . generate_exits ( exit_choice_func_nb , temp_range ) a b c 2020-01-01 False False False 2020-01-02 True True False 2020-01-03 True False False 2020-01-04 True True True 2020-01-05 True False True generate_ohlc_stop_exits method \u00b6 SignalsAccessor . generate_ohlc_stop_exits ( open , high = None , low = None , close = None , is_open_safe = True , out_dict = None , sl_stop = nan , sl_trail = False , tp_stop = nan , reverse = False , entry_wait = 1 , exit_wait = 1 , until_next = True , skip_until_exit = False , pick_first = True , chain = False , broadcast_kwargs = None , wrap_kwargs = None ) Generate exits based on when the price hits (trailing) stop loss or take profit. Hint This function is meant for signal analysis. For backtesting, consider using the stop logic integrated into Portfolio.from_signals() . If any of high , low or close is None, it will be set to open . Use out_dict as a dict to pass stop_price and stop_type arrays. You can also set out_dict to {} to produce these arrays automatically and still have access to them. For arguments, see ohlc_stop_choice_nb() . If chain is True, see generate_ohlc_stop_enex_nb() . Otherwise, see generate_ohlc_stop_ex_nb() . All array-like arguments including stops and out_dict will broadcast using broadcast() and broadcast_kwargs . For arguments, see ohlc_stop_choice_nb() . Note open isn't necessarily open price, but can be any entry price (even previous close). Stop price is calculated based solely on the entry price. Hint Default arguments will generate an exit signal strictly between two entry signals. If both entry signals are too close to each other, no exit will be generated. To ignore all entries that come between an entry and its exit, set until_next to False and skip_until_exit to True. To remove all entries that come between an entry and its exit, set chain to True. This will return two arrays: new entries and exits. Usage The same example as under generate_ohlc_stop_ex_nb() : >>> from vectorbt.signals.enums import StopType >>> price = pd . DataFrame ({ ... 'open' : [ 10 , 11 , 12 , 11 , 10 ], ... 'high' : [ 11 , 12 , 13 , 12 , 11 ], ... 'low' : [ 9 , 10 , 11 , 10 , 9 ], ... 'close' : [ 10 , 11 , 12 , 11 , 10 ] ... }) >>> out_dict = {} >>> exits = mask . vbt . signals . generate_ohlc_stop_exits ( ... price [ 'open' ], price [ 'high' ], price [ 'low' ], price [ 'close' ], ... sl_stop = 0.1 , sl_trail = True , tp_stop = 0.1 , out_dict = out_dict ) >>> exits a b c 2020-01-01 False False False 2020-01-02 True True False 2020-01-03 False False False 2020-01-04 False True True 2020-01-05 False False False >>> out_dict [ 'stop_price' ] a b c 2020-01-01 NaN NaN NaN 2020-01-02 11.0 11.0 NaN 2020-01-03 NaN NaN NaN 2020-01-04 NaN 10.8 10.8 2020-01-05 NaN NaN NaN >>> out_dict [ 'stop_type' ] . vbt ( mapping = StopType ) . apply_mapping () a b c 2020-01-01 None None None 2020-01-02 TakeProfit TakeProfit None 2020-01-03 None None None 2020-01-04 None TrailStop TrailStop 2020-01-05 None None None Notice how the first two entry signals in the third column have no exit signal - there is no room between them for an exit signal. To find an exit for the first entry and ignore all entries that are in-between them, we can pass until_next=False and skip_until_exit=True : >>> out_dict = {} >>> exits = mask . vbt . signals . generate_ohlc_stop_exits ( ... price [ 'open' ], price [ 'high' ], price [ 'low' ], price [ 'close' ], ... sl_stop = 0.1 , sl_trail = True , tp_stop = 0.1 , out_dict = out_dict , ... until_next = False , skip_until_exit = True ) >>> exits a b c 2020-01-01 False False False 2020-01-02 True True True 2020-01-03 False False False 2020-01-04 False True True 2020-01-05 False False False >>> out_dict [ 'stop_price' ] 2020-01-01 NaN NaN NaN 2020-01-02 11.0 11.0 11.0 2020-01-03 NaN NaN NaN 2020-01-04 NaN 10.8 10.8 2020-01-05 NaN NaN NaN >>> out_dict [ 'stop_type' ] . vbt ( mapping = StopType ) . apply_mapping () a b c 2020-01-01 None None None 2020-01-02 TakeProfit TakeProfit TakeProfit 2020-01-03 None None None 2020-01-04 None TrailStop TrailStop 2020-01-05 None None None Now, the first signal in the third column gets executed regardless of the entries that come next, which is very similar to the logic that is implemented in Portfolio.from_signals() . To automatically remove all ignored entry signals, pass chain=True . This will return a new entries array: >>> out_dict = {} >>> new_entries , exits = mask . vbt . signals . generate_ohlc_stop_exits ( ... price [ 'open' ], price [ 'high' ], price [ 'low' ], price [ 'close' ], ... sl_stop = 0.1 , sl_trail = True , tp_stop = 0.1 , out_dict = out_dict , ... chain = True ) >>> new_entries a b c 2020-01-01 True True True 2020-01-02 False False False << removed entry in the third column 2020-01-03 False True True 2020-01-04 False False False 2020-01-05 False True False >>> exits a b c 2020-01-01 False False False 2020-01-02 True True True 2020-01-03 False False False 2020-01-04 False True True 2020-01-05 False False False Warning The last two examples above make entries dependent upon exits - this makes only sense if you have no other exit arrays to combine this stop exit array with. generate_random class method \u00b6 SignalsAccessor . generate_random ( shape , n = None , prob = None , pick_first = False , seed = None , ** kwargs ) Generate signals randomly. If n is set, see generate_rand_nb() . If prob is set, see generate_rand_by_prob_nb() . n should be either a scalar or an array that will broadcast to the number of columns. prob should be either a single number or an array that will broadcast to match shape . **kwargs will be passed to pandas constructor. Usage For each column, generate a variable number of signals: >>> pd . DataFrame . vbt . signals . generate_random (( 5 , 3 ), n = [ 0 , 1 , 2 ], ... seed = 42 , index = mask . index , columns = mask . columns ) a b c 2020-01-01 False False True 2020-01-02 False False True 2020-01-03 False False False 2020-01-04 False True False 2020-01-05 False False False For each column and time step, pick a signal with 50% probability: >>> pd . DataFrame . vbt . signals . generate_random (( 5 , 3 ), prob = 0.5 , ... seed = 42 , index = mask . index , columns = mask . columns ) a b c 2020-01-01 True True True 2020-01-02 False True False 2020-01-03 False False False 2020-01-04 False False True 2020-01-05 True False True generate_random_both class method \u00b6 SignalsAccessor . generate_random_both ( shape , n = None , entry_prob = None , exit_prob = None , seed = None , entry_wait = 1 , exit_wait = 1 , entry_pick_first = True , exit_pick_first = True , ** kwargs ) Generate chain of entry and exit signals randomly. If n is set, see generate_rand_enex_nb() . If entry_prob and exit_prob are set, see generate_rand_enex_by_prob_nb() . For arguments, see SignalsAccessor.generate_random() . Usage For each column, generate two entries and exits randomly: >>> en , ex = pd . DataFrame . vbt . signals . generate_random_both ( ... ( 5 , 3 ), n = 2 , seed = 42 , index = mask . index , columns = mask . columns ) >>> en a b c 2020-01-01 True True True 2020-01-02 False False False 2020-01-03 True True False 2020-01-04 False False True 2020-01-05 False False False >>> ex a b c 2020-01-01 False False False 2020-01-02 True True True 2020-01-03 False False False 2020-01-04 False True False 2020-01-05 True False True For each column and time step, pick entry with 50% probability and exit right after: >>> en , ex = pd . DataFrame . vbt . signals . generate_random_both ( ... ( 5 , 3 ), entry_prob = 0.5 , exit_prob = 1. , ... seed = 42 , index = mask . index , columns = mask . columns ) >>> en a b c 2020-01-01 True True True 2020-01-02 False False False 2020-01-03 False False False 2020-01-04 False False True 2020-01-05 True False False >>> ex a b c 2020-01-01 False False False 2020-01-02 True True False 2020-01-03 False False True 2020-01-04 False True False 2020-01-05 True False True generate_random_exits method \u00b6 SignalsAccessor . generate_random_exits ( prob = None , seed = None , wait = 1 , until_next = True , skip_until_exit = False , wrap_kwargs = None ) Generate exit signals randomly. If prob is None, see generate_rand_ex_nb() . Otherwise, see generate_rand_ex_by_prob_nb() . Usage After each entry in mask , generate exactly one exit: >>> mask . vbt . signals . generate_random_exits ( seed = 42 ) a b c 2020-01-01 False False False 2020-01-02 False True False 2020-01-03 True False False 2020-01-04 False True False 2020-01-05 False False True After each entry in mask and at each time step, generate exit with 50% probability: >>> mask . vbt . signals . generate_random_exits ( prob = 0.5 , seed = 42 ) a b c 2020-01-01 False False False 2020-01-02 True False False 2020-01-03 False False False 2020-01-04 False False False 2020-01-05 False False True generate_stop_exits method \u00b6 SignalsAccessor . generate_stop_exits ( ts , stop , trailing = False , entry_wait = 1 , exit_wait = 1 , until_next = True , skip_until_exit = False , pick_first = True , chain = False , broadcast_kwargs = None , wrap_kwargs = None ) Generate exits based on when ts hits the stop. For arguments, see stop_choice_nb() . If chain is True, see generate_stop_enex_nb() . Otherwise, see generate_stop_ex_nb() . Arguments entries , ts and stop will broadcast using broadcast() and broadcast_kwargs . For arguments, see stop_choice_nb() . Hint Default arguments will generate an exit signal strictly between two entry signals. If both entry signals are too close to each other, no exit will be generated. To ignore all entries that come between an entry and its exit, set until_next to False and skip_until_exit to True. To remove all entries that come between an entry and its exit, set chain to True. This will return two arrays: new entries and exits. Usage >>> ts = pd . Series ([ 1 , 2 , 3 , 2 , 1 ]) >>> # stop loss >>> mask . vbt . signals . generate_stop_exits ( ts , - 0.1 ) a b c 2020-01-01 False False False 2020-01-02 False False False 2020-01-03 False False False 2020-01-04 False True True 2020-01-05 False False False >>> # trailing stop loss >>> mask . vbt . signals . generate_stop_exits ( ts , - 0.1 , trailing = True ) a b c 2020-01-01 False False False 2020-01-02 False False False 2020-01-03 False False False 2020-01-04 True True True 2020-01-05 False False False index_mapped method \u00b6 SignalsAccessor . index_mapped ( group_by = None , ** kwargs ) Get a mapped array of indices. See GenericAccessor.to_mapped() . Only True values will be considered. metrics class variable \u00b6 Metrics supported by SignalsAccessor . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b19d8>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1a60>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1ae8>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"total\" : { \"title\" : \"Total\" , \"calc_func\" : \"total\" , \"tags\" : \"signals\" }, \"rate\" : { \"title\" : \"Rate [%]\" , \"calc_func\" : \"rate\" , \"post_calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1b70>\" , \"tags\" : \"signals\" }, \"total_overlapping\" : { \"title\" : \"Total Overlapping\" , \"calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1bf8>\" , \"check_silent_has_other\" : true , \"tags\" : [ \"signals\" , \"other\" ] }, \"overlapping_rate\" : { \"title\" : \"Overlapping Rate [%]\" , \"calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1c80>\" , \"post_calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1d08>\" , \"check_silent_has_other\" : true , \"tags\" : [ \"signals\" , \"other\" ] }, \"first_index\" : { \"title\" : \"First Index\" , \"calc_func\" : \"nth_index\" , \"n\" : 0 , \"return_labels\" : true , \"tags\" : [ \"signals\" , \"index\" ] }, \"last_index\" : { \"title\" : \"Last Index\" , \"calc_func\" : \"nth_index\" , \"n\" : -1 , \"return_labels\" : true , \"tags\" : [ \"signals\" , \"index\" ] }, \"norm_avg_index\" : { \"title\" : \"Norm Avg Index [-1, 1]\" , \"calc_func\" : \"norm_avg_index\" , \"tags\" : [ \"signals\" , \"index\" ] }, \"distance\" : { \"title\" : \"RepEval(expression=\\\"f'Distance {\\\"<-\\\" if from_other else \\\"->\\\"} {other_name}' if other is not None else 'Distance'\\\", mapping={})\" , \"calc_func\" : \"between_ranges.duration\" , \"post_calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1d90>\" , \"apply_to_timedelta\" : true , \"tags\" : \"RepEval(expression=\\\"['signals', 'distance', 'other'] if other is not None else ['signals', 'distance']\\\", mapping={})\" }, \"total_partitions\" : { \"title\" : \"Total Partitions\" , \"calc_func\" : \"total_partitions\" , \"tags\" : [ \"signals\" , \"partitions\" ] }, \"partition_rate\" : { \"title\" : \"Partition Rate [%]\" , \"calc_func\" : \"partition_rate\" , \"post_calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1e18>\" , \"tags\" : [ \"signals\" , \"partitions\" ] }, \"partition_len\" : { \"title\" : \"Partition Length\" , \"calc_func\" : \"partition_ranges.duration\" , \"post_calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1ea0>\" , \"apply_to_timedelta\" : true , \"tags\" : [ \"signals\" , \"partitions\" , \"distance\" ] }, \"partition_distance\" : { \"title\" : \"Partition Distance\" , \"calc_func\" : \"between_partition_ranges.duration\" , \"post_calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1f28>\" , \"apply_to_timedelta\" : true , \"tags\" : [ \"signals\" , \"partitions\" , \"distance\" ] } } ) Returns SignalsAccessor._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable SignalsAccessor._metrics . norm_avg_index method \u00b6 SignalsAccessor . norm_avg_index ( group_by = None , wrap_kwargs = None ) See norm_avg_index_nb() . Normalized average index measures the average signal location relative to the middle of the column. This way, we can quickly see where the majority of signals are located. Common values are: -1.0: only the first signal is set 1.0: only the last signal is set 0.0: symmetric distribution around the middle [-1.0, 0.0): average signal is on the left (0.0, 1.0]: average signal is on the right Usage >>> pd . Series ([ True , False , False , False ]) . vbt . signals . norm_avg_index () -1.0 >>> pd . Series ([ False , False , False , True ]) . vbt . signals . norm_avg_index () 1.0 >>> pd . Series ([ True , False , False , True ]) . vbt . signals . norm_avg_index () 0.0 nth method \u00b6 SignalsAccessor . nth ( n , wrap_kwargs = None , ** kwargs ) Select signals that satisfy the condition pos_rank == n . nth_index method \u00b6 SignalsAccessor . nth_index ( n , return_labels = True , group_by = None , wrap_kwargs = None ) See nth_index_nb() . Usage >>> mask . vbt . signals . nth_index ( 0 ) a 2020-01-01 b 2020-01-01 c 2020-01-01 Name: nth_index, dtype: datetime64[ns] >>> mask . vbt . signals . nth_index ( 2 ) a NaT b 2020-01-05 c 2020-01-03 Name: nth_index, dtype: datetime64[ns] >>> mask . vbt . signals . nth_index ( - 1 ) a 2020-01-01 b 2020-01-05 c 2020-01-03 Name: nth_index, dtype: datetime64[ns] >>> mask . vbt . signals . nth_index ( - 1 , group_by = True ) Timestamp('2020-01-05 00:00:00') partition_pos_rank method \u00b6 SignalsAccessor . partition_pos_rank ( ** kwargs ) Get partition position ranks. Uses SignalsAccessor.rank() with part_pos_rank_nb() . Usage Rank each partition of True values in mask : >>> mask . vbt . signals . partition_pos_rank () a b c 2020-01-01 0 0 0 2020-01-02 -1 -1 0 2020-01-03 -1 1 0 2020-01-04 -1 -1 -1 2020-01-05 -1 2 -1 >>> mask . vbt . signals . partition_pos_rank ( after_false = True ) a b c 2020-01-01 -1 -1 -1 2020-01-02 -1 -1 -1 2020-01-03 -1 0 -1 2020-01-04 -1 -1 -1 2020-01-05 -1 1 -1 >>> mask . vbt . signals . partition_pos_rank ( reset_by = mask ) a b c 2020-01-01 0 0 0 2020-01-02 -1 -1 0 2020-01-03 -1 0 0 2020-01-04 -1 -1 -1 2020-01-05 -1 0 -1 partition_pos_rank_mapped method \u00b6 SignalsAccessor . partition_pos_rank_mapped ( group_by = None , ** kwargs ) Get a mapped array of partition position ranks. See SignalsAccessor.partition_pos_rank() . partition_ranges method \u00b6 SignalsAccessor . partition_ranges ( group_by = None , attach_ts = True , ** kwargs ) Wrap the result of partition_ranges_nb() with Ranges . If use_end_idxs is True, uses the index of the last signal in each partition as idx_arr . Otherwise, uses the index of the first signal. Usage >>> mask_sr = pd . Series ([ True , True , True , False , True , True ]) >>> mask_sr . vbt . signals . partition_ranges () . records_readable Range Id Column Start Timestamp End Timestamp Status 0 0 0 0 3 Closed 1 1 0 4 5 Open partition_rate method \u00b6 SignalsAccessor . partition_rate ( wrap_kwargs = None , group_by = None , ** kwargs ) SignalsAccessor.total_partitions() divided by SignalsAccessor.total() in each column/group. plot method \u00b6 SignalsAccessor . plot ( yref = 'y' , ** kwargs ) Plot signals. Args yref :\u2002 str Y coordinate axis. **kwargs Keyword arguments passed to GenericAccessor.lineplot() . Usage >>> mask [[ 'a' , 'c' ]] . vbt . signals . plot () plots_defaults property \u00b6 Defaults for PlotsBuilderMixin.plots() . Merges GenericAccessor.plots_defaults and signals.plots from settings . pos_rank method \u00b6 SignalsAccessor . pos_rank ( allow_gaps = False , ** kwargs ) Get signal position ranks. Uses SignalsAccessor.rank() with sig_pos_rank_nb() . Usage Rank each True value in each partition in mask : >>> mask . vbt . signals . pos_rank () a b c 2020-01-01 0 0 0 2020-01-02 -1 -1 1 2020-01-03 -1 0 2 2020-01-04 -1 -1 -1 2020-01-05 -1 0 -1 >>> mask . vbt . signals . pos_rank ( after_false = True ) a b c 2020-01-01 -1 -1 -1 2020-01-02 -1 -1 -1 2020-01-03 -1 0 -1 2020-01-04 -1 -1 -1 2020-01-05 -1 0 -1 >>> mask . vbt . signals . pos_rank ( allow_gaps = True ) a b c 2020-01-01 0 0 0 2020-01-02 -1 -1 1 2020-01-03 -1 1 2 2020-01-04 -1 -1 -1 2020-01-05 -1 2 -1 >>> mask . vbt . signals . pos_rank ( reset_by =~ mask , allow_gaps = True ) a b c 2020-01-01 0 0 0 2020-01-02 -1 -1 1 2020-01-03 -1 0 2 2020-01-04 -1 -1 -1 2020-01-05 -1 0 -1 pos_rank_mapped method \u00b6 SignalsAccessor . pos_rank_mapped ( group_by = None , ** kwargs ) Get a mapped array of signal position ranks. See SignalsAccessor.pos_rank() . rank method \u00b6 SignalsAccessor . rank ( rank_func_nb , * args , prepare_func = None , reset_by = None , after_false = False , broadcast_kwargs = None , wrap_kwargs = None , as_mapped = False , ** kwargs ) See rank_nb() . Will broadcast with reset_by using broadcast() and broadcast_kwargs . Use prepare_func to prepare further arguments to be passed before *args , such as temporary arrays. It should take both broadcasted arrays ( reset_by can be None) and return a tuple. Set as_mapped to True to return an instance of MappedArray . rate method \u00b6 SignalsAccessor . rate ( wrap_kwargs = None , group_by = None , ** kwargs ) SignalsAccessor.total() divided by the total index length in each column/group. stats_defaults property \u00b6 Defaults for StatsBuilderMixin.stats() . Merges GenericAccessor.stats_defaults and signals.stats from settings . subplots class variable \u00b6 Subplots supported by SignalsAccessor . Co nf ig( { \"plot\" : { \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"pass_trace_names\" : false , \"tags\" : \"generic\" } } ) Returns SignalsAccessor._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable SignalsAccessor._subplots . total method \u00b6 SignalsAccessor . total ( wrap_kwargs = None , group_by = None ) Total number of True values in each column/group. total_partitions method \u00b6 SignalsAccessor . total_partitions ( wrap_kwargs = None , group_by = None , ** kwargs ) Total number of partitions of True values in each column/group. SignalsDFAccessor class \u00b6 Accessor on top of signal series. For DataFrames only. Accessible through pd.DataFrame.vbt.signals . Superclasses AttrResolver BaseAccessor BaseDFAccessor Configured Documented GenericAccessor GenericDFAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin SignalsAccessor StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.diff() GenericAccessor.drawdown() GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.get_drawdowns() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.lineplot() GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.pct_change() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.resolve_self() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.zscore() GenericDFAccessor.flatten_grouped() GenericDFAccessor.heatmap() GenericDFAccessor.squeeze_grouped() GenericDFAccessor.ts_heatmap() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() SignalsAccessor.AND() SignalsAccessor.OR() SignalsAccessor.XOR() SignalsAccessor.between_partition_ranges() SignalsAccessor.between_ranges() SignalsAccessor.bshift() SignalsAccessor.clean() SignalsAccessor.config SignalsAccessor.df_accessor_cls SignalsAccessor.drawdowns SignalsAccessor.empty() SignalsAccessor.empty_like() SignalsAccessor.first() SignalsAccessor.from_nth() SignalsAccessor.fshift() SignalsAccessor.generate() SignalsAccessor.generate_both() SignalsAccessor.generate_exits() SignalsAccessor.generate_ohlc_stop_exits() SignalsAccessor.generate_random() SignalsAccessor.generate_random_both() SignalsAccessor.generate_random_exits() SignalsAccessor.generate_stop_exits() SignalsAccessor.iloc SignalsAccessor.index_mapped() SignalsAccessor.indexing_kwargs SignalsAccessor.loc SignalsAccessor.mapping SignalsAccessor.norm_avg_index() SignalsAccessor.nth() SignalsAccessor.nth_index() SignalsAccessor.obj SignalsAccessor.partition_pos_rank() SignalsAccessor.partition_pos_rank_mapped() SignalsAccessor.partition_ranges() SignalsAccessor.partition_rate() SignalsAccessor.plot() SignalsAccessor.plots_defaults SignalsAccessor.pos_rank() SignalsAccessor.pos_rank_mapped() SignalsAccessor.ranges SignalsAccessor.rank() SignalsAccessor.rate() SignalsAccessor.self_aliases SignalsAccessor.sr_accessor_cls SignalsAccessor.stats_defaults SignalsAccessor.total() SignalsAccessor.total_partitions() SignalsAccessor.wrapper SignalsAccessor.writeable_attrs StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj() SignalsSRAccessor class \u00b6 Accessor on top of signal series. For Series only. Accessible through pd.Series.vbt.signals . Superclasses AttrResolver BaseAccessor BaseSRAccessor Configured Documented GenericAccessor GenericSRAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin SignalsAccessor StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.diff() GenericAccessor.drawdown() GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.get_drawdowns() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.lineplot() GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.pct_change() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.resolve_self() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.zscore() GenericSRAccessor.flatten_grouped() GenericSRAccessor.heatmap() GenericSRAccessor.overlay_with_heatmap() GenericSRAccessor.plot_against() GenericSRAccessor.qqplot() GenericSRAccessor.squeeze_grouped() GenericSRAccessor.ts_heatmap() GenericSRAccessor.volume() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() SignalsAccessor.AND() SignalsAccessor.OR() SignalsAccessor.XOR() SignalsAccessor.between_partition_ranges() SignalsAccessor.between_ranges() SignalsAccessor.bshift() SignalsAccessor.clean() SignalsAccessor.config SignalsAccessor.df_accessor_cls SignalsAccessor.drawdowns SignalsAccessor.empty() SignalsAccessor.empty_like() SignalsAccessor.first() SignalsAccessor.from_nth() SignalsAccessor.fshift() SignalsAccessor.generate() SignalsAccessor.generate_both() SignalsAccessor.generate_exits() SignalsAccessor.generate_ohlc_stop_exits() SignalsAccessor.generate_random() SignalsAccessor.generate_random_both() SignalsAccessor.generate_random_exits() SignalsAccessor.generate_stop_exits() SignalsAccessor.iloc SignalsAccessor.index_mapped() SignalsAccessor.indexing_kwargs SignalsAccessor.loc SignalsAccessor.mapping SignalsAccessor.norm_avg_index() SignalsAccessor.nth() SignalsAccessor.nth_index() SignalsAccessor.obj SignalsAccessor.partition_pos_rank() SignalsAccessor.partition_pos_rank_mapped() SignalsAccessor.partition_ranges() SignalsAccessor.partition_rate() SignalsAccessor.plot() SignalsAccessor.plots_defaults SignalsAccessor.pos_rank() SignalsAccessor.pos_rank_mapped() SignalsAccessor.ranges SignalsAccessor.rank() SignalsAccessor.rate() SignalsAccessor.self_aliases SignalsAccessor.sr_accessor_cls SignalsAccessor.stats_defaults SignalsAccessor.total() SignalsAccessor.total_partitions() SignalsAccessor.wrapper SignalsAccessor.writeable_attrs StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj() plot_as_entry_markers method \u00b6 SignalsSRAccessor . plot_as_entry_markers ( y = None , ** kwargs ) Plot signals as entry markers. See SignalsSRAccessor.plot_as_markers() . plot_as_exit_markers method \u00b6 SignalsSRAccessor . plot_as_exit_markers ( y = None , ** kwargs ) Plot signals as exit markers. See SignalsSRAccessor.plot_as_markers() . plot_as_markers method \u00b6 SignalsSRAccessor . plot_as_markers ( y = None , ** kwargs ) Plot Series as markers. Args y :\u2002 array_like Y-axis values to plot markers on. **kwargs Keyword arguments passed to GenericAccessor.scatterplot() . Usage >>> ts = pd . Series ([ 1 , 2 , 3 , 2 , 1 ], index = mask . index ) >>> fig = ts . vbt . lineplot () >>> mask [ 'b' ] . vbt . signals . plot_as_entry_markers ( y = ts , fig = fig ) >>> ( ~ mask [ 'b' ]) . vbt . signals . plot_as_exit_markers ( y = ts , fig = fig )","title":"accessors"},{"location":"api/signals/accessors/#vectorbt.signals.accessors","text":"Custom pandas accessors for signals data. Methods can be accessed as follows: SignalsSRAccessor -> pd.Series.vbt.signals.* SignalsDFAccessor -> pd.DataFrame.vbt.signals.* >>> import pandas as pd >>> import vectorbt as vbt >>> # vectorbt.signals.accessors.SignalsAccessor.pos_rank >>> pd . Series ([ False , True , True , True , False ]) . vbt . signals . pos_rank () 0 0 1 1 2 2 3 3 4 0 dtype: int64 The accessors extend vectorbt.generic.accessors . Note The underlying Series/DataFrame should already be a signal series. Input arrays should be np.bool_ . Grouping is only supported by the methods that accept the group_by argument. Accessors do not utilize caching. Run for the examples below >>> import vectorbt as vbt >>> import numpy as np >>> import pandas as pd >>> from numba import njit >>> from datetime import datetime >>> mask = pd . DataFrame ({ ... 'a' : [ True , False , False , False , False ], ... 'b' : [ True , False , True , False , True ], ... 'c' : [ True , True , True , False , False ] ... }, index = pd . Index ([ ... datetime ( 2020 , 1 , 1 ), ... datetime ( 2020 , 1 , 2 ), ... datetime ( 2020 , 1 , 3 ), ... datetime ( 2020 , 1 , 4 ), ... datetime ( 2020 , 1 , 5 ) ... ])) >>> mask a b c 2020-01-01 True True True 2020-01-02 False False True 2020-01-03 False True True 2020-01-04 False False False 2020-01-05 False True False","title":"vectorbt.signals.accessors"},{"location":"api/signals/accessors/#stats","text":"Hint See StatsBuilderMixin.stats() and SignalsAccessor.metrics . >>> mask . vbt . signals . stats ( column = 'a' ) Start 2020-01-01 00:00:00 End 2020-01-05 00:00:00 Period 5 days 00:00:00 Total 1 Rate [%] 20 First Index 2020-01-01 00:00:00 Last Index 2020-01-01 00:00:00 Norm Avg Index [-1, 1] -1 Distance: Min NaT Distance: Max NaT Distance: Mean NaT Distance: Std NaT Total Partitions 1 Partition Rate [%] 100 Partition Length: Min 1 days 00:00:00 Partition Length: Max 1 days 00:00:00 Partition Length: Mean 1 days 00:00:00 Partition Length: Std NaT Partition Distance: Min NaT Partition Distance: Max NaT Partition Distance: Mean NaT Partition Distance: Std NaT Name: a, dtype: object We can pass another signal array to compare this array with: >>> mask . vbt . signals . stats ( column = 'a' , settings = dict ( other = mask [ 'b' ])) Start 2020-01-01 00:00:00 End 2020-01-05 00:00:00 Period 5 days 00:00:00 Total 1 Rate [%] 20 Total Overlapping 1 Overlapping Rate [%] 33.3333 First Index 2020-01-01 00:00:00 Last Index 2020-01-01 00:00:00 Norm Avg Index [-1, 1] -1 Distance -> Other: Min 0 days 00:00:00 Distance -> Other: Max 0 days 00:00:00 Distance -> Other: Mean 0 days 00:00:00 Distance -> Other: Std NaT Total Partitions 1 Partition Rate [%] 100 Partition Length: Min 1 days 00:00:00 Partition Length: Max 1 days 00:00:00 Partition Length: Mean 1 days 00:00:00 Partition Length: Std NaT Partition Distance: Min NaT Partition Distance: Max NaT Partition Distance: Mean NaT Partition Distance: Std NaT Name: a, dtype: object We can also return duration as a floating number rather than a timedelta: >>> mask . vbt . signals . stats ( column = 'a' , settings = dict ( to_timedelta = False )) Start 2020-01-01 00:00:00 End 2020-01-05 00:00:00 Period 5 Total 1 Rate [%] 20 First Index 2020-01-01 00:00:00 Last Index 2020-01-01 00:00:00 Norm Avg Index [-1, 1] -1 Distance: Min NaN Distance: Max NaN Distance: Mean NaN Distance: Std NaN Total Partitions 1 Partition Rate [%] 100 Partition Length: Min 1 Partition Length: Max 1 Partition Length: Mean 1 Partition Length: Std NaN Partition Distance: Min NaN Partition Distance: Max NaN Partition Distance: Mean NaN Partition Distance: Std NaN Name: a, dtype: object StatsBuilderMixin.stats() also supports (re-)grouping: >>> mask . vbt . signals . stats ( column = 0 , group_by = [ 0 , 0 , 1 ]) Start 2020-01-01 00:00:00 End 2020-01-05 00:00:00 Period 5 days 00:00:00 Total 4 Rate [%] 40 First Index 2020-01-01 00:00:00 Last Index 2020-01-05 00:00:00 Norm Avg Index [-1, 1] -0.25 Distance: Min 2 days 00:00:00 Distance: Max 2 days 00:00:00 Distance: Mean 2 days 00:00:00 Distance: Std 0 days 00:00:00 Total Partitions 4 Partition Rate [%] 100 Partition Length: Min 1 days 00:00:00 Partition Length: Max 1 days 00:00:00 Partition Length: Mean 1 days 00:00:00 Partition Length: Std 0 days 00:00:00 Partition Distance: Min 2 days 00:00:00 Partition Distance: Max 2 days 00:00:00 Partition Distance: Mean 2 days 00:00:00 Partition Distance: Std 0 days 00:00:00 Name: 0, dtype: object","title":"Stats"},{"location":"api/signals/accessors/#plots","text":"Hint See PlotsBuilderMixin.plots() and SignalsAccessor.subplots . This class inherits subplots from GenericAccessor .","title":"Plots"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor","text":"Accessor on top of signal series. For both, Series and DataFrames. Accessible through pd.Series.vbt.signals and pd.DataFrame.vbt.signals . Superclasses AttrResolver BaseAccessor Configured Documented GenericAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.config GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.df_accessor_cls GenericAccessor.diff() GenericAccessor.drawdown() GenericAccessor.drawdowns GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.get_drawdowns() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.iloc GenericAccessor.indexing_kwargs GenericAccessor.lineplot() GenericAccessor.loc GenericAccessor.mapping GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.obj GenericAccessor.pct_change() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.ranges GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.resolve_self() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.self_aliases GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.sr_accessor_cls GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.wrapper GenericAccessor.writeable_attrs GenericAccessor.zscore() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses SignalsDFAccessor SignalsSRAccessor","title":"SignalsAccessor"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.AND","text":"SignalsAccessor . AND ( other , ** kwargs ) Combine with other using logical AND. See BaseAccessor.combine() .","title":"AND()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.OR","text":"SignalsAccessor . OR ( other , ** kwargs ) Combine with other using logical OR. See BaseAccessor.combine() . Usage Perform two OR operations and concatenate them: >>> ts = pd . Series ([ 1 , 2 , 3 , 2 , 1 ]) >>> mask . vbt . signals . OR ([ ts > 1 , ts > 2 ], concat = True , keys = [ '>1' , '>2' ]) >1 >2 a b c a b c 2020-01-01 True True True True True True 2020-01-02 True True True False False True 2020-01-03 True True True True True True 2020-01-04 True True True False False False 2020-01-05 False True False False True False","title":"OR()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.XOR","text":"SignalsAccessor . XOR ( other , ** kwargs ) Combine with other using logical XOR. See BaseAccessor.combine() .","title":"XOR()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.between_partition_ranges","text":"SignalsAccessor . between_partition_ranges ( group_by = None , attach_ts = True , ** kwargs ) Wrap the result of between_partition_ranges_nb() with Ranges . Usage >>> mask_sr = pd . Series ([ True , False , False , True , False , True , True ]) >>> mask_sr . vbt . signals . between_partition_ranges () . records_readable Range Id Column Start Timestamp End Timestamp Status 0 0 0 0 3 Closed 1 1 0 3 5 Closed","title":"between_partition_ranges()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.between_ranges","text":"SignalsAccessor . between_ranges ( other = None , from_other = False , broadcast_kwargs = None , group_by = None , attach_ts = True , attach_other = False , ** kwargs ) Wrap the result of between_ranges_nb() with Ranges . If other specified, see between_two_ranges_nb() . Both will broadcast using broadcast() and broadcast_kwargs . Usage One array: >>> mask_sr = pd . Series ([ True , False , False , True , False , True , True ]) >>> ranges = mask_sr . vbt . signals . between_ranges () >>> ranges <vectorbt.generic.ranges.Ranges at 0x7ff29ea7c7b8> >>> ranges . records_readable Range Id Column Start Timestamp End Timestamp Status 0 0 0 0 3 Closed 1 1 0 3 5 Closed 2 2 0 5 6 Closed >>> ranges . duration . values array([3, 2, 1]) Two arrays, traversing the signals of the first array: >>> mask_sr = pd . Series ([ True , True , True , False , False ]) >>> mask_sr2 = pd . Series ([ False , False , True , False , True ]) >>> ranges = mask_sr . vbt . signals . between_ranges ( other = mask_sr2 ) >>> ranges <vectorbt.generic.ranges.Ranges at 0x7ff29e3b80f0> >>> ranges . records_readable Range Id Column Start Timestamp End Timestamp Status 0 0 0 0 2 Closed 1 1 0 1 2 Closed 2 2 0 2 2 Closed >>> ranges . duration . values array([2, 1, 0]) Two arrays, traversing the signals of the second array: >>> ranges = mask_sr . vbt . signals . between_ranges ( other = mask_sr2 , from_other = True ) >>> ranges <vectorbt.generic.ranges.Ranges at 0x7ff29eccbd68> >>> ranges . records_readable Range Id Column Start Timestamp End Timestamp Status 0 0 0 2 2 Closed 1 1 0 2 4 Closed >>> ranges . duration . values array([0, 2])","title":"between_ranges()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.bshift","text":"SignalsAccessor . bshift ( * args , fill_value = False , ** kwargs ) GenericAccessor.bshift() with fill_value=False .","title":"bshift()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.clean","text":"SignalsAccessor . clean ( * args , entry_first = True , broadcast_kwargs = None , wrap_kwargs = None ) Clean signals. If one array passed, see SignalsAccessor.first() . If two arrays passed, entries and exits, see clean_enex_nb() .","title":"clean()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.empty","text":"SignalsAccessor . empty ( * args , fill_value = False , ** kwargs ) BaseAccessor.empty() with fill_value=False .","title":"empty()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.empty_like","text":"SignalsAccessor . empty_like ( * args , fill_value = False , ** kwargs ) BaseAccessor.empty_like() with fill_value=False .","title":"empty_like()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.first","text":"SignalsAccessor . first ( wrap_kwargs = None , ** kwargs ) Select signals that satisfy the condition pos_rank == 0 .","title":"first()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.from_nth","text":"SignalsAccessor . from_nth ( n , wrap_kwargs = None , ** kwargs ) Select signals that satisfy the condition pos_rank >= n .","title":"from_nth()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.fshift","text":"SignalsAccessor . fshift ( * args , fill_value = False , ** kwargs ) GenericAccessor.fshift() with fill_value=False .","title":"fshift()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.generate","text":"SignalsAccessor . generate ( shape , choice_func_nb , * args , pick_first = False , ** kwargs ) See generate_nb() . **kwargs will be passed to pandas constructor. Usage Generate random signals manually: >>> @njit ... def choice_func_nb ( from_i , to_i , col ): ... return col + from_i >>> pd . DataFrame . vbt . signals . generate (( 5 , 3 ), ... choice_func_nb , index = mask . index , columns = mask . columns ) a b c 2020-01-01 True False False 2020-01-02 False True False 2020-01-03 False False True 2020-01-04 False False False 2020-01-05 False False False","title":"generate()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.generate_both","text":"SignalsAccessor . generate_both ( shape , entry_choice_func_nb = None , entry_args = None , exit_choice_func_nb = None , exit_args = None , entry_wait = 1 , exit_wait = 1 , entry_pick_first = True , exit_pick_first = True , ** kwargs ) See generate_enex_nb() . **kwargs will be passed to pandas constructor. Usage Generate entry and exit signals one after another. Each column increment the number of ticks to wait before placing the exit signal. >>> @njit ... def entry_choice_func_nb ( from_i , to_i , col , temp_idx_arr ): ... temp_idx_arr [ 0 ] = from_i ... return temp_idx_arr [: 1 ] # array with one signal >>> @njit ... def exit_choice_func_nb ( from_i , to_i , col , temp_idx_arr ): ... wait = col ... temp_idx_arr [ 0 ] = from_i + wait ... if temp_idx_arr [ 0 ] < to_i : ... return temp_idx_arr [: 1 ] # array with one signal ... return temp_idx_arr [: 0 ] # empty array >>> temp_idx_arr = np . empty (( 1 ,), dtype = np . int_ ) # reuse memory >>> en , ex = pd . DataFrame . vbt . signals . generate_both ( ... ( 5 , 3 ), ... entry_choice_func_nb , ( temp_idx_arr ,), ... exit_choice_func_nb , ( temp_idx_arr ,), ... index = mask . index , columns = mask . columns ) >>> en a b c 2020-01-01 True True True 2020-01-02 False False False 2020-01-03 True False False 2020-01-04 False True False 2020-01-05 True False True >>> ex a b c 2020-01-01 False False False 2020-01-02 True False False 2020-01-03 False True False 2020-01-04 True False True 2020-01-05 False False False","title":"generate_both()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.generate_exits","text":"SignalsAccessor . generate_exits ( exit_choice_func_nb , * args , wait = 1 , until_next = True , skip_until_exit = False , pick_first = False , wrap_kwargs = None ) See generate_ex_nb() . Usage Fill all space after signals in mask : >>> @njit ... def exit_choice_func_nb ( from_i , to_i , col , temp_range ): ... return temp_range [ from_i : to_i ] >>> temp_range = np . arange ( mask . shape [ 0 ]) # reuse memory >>> mask . vbt . signals . generate_exits ( exit_choice_func_nb , temp_range ) a b c 2020-01-01 False False False 2020-01-02 True True False 2020-01-03 True False False 2020-01-04 True True True 2020-01-05 True False True","title":"generate_exits()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.generate_ohlc_stop_exits","text":"SignalsAccessor . generate_ohlc_stop_exits ( open , high = None , low = None , close = None , is_open_safe = True , out_dict = None , sl_stop = nan , sl_trail = False , tp_stop = nan , reverse = False , entry_wait = 1 , exit_wait = 1 , until_next = True , skip_until_exit = False , pick_first = True , chain = False , broadcast_kwargs = None , wrap_kwargs = None ) Generate exits based on when the price hits (trailing) stop loss or take profit. Hint This function is meant for signal analysis. For backtesting, consider using the stop logic integrated into Portfolio.from_signals() . If any of high , low or close is None, it will be set to open . Use out_dict as a dict to pass stop_price and stop_type arrays. You can also set out_dict to {} to produce these arrays automatically and still have access to them. For arguments, see ohlc_stop_choice_nb() . If chain is True, see generate_ohlc_stop_enex_nb() . Otherwise, see generate_ohlc_stop_ex_nb() . All array-like arguments including stops and out_dict will broadcast using broadcast() and broadcast_kwargs . For arguments, see ohlc_stop_choice_nb() . Note open isn't necessarily open price, but can be any entry price (even previous close). Stop price is calculated based solely on the entry price. Hint Default arguments will generate an exit signal strictly between two entry signals. If both entry signals are too close to each other, no exit will be generated. To ignore all entries that come between an entry and its exit, set until_next to False and skip_until_exit to True. To remove all entries that come between an entry and its exit, set chain to True. This will return two arrays: new entries and exits. Usage The same example as under generate_ohlc_stop_ex_nb() : >>> from vectorbt.signals.enums import StopType >>> price = pd . DataFrame ({ ... 'open' : [ 10 , 11 , 12 , 11 , 10 ], ... 'high' : [ 11 , 12 , 13 , 12 , 11 ], ... 'low' : [ 9 , 10 , 11 , 10 , 9 ], ... 'close' : [ 10 , 11 , 12 , 11 , 10 ] ... }) >>> out_dict = {} >>> exits = mask . vbt . signals . generate_ohlc_stop_exits ( ... price [ 'open' ], price [ 'high' ], price [ 'low' ], price [ 'close' ], ... sl_stop = 0.1 , sl_trail = True , tp_stop = 0.1 , out_dict = out_dict ) >>> exits a b c 2020-01-01 False False False 2020-01-02 True True False 2020-01-03 False False False 2020-01-04 False True True 2020-01-05 False False False >>> out_dict [ 'stop_price' ] a b c 2020-01-01 NaN NaN NaN 2020-01-02 11.0 11.0 NaN 2020-01-03 NaN NaN NaN 2020-01-04 NaN 10.8 10.8 2020-01-05 NaN NaN NaN >>> out_dict [ 'stop_type' ] . vbt ( mapping = StopType ) . apply_mapping () a b c 2020-01-01 None None None 2020-01-02 TakeProfit TakeProfit None 2020-01-03 None None None 2020-01-04 None TrailStop TrailStop 2020-01-05 None None None Notice how the first two entry signals in the third column have no exit signal - there is no room between them for an exit signal. To find an exit for the first entry and ignore all entries that are in-between them, we can pass until_next=False and skip_until_exit=True : >>> out_dict = {} >>> exits = mask . vbt . signals . generate_ohlc_stop_exits ( ... price [ 'open' ], price [ 'high' ], price [ 'low' ], price [ 'close' ], ... sl_stop = 0.1 , sl_trail = True , tp_stop = 0.1 , out_dict = out_dict , ... until_next = False , skip_until_exit = True ) >>> exits a b c 2020-01-01 False False False 2020-01-02 True True True 2020-01-03 False False False 2020-01-04 False True True 2020-01-05 False False False >>> out_dict [ 'stop_price' ] 2020-01-01 NaN NaN NaN 2020-01-02 11.0 11.0 11.0 2020-01-03 NaN NaN NaN 2020-01-04 NaN 10.8 10.8 2020-01-05 NaN NaN NaN >>> out_dict [ 'stop_type' ] . vbt ( mapping = StopType ) . apply_mapping () a b c 2020-01-01 None None None 2020-01-02 TakeProfit TakeProfit TakeProfit 2020-01-03 None None None 2020-01-04 None TrailStop TrailStop 2020-01-05 None None None Now, the first signal in the third column gets executed regardless of the entries that come next, which is very similar to the logic that is implemented in Portfolio.from_signals() . To automatically remove all ignored entry signals, pass chain=True . This will return a new entries array: >>> out_dict = {} >>> new_entries , exits = mask . vbt . signals . generate_ohlc_stop_exits ( ... price [ 'open' ], price [ 'high' ], price [ 'low' ], price [ 'close' ], ... sl_stop = 0.1 , sl_trail = True , tp_stop = 0.1 , out_dict = out_dict , ... chain = True ) >>> new_entries a b c 2020-01-01 True True True 2020-01-02 False False False << removed entry in the third column 2020-01-03 False True True 2020-01-04 False False False 2020-01-05 False True False >>> exits a b c 2020-01-01 False False False 2020-01-02 True True True 2020-01-03 False False False 2020-01-04 False True True 2020-01-05 False False False Warning The last two examples above make entries dependent upon exits - this makes only sense if you have no other exit arrays to combine this stop exit array with.","title":"generate_ohlc_stop_exits()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.generate_random","text":"SignalsAccessor . generate_random ( shape , n = None , prob = None , pick_first = False , seed = None , ** kwargs ) Generate signals randomly. If n is set, see generate_rand_nb() . If prob is set, see generate_rand_by_prob_nb() . n should be either a scalar or an array that will broadcast to the number of columns. prob should be either a single number or an array that will broadcast to match shape . **kwargs will be passed to pandas constructor. Usage For each column, generate a variable number of signals: >>> pd . DataFrame . vbt . signals . generate_random (( 5 , 3 ), n = [ 0 , 1 , 2 ], ... seed = 42 , index = mask . index , columns = mask . columns ) a b c 2020-01-01 False False True 2020-01-02 False False True 2020-01-03 False False False 2020-01-04 False True False 2020-01-05 False False False For each column and time step, pick a signal with 50% probability: >>> pd . DataFrame . vbt . signals . generate_random (( 5 , 3 ), prob = 0.5 , ... seed = 42 , index = mask . index , columns = mask . columns ) a b c 2020-01-01 True True True 2020-01-02 False True False 2020-01-03 False False False 2020-01-04 False False True 2020-01-05 True False True","title":"generate_random()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.generate_random_both","text":"SignalsAccessor . generate_random_both ( shape , n = None , entry_prob = None , exit_prob = None , seed = None , entry_wait = 1 , exit_wait = 1 , entry_pick_first = True , exit_pick_first = True , ** kwargs ) Generate chain of entry and exit signals randomly. If n is set, see generate_rand_enex_nb() . If entry_prob and exit_prob are set, see generate_rand_enex_by_prob_nb() . For arguments, see SignalsAccessor.generate_random() . Usage For each column, generate two entries and exits randomly: >>> en , ex = pd . DataFrame . vbt . signals . generate_random_both ( ... ( 5 , 3 ), n = 2 , seed = 42 , index = mask . index , columns = mask . columns ) >>> en a b c 2020-01-01 True True True 2020-01-02 False False False 2020-01-03 True True False 2020-01-04 False False True 2020-01-05 False False False >>> ex a b c 2020-01-01 False False False 2020-01-02 True True True 2020-01-03 False False False 2020-01-04 False True False 2020-01-05 True False True For each column and time step, pick entry with 50% probability and exit right after: >>> en , ex = pd . DataFrame . vbt . signals . generate_random_both ( ... ( 5 , 3 ), entry_prob = 0.5 , exit_prob = 1. , ... seed = 42 , index = mask . index , columns = mask . columns ) >>> en a b c 2020-01-01 True True True 2020-01-02 False False False 2020-01-03 False False False 2020-01-04 False False True 2020-01-05 True False False >>> ex a b c 2020-01-01 False False False 2020-01-02 True True False 2020-01-03 False False True 2020-01-04 False True False 2020-01-05 True False True","title":"generate_random_both()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.generate_random_exits","text":"SignalsAccessor . generate_random_exits ( prob = None , seed = None , wait = 1 , until_next = True , skip_until_exit = False , wrap_kwargs = None ) Generate exit signals randomly. If prob is None, see generate_rand_ex_nb() . Otherwise, see generate_rand_ex_by_prob_nb() . Usage After each entry in mask , generate exactly one exit: >>> mask . vbt . signals . generate_random_exits ( seed = 42 ) a b c 2020-01-01 False False False 2020-01-02 False True False 2020-01-03 True False False 2020-01-04 False True False 2020-01-05 False False True After each entry in mask and at each time step, generate exit with 50% probability: >>> mask . vbt . signals . generate_random_exits ( prob = 0.5 , seed = 42 ) a b c 2020-01-01 False False False 2020-01-02 True False False 2020-01-03 False False False 2020-01-04 False False False 2020-01-05 False False True","title":"generate_random_exits()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.generate_stop_exits","text":"SignalsAccessor . generate_stop_exits ( ts , stop , trailing = False , entry_wait = 1 , exit_wait = 1 , until_next = True , skip_until_exit = False , pick_first = True , chain = False , broadcast_kwargs = None , wrap_kwargs = None ) Generate exits based on when ts hits the stop. For arguments, see stop_choice_nb() . If chain is True, see generate_stop_enex_nb() . Otherwise, see generate_stop_ex_nb() . Arguments entries , ts and stop will broadcast using broadcast() and broadcast_kwargs . For arguments, see stop_choice_nb() . Hint Default arguments will generate an exit signal strictly between two entry signals. If both entry signals are too close to each other, no exit will be generated. To ignore all entries that come between an entry and its exit, set until_next to False and skip_until_exit to True. To remove all entries that come between an entry and its exit, set chain to True. This will return two arrays: new entries and exits. Usage >>> ts = pd . Series ([ 1 , 2 , 3 , 2 , 1 ]) >>> # stop loss >>> mask . vbt . signals . generate_stop_exits ( ts , - 0.1 ) a b c 2020-01-01 False False False 2020-01-02 False False False 2020-01-03 False False False 2020-01-04 False True True 2020-01-05 False False False >>> # trailing stop loss >>> mask . vbt . signals . generate_stop_exits ( ts , - 0.1 , trailing = True ) a b c 2020-01-01 False False False 2020-01-02 False False False 2020-01-03 False False False 2020-01-04 True True True 2020-01-05 False False False","title":"generate_stop_exits()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.index_mapped","text":"SignalsAccessor . index_mapped ( group_by = None , ** kwargs ) Get a mapped array of indices. See GenericAccessor.to_mapped() . Only True values will be considered.","title":"index_mapped()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.metrics","text":"Metrics supported by SignalsAccessor . Co nf ig( { \"start\" : { \"title\" : \"Start\" , \"calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b19d8>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"end\" : { \"title\" : \"End\" , \"calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1a60>\" , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"period\" : { \"title\" : \"Period\" , \"calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1ae8>\" , \"apply_to_timedelta\" : true , \"agg_func\" : null , \"tags\" : \"wrapper\" }, \"total\" : { \"title\" : \"Total\" , \"calc_func\" : \"total\" , \"tags\" : \"signals\" }, \"rate\" : { \"title\" : \"Rate [%]\" , \"calc_func\" : \"rate\" , \"post_calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1b70>\" , \"tags\" : \"signals\" }, \"total_overlapping\" : { \"title\" : \"Total Overlapping\" , \"calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1bf8>\" , \"check_silent_has_other\" : true , \"tags\" : [ \"signals\" , \"other\" ] }, \"overlapping_rate\" : { \"title\" : \"Overlapping Rate [%]\" , \"calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1c80>\" , \"post_calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1d08>\" , \"check_silent_has_other\" : true , \"tags\" : [ \"signals\" , \"other\" ] }, \"first_index\" : { \"title\" : \"First Index\" , \"calc_func\" : \"nth_index\" , \"n\" : 0 , \"return_labels\" : true , \"tags\" : [ \"signals\" , \"index\" ] }, \"last_index\" : { \"title\" : \"Last Index\" , \"calc_func\" : \"nth_index\" , \"n\" : -1 , \"return_labels\" : true , \"tags\" : [ \"signals\" , \"index\" ] }, \"norm_avg_index\" : { \"title\" : \"Norm Avg Index [-1, 1]\" , \"calc_func\" : \"norm_avg_index\" , \"tags\" : [ \"signals\" , \"index\" ] }, \"distance\" : { \"title\" : \"RepEval(expression=\\\"f'Distance {\\\"<-\\\" if from_other else \\\"->\\\"} {other_name}' if other is not None else 'Distance'\\\", mapping={})\" , \"calc_func\" : \"between_ranges.duration\" , \"post_calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1d90>\" , \"apply_to_timedelta\" : true , \"tags\" : \"RepEval(expression=\\\"['signals', 'distance', 'other'] if other is not None else ['signals', 'distance']\\\", mapping={})\" }, \"total_partitions\" : { \"title\" : \"Total Partitions\" , \"calc_func\" : \"total_partitions\" , \"tags\" : [ \"signals\" , \"partitions\" ] }, \"partition_rate\" : { \"title\" : \"Partition Rate [%]\" , \"calc_func\" : \"partition_rate\" , \"post_calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1e18>\" , \"tags\" : [ \"signals\" , \"partitions\" ] }, \"partition_len\" : { \"title\" : \"Partition Length\" , \"calc_func\" : \"partition_ranges.duration\" , \"post_calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1ea0>\" , \"apply_to_timedelta\" : true , \"tags\" : [ \"signals\" , \"partitions\" , \"distance\" ] }, \"partition_distance\" : { \"title\" : \"Partition Distance\" , \"calc_func\" : \"between_partition_ranges.duration\" , \"post_calc_func\" : \"<function SignalsAccessor.<lambda> at 0x7fac990b1f28>\" , \"apply_to_timedelta\" : true , \"tags\" : [ \"signals\" , \"partitions\" , \"distance\" ] } } ) Returns SignalsAccessor._metrics , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change metrics, you can either change the config in-place, override this property, or overwrite the instance variable SignalsAccessor._metrics .","title":"metrics"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.norm_avg_index","text":"SignalsAccessor . norm_avg_index ( group_by = None , wrap_kwargs = None ) See norm_avg_index_nb() . Normalized average index measures the average signal location relative to the middle of the column. This way, we can quickly see where the majority of signals are located. Common values are: -1.0: only the first signal is set 1.0: only the last signal is set 0.0: symmetric distribution around the middle [-1.0, 0.0): average signal is on the left (0.0, 1.0]: average signal is on the right Usage >>> pd . Series ([ True , False , False , False ]) . vbt . signals . norm_avg_index () -1.0 >>> pd . Series ([ False , False , False , True ]) . vbt . signals . norm_avg_index () 1.0 >>> pd . Series ([ True , False , False , True ]) . vbt . signals . norm_avg_index () 0.0","title":"norm_avg_index()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.nth","text":"SignalsAccessor . nth ( n , wrap_kwargs = None , ** kwargs ) Select signals that satisfy the condition pos_rank == n .","title":"nth()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.nth_index","text":"SignalsAccessor . nth_index ( n , return_labels = True , group_by = None , wrap_kwargs = None ) See nth_index_nb() . Usage >>> mask . vbt . signals . nth_index ( 0 ) a 2020-01-01 b 2020-01-01 c 2020-01-01 Name: nth_index, dtype: datetime64[ns] >>> mask . vbt . signals . nth_index ( 2 ) a NaT b 2020-01-05 c 2020-01-03 Name: nth_index, dtype: datetime64[ns] >>> mask . vbt . signals . nth_index ( - 1 ) a 2020-01-01 b 2020-01-05 c 2020-01-03 Name: nth_index, dtype: datetime64[ns] >>> mask . vbt . signals . nth_index ( - 1 , group_by = True ) Timestamp('2020-01-05 00:00:00')","title":"nth_index()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.partition_pos_rank","text":"SignalsAccessor . partition_pos_rank ( ** kwargs ) Get partition position ranks. Uses SignalsAccessor.rank() with part_pos_rank_nb() . Usage Rank each partition of True values in mask : >>> mask . vbt . signals . partition_pos_rank () a b c 2020-01-01 0 0 0 2020-01-02 -1 -1 0 2020-01-03 -1 1 0 2020-01-04 -1 -1 -1 2020-01-05 -1 2 -1 >>> mask . vbt . signals . partition_pos_rank ( after_false = True ) a b c 2020-01-01 -1 -1 -1 2020-01-02 -1 -1 -1 2020-01-03 -1 0 -1 2020-01-04 -1 -1 -1 2020-01-05 -1 1 -1 >>> mask . vbt . signals . partition_pos_rank ( reset_by = mask ) a b c 2020-01-01 0 0 0 2020-01-02 -1 -1 0 2020-01-03 -1 0 0 2020-01-04 -1 -1 -1 2020-01-05 -1 0 -1","title":"partition_pos_rank()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.partition_pos_rank_mapped","text":"SignalsAccessor . partition_pos_rank_mapped ( group_by = None , ** kwargs ) Get a mapped array of partition position ranks. See SignalsAccessor.partition_pos_rank() .","title":"partition_pos_rank_mapped()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.partition_ranges","text":"SignalsAccessor . partition_ranges ( group_by = None , attach_ts = True , ** kwargs ) Wrap the result of partition_ranges_nb() with Ranges . If use_end_idxs is True, uses the index of the last signal in each partition as idx_arr . Otherwise, uses the index of the first signal. Usage >>> mask_sr = pd . Series ([ True , True , True , False , True , True ]) >>> mask_sr . vbt . signals . partition_ranges () . records_readable Range Id Column Start Timestamp End Timestamp Status 0 0 0 0 3 Closed 1 1 0 4 5 Open","title":"partition_ranges()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.partition_rate","text":"SignalsAccessor . partition_rate ( wrap_kwargs = None , group_by = None , ** kwargs ) SignalsAccessor.total_partitions() divided by SignalsAccessor.total() in each column/group.","title":"partition_rate()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.plot","text":"SignalsAccessor . plot ( yref = 'y' , ** kwargs ) Plot signals. Args yref :\u2002 str Y coordinate axis. **kwargs Keyword arguments passed to GenericAccessor.lineplot() . Usage >>> mask [[ 'a' , 'c' ]] . vbt . signals . plot ()","title":"plot()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.plots_defaults","text":"Defaults for PlotsBuilderMixin.plots() . Merges GenericAccessor.plots_defaults and signals.plots from settings .","title":"plots_defaults"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.pos_rank","text":"SignalsAccessor . pos_rank ( allow_gaps = False , ** kwargs ) Get signal position ranks. Uses SignalsAccessor.rank() with sig_pos_rank_nb() . Usage Rank each True value in each partition in mask : >>> mask . vbt . signals . pos_rank () a b c 2020-01-01 0 0 0 2020-01-02 -1 -1 1 2020-01-03 -1 0 2 2020-01-04 -1 -1 -1 2020-01-05 -1 0 -1 >>> mask . vbt . signals . pos_rank ( after_false = True ) a b c 2020-01-01 -1 -1 -1 2020-01-02 -1 -1 -1 2020-01-03 -1 0 -1 2020-01-04 -1 -1 -1 2020-01-05 -1 0 -1 >>> mask . vbt . signals . pos_rank ( allow_gaps = True ) a b c 2020-01-01 0 0 0 2020-01-02 -1 -1 1 2020-01-03 -1 1 2 2020-01-04 -1 -1 -1 2020-01-05 -1 2 -1 >>> mask . vbt . signals . pos_rank ( reset_by =~ mask , allow_gaps = True ) a b c 2020-01-01 0 0 0 2020-01-02 -1 -1 1 2020-01-03 -1 0 2 2020-01-04 -1 -1 -1 2020-01-05 -1 0 -1","title":"pos_rank()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.pos_rank_mapped","text":"SignalsAccessor . pos_rank_mapped ( group_by = None , ** kwargs ) Get a mapped array of signal position ranks. See SignalsAccessor.pos_rank() .","title":"pos_rank_mapped()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.rank","text":"SignalsAccessor . rank ( rank_func_nb , * args , prepare_func = None , reset_by = None , after_false = False , broadcast_kwargs = None , wrap_kwargs = None , as_mapped = False , ** kwargs ) See rank_nb() . Will broadcast with reset_by using broadcast() and broadcast_kwargs . Use prepare_func to prepare further arguments to be passed before *args , such as temporary arrays. It should take both broadcasted arrays ( reset_by can be None) and return a tuple. Set as_mapped to True to return an instance of MappedArray .","title":"rank()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.rate","text":"SignalsAccessor . rate ( wrap_kwargs = None , group_by = None , ** kwargs ) SignalsAccessor.total() divided by the total index length in each column/group.","title":"rate()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.stats_defaults","text":"Defaults for StatsBuilderMixin.stats() . Merges GenericAccessor.stats_defaults and signals.stats from settings .","title":"stats_defaults"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.subplots","text":"Subplots supported by SignalsAccessor . Co nf ig( { \"plot\" : { \"check_is_not_grouped\" : true , \"plot_func\" : \"plot\" , \"pass_trace_names\" : false , \"tags\" : \"generic\" } } ) Returns SignalsAccessor._subplots , which gets (deep) copied upon creation of each instance. Thus, changing this config won't affect the class. To change subplots, you can either change the config in-place, override this property, or overwrite the instance variable SignalsAccessor._subplots .","title":"subplots"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.total","text":"SignalsAccessor . total ( wrap_kwargs = None , group_by = None ) Total number of True values in each column/group.","title":"total()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsAccessor.total_partitions","text":"SignalsAccessor . total_partitions ( wrap_kwargs = None , group_by = None , ** kwargs ) Total number of partitions of True values in each column/group.","title":"total_partitions()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsDFAccessor","text":"Accessor on top of signal series. For DataFrames only. Accessible through pd.DataFrame.vbt.signals . Superclasses AttrResolver BaseAccessor BaseDFAccessor Configured Documented GenericAccessor GenericDFAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin SignalsAccessor StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.diff() GenericAccessor.drawdown() GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.get_drawdowns() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.lineplot() GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.pct_change() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.resolve_self() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.zscore() GenericDFAccessor.flatten_grouped() GenericDFAccessor.heatmap() GenericDFAccessor.squeeze_grouped() GenericDFAccessor.ts_heatmap() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() SignalsAccessor.AND() SignalsAccessor.OR() SignalsAccessor.XOR() SignalsAccessor.between_partition_ranges() SignalsAccessor.between_ranges() SignalsAccessor.bshift() SignalsAccessor.clean() SignalsAccessor.config SignalsAccessor.df_accessor_cls SignalsAccessor.drawdowns SignalsAccessor.empty() SignalsAccessor.empty_like() SignalsAccessor.first() SignalsAccessor.from_nth() SignalsAccessor.fshift() SignalsAccessor.generate() SignalsAccessor.generate_both() SignalsAccessor.generate_exits() SignalsAccessor.generate_ohlc_stop_exits() SignalsAccessor.generate_random() SignalsAccessor.generate_random_both() SignalsAccessor.generate_random_exits() SignalsAccessor.generate_stop_exits() SignalsAccessor.iloc SignalsAccessor.index_mapped() SignalsAccessor.indexing_kwargs SignalsAccessor.loc SignalsAccessor.mapping SignalsAccessor.norm_avg_index() SignalsAccessor.nth() SignalsAccessor.nth_index() SignalsAccessor.obj SignalsAccessor.partition_pos_rank() SignalsAccessor.partition_pos_rank_mapped() SignalsAccessor.partition_ranges() SignalsAccessor.partition_rate() SignalsAccessor.plot() SignalsAccessor.plots_defaults SignalsAccessor.pos_rank() SignalsAccessor.pos_rank_mapped() SignalsAccessor.ranges SignalsAccessor.rank() SignalsAccessor.rate() SignalsAccessor.self_aliases SignalsAccessor.sr_accessor_cls SignalsAccessor.stats_defaults SignalsAccessor.total() SignalsAccessor.total_partitions() SignalsAccessor.wrapper SignalsAccessor.writeable_attrs StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"SignalsDFAccessor"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsSRAccessor","text":"Accessor on top of signal series. For Series only. Accessible through pd.Series.vbt.signals . Superclasses AttrResolver BaseAccessor BaseSRAccessor Configured Documented GenericAccessor GenericSRAccessor IndexingBase PandasIndexer Pickleable PlotsBuilderMixin SignalsAccessor StatsBuilderMixin Wrapping Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() BaseAccessor.align_to() BaseAccessor.apply() BaseAccessor.apply_and_concat() BaseAccessor.apply_on_index() BaseAccessor.broadcast() BaseAccessor.broadcast_to() BaseAccessor.combine() BaseAccessor.concat() BaseAccessor.drop_duplicate_levels() BaseAccessor.drop_levels() BaseAccessor.drop_redundant_levels() BaseAccessor.indexing_func() BaseAccessor.make_symmetric() BaseAccessor.rename_levels() BaseAccessor.repeat() BaseAccessor.select_levels() BaseAccessor.stack_index() BaseAccessor.tile() BaseAccessor.to_1d_array() BaseAccessor.to_2d_array() BaseAccessor.to_dict() BaseAccessor.unstack_to_array() BaseAccessor.unstack_to_df() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() GenericAccessor.apply_along_axis() GenericAccessor.apply_and_reduce() GenericAccessor.apply_mapping() GenericAccessor.applymap() GenericAccessor.barplot() GenericAccessor.bfill() GenericAccessor.binarize() GenericAccessor.boxplot() GenericAccessor.count() GenericAccessor.crossed_above() GenericAccessor.crossed_below() GenericAccessor.cumprod() GenericAccessor.cumsum() GenericAccessor.describe() GenericAccessor.diff() GenericAccessor.drawdown() GenericAccessor.ewm_mean() GenericAccessor.ewm_std() GenericAccessor.expanding_apply() GenericAccessor.expanding_max() GenericAccessor.expanding_mean() GenericAccessor.expanding_min() GenericAccessor.expanding_split() GenericAccessor.expanding_std() GenericAccessor.ffill() GenericAccessor.fillna() GenericAccessor.filter() GenericAccessor.get_drawdowns() GenericAccessor.get_ranges() GenericAccessor.groupby_apply() GenericAccessor.histplot() GenericAccessor.idxmax() GenericAccessor.idxmin() GenericAccessor.lineplot() GenericAccessor.max() GenericAccessor.maxabs_scale() GenericAccessor.mean() GenericAccessor.median() GenericAccessor.min() GenericAccessor.minmax_scale() GenericAccessor.normalize() GenericAccessor.pct_change() GenericAccessor.power_transform() GenericAccessor.product() GenericAccessor.quantile_transform() GenericAccessor.range_split() GenericAccessor.rebase() GenericAccessor.reduce() GenericAccessor.resample_apply() GenericAccessor.resolve_self() GenericAccessor.robust_scale() GenericAccessor.rolling_apply() GenericAccessor.rolling_max() GenericAccessor.rolling_mean() GenericAccessor.rolling_min() GenericAccessor.rolling_split() GenericAccessor.rolling_std() GenericAccessor.scale() GenericAccessor.scatterplot() GenericAccessor.shuffle() GenericAccessor.split() GenericAccessor.std() GenericAccessor.sum() GenericAccessor.to_mapped() GenericAccessor.to_returns() GenericAccessor.transform() GenericAccessor.value_counts() GenericAccessor.zscore() GenericSRAccessor.flatten_grouped() GenericSRAccessor.heatmap() GenericSRAccessor.overlay_with_heatmap() GenericSRAccessor.plot_against() GenericSRAccessor.qqplot() GenericSRAccessor.squeeze_grouped() GenericSRAccessor.ts_heatmap() GenericSRAccessor.volume() PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() SignalsAccessor.AND() SignalsAccessor.OR() SignalsAccessor.XOR() SignalsAccessor.between_partition_ranges() SignalsAccessor.between_ranges() SignalsAccessor.bshift() SignalsAccessor.clean() SignalsAccessor.config SignalsAccessor.df_accessor_cls SignalsAccessor.drawdowns SignalsAccessor.empty() SignalsAccessor.empty_like() SignalsAccessor.first() SignalsAccessor.from_nth() SignalsAccessor.fshift() SignalsAccessor.generate() SignalsAccessor.generate_both() SignalsAccessor.generate_exits() SignalsAccessor.generate_ohlc_stop_exits() SignalsAccessor.generate_random() SignalsAccessor.generate_random_both() SignalsAccessor.generate_random_exits() SignalsAccessor.generate_stop_exits() SignalsAccessor.iloc SignalsAccessor.index_mapped() SignalsAccessor.indexing_kwargs SignalsAccessor.loc SignalsAccessor.mapping SignalsAccessor.norm_avg_index() SignalsAccessor.nth() SignalsAccessor.nth_index() SignalsAccessor.obj SignalsAccessor.partition_pos_rank() SignalsAccessor.partition_pos_rank_mapped() SignalsAccessor.partition_ranges() SignalsAccessor.partition_rate() SignalsAccessor.plot() SignalsAccessor.plots_defaults SignalsAccessor.pos_rank() SignalsAccessor.pos_rank_mapped() SignalsAccessor.ranges SignalsAccessor.rank() SignalsAccessor.rate() SignalsAccessor.self_aliases SignalsAccessor.sr_accessor_cls SignalsAccessor.stats_defaults SignalsAccessor.total() SignalsAccessor.total_partitions() SignalsAccessor.wrapper SignalsAccessor.writeable_attrs StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.select_one() Wrapping.select_one_from_obj()","title":"SignalsSRAccessor"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsSRAccessor.plot_as_entry_markers","text":"SignalsSRAccessor . plot_as_entry_markers ( y = None , ** kwargs ) Plot signals as entry markers. See SignalsSRAccessor.plot_as_markers() .","title":"plot_as_entry_markers()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsSRAccessor.plot_as_exit_markers","text":"SignalsSRAccessor . plot_as_exit_markers ( y = None , ** kwargs ) Plot signals as exit markers. See SignalsSRAccessor.plot_as_markers() .","title":"plot_as_exit_markers()"},{"location":"api/signals/accessors/#vectorbt.signals.accessors.SignalsSRAccessor.plot_as_markers","text":"SignalsSRAccessor . plot_as_markers ( y = None , ** kwargs ) Plot Series as markers. Args y :\u2002 array_like Y-axis values to plot markers on. **kwargs Keyword arguments passed to GenericAccessor.scatterplot() . Usage >>> ts = pd . Series ([ 1 , 2 , 3 , 2 , 1 ], index = mask . index ) >>> fig = ts . vbt . lineplot () >>> mask [ 'b' ] . vbt . signals . plot_as_entry_markers ( y = ts , fig = fig ) >>> ( ~ mask [ 'b' ]) . vbt . signals . plot_as_exit_markers ( y = ts , fig = fig )","title":"plot_as_markers()"},{"location":"api/signals/enums/","text":"enums module \u00b6 Named tuples and enumerated types. Defines enums and other schemas for vectorbt.signals . FactoryMode FactoryModeT \u00b6 Factory mode. { \"Entries\" : 0 , \"Exits\" : 1 , \"Both\" : 2 , \"Chain\" : 3 } Attributes Entries Generate entries only using generate_func . Takes no input signal arrays. Produces one output signal array - entries . Such generators often have no suffix. Exits Generate exits only using generate_ex_func . Takes one input signal array - entries . Produces one output signal array - exits . Such generators often have suffix 'X'. Both Generate both entries and exits using generate_enex_func . Takes no input signal arrays. Produces two output signal arrays - entries and exits . Such generators often have suffix 'NX'. Chain Generate chain of entries and exits using generate_enex_func . Takes one input signal array - entries . Produces two output signal arrays - new_entries and exits . Such generators often have suffix 'CX'. StopType StopTypeT \u00b6 Stop type. { \"StopLoss\" : 0 , \"TrailStop\" : 1 , \"TakeProfit\" : 2 }","title":"enums"},{"location":"api/signals/enums/#vectorbt.signals.enums","text":"Named tuples and enumerated types. Defines enums and other schemas for vectorbt.signals .","title":"vectorbt.signals.enums"},{"location":"api/signals/enums/#vectorbt.signals.enums.FactoryMode","text":"Factory mode. { \"Entries\" : 0 , \"Exits\" : 1 , \"Both\" : 2 , \"Chain\" : 3 } Attributes Entries Generate entries only using generate_func . Takes no input signal arrays. Produces one output signal array - entries . Such generators often have no suffix. Exits Generate exits only using generate_ex_func . Takes one input signal array - entries . Produces one output signal array - exits . Such generators often have suffix 'X'. Both Generate both entries and exits using generate_enex_func . Takes no input signal arrays. Produces two output signal arrays - entries and exits . Such generators often have suffix 'NX'. Chain Generate chain of entries and exits using generate_enex_func . Takes one input signal array - entries . Produces two output signal arrays - new_entries and exits . Such generators often have suffix 'CX'.","title":"FactoryMode"},{"location":"api/signals/enums/#vectorbt.signals.enums.StopType","text":"Stop type. { \"StopLoss\" : 0 , \"TrailStop\" : 1 , \"TakeProfit\" : 2 }","title":"StopType"},{"location":"api/signals/factory/","text":"factory module \u00b6 A factory for building new signal generators with ease. The signal factory class SignalFactory extends IndicatorFactory to offer a convenient way to create signal generators of any complexity. By providing it with information such as entry and exit functions and the names of inputs, parameters, and outputs, it will create a stand-alone class capable of generating signals for an arbitrary combination of inputs and parameters. SignalFactory class \u00b6 A factory for building signal generators. Extends IndicatorFactory with choice functions. Generates a fixed number of outputs (depending upon mode ). If you need to generate other outputs, use in-place outputs (via in_output_names ). See FactoryMode for supported generation modes. Other arguments are passed to IndicatorFactory . A factory for creating new indicators. Initialize `IndicatorFactory` to create a skeleton and then use a class method such as `IndicatorFactory.from_custom_func` to bind a calculation function to the skeleton. __Args__ **```class_name```** :&ensp;`str` : Name for the created indicator class. **```class_docstring```** :&ensp;`str` : Docstring for the created indicator class. **```module_name```** :&ensp;`str` : Specify the module the class originates from. **```short_name```** :&ensp;`str` : A short name of the indicator. Defaults to lower-case `class_name`. **```prepend_name```** :&ensp;`bool` : Whether to prepend `short_name` to each parameter level. **```input_names```** :&ensp;`list` of `str` : A list of names of input arrays. **```param_names```** :&ensp;`list` of `str` : A list of names of parameters. **```in_output_names```** :&ensp;`list` of `str` : A list of names of in-place output arrays. An in-place output is an output that is not returned but modified in-place. Some advantages of such outputs include: 1) they don't need to be returned, 2) they can be passed between functions as easily as inputs, 3) they can be provided with already allocated data to safe memory, 4) if data or default value are not provided, they are created empty to not occupy memory. **```output_names```** :&ensp;`list` of `str` : A list of names of output arrays. **```output_flags```** :&ensp;`dict` : A dictionary of in-place and regular output flags. **```custom_output_props```** :&ensp;`dict` : A dictionary with user-defined functions that will be bound to the indicator class and wrapped with `@cached_property`. **```attr_settings```** :&ensp;`dict` : A dictionary of settings by attribute name. Attributes can be `input_names`, `in_output_names`, `output_names` and `custom_output_props`. Following keys are accepted: * `dtype`: Data type used to determine which methods to generate around this attribute. Set to None to disable. Default is `np.float_`. Can be set to instance of `collections.namedtuple` acting as enumerated type, or any other mapping; It will then create a property with suffix `readable` that contains data in a string format. **```metrics```** :&ensp;`dict` : Metrics supported by [StatsBuilderMixin.stats()](/api/generic/stats_builder/#vectorbt.generic.stats_builder.StatsBuilderMixin.stats \"vectorbt.generic.stats_builder.StatsBuilderMixin.stats\"). If dict, will be converted to [Config](/api/utils/config/#vectorbt.utils.config.Config \"vectorbt.utils.config.Config\"). **```stats_defaults```** :&ensp;`callable` or `dict` : Defaults for [StatsBuilderMixin.stats()](/api/generic/stats_builder/#vectorbt.generic.stats_builder.StatsBuilderMixin.stats \"vectorbt.generic.stats_builder.StatsBuilderMixin.stats\"). If dict, will be converted into a property. **```subplots```** :&ensp;`dict` : Subplots supported by [PlotsBuilderMixin.plots()](/api/generic/plots_builder/#vectorbt.generic.plots_builder.PlotsBuilderMixin.plots \"vectorbt.generic.plots_builder.PlotsBuilderMixin.plots\"). If dict, will be converted to [Config](/api/utils/config/#vectorbt.utils.config.Config \"vectorbt.utils.config.Config\"). **```plots_defaults```** :&ensp;`callable` or `dict` : Defaults for [PlotsBuilderMixin.plots()](/api/generic/plots_builder/#vectorbt.generic.plots_builder.PlotsBuilderMixin.plots \"vectorbt.generic.plots_builder.PlotsBuilderMixin.plots\"). If dict, will be converted into a property. !!! note The `__init__` method is not used for running the indicator, for this use `run`. The reason for this is indexing, which requires a clean `__init__` method for creating a new indicator object with newly indexed attributes. __Superclasses__ * [IndicatorFactory](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory \"vectorbt.indicators.factory.IndicatorFactory\") __Inherited members__ * [IndicatorFactory.find_ta_indicator()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.find_ta_indicator \"vectorbt.indicators.factory.IndicatorFactory.find_ta_indicator\") * [IndicatorFactory.from_apply_func()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.from_apply_func \"vectorbt.indicators.factory.IndicatorFactory.from_apply_func\") * [IndicatorFactory.from_custom_func()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.from_custom_func \"vectorbt.indicators.factory.IndicatorFactory.from_custom_func\") * [IndicatorFactory.from_pandas_ta()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.from_pandas_ta \"vectorbt.indicators.factory.IndicatorFactory.from_pandas_ta\") * [IndicatorFactory.from_ta()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.from_ta \"vectorbt.indicators.factory.IndicatorFactory.from_ta\") * [IndicatorFactory.from_talib()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.from_talib \"vectorbt.indicators.factory.IndicatorFactory.from_talib\") * [IndicatorFactory.get_pandas_ta_indicators()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.get_pandas_ta_indicators \"vectorbt.indicators.factory.IndicatorFactory.get_pandas_ta_indicators\") * [IndicatorFactory.get_ta_indicators()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.get_ta_indicators \"vectorbt.indicators.factory.IndicatorFactory.get_ta_indicators\") * [IndicatorFactory.get_talib_indicators()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.get_talib_indicators \"vectorbt.indicators.factory.IndicatorFactory.get_talib_indicators\") * [IndicatorFactory.parse_pandas_ta_config()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.parse_pandas_ta_config \"vectorbt.indicators.factory.IndicatorFactory.parse_pandas_ta_config\") * [IndicatorFactory.parse_ta_config()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.parse_ta_config \"vectorbt.indicators.factory.IndicatorFactory.parse_ta_config\") --- ### from_choice_func <span class=\"dobjtype\">method</span><a class=\"githublink\" href=\"https://github.com/polakowo/vectorbt/blob/b7b6e48cd3a7fbc185980f6b83f3c15b2ad0d4b6/vectorbt/signals/factory.py#L165-L950\" target=\"_blank\" title=\"Jump to source\">:material-github:</a> { #vectorbt.signals.factory.SignalFactory.from_choice_func data-toc-label='from_choice_func()' } ```python SignalFactory.from_choice_func( entry_choice_func=None, exit_choice_func=None, generate_func=generate_nb, generate_ex_func=generate_ex_nb, generate_enex_func=generate_enex_nb, cache_func=None, entry_settings=None, exit_settings=None, cache_settings=None, numba_loop=False, **kwargs ) Build signal generator class around entry and exit choice functions. A choice function is simply a function that returns indices of signals. There are two types of it: entry choice function and exit choice function. Each choice function takes broadcast time series, broadcast in-place output time series, broadcast parameter arrays, and other arguments, and returns an array of indices corresponding to chosen signals. See generate_nb() . Args entry_choice_func :\u2002 callable choice_func_nb that returns indices of entries. Defaults to first_choice_nb() for FactoryMode.Chain . exit_choice_func :\u2002 callable choice_func_nb that returns indices of exits. generate_func :\u2002 callable Entry generation function. Defaults to generate_nb() . generate_ex_func :\u2002 callable Exit generation function. Defaults to generate_ex_nb() . generate_enex_func :\u2002 callable Entry and exit generation function. Defaults to generate_enex_nb() . cache_func :\u2002 callable A caching function to preprocess data beforehand. All returned objects will be passed as last arguments to choice functions. entry_settings :\u2002 dict Settings dict for entry_choice_func . exit_settings :\u2002 dict Settings dict for exit_choice_func . cache_settings :\u2002 dict Settings dict for cache_func . numba_loop :\u2002 bool Whether to loop using Numba. Set to True when iterating large number of times over small input. **kwargs Keyword arguments passed to IndicatorFactory.from_custom_func . Note Choice functions should be Numba-compiled. Which inputs, parameters and arguments to pass to each function should be explicitly indicated in the function's settings dict. By default, nothing is passed. Passing keyword arguments directly to the choice functions is not supported. Use pass_kwargs in a settings dict to pass keyword arguments as positional. Settings dict of each function can have the following keys: Attributes pass_inputs :\u2002 list of str Input names to pass to the choice function. Defaults to []. Order matters. Each name must be in input_names . pass_in_outputs :\u2002 list of str In-place output names to pass to the choice function. Defaults to []. Order matters. Each name must be in in_output_names . pass_params :\u2002 list of str Parameter names to pass to the choice function. Defaults to []. Order matters. Each name must be in param_names . pass_kwargs :\u2002 dict , list of str or list of tuple Keyword arguments from kwargs dict to pass as positional arguments to the choice function. Defaults to []. Order matters. If any element is a tuple, should contain the name and the default value. If any element is a string, the default value is None. Built-in keys include: input_shape : Input shape if no input time series passed. Default is provided by the pipeline if pass_input_shape is True. wait : Number of ticks to wait before placing signals. Default is 1. until_next : Whether to place signals up to the next entry signal. Default is True. Applied in generate_ex_func only. * skip_until_exit : Whether to skip processing entry signals until the next exit. Default is False. Applied in generate_ex_func only. * pick_first : Whether to stop as soon as the first exit signal is found. Default is False with FactoryMode.Entries , otherwise is True. * temp_idx_arr : Empty integer array used to temporarily store indices. Default is an automatically generated array of shape input_shape[0] . You can also pass temp_idx_arr1 , temp_idx_arr2 , etc. to generate multiple. * flex_2d : See flex_select_auto_nb() . Default is provided by the pipeline if pass_flex_2d is True. pass_cache :\u2002 bool Whether to pass cache from cache_func to the choice function. Defaults to False. Cache is passed unpacked. The following arguments can be passed to run and run_combs methods: Args *args Should be used instead of entry_args with FactoryMode.Entries and instead of exit_args with FactoryMode.Exits and FactoryMode.Chain with default entry_choice_func . entry_args :\u2002 tuple Arguments passed to the entry choice function. exit_args :\u2002 tuple Arguments passed to the exit choice function. cache_args :\u2002 tuple Arguments passed to the cache function. entry_kwargs :\u2002 tuple Settings for the entry choice function. Also contains arguments passed as positional if in pass_kwargs . exit_kwargs :\u2002 tuple Settings for the exit choice function. Also contains arguments passed as positional if in pass_kwargs . cache_kwargs :\u2002 tuple Settings for the cache function. Also contains arguments passed as positional if in pass_kwargs . return_cache :\u2002 bool Whether to return only cache. use_cache :\u2002 any Cache to use. **kwargs Should be used instead of entry_kwargs with FactoryMode.Entries and instead of exit_kwargs with FactoryMode.Exits and FactoryMode.Chain with default entry_choice_func . For more arguments, see run_pipeline() . Usage The simplest signal indicator that places True at the very first index: >>> from numba import njit >>> import vectorbt as vbt >>> import numpy as np >>> @njit ... def entry_choice_func ( from_i , to_i , col ): ... return np . array ([ from_i ]) >>> @njit ... def exit_choice_func ( from_i , to_i , col ): ... return np . array ([ from_i ]) >>> MySignals = vbt . SignalFactory () . from_choice_func ( ... entry_choice_func = entry_choice_func , ... exit_choice_func = exit_choice_func , ... entry_kwargs = dict ( wait = 1 ), ... exit_kwargs = dict ( wait = 1 ) ... ) >>> my_sig = MySignals . run ( input_shape = ( 3 , 3 )) >>> my_sig . entries 0 1 2 0 True True True 1 False False False 2 True True True >>> my_sig . exits 0 1 2 0 False False False 1 True True True 2 False False False Take the first entry and place an exit after waiting n ticks. Find the next entry and repeat. Test three different n values. >>> from numba import njit >>> from vectorbt.signals.factory import SignalFactory >>> @njit ... def wait_choice_nb ( from_i , to_i , col , n , temp_idx_arr ): ... temp_idx_arr [ 0 ] = from_i + n # index of next exit ... if temp_idx_arr [ 0 ] < to_i : ... return temp_idx_arr [: 1 ] ... return temp_idx_arr [: 0 ] # must return array anyway >>> # Build signal generator >>> MySignals = SignalFactory ( ... mode = 'chain' , ... param_names = [ 'n' ] ... ) . from_choice_func ( ... exit_choice_func = wait_choice_nb , ... exit_settings = dict ( ... pass_params = [ 'n' ], ... pass_kwargs = [ 'temp_idx_arr' ] # built-in kwarg ... ) ... ) >>> # Run signal generator >>> entries = [ True , True , True , True , True ] >>> my_sig = MySignals . run ( entries , [ 0 , 1 , 2 ]) >>> my_sig . entries # input entries custom_n 0 1 2 0 True True True 1 True True True 2 True True True 3 True True True 4 True True True >>> my_sig . new_entries # output entries custom_n 0 1 2 0 True True True 1 False False False 2 True False False 3 False True False 4 True False True >>> my_sig . exits # output exits custom_n 0 1 2 0 False False False 1 True False False 2 False True False 3 True False True 4 False False False To combine multiple iterative signals, you would need to create a custom choice function. Here is an example of combining two random generators using \"OR\" rule (the first signal wins): >>> from numba import njit >>> from collections import namedtuple >>> from vectorbt.indicators.configs import flex_elem_param_config >>> from vectorbt.signals.factory import SignalFactory >>> from vectorbt.signals.nb import rand_by_prob_choice_nb >>> # Enum to distinguish random generators >>> RandType = namedtuple ( 'RandType' , [ 'R1' , 'R2' ])( 0 , 1 ) >>> # Define exit choice function >>> @njit ... def rand_exit_choice_nb ( from_i , to_i , col , rand_type , prob1 , ... prob2 , temp_idx_arr1 , temp_idx_arr2 , flex_2d ): ... idxs1 = rand_by_prob_choice_nb ( from_i , to_i , col , prob1 , True , temp_idx_arr1 , flex_2d ) ... if len ( idxs1 ) > 0 : ... to_i = idxs1 [ 0 ] # no need to go beyond first the first found signal ... idxs2 = rand_by_prob_choice_nb ( from_i , to_i , col , prob2 , True , temp_idx_arr2 , flex_2d ) ... if len ( idxs2 ) > 0 : ... rand_type [ idxs2 [ 0 ], col ] = RandType . R2 ... return idxs2 ... if len ( idxs1 ) > 0 : ... rand_type [ idxs1 [ 0 ], col ] = RandType . R1 ... return idxs1 ... return temp_idx_arr1 [: 0 ] >>> # Build signal generator >>> MySignals = SignalFactory ( ... mode = 'chain' , ... in_output_names = [ 'rand_type' ], ... param_names = [ 'prob1' , 'prob2' ], ... attr_settings = dict ( ... rand_type = dict ( dtype = RandType ) # creates rand_type_readable ... ) ... ) . from_choice_func ( ... exit_choice_func = rand_exit_choice_nb , ... exit_settings = dict ( ... pass_in_outputs = [ 'rand_type' ], ... pass_params = [ 'prob1' , 'prob2' ], ... pass_kwargs = [ 'temp_idx_arr1' , 'temp_idx_arr2' , 'flex_2d' ] ... ), ... param_settings = dict ( ... prob1 = flex_elem_param_config , # param per frame/row/col/element ... prob2 = flex_elem_param_config ... ), ... pass_flex_2d = True , ... rand_type =- 1 # fill with this value ... ) >>> # Run signal generator >>> entries = [ True , True , True , True , True ] >>> my_sig = MySignals . run ( entries , [ 0. , 1. ], [ 0. , 1. ], param_product = True ) >>> my_sig . new_entries custom_prob1 0.0 1.0 custom_prob2 0.0 1.0 0.0 1.0 0 True True True True 1 False False False False 2 False True True True 3 False False False False 4 False True True True >>> my_sig . exits custom_prob1 0.0 1.0 custom_prob2 0.0 1.0 0.0 1.0 0 False False False False 1 False True True True 2 False False False False 3 False True True True 4 False False False False >>> my_sig . rand_type_readable custom_prob1 0.0 1.0 custom_prob2 0.0 1.0 0.0 1.0 0 1 R2 R1 R1 2 3 R2 R1 R1 4","title":"factory"},{"location":"api/signals/factory/#vectorbt.signals.factory","text":"A factory for building new signal generators with ease. The signal factory class SignalFactory extends IndicatorFactory to offer a convenient way to create signal generators of any complexity. By providing it with information such as entry and exit functions and the names of inputs, parameters, and outputs, it will create a stand-alone class capable of generating signals for an arbitrary combination of inputs and parameters.","title":"vectorbt.signals.factory"},{"location":"api/signals/factory/#vectorbt.signals.factory.SignalFactory","text":"A factory for building signal generators. Extends IndicatorFactory with choice functions. Generates a fixed number of outputs (depending upon mode ). If you need to generate other outputs, use in-place outputs (via in_output_names ). See FactoryMode for supported generation modes. Other arguments are passed to IndicatorFactory . A factory for creating new indicators. Initialize `IndicatorFactory` to create a skeleton and then use a class method such as `IndicatorFactory.from_custom_func` to bind a calculation function to the skeleton. __Args__ **```class_name```** :&ensp;`str` : Name for the created indicator class. **```class_docstring```** :&ensp;`str` : Docstring for the created indicator class. **```module_name```** :&ensp;`str` : Specify the module the class originates from. **```short_name```** :&ensp;`str` : A short name of the indicator. Defaults to lower-case `class_name`. **```prepend_name```** :&ensp;`bool` : Whether to prepend `short_name` to each parameter level. **```input_names```** :&ensp;`list` of `str` : A list of names of input arrays. **```param_names```** :&ensp;`list` of `str` : A list of names of parameters. **```in_output_names```** :&ensp;`list` of `str` : A list of names of in-place output arrays. An in-place output is an output that is not returned but modified in-place. Some advantages of such outputs include: 1) they don't need to be returned, 2) they can be passed between functions as easily as inputs, 3) they can be provided with already allocated data to safe memory, 4) if data or default value are not provided, they are created empty to not occupy memory. **```output_names```** :&ensp;`list` of `str` : A list of names of output arrays. **```output_flags```** :&ensp;`dict` : A dictionary of in-place and regular output flags. **```custom_output_props```** :&ensp;`dict` : A dictionary with user-defined functions that will be bound to the indicator class and wrapped with `@cached_property`. **```attr_settings```** :&ensp;`dict` : A dictionary of settings by attribute name. Attributes can be `input_names`, `in_output_names`, `output_names` and `custom_output_props`. Following keys are accepted: * `dtype`: Data type used to determine which methods to generate around this attribute. Set to None to disable. Default is `np.float_`. Can be set to instance of `collections.namedtuple` acting as enumerated type, or any other mapping; It will then create a property with suffix `readable` that contains data in a string format. **```metrics```** :&ensp;`dict` : Metrics supported by [StatsBuilderMixin.stats()](/api/generic/stats_builder/#vectorbt.generic.stats_builder.StatsBuilderMixin.stats \"vectorbt.generic.stats_builder.StatsBuilderMixin.stats\"). If dict, will be converted to [Config](/api/utils/config/#vectorbt.utils.config.Config \"vectorbt.utils.config.Config\"). **```stats_defaults```** :&ensp;`callable` or `dict` : Defaults for [StatsBuilderMixin.stats()](/api/generic/stats_builder/#vectorbt.generic.stats_builder.StatsBuilderMixin.stats \"vectorbt.generic.stats_builder.StatsBuilderMixin.stats\"). If dict, will be converted into a property. **```subplots```** :&ensp;`dict` : Subplots supported by [PlotsBuilderMixin.plots()](/api/generic/plots_builder/#vectorbt.generic.plots_builder.PlotsBuilderMixin.plots \"vectorbt.generic.plots_builder.PlotsBuilderMixin.plots\"). If dict, will be converted to [Config](/api/utils/config/#vectorbt.utils.config.Config \"vectorbt.utils.config.Config\"). **```plots_defaults```** :&ensp;`callable` or `dict` : Defaults for [PlotsBuilderMixin.plots()](/api/generic/plots_builder/#vectorbt.generic.plots_builder.PlotsBuilderMixin.plots \"vectorbt.generic.plots_builder.PlotsBuilderMixin.plots\"). If dict, will be converted into a property. !!! note The `__init__` method is not used for running the indicator, for this use `run`. The reason for this is indexing, which requires a clean `__init__` method for creating a new indicator object with newly indexed attributes. __Superclasses__ * [IndicatorFactory](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory \"vectorbt.indicators.factory.IndicatorFactory\") __Inherited members__ * [IndicatorFactory.find_ta_indicator()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.find_ta_indicator \"vectorbt.indicators.factory.IndicatorFactory.find_ta_indicator\") * [IndicatorFactory.from_apply_func()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.from_apply_func \"vectorbt.indicators.factory.IndicatorFactory.from_apply_func\") * [IndicatorFactory.from_custom_func()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.from_custom_func \"vectorbt.indicators.factory.IndicatorFactory.from_custom_func\") * [IndicatorFactory.from_pandas_ta()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.from_pandas_ta \"vectorbt.indicators.factory.IndicatorFactory.from_pandas_ta\") * [IndicatorFactory.from_ta()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.from_ta \"vectorbt.indicators.factory.IndicatorFactory.from_ta\") * [IndicatorFactory.from_talib()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.from_talib \"vectorbt.indicators.factory.IndicatorFactory.from_talib\") * [IndicatorFactory.get_pandas_ta_indicators()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.get_pandas_ta_indicators \"vectorbt.indicators.factory.IndicatorFactory.get_pandas_ta_indicators\") * [IndicatorFactory.get_ta_indicators()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.get_ta_indicators \"vectorbt.indicators.factory.IndicatorFactory.get_ta_indicators\") * [IndicatorFactory.get_talib_indicators()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.get_talib_indicators \"vectorbt.indicators.factory.IndicatorFactory.get_talib_indicators\") * [IndicatorFactory.parse_pandas_ta_config()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.parse_pandas_ta_config \"vectorbt.indicators.factory.IndicatorFactory.parse_pandas_ta_config\") * [IndicatorFactory.parse_ta_config()](/api/indicators/factory/#vectorbt.indicators.factory.IndicatorFactory.parse_ta_config \"vectorbt.indicators.factory.IndicatorFactory.parse_ta_config\") --- ### from_choice_func <span class=\"dobjtype\">method</span><a class=\"githublink\" href=\"https://github.com/polakowo/vectorbt/blob/b7b6e48cd3a7fbc185980f6b83f3c15b2ad0d4b6/vectorbt/signals/factory.py#L165-L950\" target=\"_blank\" title=\"Jump to source\">:material-github:</a> { #vectorbt.signals.factory.SignalFactory.from_choice_func data-toc-label='from_choice_func()' } ```python SignalFactory.from_choice_func( entry_choice_func=None, exit_choice_func=None, generate_func=generate_nb, generate_ex_func=generate_ex_nb, generate_enex_func=generate_enex_nb, cache_func=None, entry_settings=None, exit_settings=None, cache_settings=None, numba_loop=False, **kwargs ) Build signal generator class around entry and exit choice functions. A choice function is simply a function that returns indices of signals. There are two types of it: entry choice function and exit choice function. Each choice function takes broadcast time series, broadcast in-place output time series, broadcast parameter arrays, and other arguments, and returns an array of indices corresponding to chosen signals. See generate_nb() . Args entry_choice_func :\u2002 callable choice_func_nb that returns indices of entries. Defaults to first_choice_nb() for FactoryMode.Chain . exit_choice_func :\u2002 callable choice_func_nb that returns indices of exits. generate_func :\u2002 callable Entry generation function. Defaults to generate_nb() . generate_ex_func :\u2002 callable Exit generation function. Defaults to generate_ex_nb() . generate_enex_func :\u2002 callable Entry and exit generation function. Defaults to generate_enex_nb() . cache_func :\u2002 callable A caching function to preprocess data beforehand. All returned objects will be passed as last arguments to choice functions. entry_settings :\u2002 dict Settings dict for entry_choice_func . exit_settings :\u2002 dict Settings dict for exit_choice_func . cache_settings :\u2002 dict Settings dict for cache_func . numba_loop :\u2002 bool Whether to loop using Numba. Set to True when iterating large number of times over small input. **kwargs Keyword arguments passed to IndicatorFactory.from_custom_func . Note Choice functions should be Numba-compiled. Which inputs, parameters and arguments to pass to each function should be explicitly indicated in the function's settings dict. By default, nothing is passed. Passing keyword arguments directly to the choice functions is not supported. Use pass_kwargs in a settings dict to pass keyword arguments as positional. Settings dict of each function can have the following keys: Attributes pass_inputs :\u2002 list of str Input names to pass to the choice function. Defaults to []. Order matters. Each name must be in input_names . pass_in_outputs :\u2002 list of str In-place output names to pass to the choice function. Defaults to []. Order matters. Each name must be in in_output_names . pass_params :\u2002 list of str Parameter names to pass to the choice function. Defaults to []. Order matters. Each name must be in param_names . pass_kwargs :\u2002 dict , list of str or list of tuple Keyword arguments from kwargs dict to pass as positional arguments to the choice function. Defaults to []. Order matters. If any element is a tuple, should contain the name and the default value. If any element is a string, the default value is None. Built-in keys include: input_shape : Input shape if no input time series passed. Default is provided by the pipeline if pass_input_shape is True. wait : Number of ticks to wait before placing signals. Default is 1. until_next : Whether to place signals up to the next entry signal. Default is True. Applied in generate_ex_func only. * skip_until_exit : Whether to skip processing entry signals until the next exit. Default is False. Applied in generate_ex_func only. * pick_first : Whether to stop as soon as the first exit signal is found. Default is False with FactoryMode.Entries , otherwise is True. * temp_idx_arr : Empty integer array used to temporarily store indices. Default is an automatically generated array of shape input_shape[0] . You can also pass temp_idx_arr1 , temp_idx_arr2 , etc. to generate multiple. * flex_2d : See flex_select_auto_nb() . Default is provided by the pipeline if pass_flex_2d is True. pass_cache :\u2002 bool Whether to pass cache from cache_func to the choice function. Defaults to False. Cache is passed unpacked. The following arguments can be passed to run and run_combs methods: Args *args Should be used instead of entry_args with FactoryMode.Entries and instead of exit_args with FactoryMode.Exits and FactoryMode.Chain with default entry_choice_func . entry_args :\u2002 tuple Arguments passed to the entry choice function. exit_args :\u2002 tuple Arguments passed to the exit choice function. cache_args :\u2002 tuple Arguments passed to the cache function. entry_kwargs :\u2002 tuple Settings for the entry choice function. Also contains arguments passed as positional if in pass_kwargs . exit_kwargs :\u2002 tuple Settings for the exit choice function. Also contains arguments passed as positional if in pass_kwargs . cache_kwargs :\u2002 tuple Settings for the cache function. Also contains arguments passed as positional if in pass_kwargs . return_cache :\u2002 bool Whether to return only cache. use_cache :\u2002 any Cache to use. **kwargs Should be used instead of entry_kwargs with FactoryMode.Entries and instead of exit_kwargs with FactoryMode.Exits and FactoryMode.Chain with default entry_choice_func . For more arguments, see run_pipeline() . Usage The simplest signal indicator that places True at the very first index: >>> from numba import njit >>> import vectorbt as vbt >>> import numpy as np >>> @njit ... def entry_choice_func ( from_i , to_i , col ): ... return np . array ([ from_i ]) >>> @njit ... def exit_choice_func ( from_i , to_i , col ): ... return np . array ([ from_i ]) >>> MySignals = vbt . SignalFactory () . from_choice_func ( ... entry_choice_func = entry_choice_func , ... exit_choice_func = exit_choice_func , ... entry_kwargs = dict ( wait = 1 ), ... exit_kwargs = dict ( wait = 1 ) ... ) >>> my_sig = MySignals . run ( input_shape = ( 3 , 3 )) >>> my_sig . entries 0 1 2 0 True True True 1 False False False 2 True True True >>> my_sig . exits 0 1 2 0 False False False 1 True True True 2 False False False Take the first entry and place an exit after waiting n ticks. Find the next entry and repeat. Test three different n values. >>> from numba import njit >>> from vectorbt.signals.factory import SignalFactory >>> @njit ... def wait_choice_nb ( from_i , to_i , col , n , temp_idx_arr ): ... temp_idx_arr [ 0 ] = from_i + n # index of next exit ... if temp_idx_arr [ 0 ] < to_i : ... return temp_idx_arr [: 1 ] ... return temp_idx_arr [: 0 ] # must return array anyway >>> # Build signal generator >>> MySignals = SignalFactory ( ... mode = 'chain' , ... param_names = [ 'n' ] ... ) . from_choice_func ( ... exit_choice_func = wait_choice_nb , ... exit_settings = dict ( ... pass_params = [ 'n' ], ... pass_kwargs = [ 'temp_idx_arr' ] # built-in kwarg ... ) ... ) >>> # Run signal generator >>> entries = [ True , True , True , True , True ] >>> my_sig = MySignals . run ( entries , [ 0 , 1 , 2 ]) >>> my_sig . entries # input entries custom_n 0 1 2 0 True True True 1 True True True 2 True True True 3 True True True 4 True True True >>> my_sig . new_entries # output entries custom_n 0 1 2 0 True True True 1 False False False 2 True False False 3 False True False 4 True False True >>> my_sig . exits # output exits custom_n 0 1 2 0 False False False 1 True False False 2 False True False 3 True False True 4 False False False To combine multiple iterative signals, you would need to create a custom choice function. Here is an example of combining two random generators using \"OR\" rule (the first signal wins): >>> from numba import njit >>> from collections import namedtuple >>> from vectorbt.indicators.configs import flex_elem_param_config >>> from vectorbt.signals.factory import SignalFactory >>> from vectorbt.signals.nb import rand_by_prob_choice_nb >>> # Enum to distinguish random generators >>> RandType = namedtuple ( 'RandType' , [ 'R1' , 'R2' ])( 0 , 1 ) >>> # Define exit choice function >>> @njit ... def rand_exit_choice_nb ( from_i , to_i , col , rand_type , prob1 , ... prob2 , temp_idx_arr1 , temp_idx_arr2 , flex_2d ): ... idxs1 = rand_by_prob_choice_nb ( from_i , to_i , col , prob1 , True , temp_idx_arr1 , flex_2d ) ... if len ( idxs1 ) > 0 : ... to_i = idxs1 [ 0 ] # no need to go beyond first the first found signal ... idxs2 = rand_by_prob_choice_nb ( from_i , to_i , col , prob2 , True , temp_idx_arr2 , flex_2d ) ... if len ( idxs2 ) > 0 : ... rand_type [ idxs2 [ 0 ], col ] = RandType . R2 ... return idxs2 ... if len ( idxs1 ) > 0 : ... rand_type [ idxs1 [ 0 ], col ] = RandType . R1 ... return idxs1 ... return temp_idx_arr1 [: 0 ] >>> # Build signal generator >>> MySignals = SignalFactory ( ... mode = 'chain' , ... in_output_names = [ 'rand_type' ], ... param_names = [ 'prob1' , 'prob2' ], ... attr_settings = dict ( ... rand_type = dict ( dtype = RandType ) # creates rand_type_readable ... ) ... ) . from_choice_func ( ... exit_choice_func = rand_exit_choice_nb , ... exit_settings = dict ( ... pass_in_outputs = [ 'rand_type' ], ... pass_params = [ 'prob1' , 'prob2' ], ... pass_kwargs = [ 'temp_idx_arr1' , 'temp_idx_arr2' , 'flex_2d' ] ... ), ... param_settings = dict ( ... prob1 = flex_elem_param_config , # param per frame/row/col/element ... prob2 = flex_elem_param_config ... ), ... pass_flex_2d = True , ... rand_type =- 1 # fill with this value ... ) >>> # Run signal generator >>> entries = [ True , True , True , True , True ] >>> my_sig = MySignals . run ( entries , [ 0. , 1. ], [ 0. , 1. ], param_product = True ) >>> my_sig . new_entries custom_prob1 0.0 1.0 custom_prob2 0.0 1.0 0.0 1.0 0 True True True True 1 False False False False 2 False True True True 3 False False False False 4 False True True True >>> my_sig . exits custom_prob1 0.0 1.0 custom_prob2 0.0 1.0 0.0 1.0 0 False False False False 1 False True True True 2 False False False False 3 False True True True 4 False False False False >>> my_sig . rand_type_readable custom_prob1 0.0 1.0 custom_prob2 0.0 1.0 0.0 1.0 0 1 R2 R1 R1 2 3 R2 R1 R1 4","title":"SignalFactory"},{"location":"api/signals/generators/","text":"generators module \u00b6 Signal generators built with SignalFactory . ohlcstx_config Config \u00b6 Factory config for OHLCSTX . ohlcstx_func_config Config \u00b6 Exit function config for OHLCSTX . rprobx_config Config \u00b6 Factory config for RPROBX . rprobx_func_config Config \u00b6 Exit function config for RPROBX . stx_config Config \u00b6 Factory config for STX . stx_func_config Config \u00b6 Exit function config for STX . OHLCSTCX class \u00b6 Exit signal generator based on OHLC and stop values. Generates chain of new_entries and exits based on entries and ohlc_stop_choice_nb() . See OHLCSTX for notes on parameters. Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._OHLCSTCX close method \u00b6 Input array. close_above method \u00b6 OHLCSTCX . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 OHLCSTCX . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 OHLCSTCX . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 OHLCSTCX . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 OHLCSTCX . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 OHLCSTCX . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs ) entries method \u00b6 Input array. entries_and method \u00b6 OHLCSTCX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() . entries_or method \u00b6 OHLCSTCX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() . entries_stats method \u00b6 OHLCSTCX . entries_stats ( * args , ** kwargs ) Stats of entries as signals. entries_xor method \u00b6 OHLCSTCX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() . exits property \u00b6 Output array. exits_and method \u00b6 OHLCSTCX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() . exits_or method \u00b6 OHLCSTCX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() . exits_stats method \u00b6 OHLCSTCX . exits_stats ( * args , ** kwargs ) Stats of exits as signals. exits_xor method \u00b6 OHLCSTCX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() . high method \u00b6 Input array. high_above method \u00b6 OHLCSTCX . high_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is above other . See combine_objs() . high_below method \u00b6 OHLCSTCX . high_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is below other . See combine_objs() . high_crossed_above method \u00b6 OHLCSTCX . high_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is crossed_above other . See combine_objs() . high_crossed_below method \u00b6 OHLCSTCX . high_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is crossed_below other . See combine_objs() . high_equal method \u00b6 OHLCSTCX . high_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is equal other . See combine_objs() . high_stats method \u00b6 OHLCSTCX . high_stats ( * args , ** kwargs ) Stats of high as generic. low method \u00b6 Input array. low_above method \u00b6 OHLCSTCX . low_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is above other . See combine_objs() . low_below method \u00b6 OHLCSTCX . low_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is below other . See combine_objs() . low_crossed_above method \u00b6 OHLCSTCX . low_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is crossed_above other . See combine_objs() . low_crossed_below method \u00b6 OHLCSTCX . low_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is crossed_below other . See combine_objs() . low_equal method \u00b6 OHLCSTCX . low_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is equal other . See combine_objs() . low_stats method \u00b6 OHLCSTCX . low_stats ( * args , ** kwargs ) Stats of low as generic. new_entries property \u00b6 Output array. new_entries_and method \u00b6 OHLCSTCX . new_entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries AND other . See combine_objs() . new_entries_or method \u00b6 OHLCSTCX . new_entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries OR other . See combine_objs() . new_entries_stats method \u00b6 OHLCSTCX . new_entries_stats ( * args , ** kwargs ) Stats of new_entries as signals. new_entries_xor method \u00b6 OHLCSTCX . new_entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries XOR other . See combine_objs() . open method \u00b6 Input array. open_above method \u00b6 OHLCSTCX . open_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is above other . See combine_objs() . open_below method \u00b6 OHLCSTCX . open_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is below other . See combine_objs() . open_crossed_above method \u00b6 OHLCSTCX . open_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is crossed_above other . See combine_objs() . open_crossed_below method \u00b6 OHLCSTCX . open_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is crossed_below other . See combine_objs() . open_equal method \u00b6 OHLCSTCX . open_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is equal other . See combine_objs() . open_stats method \u00b6 OHLCSTCX . open_stats ( * args , ** kwargs ) Stats of open as generic. plot method \u00b6 _bind_ohlcstx_plot .< locals >. plot ( plot_type = None , ohlc_kwargs = None , entry_trace_kwargs = None , exit_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot OHLC, OHLCSTCX.new_entries and OHLCSTCX.exits . Args plot_type Either 'OHLC', 'Candlestick' or Plotly trace. ohlc_kwargs :\u2002 dict Keyword arguments passed to plot_type . entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for OHLCSTCX.new_entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for OHLCSTCX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. reverse_list property \u00b6 List of reverse values. run class method \u00b6 OHLCSTCX . run ( entries , open , high , low , close , sl_stop = Default ( nan ), sl_trail = Default ( False ), tp_stop = Default ( nan ), reverse = Default ( False ), stop_price = nan , stop_type =- 1 , short_name = 'ohlcstcx' , hide_params = None , hide_default = True , ** kwargs ) Run OHLCSTCX indicator. Inputs: entries , open , high , low , close In-place outputs: stop_price , stop_type Parameters: sl_stop , sl_trail , tp_stop , reverse Outputs: new_entries , exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 OHLCSTCX . run_combs ( entries , open , high , low , close , sl_stop = Default ( nan ), sl_trail = Default ( False ), tp_stop = Default ( nan ), reverse = Default ( False ), stop_price = nan , stop_type =- 1 , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple OHLCSTCX indicators using function comb_func . Inputs: entries , open , high , low , close In-place outputs: stop_price , stop_type Parameters: sl_stop , sl_trail , tp_stop , reverse Outputs: new_entries , exits comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to OHLCSTCX.run() . sl_stop_list property \u00b6 List of sl_stop values. sl_trail_list property \u00b6 List of sl_trail values. stop_price property \u00b6 In-place output array. stop_price_above method \u00b6 OHLCSTCX . stop_price_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is above other . See combine_objs() . stop_price_below method \u00b6 OHLCSTCX . stop_price_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is below other . See combine_objs() . stop_price_crossed_above method \u00b6 OHLCSTCX . stop_price_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is crossed_above other . See combine_objs() . stop_price_crossed_below method \u00b6 OHLCSTCX . stop_price_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is crossed_below other . See combine_objs() . stop_price_equal method \u00b6 OHLCSTCX . stop_price_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is equal other . See combine_objs() . stop_price_stats method \u00b6 OHLCSTCX . stop_price_stats ( * args , ** kwargs ) Stats of stop_price as generic. stop_type property \u00b6 In-place output array. stop_type_readable property \u00b6 stop_type in readable format based on the following mapping: { \"0\" : \"StopLoss\" , \"1\" : \"TrailStop\" , \"2\" : \"TakeProfit\" , \"-1\" : null } stop_type_stats method \u00b6 OHLCSTCX . stop_type_stats ( * args , ** kwargs ) Stats of stop_type based on the following mapping: { \"0\" : \"StopLoss\" , \"1\" : \"TrailStop\" , \"2\" : \"TakeProfit\" , \"-1\" : null } tp_stop_list property \u00b6 List of tp_stop values. OHLCSTX class \u00b6 Exit signal generator based on OHLC and stop values. Generates exits based on entries and ohlc_stop_choice_nb() . Hint All parameters can be either a single value (per frame) or a NumPy array (per row, column, or element). To generate multiple combinations, pass them as lists. Usage Test each stop type: >>> import vectorbt as vbt >>> import pandas as pd >>> import numpy as np >>> entries = pd . Series ([ True , False , False , False , False , False ]) >>> price = pd . DataFrame ({ ... 'open' : [ 10 , 11 , 12 , 11 , 10 , 9 ], ... 'high' : [ 11 , 12 , 13 , 12 , 11 , 10 ], ... 'low' : [ 9 , 10 , 11 , 10 , 9 , 8 ], ... 'close' : [ 10 , 11 , 12 , 11 , 10 , 9 ] ... }) >>> ohlcstx = vbt . OHLCSTX . run ( ... entries , ... price [ 'open' ], price [ 'high' ], price [ 'low' ], price [ 'close' ], ... sl_stop = [ 0.1 , 0.1 , np . nan ], ... sl_trail = [ False , True , False ], ... tp_stop = [ np . nan , np . nan , 0.1 ]) >>> ohlcstx . entries ohlcstx_sl_stop 0.1 0.1 NaN ohlcstx_sl_trail False True False ohlcstx_tp_stop NaN NaN 0.1 0 True True True 1 False False False 2 False False False 3 False False False 4 False False False 5 False False False >>> ohlcstx . exits ohlcstx_sl_stop 0.1 0.1 NaN ohlcstx_sl_trail False True False ohlcstx_tp_stop NaN NaN 0.1 0 False False False 1 False False True 2 False False False 3 False True False 4 True False False 5 False False False >>> ohlcstx . stop_price ohlcstx_sl_stop 0.1 0.1 NaN ohlcstx_sl_trail False True False ohlcstx_tp_stop NaN NaN 0.1 0 NaN NaN NaN 1 NaN NaN 11.0 2 NaN NaN NaN 3 NaN 11.7 NaN 4 9.0 NaN NaN 5 NaN NaN NaN >>> ohlcstx . stop_type_readable ohlcstx_sl_stop 0.1 0.1 NaN ohlcstx_sl_trail False True False ohlcstx_tp_stop NaN NaN 0.1 0 None None None 1 None None TakeProfit 2 None None None 3 None TrailStop None 4 StopLoss None None 5 None None None Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._OHLCSTX close method \u00b6 Input array. close_above method \u00b6 OHLCSTX . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() . close_below method \u00b6 OHLCSTX . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() . close_crossed_above method \u00b6 OHLCSTX . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() . close_crossed_below method \u00b6 OHLCSTX . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() . close_equal method \u00b6 OHLCSTX . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() . close_stats method \u00b6 OHLCSTX . close_stats ( * args , ** kwargs ) Stats of close as generic. custom_func method \u00b6 SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs ) entries method \u00b6 Input array. entries_and method \u00b6 OHLCSTX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() . entries_or method \u00b6 OHLCSTX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() . entries_stats method \u00b6 OHLCSTX . entries_stats ( * args , ** kwargs ) Stats of entries as signals. entries_xor method \u00b6 OHLCSTX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() . exits property \u00b6 Output array. exits_and method \u00b6 OHLCSTX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() . exits_or method \u00b6 OHLCSTX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() . exits_stats method \u00b6 OHLCSTX . exits_stats ( * args , ** kwargs ) Stats of exits as signals. exits_xor method \u00b6 OHLCSTX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() . high method \u00b6 Input array. high_above method \u00b6 OHLCSTX . high_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is above other . See combine_objs() . high_below method \u00b6 OHLCSTX . high_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is below other . See combine_objs() . high_crossed_above method \u00b6 OHLCSTX . high_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is crossed_above other . See combine_objs() . high_crossed_below method \u00b6 OHLCSTX . high_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is crossed_below other . See combine_objs() . high_equal method \u00b6 OHLCSTX . high_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is equal other . See combine_objs() . high_stats method \u00b6 OHLCSTX . high_stats ( * args , ** kwargs ) Stats of high as generic. low method \u00b6 Input array. low_above method \u00b6 OHLCSTX . low_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is above other . See combine_objs() . low_below method \u00b6 OHLCSTX . low_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is below other . See combine_objs() . low_crossed_above method \u00b6 OHLCSTX . low_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is crossed_above other . See combine_objs() . low_crossed_below method \u00b6 OHLCSTX . low_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is crossed_below other . See combine_objs() . low_equal method \u00b6 OHLCSTX . low_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is equal other . See combine_objs() . low_stats method \u00b6 OHLCSTX . low_stats ( * args , ** kwargs ) Stats of low as generic. open method \u00b6 Input array. open_above method \u00b6 OHLCSTX . open_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is above other . See combine_objs() . open_below method \u00b6 OHLCSTX . open_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is below other . See combine_objs() . open_crossed_above method \u00b6 OHLCSTX . open_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is crossed_above other . See combine_objs() . open_crossed_below method \u00b6 OHLCSTX . open_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is crossed_below other . See combine_objs() . open_equal method \u00b6 OHLCSTX . open_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is equal other . See combine_objs() . open_stats method \u00b6 OHLCSTX . open_stats ( * args , ** kwargs ) Stats of open as generic. plot method \u00b6 _bind_ohlcstx_plot .< locals >. plot ( plot_type = None , ohlc_kwargs = None , entry_trace_kwargs = None , exit_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot OHLC, OHLCSTX.entries and OHLCSTX.exits . Args plot_type Either 'OHLC', 'Candlestick' or Plotly trace. ohlc_kwargs :\u2002 dict Keyword arguments passed to plot_type . entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for OHLCSTX.entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for OHLCSTX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> ohlcstx . iloc [:, 0 ] . plot () reverse_list property \u00b6 List of reverse values. run class method \u00b6 OHLCSTX . run ( entries , open , high , low , close , sl_stop = Default ( nan ), sl_trail = Default ( False ), tp_stop = Default ( nan ), reverse = Default ( False ), stop_price = nan , stop_type =- 1 , short_name = 'ohlcstx' , hide_params = None , hide_default = True , ** kwargs ) Run OHLCSTX indicator. Inputs: entries , open , high , low , close In-place outputs: stop_price , stop_type Parameters: sl_stop , sl_trail , tp_stop , reverse Outputs: exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 OHLCSTX . run_combs ( entries , open , high , low , close , sl_stop = Default ( nan ), sl_trail = Default ( False ), tp_stop = Default ( nan ), reverse = Default ( False ), stop_price = nan , stop_type =- 1 , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple OHLCSTX indicators using function comb_func . Inputs: entries , open , high , low , close In-place outputs: stop_price , stop_type Parameters: sl_stop , sl_trail , tp_stop , reverse Outputs: exits comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to OHLCSTX.run() . sl_stop_list property \u00b6 List of sl_stop values. sl_trail_list property \u00b6 List of sl_trail values. stop_price property \u00b6 In-place output array. stop_price_above method \u00b6 OHLCSTX . stop_price_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is above other . See combine_objs() . stop_price_below method \u00b6 OHLCSTX . stop_price_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is below other . See combine_objs() . stop_price_crossed_above method \u00b6 OHLCSTX . stop_price_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is crossed_above other . See combine_objs() . stop_price_crossed_below method \u00b6 OHLCSTX . stop_price_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is crossed_below other . See combine_objs() . stop_price_equal method \u00b6 OHLCSTX . stop_price_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is equal other . See combine_objs() . stop_price_stats method \u00b6 OHLCSTX . stop_price_stats ( * args , ** kwargs ) Stats of stop_price as generic. stop_type property \u00b6 In-place output array. stop_type_readable property \u00b6 stop_type in readable format based on the following mapping: { \"0\" : \"StopLoss\" , \"1\" : \"TrailStop\" , \"2\" : \"TakeProfit\" , \"-1\" : null } stop_type_stats method \u00b6 OHLCSTX . stop_type_stats ( * args , ** kwargs ) Stats of stop_type based on the following mapping: { \"0\" : \"StopLoss\" , \"1\" : \"TrailStop\" , \"2\" : \"TakeProfit\" , \"-1\" : null } tp_stop_list property \u00b6 List of tp_stop values. RAND class \u00b6 Random entry signal generator based on the number of signals. Generates entries based on rand_choice_nb() . Hint Parameter n can be either a single value (per frame) or a NumPy array (per column). To generate multiple combinations, pass it as a list. Usage Test three different entry counts values: >>> import vectorbt as vbt >>> rand = vbt . RAND . run ( input_shape = ( 6 ,), n = [ 1 , 2 , 3 ], seed = 42 ) >>> rand . entries rand_n 1 2 3 0 True True True 1 False False True 2 False False False 3 False True False 4 False False True 5 False False False Entry count can also be set per column: >>> import numpy as np >>> rand = vbt . RAND . run ( input_shape = ( 8 , 2 ), n = [ np . array ([ 1 , 2 ]), 3 ], seed = 42 ) >>> rand . entries rand_n 1 2 3 3 0 1 0 1 0 False False True False 1 True False False False 2 False False False True 3 False True True False 4 False False False False 5 False False False True 6 False False True False 7 False True False True Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._RAND custom_func method \u00b6 SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs ) entries property \u00b6 Output array. entries_and method \u00b6 RAND . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() . entries_or method \u00b6 RAND . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() . entries_stats method \u00b6 RAND . entries_stats ( * args , ** kwargs ) Stats of entries as signals. entries_xor method \u00b6 RAND . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() . n_list property \u00b6 List of n values. plot method \u00b6 SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot RAND.entries and RAND.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for RAND.entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for RAND.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() . run class method \u00b6 RAND . run ( input_shape , n , short_name = 'rand' , hide_params = None , hide_default = True , ** kwargs ) Run RAND indicator. Parameters: n Outputs: entries Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 RAND . run_combs ( input_shape , n , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple RAND indicators using function comb_func . Parameters: n Outputs: entries comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to RAND.run() . RANDNX class \u00b6 Random entry and exit signal generator based on the number of signals. Generates entries and exits based on rand_enex_apply_nb() . See RAND for notes on parameters. Usage Test three different entry and exit counts: >>> import vectorbt as vbt >>> randnx = vbt . RANDNX . run ( ... input_shape = ( 6 ,), ... n = [ 1 , 2 , 3 ], ... seed = 42 ) >>> randnx . entries randnx_n 1 2 3 0 True True True 1 False False False 2 False True True 3 False False False 4 False False True 5 False False False >>> randnx . exits randnx_n 1 2 3 0 False False False 1 True True True 2 False False False 3 False True True 4 False False False 5 False False True Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._RANDNX apply_func method \u00b6 RANDNX . apply_func ( input_shape , n , entry_wait , exit_wait ) apply_func_nb that calls generate_rand_enex_nb . custom_func method \u00b6 IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func . entries property \u00b6 Output array. entries_and method \u00b6 RANDNX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() . entries_or method \u00b6 RANDNX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() . entries_stats method \u00b6 RANDNX . entries_stats ( * args , ** kwargs ) Stats of entries as signals. entries_xor method \u00b6 RANDNX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() . exits property \u00b6 Output array. exits_and method \u00b6 RANDNX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() . exits_or method \u00b6 RANDNX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() . exits_stats method \u00b6 RANDNX . exits_stats ( * args , ** kwargs ) Stats of exits as signals. exits_xor method \u00b6 RANDNX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() . n_list property \u00b6 List of n values. plot method \u00b6 SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot RANDNX.entries and RANDNX.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for RANDNX.entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for RANDNX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() . run class method \u00b6 RANDNX . run ( input_shape , n , short_name = 'randnx' , hide_params = None , hide_default = True , ** kwargs ) Run RANDNX indicator. Parameters: n Outputs: entries , exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 RANDNX . run_combs ( input_shape , n , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple RANDNX indicators using function comb_func . Parameters: n Outputs: entries , exits comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to RANDNX.run() . RANDX class \u00b6 Random exit signal generator based on the number of signals. Generates exits based on entries and rand_choice_nb() . See RAND for notes on parameters. Usage Generate an exit for each entry: >>> import vectorbt as vbt >>> import pandas as pd >>> entries = pd . Series ([ True , False , False , True , False , False ]) >>> randx = vbt . RANDX . run ( entries , seed = 42 ) >>> randx . exits 0 False 1 False 2 True 3 False 4 True 5 False dtype: bool Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.run_combs() IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._RANDX custom_func method \u00b6 SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs ) entries method \u00b6 Input array. entries_and method \u00b6 RANDX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() . entries_or method \u00b6 RANDX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() . entries_stats method \u00b6 RANDX . entries_stats ( * args , ** kwargs ) Stats of entries as signals. entries_xor method \u00b6 RANDX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() . exits property \u00b6 Output array. exits_and method \u00b6 RANDX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() . exits_or method \u00b6 RANDX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() . exits_stats method \u00b6 RANDX . exits_stats ( * args , ** kwargs ) Stats of exits as signals. exits_xor method \u00b6 RANDX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() . plot method \u00b6 SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot RANDX.entries and RANDX.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for RANDX.entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for RANDX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() . run class method \u00b6 RANDX . run ( entries , short_name = 'randx' , hide_params = None , hide_default = True , ** kwargs ) Run RANDX indicator. Inputs: entries Outputs: exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . RPROB class \u00b6 Random entry signal generator based on probabilities. Generates entries based on rand_by_prob_choice_nb() . Hint All parameters can be either a single value (per frame) or a NumPy array (per row, column, or element). To generate multiple combinations, pass them as lists. Usage Generate three columns with different entry probabilities: >>> import vectorbt as vbt >>> rprob = vbt . RPROB . run ( input_shape = ( 5 ,), prob = [ 0. , 0.5 , 1. ], seed = 42 ) >>> rprob . entries rprob_prob 0.0 0.5 1.0 0 False True True 1 False True True 2 False False True 3 False False True 4 False False True Probability can also be set per row, column, or element: >>> import numpy as np >>> rprob = vbt . RPROB . run ( input_shape = ( 5 ,), prob = np . array ([ 0. , 0. , 1. , 1. , 1. ]), seed = 42 ) >>> rprob . entries 0 False 1 False 2 True 3 True 4 True Name: array_0, dtype: bool Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._RPROB custom_func method \u00b6 SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs ) entries property \u00b6 Output array. entries_and method \u00b6 RPROB . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() . entries_or method \u00b6 RPROB . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() . entries_stats method \u00b6 RPROB . entries_stats ( * args , ** kwargs ) Stats of entries as signals. entries_xor method \u00b6 RPROB . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() . plot method \u00b6 SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot RPROB.entries and RPROB.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for RPROB.entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for RPROB.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() . prob_list property \u00b6 List of prob values. run class method \u00b6 RPROB . run ( input_shape , prob , short_name = 'rprob' , hide_params = None , hide_default = True , ** kwargs ) Run RPROB indicator. Parameters: prob Outputs: entries Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 RPROB . run_combs ( input_shape , prob , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple RPROB indicators using function comb_func . Parameters: prob Outputs: entries comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to RPROB.run() . RPROBCX class \u00b6 Random exit signal generator based on probabilities. Generates chain of new_entries and exits based on entries and rand_by_prob_choice_nb() . See RPROB for notes on parameters. Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._RPROBCX custom_func method \u00b6 SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs ) entries method \u00b6 Input array. entries_and method \u00b6 RPROBCX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() . entries_or method \u00b6 RPROBCX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() . entries_stats method \u00b6 RPROBCX . entries_stats ( * args , ** kwargs ) Stats of entries as signals. entries_xor method \u00b6 RPROBCX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() . exits property \u00b6 Output array. exits_and method \u00b6 RPROBCX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() . exits_or method \u00b6 RPROBCX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() . exits_stats method \u00b6 RPROBCX . exits_stats ( * args , ** kwargs ) Stats of exits as signals. exits_xor method \u00b6 RPROBCX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() . new_entries property \u00b6 Output array. new_entries_and method \u00b6 RPROBCX . new_entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries AND other . See combine_objs() . new_entries_or method \u00b6 RPROBCX . new_entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries OR other . See combine_objs() . new_entries_stats method \u00b6 RPROBCX . new_entries_stats ( * args , ** kwargs ) Stats of new_entries as signals. new_entries_xor method \u00b6 RPROBCX . new_entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries XOR other . See combine_objs() . plot method \u00b6 SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot RPROBCX.new_entries and RPROBCX.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for RPROBCX.new_entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for RPROBCX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() . prob_list property \u00b6 List of prob values. run class method \u00b6 RPROBCX . run ( entries , prob , short_name = 'rprobcx' , hide_params = None , hide_default = True , ** kwargs ) Run RPROBCX indicator. Inputs: entries Parameters: prob Outputs: new_entries , exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 RPROBCX . run_combs ( entries , prob , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple RPROBCX indicators using function comb_func . Inputs: entries Parameters: prob Outputs: new_entries , exits comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to RPROBCX.run() . RPROBNX class \u00b6 Random entry and exit signal generator based on probabilities. Generates entries and exits based on rand_by_prob_choice_nb() . See RPROB for notes on parameters. Usage Test all probability combinations: >>> import vectorbt as vbt >>> rprobnx = vbt . RPROBNX . run ( ... input_shape = ( 5 ,), ... entry_prob = [ 0.5 , 1. ], ... exit_prob = [ 0.5 , 1. ], ... param_product = True , ... seed = 42 ) >>> rprobnx . entries rprobnx_entry_prob 0.5 0.5 1.0 0.5 rprobnx_exit_prob 0.5 1.0 0.5 1.0 0 True True True True 1 False False False False 2 False False False True 3 False False False False 4 False False True True >>> rprobnx . exits rprobnx_entry_prob 0.5 0.5 1.0 1.0 rprobnx_exit_prob 0.5 1.0 0.5 1.0 0 False False False False 1 False True False True 2 False False False False 3 False False True True 4 True False False False Probabilities can also be set per row, column, or element: >>> import numpy as np >>> entry_prob1 = np . asarray ([ 1. , 0. , 1. , 0. , 1. ]) >>> entry_prob2 = np . asarray ([ 0. , 1. , 0. , 1. , 0. ]) >>> rprobnx = vbt . RPROBNX . run ( ... input_shape = ( 5 ,), ... entry_prob = [ entry_prob1 , entry_prob2 ], ... exit_prob = 1. , ... seed = 42 ) >>> rprobnx . entries rprobnx_entry_prob array_0 array_1 rprobnx_exit_prob 1.0 1.0 0 True False 1 False True 2 True False 3 False True 4 True False >>> rprobnx . exits rprobnx_entry_prob array_0 array_1 rprobnx_exit_prob 1.0 1.0 0 False False 1 True False 2 False True 3 True False 4 False True Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._RPROBNX custom_func method \u00b6 SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs ) entries property \u00b6 Output array. entries_and method \u00b6 RPROBNX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() . entries_or method \u00b6 RPROBNX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() . entries_stats method \u00b6 RPROBNX . entries_stats ( * args , ** kwargs ) Stats of entries as signals. entries_xor method \u00b6 RPROBNX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() . entry_prob_list property \u00b6 List of entry_prob values. exit_prob_list property \u00b6 List of exit_prob values. exits property \u00b6 Output array. exits_and method \u00b6 RPROBNX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() . exits_or method \u00b6 RPROBNX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() . exits_stats method \u00b6 RPROBNX . exits_stats ( * args , ** kwargs ) Stats of exits as signals. exits_xor method \u00b6 RPROBNX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() . plot method \u00b6 SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot RPROBNX.entries and RPROBNX.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for RPROBNX.entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for RPROBNX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() . run class method \u00b6 RPROBNX . run ( input_shape , entry_prob , exit_prob , short_name = 'rprobnx' , hide_params = None , hide_default = True , ** kwargs ) Run RPROBNX indicator. Parameters: entry_prob , exit_prob Outputs: entries , exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 RPROBNX . run_combs ( input_shape , entry_prob , exit_prob , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple RPROBNX indicators using function comb_func . Parameters: entry_prob , exit_prob Outputs: entries , exits comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to RPROBNX.run() . RPROBX class \u00b6 Random exit signal generator based on probabilities. Generates exits based on entries and rand_by_prob_choice_nb() . See RPROB for notes on parameters. Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._RPROBX custom_func method \u00b6 SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs ) entries method \u00b6 Input array. entries_and method \u00b6 RPROBX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() . entries_or method \u00b6 RPROBX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() . entries_stats method \u00b6 RPROBX . entries_stats ( * args , ** kwargs ) Stats of entries as signals. entries_xor method \u00b6 RPROBX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() . exits property \u00b6 Output array. exits_and method \u00b6 RPROBX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() . exits_or method \u00b6 RPROBX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() . exits_stats method \u00b6 RPROBX . exits_stats ( * args , ** kwargs ) Stats of exits as signals. exits_xor method \u00b6 RPROBX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() . plot method \u00b6 SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot RPROBX.entries and RPROBX.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for RPROBX.entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for RPROBX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() . prob_list property \u00b6 List of prob values. run class method \u00b6 RPROBX . run ( entries , prob , short_name = 'rprobx' , hide_params = None , hide_default = True , ** kwargs ) Run RPROBX indicator. Inputs: entries Parameters: prob Outputs: exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 RPROBX . run_combs ( entries , prob , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple RPROBX indicators using function comb_func . Inputs: entries Parameters: prob Outputs: exits comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to RPROBX.run() . STCX class \u00b6 Exit signal generator based on stop values. Generates chain of new_entries and exits based on entries and stop_choice_nb() . See STX for notes on parameters. Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._STCX custom_func method \u00b6 SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs ) entries method \u00b6 Input array. entries_and method \u00b6 STCX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() . entries_or method \u00b6 STCX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() . entries_stats method \u00b6 STCX . entries_stats ( * args , ** kwargs ) Stats of entries as signals. entries_xor method \u00b6 STCX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() . exits property \u00b6 Output array. exits_and method \u00b6 STCX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() . exits_or method \u00b6 STCX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() . exits_stats method \u00b6 STCX . exits_stats ( * args , ** kwargs ) Stats of exits as signals. exits_xor method \u00b6 STCX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() . new_entries property \u00b6 Output array. new_entries_and method \u00b6 STCX . new_entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries AND other . See combine_objs() . new_entries_or method \u00b6 STCX . new_entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries OR other . See combine_objs() . new_entries_stats method \u00b6 STCX . new_entries_stats ( * args , ** kwargs ) Stats of new_entries as signals. new_entries_xor method \u00b6 STCX . new_entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries XOR other . See combine_objs() . plot method \u00b6 SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot STCX.new_entries and STCX.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for STCX.new_entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for STCX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() . run class method \u00b6 STCX . run ( entries , ts , stop , trailing = Default ( False ), short_name = 'stcx' , hide_params = None , hide_default = True , ** kwargs ) Run STCX indicator. Inputs: entries , ts Parameters: stop , trailing Outputs: new_entries , exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 STCX . run_combs ( entries , ts , stop , trailing = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple STCX indicators using function comb_func . Inputs: entries , ts Parameters: stop , trailing Outputs: new_entries , exits comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to STCX.run() . stop_list property \u00b6 List of stop values. trailing_list property \u00b6 List of trailing values. ts method \u00b6 Input array. ts_above method \u00b6 STCX . ts_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is above other . See combine_objs() . ts_below method \u00b6 STCX . ts_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is below other . See combine_objs() . ts_crossed_above method \u00b6 STCX . ts_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is crossed_above other . See combine_objs() . ts_crossed_below method \u00b6 STCX . ts_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is crossed_below other . See combine_objs() . ts_equal method \u00b6 STCX . ts_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is equal other . See combine_objs() . ts_stats method \u00b6 STCX . ts_stats ( * args , ** kwargs ) Stats of ts as generic. STX class \u00b6 Exit signal generator based on stop values. Generates exits based on entries and stop_choice_nb() . Hint All parameters can be either a single value (per frame) or a NumPy array (per row, column, or element). To generate multiple combinations, pass them as lists. Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._STX custom_func method \u00b6 SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs ) entries method \u00b6 Input array. entries_and method \u00b6 STX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() . entries_or method \u00b6 STX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() . entries_stats method \u00b6 STX . entries_stats ( * args , ** kwargs ) Stats of entries as signals. entries_xor method \u00b6 STX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() . exits property \u00b6 Output array. exits_and method \u00b6 STX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() . exits_or method \u00b6 STX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() . exits_stats method \u00b6 STX . exits_stats ( * args , ** kwargs ) Stats of exits as signals. exits_xor method \u00b6 STX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() . plot method \u00b6 SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot STX.entries and STX.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for STX.entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for STX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() . run class method \u00b6 STX . run ( entries , ts , stop , trailing = Default ( False ), short_name = 'stx' , hide_params = None , hide_default = True , ** kwargs ) Run STX indicator. Inputs: entries , ts Parameters: stop , trailing Outputs: exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() . run_combs class method \u00b6 STX . run_combs ( entries , ts , stop , trailing = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple STX indicators using function comb_func . Inputs: entries , ts Parameters: stop , trailing Outputs: exits comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to STX.run() . stop_list property \u00b6 List of stop values. trailing_list property \u00b6 List of trailing values. ts method \u00b6 Input array. ts_above method \u00b6 STX . ts_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is above other . See combine_objs() . ts_below method \u00b6 STX . ts_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is below other . See combine_objs() . ts_crossed_above method \u00b6 STX . ts_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is crossed_above other . See combine_objs() . ts_crossed_below method \u00b6 STX . ts_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is crossed_below other . See combine_objs() . ts_equal method \u00b6 STX . ts_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is equal other . See combine_objs() . ts_stats method \u00b6 STX . ts_stats ( * args , ** kwargs ) Stats of ts as generic.","title":"generators"},{"location":"api/signals/generators/#vectorbt.signals.generators","text":"Signal generators built with SignalFactory .","title":"vectorbt.signals.generators"},{"location":"api/signals/generators/#vectorbt.signals.generators.ohlcstx_config","text":"Factory config for OHLCSTX .","title":"ohlcstx_config"},{"location":"api/signals/generators/#vectorbt.signals.generators.ohlcstx_func_config","text":"Exit function config for OHLCSTX .","title":"ohlcstx_func_config"},{"location":"api/signals/generators/#vectorbt.signals.generators.rprobx_config","text":"Factory config for RPROBX .","title":"rprobx_config"},{"location":"api/signals/generators/#vectorbt.signals.generators.rprobx_func_config","text":"Exit function config for RPROBX .","title":"rprobx_func_config"},{"location":"api/signals/generators/#vectorbt.signals.generators.stx_config","text":"Factory config for STX .","title":"stx_config"},{"location":"api/signals/generators/#vectorbt.signals.generators.stx_func_config","text":"Exit function config for STX .","title":"stx_func_config"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX","text":"Exit signal generator based on OHLC and stop values. Generates chain of new_entries and exits based on entries and ohlc_stop_choice_nb() . See OHLCSTX for notes on parameters. Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._OHLCSTCX","title":"OHLCSTCX"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.close","text":"Input array.","title":"close"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.close_above","text":"OHLCSTCX . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.close_below","text":"OHLCSTCX . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.close_crossed_above","text":"OHLCSTCX . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.close_crossed_below","text":"OHLCSTCX . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.close_equal","text":"OHLCSTCX . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.close_stats","text":"OHLCSTCX . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.custom_func","text":"SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs )","title":"custom_func()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.entries","text":"Input array.","title":"entries"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.entries_and","text":"OHLCSTCX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() .","title":"entries_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.entries_or","text":"OHLCSTCX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() .","title":"entries_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.entries_stats","text":"OHLCSTCX . entries_stats ( * args , ** kwargs ) Stats of entries as signals.","title":"entries_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.entries_xor","text":"OHLCSTCX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() .","title":"entries_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.exits","text":"Output array.","title":"exits"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.exits_and","text":"OHLCSTCX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() .","title":"exits_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.exits_or","text":"OHLCSTCX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() .","title":"exits_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.exits_stats","text":"OHLCSTCX . exits_stats ( * args , ** kwargs ) Stats of exits as signals.","title":"exits_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.exits_xor","text":"OHLCSTCX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() .","title":"exits_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.high","text":"Input array.","title":"high"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.high_above","text":"OHLCSTCX . high_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is above other . See combine_objs() .","title":"high_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.high_below","text":"OHLCSTCX . high_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is below other . See combine_objs() .","title":"high_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.high_crossed_above","text":"OHLCSTCX . high_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is crossed_above other . See combine_objs() .","title":"high_crossed_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.high_crossed_below","text":"OHLCSTCX . high_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is crossed_below other . See combine_objs() .","title":"high_crossed_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.high_equal","text":"OHLCSTCX . high_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is equal other . See combine_objs() .","title":"high_equal()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.high_stats","text":"OHLCSTCX . high_stats ( * args , ** kwargs ) Stats of high as generic.","title":"high_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.low","text":"Input array.","title":"low"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.low_above","text":"OHLCSTCX . low_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is above other . See combine_objs() .","title":"low_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.low_below","text":"OHLCSTCX . low_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is below other . See combine_objs() .","title":"low_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.low_crossed_above","text":"OHLCSTCX . low_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is crossed_above other . See combine_objs() .","title":"low_crossed_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.low_crossed_below","text":"OHLCSTCX . low_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is crossed_below other . See combine_objs() .","title":"low_crossed_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.low_equal","text":"OHLCSTCX . low_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is equal other . See combine_objs() .","title":"low_equal()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.low_stats","text":"OHLCSTCX . low_stats ( * args , ** kwargs ) Stats of low as generic.","title":"low_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.new_entries","text":"Output array.","title":"new_entries"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.new_entries_and","text":"OHLCSTCX . new_entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries AND other . See combine_objs() .","title":"new_entries_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.new_entries_or","text":"OHLCSTCX . new_entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries OR other . See combine_objs() .","title":"new_entries_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.new_entries_stats","text":"OHLCSTCX . new_entries_stats ( * args , ** kwargs ) Stats of new_entries as signals.","title":"new_entries_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.new_entries_xor","text":"OHLCSTCX . new_entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries XOR other . See combine_objs() .","title":"new_entries_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.open","text":"Input array.","title":"open"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.open_above","text":"OHLCSTCX . open_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is above other . See combine_objs() .","title":"open_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.open_below","text":"OHLCSTCX . open_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is below other . See combine_objs() .","title":"open_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.open_crossed_above","text":"OHLCSTCX . open_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is crossed_above other . See combine_objs() .","title":"open_crossed_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.open_crossed_below","text":"OHLCSTCX . open_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is crossed_below other . See combine_objs() .","title":"open_crossed_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.open_equal","text":"OHLCSTCX . open_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is equal other . See combine_objs() .","title":"open_equal()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.open_stats","text":"OHLCSTCX . open_stats ( * args , ** kwargs ) Stats of open as generic.","title":"open_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.plot","text":"_bind_ohlcstx_plot .< locals >. plot ( plot_type = None , ohlc_kwargs = None , entry_trace_kwargs = None , exit_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot OHLC, OHLCSTCX.new_entries and OHLCSTCX.exits . Args plot_type Either 'OHLC', 'Candlestick' or Plotly trace. ohlc_kwargs :\u2002 dict Keyword arguments passed to plot_type . entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for OHLCSTCX.new_entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for OHLCSTCX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout.","title":"plot()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.reverse_list","text":"List of reverse values.","title":"reverse_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.run","text":"OHLCSTCX . run ( entries , open , high , low , close , sl_stop = Default ( nan ), sl_trail = Default ( False ), tp_stop = Default ( nan ), reverse = Default ( False ), stop_price = nan , stop_type =- 1 , short_name = 'ohlcstcx' , hide_params = None , hide_default = True , ** kwargs ) Run OHLCSTCX indicator. Inputs: entries , open , high , low , close In-place outputs: stop_price , stop_type Parameters: sl_stop , sl_trail , tp_stop , reverse Outputs: new_entries , exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.run_combs","text":"OHLCSTCX . run_combs ( entries , open , high , low , close , sl_stop = Default ( nan ), sl_trail = Default ( False ), tp_stop = Default ( nan ), reverse = Default ( False ), stop_price = nan , stop_type =- 1 , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple OHLCSTCX indicators using function comb_func . Inputs: entries , open , high , low , close In-place outputs: stop_price , stop_type Parameters: sl_stop , sl_trail , tp_stop , reverse Outputs: new_entries , exits comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to OHLCSTCX.run() .","title":"run_combs()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.sl_stop_list","text":"List of sl_stop values.","title":"sl_stop_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.sl_trail_list","text":"List of sl_trail values.","title":"sl_trail_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.stop_price","text":"In-place output array.","title":"stop_price"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.stop_price_above","text":"OHLCSTCX . stop_price_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is above other . See combine_objs() .","title":"stop_price_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.stop_price_below","text":"OHLCSTCX . stop_price_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is below other . See combine_objs() .","title":"stop_price_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.stop_price_crossed_above","text":"OHLCSTCX . stop_price_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is crossed_above other . See combine_objs() .","title":"stop_price_crossed_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.stop_price_crossed_below","text":"OHLCSTCX . stop_price_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is crossed_below other . See combine_objs() .","title":"stop_price_crossed_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.stop_price_equal","text":"OHLCSTCX . stop_price_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is equal other . See combine_objs() .","title":"stop_price_equal()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.stop_price_stats","text":"OHLCSTCX . stop_price_stats ( * args , ** kwargs ) Stats of stop_price as generic.","title":"stop_price_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.stop_type","text":"In-place output array.","title":"stop_type"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.stop_type_readable","text":"stop_type in readable format based on the following mapping: { \"0\" : \"StopLoss\" , \"1\" : \"TrailStop\" , \"2\" : \"TakeProfit\" , \"-1\" : null }","title":"stop_type_readable"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.stop_type_stats","text":"OHLCSTCX . stop_type_stats ( * args , ** kwargs ) Stats of stop_type based on the following mapping: { \"0\" : \"StopLoss\" , \"1\" : \"TrailStop\" , \"2\" : \"TakeProfit\" , \"-1\" : null }","title":"stop_type_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTCX.tp_stop_list","text":"List of tp_stop values.","title":"tp_stop_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX","text":"Exit signal generator based on OHLC and stop values. Generates exits based on entries and ohlc_stop_choice_nb() . Hint All parameters can be either a single value (per frame) or a NumPy array (per row, column, or element). To generate multiple combinations, pass them as lists. Usage Test each stop type: >>> import vectorbt as vbt >>> import pandas as pd >>> import numpy as np >>> entries = pd . Series ([ True , False , False , False , False , False ]) >>> price = pd . DataFrame ({ ... 'open' : [ 10 , 11 , 12 , 11 , 10 , 9 ], ... 'high' : [ 11 , 12 , 13 , 12 , 11 , 10 ], ... 'low' : [ 9 , 10 , 11 , 10 , 9 , 8 ], ... 'close' : [ 10 , 11 , 12 , 11 , 10 , 9 ] ... }) >>> ohlcstx = vbt . OHLCSTX . run ( ... entries , ... price [ 'open' ], price [ 'high' ], price [ 'low' ], price [ 'close' ], ... sl_stop = [ 0.1 , 0.1 , np . nan ], ... sl_trail = [ False , True , False ], ... tp_stop = [ np . nan , np . nan , 0.1 ]) >>> ohlcstx . entries ohlcstx_sl_stop 0.1 0.1 NaN ohlcstx_sl_trail False True False ohlcstx_tp_stop NaN NaN 0.1 0 True True True 1 False False False 2 False False False 3 False False False 4 False False False 5 False False False >>> ohlcstx . exits ohlcstx_sl_stop 0.1 0.1 NaN ohlcstx_sl_trail False True False ohlcstx_tp_stop NaN NaN 0.1 0 False False False 1 False False True 2 False False False 3 False True False 4 True False False 5 False False False >>> ohlcstx . stop_price ohlcstx_sl_stop 0.1 0.1 NaN ohlcstx_sl_trail False True False ohlcstx_tp_stop NaN NaN 0.1 0 NaN NaN NaN 1 NaN NaN 11.0 2 NaN NaN NaN 3 NaN 11.7 NaN 4 9.0 NaN NaN 5 NaN NaN NaN >>> ohlcstx . stop_type_readable ohlcstx_sl_stop 0.1 0.1 NaN ohlcstx_sl_trail False True False ohlcstx_tp_stop NaN NaN 0.1 0 None None None 1 None None TakeProfit 2 None None None 3 None TrailStop None 4 StopLoss None None 5 None None None Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._OHLCSTX","title":"OHLCSTX"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.close","text":"Input array.","title":"close"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.close_above","text":"OHLCSTX . close_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is above other . See combine_objs() .","title":"close_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.close_below","text":"OHLCSTX . close_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is below other . See combine_objs() .","title":"close_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.close_crossed_above","text":"OHLCSTX . close_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_above other . See combine_objs() .","title":"close_crossed_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.close_crossed_below","text":"OHLCSTX . close_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is crossed_below other . See combine_objs() .","title":"close_crossed_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.close_equal","text":"OHLCSTX . close_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where close is equal other . See combine_objs() .","title":"close_equal()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.close_stats","text":"OHLCSTX . close_stats ( * args , ** kwargs ) Stats of close as generic.","title":"close_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.custom_func","text":"SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs )","title":"custom_func()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.entries","text":"Input array.","title":"entries"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.entries_and","text":"OHLCSTX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() .","title":"entries_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.entries_or","text":"OHLCSTX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() .","title":"entries_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.entries_stats","text":"OHLCSTX . entries_stats ( * args , ** kwargs ) Stats of entries as signals.","title":"entries_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.entries_xor","text":"OHLCSTX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() .","title":"entries_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.exits","text":"Output array.","title":"exits"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.exits_and","text":"OHLCSTX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() .","title":"exits_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.exits_or","text":"OHLCSTX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() .","title":"exits_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.exits_stats","text":"OHLCSTX . exits_stats ( * args , ** kwargs ) Stats of exits as signals.","title":"exits_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.exits_xor","text":"OHLCSTX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() .","title":"exits_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.high","text":"Input array.","title":"high"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.high_above","text":"OHLCSTX . high_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is above other . See combine_objs() .","title":"high_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.high_below","text":"OHLCSTX . high_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is below other . See combine_objs() .","title":"high_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.high_crossed_above","text":"OHLCSTX . high_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is crossed_above other . See combine_objs() .","title":"high_crossed_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.high_crossed_below","text":"OHLCSTX . high_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is crossed_below other . See combine_objs() .","title":"high_crossed_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.high_equal","text":"OHLCSTX . high_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where high is equal other . See combine_objs() .","title":"high_equal()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.high_stats","text":"OHLCSTX . high_stats ( * args , ** kwargs ) Stats of high as generic.","title":"high_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.low","text":"Input array.","title":"low"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.low_above","text":"OHLCSTX . low_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is above other . See combine_objs() .","title":"low_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.low_below","text":"OHLCSTX . low_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is below other . See combine_objs() .","title":"low_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.low_crossed_above","text":"OHLCSTX . low_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is crossed_above other . See combine_objs() .","title":"low_crossed_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.low_crossed_below","text":"OHLCSTX . low_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is crossed_below other . See combine_objs() .","title":"low_crossed_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.low_equal","text":"OHLCSTX . low_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where low is equal other . See combine_objs() .","title":"low_equal()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.low_stats","text":"OHLCSTX . low_stats ( * args , ** kwargs ) Stats of low as generic.","title":"low_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.open","text":"Input array.","title":"open"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.open_above","text":"OHLCSTX . open_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is above other . See combine_objs() .","title":"open_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.open_below","text":"OHLCSTX . open_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is below other . See combine_objs() .","title":"open_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.open_crossed_above","text":"OHLCSTX . open_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is crossed_above other . See combine_objs() .","title":"open_crossed_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.open_crossed_below","text":"OHLCSTX . open_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is crossed_below other . See combine_objs() .","title":"open_crossed_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.open_equal","text":"OHLCSTX . open_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where open is equal other . See combine_objs() .","title":"open_equal()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.open_stats","text":"OHLCSTX . open_stats ( * args , ** kwargs ) Stats of open as generic.","title":"open_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.plot","text":"_bind_ohlcstx_plot .< locals >. plot ( plot_type = None , ohlc_kwargs = None , entry_trace_kwargs = None , exit_trace_kwargs = None , add_trace_kwargs = None , fig = None , ** layout_kwargs ) Plot OHLC, OHLCSTX.entries and OHLCSTX.exits . Args plot_type Either 'OHLC', 'Candlestick' or Plotly trace. ohlc_kwargs :\u2002 dict Keyword arguments passed to plot_type . entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for OHLCSTX.entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for OHLCSTX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **layout_kwargs Keyword arguments for layout. Usage >>> ohlcstx . iloc [:, 0 ] . plot ()","title":"plot()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.reverse_list","text":"List of reverse values.","title":"reverse_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.run","text":"OHLCSTX . run ( entries , open , high , low , close , sl_stop = Default ( nan ), sl_trail = Default ( False ), tp_stop = Default ( nan ), reverse = Default ( False ), stop_price = nan , stop_type =- 1 , short_name = 'ohlcstx' , hide_params = None , hide_default = True , ** kwargs ) Run OHLCSTX indicator. Inputs: entries , open , high , low , close In-place outputs: stop_price , stop_type Parameters: sl_stop , sl_trail , tp_stop , reverse Outputs: exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.run_combs","text":"OHLCSTX . run_combs ( entries , open , high , low , close , sl_stop = Default ( nan ), sl_trail = Default ( False ), tp_stop = Default ( nan ), reverse = Default ( False ), stop_price = nan , stop_type =- 1 , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple OHLCSTX indicators using function comb_func . Inputs: entries , open , high , low , close In-place outputs: stop_price , stop_type Parameters: sl_stop , sl_trail , tp_stop , reverse Outputs: exits comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to OHLCSTX.run() .","title":"run_combs()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.sl_stop_list","text":"List of sl_stop values.","title":"sl_stop_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.sl_trail_list","text":"List of sl_trail values.","title":"sl_trail_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.stop_price","text":"In-place output array.","title":"stop_price"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.stop_price_above","text":"OHLCSTX . stop_price_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is above other . See combine_objs() .","title":"stop_price_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.stop_price_below","text":"OHLCSTX . stop_price_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is below other . See combine_objs() .","title":"stop_price_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.stop_price_crossed_above","text":"OHLCSTX . stop_price_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is crossed_above other . See combine_objs() .","title":"stop_price_crossed_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.stop_price_crossed_below","text":"OHLCSTX . stop_price_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is crossed_below other . See combine_objs() .","title":"stop_price_crossed_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.stop_price_equal","text":"OHLCSTX . stop_price_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where stop_price is equal other . See combine_objs() .","title":"stop_price_equal()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.stop_price_stats","text":"OHLCSTX . stop_price_stats ( * args , ** kwargs ) Stats of stop_price as generic.","title":"stop_price_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.stop_type","text":"In-place output array.","title":"stop_type"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.stop_type_readable","text":"stop_type in readable format based on the following mapping: { \"0\" : \"StopLoss\" , \"1\" : \"TrailStop\" , \"2\" : \"TakeProfit\" , \"-1\" : null }","title":"stop_type_readable"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.stop_type_stats","text":"OHLCSTX . stop_type_stats ( * args , ** kwargs ) Stats of stop_type based on the following mapping: { \"0\" : \"StopLoss\" , \"1\" : \"TrailStop\" , \"2\" : \"TakeProfit\" , \"-1\" : null }","title":"stop_type_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.OHLCSTX.tp_stop_list","text":"List of tp_stop values.","title":"tp_stop_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.RAND","text":"Random entry signal generator based on the number of signals. Generates entries based on rand_choice_nb() . Hint Parameter n can be either a single value (per frame) or a NumPy array (per column). To generate multiple combinations, pass it as a list. Usage Test three different entry counts values: >>> import vectorbt as vbt >>> rand = vbt . RAND . run ( input_shape = ( 6 ,), n = [ 1 , 2 , 3 ], seed = 42 ) >>> rand . entries rand_n 1 2 3 0 True True True 1 False False True 2 False False False 3 False True False 4 False False True 5 False False False Entry count can also be set per column: >>> import numpy as np >>> rand = vbt . RAND . run ( input_shape = ( 8 , 2 ), n = [ np . array ([ 1 , 2 ]), 3 ], seed = 42 ) >>> rand . entries rand_n 1 2 3 3 0 1 0 1 0 False False True False 1 True False False False 2 False False False True 3 False True True False 4 False False False False 5 False False False True 6 False False True False 7 False True False True Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._RAND","title":"RAND"},{"location":"api/signals/generators/#vectorbt.signals.generators.RAND.custom_func","text":"SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs )","title":"custom_func()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RAND.entries","text":"Output array.","title":"entries"},{"location":"api/signals/generators/#vectorbt.signals.generators.RAND.entries_and","text":"RAND . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() .","title":"entries_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RAND.entries_or","text":"RAND . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() .","title":"entries_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RAND.entries_stats","text":"RAND . entries_stats ( * args , ** kwargs ) Stats of entries as signals.","title":"entries_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RAND.entries_xor","text":"RAND . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() .","title":"entries_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RAND.n_list","text":"List of n values.","title":"n_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.RAND.plot","text":"SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot RAND.entries and RAND.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for RAND.entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for RAND.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() .","title":"plot()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RAND.run","text":"RAND . run ( input_shape , n , short_name = 'rand' , hide_params = None , hide_default = True , ** kwargs ) Run RAND indicator. Parameters: n Outputs: entries Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RAND.run_combs","text":"RAND . run_combs ( input_shape , n , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple RAND indicators using function comb_func . Parameters: n Outputs: entries comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to RAND.run() .","title":"run_combs()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDNX","text":"Random entry and exit signal generator based on the number of signals. Generates entries and exits based on rand_enex_apply_nb() . See RAND for notes on parameters. Usage Test three different entry and exit counts: >>> import vectorbt as vbt >>> randnx = vbt . RANDNX . run ( ... input_shape = ( 6 ,), ... n = [ 1 , 2 , 3 ], ... seed = 42 ) >>> randnx . entries randnx_n 1 2 3 0 True True True 1 False False False 2 False True True 3 False False False 4 False False True 5 False False False >>> randnx . exits randnx_n 1 2 3 0 False False False 1 True True True 2 False False False 3 False True True 4 False False False 5 False False True Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._RANDNX","title":"RANDNX"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDNX.apply_func","text":"RANDNX . apply_func ( input_shape , n , entry_wait , exit_wait ) apply_func_nb that calls generate_rand_enex_nb .","title":"apply_func()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDNX.custom_func","text":"IndicatorFactory . from_apply_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , col = None , flex_2d = None , return_cache = False , use_cache = None , use_ray = False , ** _kwargs ) Custom function that forwards inputs and parameters to apply_func .","title":"custom_func()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDNX.entries","text":"Output array.","title":"entries"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDNX.entries_and","text":"RANDNX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() .","title":"entries_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDNX.entries_or","text":"RANDNX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() .","title":"entries_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDNX.entries_stats","text":"RANDNX . entries_stats ( * args , ** kwargs ) Stats of entries as signals.","title":"entries_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDNX.entries_xor","text":"RANDNX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() .","title":"entries_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDNX.exits","text":"Output array.","title":"exits"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDNX.exits_and","text":"RANDNX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() .","title":"exits_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDNX.exits_or","text":"RANDNX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() .","title":"exits_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDNX.exits_stats","text":"RANDNX . exits_stats ( * args , ** kwargs ) Stats of exits as signals.","title":"exits_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDNX.exits_xor","text":"RANDNX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() .","title":"exits_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDNX.n_list","text":"List of n values.","title":"n_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDNX.plot","text":"SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot RANDNX.entries and RANDNX.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for RANDNX.entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for RANDNX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() .","title":"plot()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDNX.run","text":"RANDNX . run ( input_shape , n , short_name = 'randnx' , hide_params = None , hide_default = True , ** kwargs ) Run RANDNX indicator. Parameters: n Outputs: entries , exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDNX.run_combs","text":"RANDNX . run_combs ( input_shape , n , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple RANDNX indicators using function comb_func . Parameters: n Outputs: entries , exits comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to RANDNX.run() .","title":"run_combs()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDX","text":"Random exit signal generator based on the number of signals. Generates exits based on entries and rand_choice_nb() . See RAND for notes on parameters. Usage Generate an exit for each entry: >>> import vectorbt as vbt >>> import pandas as pd >>> entries = pd . Series ([ True , False , False , True , False , False ]) >>> randx = vbt . RANDX . run ( entries , seed = 42 ) >>> randx . exits 0 False 1 False 2 True 3 False 4 True 5 False dtype: bool Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.run_combs() IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._RANDX","title":"RANDX"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDX.custom_func","text":"SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs )","title":"custom_func()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDX.entries","text":"Input array.","title":"entries"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDX.entries_and","text":"RANDX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() .","title":"entries_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDX.entries_or","text":"RANDX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() .","title":"entries_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDX.entries_stats","text":"RANDX . entries_stats ( * args , ** kwargs ) Stats of entries as signals.","title":"entries_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDX.entries_xor","text":"RANDX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() .","title":"entries_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDX.exits","text":"Output array.","title":"exits"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDX.exits_and","text":"RANDX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() .","title":"exits_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDX.exits_or","text":"RANDX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() .","title":"exits_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDX.exits_stats","text":"RANDX . exits_stats ( * args , ** kwargs ) Stats of exits as signals.","title":"exits_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDX.exits_xor","text":"RANDX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() .","title":"exits_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDX.plot","text":"SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot RANDX.entries and RANDX.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for RANDX.entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for RANDX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() .","title":"plot()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RANDX.run","text":"RANDX . run ( entries , short_name = 'randx' , hide_params = None , hide_default = True , ** kwargs ) Run RANDX indicator. Inputs: entries Outputs: exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROB","text":"Random entry signal generator based on probabilities. Generates entries based on rand_by_prob_choice_nb() . Hint All parameters can be either a single value (per frame) or a NumPy array (per row, column, or element). To generate multiple combinations, pass them as lists. Usage Generate three columns with different entry probabilities: >>> import vectorbt as vbt >>> rprob = vbt . RPROB . run ( input_shape = ( 5 ,), prob = [ 0. , 0.5 , 1. ], seed = 42 ) >>> rprob . entries rprob_prob 0.0 0.5 1.0 0 False True True 1 False True True 2 False False True 3 False False True 4 False False True Probability can also be set per row, column, or element: >>> import numpy as np >>> rprob = vbt . RPROB . run ( input_shape = ( 5 ,), prob = np . array ([ 0. , 0. , 1. , 1. , 1. ]), seed = 42 ) >>> rprob . entries 0 False 1 False 2 True 3 True 4 True Name: array_0, dtype: bool Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._RPROB","title":"RPROB"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROB.custom_func","text":"SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs )","title":"custom_func()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROB.entries","text":"Output array.","title":"entries"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROB.entries_and","text":"RPROB . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() .","title":"entries_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROB.entries_or","text":"RPROB . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() .","title":"entries_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROB.entries_stats","text":"RPROB . entries_stats ( * args , ** kwargs ) Stats of entries as signals.","title":"entries_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROB.entries_xor","text":"RPROB . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() .","title":"entries_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROB.plot","text":"SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot RPROB.entries and RPROB.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for RPROB.entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for RPROB.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() .","title":"plot()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROB.prob_list","text":"List of prob values.","title":"prob_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROB.run","text":"RPROB . run ( input_shape , prob , short_name = 'rprob' , hide_params = None , hide_default = True , ** kwargs ) Run RPROB indicator. Parameters: prob Outputs: entries Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROB.run_combs","text":"RPROB . run_combs ( input_shape , prob , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple RPROB indicators using function comb_func . Parameters: prob Outputs: entries comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to RPROB.run() .","title":"run_combs()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX","text":"Random exit signal generator based on probabilities. Generates chain of new_entries and exits based on entries and rand_by_prob_choice_nb() . See RPROB for notes on parameters. Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._RPROBCX","title":"RPROBCX"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.custom_func","text":"SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs )","title":"custom_func()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.entries","text":"Input array.","title":"entries"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.entries_and","text":"RPROBCX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() .","title":"entries_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.entries_or","text":"RPROBCX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() .","title":"entries_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.entries_stats","text":"RPROBCX . entries_stats ( * args , ** kwargs ) Stats of entries as signals.","title":"entries_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.entries_xor","text":"RPROBCX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() .","title":"entries_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.exits","text":"Output array.","title":"exits"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.exits_and","text":"RPROBCX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() .","title":"exits_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.exits_or","text":"RPROBCX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() .","title":"exits_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.exits_stats","text":"RPROBCX . exits_stats ( * args , ** kwargs ) Stats of exits as signals.","title":"exits_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.exits_xor","text":"RPROBCX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() .","title":"exits_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.new_entries","text":"Output array.","title":"new_entries"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.new_entries_and","text":"RPROBCX . new_entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries AND other . See combine_objs() .","title":"new_entries_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.new_entries_or","text":"RPROBCX . new_entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries OR other . See combine_objs() .","title":"new_entries_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.new_entries_stats","text":"RPROBCX . new_entries_stats ( * args , ** kwargs ) Stats of new_entries as signals.","title":"new_entries_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.new_entries_xor","text":"RPROBCX . new_entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries XOR other . See combine_objs() .","title":"new_entries_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.plot","text":"SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot RPROBCX.new_entries and RPROBCX.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for RPROBCX.new_entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for RPROBCX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() .","title":"plot()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.prob_list","text":"List of prob values.","title":"prob_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.run","text":"RPROBCX . run ( entries , prob , short_name = 'rprobcx' , hide_params = None , hide_default = True , ** kwargs ) Run RPROBCX indicator. Inputs: entries Parameters: prob Outputs: new_entries , exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBCX.run_combs","text":"RPROBCX . run_combs ( entries , prob , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple RPROBCX indicators using function comb_func . Inputs: entries Parameters: prob Outputs: new_entries , exits comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to RPROBCX.run() .","title":"run_combs()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBNX","text":"Random entry and exit signal generator based on probabilities. Generates entries and exits based on rand_by_prob_choice_nb() . See RPROB for notes on parameters. Usage Test all probability combinations: >>> import vectorbt as vbt >>> rprobnx = vbt . RPROBNX . run ( ... input_shape = ( 5 ,), ... entry_prob = [ 0.5 , 1. ], ... exit_prob = [ 0.5 , 1. ], ... param_product = True , ... seed = 42 ) >>> rprobnx . entries rprobnx_entry_prob 0.5 0.5 1.0 0.5 rprobnx_exit_prob 0.5 1.0 0.5 1.0 0 True True True True 1 False False False False 2 False False False True 3 False False False False 4 False False True True >>> rprobnx . exits rprobnx_entry_prob 0.5 0.5 1.0 1.0 rprobnx_exit_prob 0.5 1.0 0.5 1.0 0 False False False False 1 False True False True 2 False False False False 3 False False True True 4 True False False False Probabilities can also be set per row, column, or element: >>> import numpy as np >>> entry_prob1 = np . asarray ([ 1. , 0. , 1. , 0. , 1. ]) >>> entry_prob2 = np . asarray ([ 0. , 1. , 0. , 1. , 0. ]) >>> rprobnx = vbt . RPROBNX . run ( ... input_shape = ( 5 ,), ... entry_prob = [ entry_prob1 , entry_prob2 ], ... exit_prob = 1. , ... seed = 42 ) >>> rprobnx . entries rprobnx_entry_prob array_0 array_1 rprobnx_exit_prob 1.0 1.0 0 True False 1 False True 2 True False 3 False True 4 True False >>> rprobnx . exits rprobnx_entry_prob array_0 array_1 rprobnx_exit_prob 1.0 1.0 0 False False 1 True False 2 False True 3 True False 4 False True Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._RPROBNX","title":"RPROBNX"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBNX.custom_func","text":"SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs )","title":"custom_func()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBNX.entries","text":"Output array.","title":"entries"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBNX.entries_and","text":"RPROBNX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() .","title":"entries_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBNX.entries_or","text":"RPROBNX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() .","title":"entries_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBNX.entries_stats","text":"RPROBNX . entries_stats ( * args , ** kwargs ) Stats of entries as signals.","title":"entries_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBNX.entries_xor","text":"RPROBNX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() .","title":"entries_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBNX.entry_prob_list","text":"List of entry_prob values.","title":"entry_prob_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBNX.exit_prob_list","text":"List of exit_prob values.","title":"exit_prob_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBNX.exits","text":"Output array.","title":"exits"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBNX.exits_and","text":"RPROBNX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() .","title":"exits_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBNX.exits_or","text":"RPROBNX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() .","title":"exits_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBNX.exits_stats","text":"RPROBNX . exits_stats ( * args , ** kwargs ) Stats of exits as signals.","title":"exits_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBNX.exits_xor","text":"RPROBNX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() .","title":"exits_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBNX.plot","text":"SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot RPROBNX.entries and RPROBNX.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for RPROBNX.entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for RPROBNX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() .","title":"plot()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBNX.run","text":"RPROBNX . run ( input_shape , entry_prob , exit_prob , short_name = 'rprobnx' , hide_params = None , hide_default = True , ** kwargs ) Run RPROBNX indicator. Parameters: entry_prob , exit_prob Outputs: entries , exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBNX.run_combs","text":"RPROBNX . run_combs ( input_shape , entry_prob , exit_prob , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple RPROBNX indicators using function comb_func . Parameters: entry_prob , exit_prob Outputs: entries , exits comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to RPROBNX.run() .","title":"run_combs()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBX","text":"Random exit signal generator based on probabilities. Generates exits based on entries and rand_by_prob_choice_nb() . See RPROB for notes on parameters. Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._RPROBX","title":"RPROBX"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBX.custom_func","text":"SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs )","title":"custom_func()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBX.entries","text":"Input array.","title":"entries"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBX.entries_and","text":"RPROBX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() .","title":"entries_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBX.entries_or","text":"RPROBX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() .","title":"entries_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBX.entries_stats","text":"RPROBX . entries_stats ( * args , ** kwargs ) Stats of entries as signals.","title":"entries_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBX.entries_xor","text":"RPROBX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() .","title":"entries_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBX.exits","text":"Output array.","title":"exits"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBX.exits_and","text":"RPROBX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() .","title":"exits_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBX.exits_or","text":"RPROBX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() .","title":"exits_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBX.exits_stats","text":"RPROBX . exits_stats ( * args , ** kwargs ) Stats of exits as signals.","title":"exits_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBX.exits_xor","text":"RPROBX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() .","title":"exits_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBX.plot","text":"SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot RPROBX.entries and RPROBX.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for RPROBX.entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for RPROBX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() .","title":"plot()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBX.prob_list","text":"List of prob values.","title":"prob_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBX.run","text":"RPROBX . run ( entries , prob , short_name = 'rprobx' , hide_params = None , hide_default = True , ** kwargs ) Run RPROBX indicator. Inputs: entries Parameters: prob Outputs: exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/signals/generators/#vectorbt.signals.generators.RPROBX.run_combs","text":"RPROBX . run_combs ( entries , prob , r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple RPROBX indicators using function comb_func . Inputs: entries Parameters: prob Outputs: exits comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to RPROBX.run() .","title":"run_combs()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX","text":"Exit signal generator based on stop values. Generates chain of new_entries and exits based on entries and stop_choice_nb() . See STX for notes on parameters. Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._STCX","title":"STCX"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.custom_func","text":"SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs )","title":"custom_func()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.entries","text":"Input array.","title":"entries"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.entries_and","text":"STCX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() .","title":"entries_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.entries_or","text":"STCX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() .","title":"entries_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.entries_stats","text":"STCX . entries_stats ( * args , ** kwargs ) Stats of entries as signals.","title":"entries_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.entries_xor","text":"STCX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() .","title":"entries_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.exits","text":"Output array.","title":"exits"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.exits_and","text":"STCX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() .","title":"exits_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.exits_or","text":"STCX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() .","title":"exits_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.exits_stats","text":"STCX . exits_stats ( * args , ** kwargs ) Stats of exits as signals.","title":"exits_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.exits_xor","text":"STCX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() .","title":"exits_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.new_entries","text":"Output array.","title":"new_entries"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.new_entries_and","text":"STCX . new_entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries AND other . See combine_objs() .","title":"new_entries_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.new_entries_or","text":"STCX . new_entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries OR other . See combine_objs() .","title":"new_entries_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.new_entries_stats","text":"STCX . new_entries_stats ( * args , ** kwargs ) Stats of new_entries as signals.","title":"new_entries_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.new_entries_xor","text":"STCX . new_entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return new_entries XOR other . See combine_objs() .","title":"new_entries_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.plot","text":"SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot STCX.new_entries and STCX.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for STCX.new_entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for STCX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() .","title":"plot()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.run","text":"STCX . run ( entries , ts , stop , trailing = Default ( False ), short_name = 'stcx' , hide_params = None , hide_default = True , ** kwargs ) Run STCX indicator. Inputs: entries , ts Parameters: stop , trailing Outputs: new_entries , exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.run_combs","text":"STCX . run_combs ( entries , ts , stop , trailing = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple STCX indicators using function comb_func . Inputs: entries , ts Parameters: stop , trailing Outputs: new_entries , exits comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to STCX.run() .","title":"run_combs()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.stop_list","text":"List of stop values.","title":"stop_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.trailing_list","text":"List of trailing values.","title":"trailing_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.ts","text":"Input array.","title":"ts"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.ts_above","text":"STCX . ts_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is above other . See combine_objs() .","title":"ts_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.ts_below","text":"STCX . ts_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is below other . See combine_objs() .","title":"ts_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.ts_crossed_above","text":"STCX . ts_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is crossed_above other . See combine_objs() .","title":"ts_crossed_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.ts_crossed_below","text":"STCX . ts_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is crossed_below other . See combine_objs() .","title":"ts_crossed_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.ts_equal","text":"STCX . ts_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is equal other . See combine_objs() .","title":"ts_equal()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STCX.ts_stats","text":"STCX . ts_stats ( * args , ** kwargs ) Stats of ts as generic.","title":"ts_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX","text":"Exit signal generator based on stop values. Generates exits based on entries and stop_choice_nb() . Hint All parameters can be either a single value (per frame) or a NumPy array (per row, column, or element). To generate multiple combinations, pass them as lists. Superclasses AttrResolver Configured Documented IndexingBase IndicatorBase PandasIndexer Pickleable PlotsBuilderMixin StatsBuilderMixin Wrapping vectorbt.signals.generators.ParamIndexer Inherited members AttrResolver.deep_getattr() AttrResolver.post_resolve_attr() AttrResolver.pre_resolve_attr() AttrResolver.resolve_attr() Configured.copy() Configured.dumps() Configured.loads() Configured.replace() Configured.to_doc() Configured.update_config() IndicatorBase.config IndicatorBase.iloc IndicatorBase.in_output_names IndicatorBase.indexing_func() IndicatorBase.indexing_kwargs IndicatorBase.input_names IndicatorBase.level_names IndicatorBase.loc IndicatorBase.output_flags IndicatorBase.output_names IndicatorBase.param_names IndicatorBase.plots_defaults IndicatorBase.self_aliases IndicatorBase.short_name IndicatorBase.stats_defaults IndicatorBase.wrapper IndicatorBase.writeable_attrs PandasIndexer.xs() Pickleable.load() Pickleable.save() PlotsBuilderMixin.build_subplots_doc() PlotsBuilderMixin.override_subplots_doc() PlotsBuilderMixin.plots() StatsBuilderMixin.build_metrics_doc() StatsBuilderMixin.override_metrics_doc() StatsBuilderMixin.stats() Wrapping.regroup() Wrapping.resolve_self() Wrapping.select_one() Wrapping.select_one_from_obj() Subclasses vectorbt.signals.generators._STX","title":"STX"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.custom_func","text":"SignalFactory . from_choice_func .< locals >. custom_func ( input_list , in_output_list , param_list , * args , input_shape = None , flex_2d = None , entry_args = None , exit_args = None , cache_args = None , entry_kwargs = None , exit_kwargs = None , cache_kwargs = None , return_cache = False , use_cache = None , ** _kwargs )","title":"custom_func()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.entries","text":"Input array.","title":"entries"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.entries_and","text":"STX . entries_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries AND other . See combine_objs() .","title":"entries_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.entries_or","text":"STX . entries_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries OR other . See combine_objs() .","title":"entries_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.entries_stats","text":"STX . entries_stats ( * args , ** kwargs ) Stats of entries as signals.","title":"entries_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.entries_xor","text":"STX . entries_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return entries XOR other . See combine_objs() .","title":"entries_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.exits","text":"Output array.","title":"exits"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.exits_and","text":"STX . exits_and ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits AND other . See combine_objs() .","title":"exits_and()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.exits_or","text":"STX . exits_or ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits OR other . See combine_objs() .","title":"exits_or()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.exits_stats","text":"STX . exits_stats ( * args , ** kwargs ) Stats of exits as signals.","title":"exits_stats()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.exits_xor","text":"STX . exits_xor ( other , level_name = None , allow_multiple = True , ** kwargs ) Return exits XOR other . See combine_objs() .","title":"exits_xor()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.plot","text":"SignalFactory . __init__ .< locals >. plot ( _self , entry_y = None , exit_y = None , entry_types = None , exit_types = None , entry_trace_kwargs = None , exit_trace_kwargs = None , fig = None , ** kwargs ) Plot STX.entries and STX.exits . Args entry_y :\u2002 array_like Y-axis values to plot entry markers on. exit_y :\u2002 array_like Y-axis values to plot exit markers on. entry_types :\u2002 array_like Entry types in string format. exit_types :\u2002 array_like Exit types in string format. entry_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_entry_markers() for STX.entries . exit_trace_kwargs :\u2002 dict Keyword arguments passed to SignalsSRAccessor.plot_as_exit_markers() for STX.exits . fig :\u2002 Figure or FigureWidget Figure to add traces to. **kwargs Keyword arguments passed to SignalsSRAccessor.plot_as_markers() .","title":"plot()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.run","text":"STX . run ( entries , ts , stop , trailing = Default ( False ), short_name = 'stx' , hide_params = None , hide_default = True , ** kwargs ) Run STX indicator. Inputs: entries , ts Parameters: stop , trailing Outputs: exits Pass a list of parameter names as hide_params to hide their column levels. Set hide_default to False to show the column levels of the parameters with a default value. Other keyword arguments are passed to run_pipeline() .","title":"run()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.run_combs","text":"STX . run_combs ( entries , ts , stop , trailing = Default ( False ), r = 2 , param_product = False , comb_func = itertools . combinations , run_unique = True , short_names = None , hide_params = None , hide_default = True , ** kwargs ) Create a combination of multiple STX indicators using function comb_func . Inputs: entries , ts Parameters: stop , trailing Outputs: exits comb_func must accept an iterable of parameter tuples and r . Also accepts all combinatoric iterators from itertools such as itertools.combinations . Pass r to specify how many indicators to run. Pass short_names to specify the short name for each indicator. Set run_unique to True to first compute raw outputs for all parameters, and then use them to build each indicator (faster). Other keyword arguments are passed to STX.run() .","title":"run_combs()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.stop_list","text":"List of stop values.","title":"stop_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.trailing_list","text":"List of trailing values.","title":"trailing_list"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.ts","text":"Input array.","title":"ts"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.ts_above","text":"STX . ts_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is above other . See combine_objs() .","title":"ts_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.ts_below","text":"STX . ts_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is below other . See combine_objs() .","title":"ts_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.ts_crossed_above","text":"STX . ts_crossed_above ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is crossed_above other . See combine_objs() .","title":"ts_crossed_above()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.ts_crossed_below","text":"STX . ts_crossed_below ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is crossed_below other . See combine_objs() .","title":"ts_crossed_below()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.ts_equal","text":"STX . ts_equal ( other , level_name = None , allow_multiple = True , ** kwargs ) Return True for each element where ts is equal other . See combine_objs() .","title":"ts_equal()"},{"location":"api/signals/generators/#vectorbt.signals.generators.STX.ts_stats","text":"STX . ts_stats ( * args , ** kwargs ) Stats of ts as generic.","title":"ts_stats()"},{"location":"api/signals/nb/","text":"nb module \u00b6 Numba-compiled functions. Provides an arsenal of Numba-compiled functions that are used by accessors and in many other parts of the backtesting pipeline, such as technical indicators. These only accept NumPy arrays and other Numba-compatible types. >>> import numpy as np >>> import vectorbt as vbt >>> # vectorbt.signals.nb.pos_rank_nb >>> vbt . signals . nb . pos_rank_nb ( np . array ([ False , True , True , True , False ])[:, None ])[:, 0 ] [-1 0 1 2 -1] Note vectorbt treats matrices as first-class citizens and expects input arrays to be 2-dim, unless function has suffix _1d or is meant to be input to another function. Data is processed along index (axis 0). All functions passed as argument should be Numba-compiled. Returned indices should be absolute. between_partition_ranges_nb function \u00b6 between_partition_ranges_nb ( a ) Create a record of type range_dt for each range between two partitions in a . between_ranges_nb function \u00b6 between_ranges_nb ( a ) Create a record of type range_dt for each range between two signals in a . between_two_ranges_nb function \u00b6 between_two_ranges_nb ( a , b , from_other = False ) Create a record of type range_dt for each range between two signals in a and b . If from_other is False, returns ranges from each in a to the succeeding in b . Otherwise, returns ranges from each in b to the preceding in a . When a and b overlap (two signals at the same time), the distance between overlapping signals is still considered and from_i would match to_i . clean_enex_1d_nb function \u00b6 clean_enex_1d_nb ( entries , exits , entry_first ) Clean entry and exit arrays by picking the first signal out of each. Entry signal must be picked first. If both signals are present, selects none. clean_enex_nb function \u00b6 clean_enex_nb ( entries , exits , entry_first ) 2-dim version of clean_enex_1d_nb() . first_choice_nb function \u00b6 first_choice_nb ( from_i , to_i , col , a ) choice_func_nb that returns the index of the first signal in a . generate_enex_nb function \u00b6 generate_enex_nb ( shape , entry_wait , exit_wait , entry_pick_first , exit_pick_first , entry_choice_func_nb , entry_args , exit_choice_func_nb , exit_args ) Pick entry signals using entry_choice_func_nb and exit signals using exit_choice_func_nb one after another. Args shape :\u2002 array Target shape. entry_wait :\u2002 int Number of ticks to wait before placing entries. Note Setting entry_wait to 0 or False assumes that both entry and exit can be processed within the same bar, and exit can be processed before entry. exit_wait :\u2002 int Number of ticks to wait before placing exits. Note Setting exit_wait to 0 or False assumes that both entry and exit can be processed within the same bar, and entry can be processed before exit. entry_pick_first :\u2002 bool Whether to pick the first entry out of all returned by entry_choice_func_nb . exit_pick_first :\u2002 bool Whether to pick the first exit out of all returned by exit_choice_func_nb . Setting it to False acts similarly to setting skip_until_exit to True in generate_ex_nb() . entry_choice_func_nb :\u2002 callable Entry choice function. See choice_func_nb in generate_nb() . entry_args :\u2002 tuple Arguments unpacked and passed to entry_choice_func_nb . exit_choice_func_nb :\u2002 callable Exit choice function. See choice_func_nb in generate_nb() . exit_args :\u2002 tuple Arguments unpacked and passed to exit_choice_func_nb . generate_ex_nb function \u00b6 generate_ex_nb ( entries , wait , until_next , skip_until_exit , pick_first , exit_choice_func_nb , * args ) Pick exit signals using exit_choice_func_nb after each signal in entries . Args entries :\u2002 array Boolean array with entry signals. wait :\u2002 int Number of ticks to wait before placing exits. Note Setting wait to 0 or False may result in two signals at one bar. until_next :\u2002 int Whether to place signals up to the next entry signal. Note Setting it to False makes it difficult to tell which exit belongs to which entry. skip_until_exit :\u2002 bool Whether to skip processing entry signals until the next exit. Has only effect when until_next is disabled. Note Setting it to True makes it difficult to tell which exit belongs to which entry. pick_first :\u2002 bool Whether to pick the first signal out of all returned by exit_choice_func_nb . exit_choice_func_nb :\u2002 callable Exit choice function. See choice_func_nb in generate_nb() . *args :\u2002 callable Arguments passed to exit_choice_func_nb . generate_nb function \u00b6 generate_nb ( shape , pick_first , choice_func_nb , * args ) Create a boolean matrix of shape and pick signals using choice_func_nb . Args shape :\u2002 array Target shape. pick_first :\u2002 bool Whether to pick the first signal out of all returned by choice_func_nb . choice_func_nb :\u2002 callable Choice function. choice_func_nb should accept index of the start of the range from_i , index of the end of the range to_i , index of the column col , and *args . It should return an array of indices from [from_i, to_i) (can be empty). *args Arguments passed to choice_func_nb . Usage >>> from numba import njit >>> import numpy as np >>> from vectorbt.signals.nb import generate_nb >>> @njit ... def choice_func_nb ( from_i , to_i , col ): ... return np . array ([ from_i + col ]) >>> generate_nb (( 5 , 3 ), choice_func_nb ) [[ True False False] [False True False] [False False True] [False False False] [False False False]] generate_ohlc_stop_enex_nb function \u00b6 generate_ohlc_stop_enex_nb ( entries , open , high , low , close , stop_price_out , stop_type_out , sl_stop , sl_trail , tp_stop , reverse , is_open_safe , entry_wait , exit_wait , pick_first , flex_2d ) Generate one after another using generate_enex_nb() and ohlc_stop_choice_nb() . Returns two arrays: new entries and exits. Note Has the same logic as calling generate_ohlc_stop_ex_nb() with skip_until_exit=True , but removes all entries that come before the next exit. generate_ohlc_stop_ex_nb function \u00b6 generate_ohlc_stop_ex_nb ( entries , open , high , low , close , stop_price_out , stop_type_out , sl_stop , sl_trail , tp_stop , reverse , is_open_safe , wait , until_next , skip_until_exit , pick_first , flex_2d ) Generate using generate_ex_nb() and ohlc_stop_choice_nb() . Usage Generate trailing stop loss and take profit signals for 10%. Illustrates how exit signal can be generated within the same bar as entry. >>> import numpy as np >>> from vectorbt.signals.nb import generate_ohlc_stop_ex_nb >>> entries = np . asarray ([ True , False , True , False , False ])[:, None ] >>> entry_price = np . asarray ([ 10 , 11 , 12 , 11 , 10 ])[:, None ] >>> high_price = entry_price + 1 >>> low_price = entry_price - 1 >>> close_price = entry_price >>> stop_price_out = np . full_like ( entries , np . nan , dtype = np . float_ ) >>> stop_type_out = np . full_like ( entries , - 1 , dtype = np . int_ ) >>> generate_ohlc_stop_ex_nb ( ... entries = entries , ... open = entry_price , ... high = high_price , ... low = low_price , ... close = close_price , ... stop_price_out = stop_price_out , ... stop_type_out = stop_type_out , ... sl_stop = 0.1 , ... sl_trail = True , ... tp_stop = 0.1 , ... reverse = False , ... is_open_safe = True , ... wait = 1 , ... until_next = True , ... skip_until_exit = False , ... pick_first = True , ... flex_2d = True ... ) array([[ True], [False], [False], [ True], [False]]) >>> stop_price_out array([[ 9. ], << trailing SL from 10 (entry_price) [ nan], [ nan], [11.7], << trailing SL from 13 (high_price) [ nan]]) >>> stop_type_out array([[ 1], [-1], [-1], [ 1], [-1]]) Note that if is_open_safe was False, the first exit would be executed at the second bar. This is because we don't know whether the entry price comes before the high and low price at the first bar, and so the trailing stop isn't triggered for the low price of 9.0. generate_rand_by_prob_nb function \u00b6 generate_rand_by_prob_nb ( shape , prob , pick_first , flex_2d , seed = None ) Create a boolean matrix of shape and pick signals randomly by probability prob . prob should be a 2-dim array of shape shape . Specify seed to make output deterministic. See rand_by_prob_choice_nb() . generate_rand_enex_by_prob_nb function \u00b6 generate_rand_enex_by_prob_nb ( shape , entry_prob , exit_prob , entry_wait , exit_wait , entry_pick_first , exit_pick_first , flex_2d , seed = None ) Pick entries by probability entry_prob and exits by probability exit_prob one after another. entry_prob and exit_prob should be 2-dim arrays of shape shape . Specify seed to make output deterministic. generate_rand_enex_nb function \u00b6 generate_rand_enex_nb ( shape , n , entry_wait , exit_wait , seed = None ) Pick a number of entries and the same number of exits one after another. Respects entry_wait and exit_wait constraints through a number of tricks. Tries to mimic a uniform distribution as much as possible. The idea is the following: with constraints, there is some fixed amount of total space required between first entry and last exit. Upscale this space in a way that distribution of entries and exit is similar to a uniform distribution. This means randomizing the position of first entry, last exit, and all signals between them. n uses flexible indexing. Specify seed to make output deterministic. generate_rand_ex_by_prob_nb function \u00b6 generate_rand_ex_by_prob_nb ( entries , prob , wait , until_next , skip_until_exit , flex_2d , seed = None ) Pick an exit after each entry in entries by probability prob . prob should be a 2-dim array of shape shape . Specify seed to make output deterministic. generate_rand_ex_nb function \u00b6 generate_rand_ex_nb ( entries , wait , until_next , skip_until_exit , seed = None ) Pick an exit after each entry in entries . Specify seed to make output deterministic. generate_rand_nb function \u00b6 generate_rand_nb ( shape , n , seed = None ) Create a boolean matrix of shape and pick a number of signals randomly. Specify seed to make output deterministic. See rand_choice_nb() . generate_stop_enex_nb function \u00b6 generate_stop_enex_nb ( entries , ts , stop , trailing , entry_wait , exit_wait , pick_first , flex_2d ) Generate one after another using generate_enex_nb() and stop_choice_nb() . Returns two arrays: new entries and exits. Note Has the same logic as calling generate_stop_ex_nb() with skip_until_exit=True , but removes all entries that come before the next exit. generate_stop_ex_nb function \u00b6 generate_stop_ex_nb ( entries , ts , stop , trailing , wait , until_next , skip_until_exit , pick_first , flex_2d ) Generate using generate_ex_nb() and stop_choice_nb() . Usage Generate trailing stop loss and take profit signals for 10%. >>> import numpy as np >>> from vectorbt.signals.nb import generate_stop_ex_nb >>> entries = np . asarray ([ False , True , False , False , False ])[:, None ] >>> ts = np . asarray ([ 1 , 2 , 3 , 2 , 1 ])[:, None ] >>> generate_stop_ex_nb ( entries , ts , - 0.1 , True , 1 , True , True ) array([[False], [False], [False], [ True], [False]]) >>> generate_stop_ex_nb ( entries , ts , 0.1 , False , 1 , True , True ) array([[False], [False], [ True], [False], [False]]) norm_avg_index_1d_nb function \u00b6 norm_avg_index_1d_nb ( a ) Get mean index normalized to (-1, 1). norm_avg_index_nb function \u00b6 norm_avg_index_nb ( a ) 2-dim version of norm_avg_index_1d_nb() . nth_index_1d_nb function \u00b6 nth_index_1d_nb ( a , n ) Get the index of the n-th True value. Note n starts with 0 and can be negative. nth_index_nb function \u00b6 nth_index_nb ( a , n ) 2-dim version of nth_index_1d_nb() . ohlc_stop_choice_nb function \u00b6 ohlc_stop_choice_nb ( from_i , to_i , col , open , high , low , close , stop_price_out , stop_type_out , sl_stop , sl_trail , tp_stop , reverse , is_open_safe , wait , pick_first , temp_idx_arr , flex_2d ) choice_func_nb that returns the indices of the stop price being hit within OHLC. Compared to stop_choice_nb() , takes into account the whole bar, can check for both (trailing) stop loss and take profit simultaneously, and tracks hit price and stop type. Note We don't have intra-candle data. If there was a huge price fluctuation in both directions, we can't determine whether SL was triggered before TP and vice versa. So some assumptions need to be made: 1) trailing stop can only be based on previous close/high, and 2) we pessimistically assume that SL comes before TP. Args col :\u2002 int Current column. from_i :\u2002 int Index to start generation from (inclusive). to_i :\u2002 int Index to run generation to (exclusive). open :\u2002 array of float Entry price such as open or previous close. high :\u2002 array of float High price. low :\u2002 array of float Low price. close :\u2002 array of float Close price. stop_price_out :\u2002 array of float Array where hit price of each exit will be stored. stop_type_out :\u2002 array of int Array where stop type of each exit will be stored. 0 for stop loss, 1 for take profit. sl_stop :\u2002 float or array_like Percentage value for stop loss. Can be per frame, column, row, or element-wise. Set to np.nan to disable. sl_trail :\u2002 bool or array_like Whether sl_stop is trailing. Can be per frame, column, row, or element-wise. Set to False to disable. tp_stop :\u2002 float or array_like Percentage value for take profit. Can be per frame, column, row, or element-wise. Set to np.nan to disable. reverse :\u2002 bool or array_like Whether to do the opposite, i.e.: prices are followed downwards. is_open_safe :\u2002 bool Whether entry price comes right at or before open. If True and wait is 0, can use high/low at entry bar. Otherwise uses only close. wait :\u2002 int Number of ticks to wait before placing exits. Setting False or 0 may result in entry and exit signal at one bar. Note If wait is greater than 0, even with is_open_safe set to True, trailing stop won't update at bars that come before from_i . pick_first :\u2002 bool Whether to stop as soon as the first exit signal is found. temp_idx_arr :\u2002 array of int Empty integer array used to temporarily store indices. flex_2d :\u2002 bool See flex_select_auto_nb() . part_pos_rank_nb function \u00b6 part_pos_rank_nb ( i , col , reset_i , prev_part_end_i , part_start_i , part_pos_temp ) rank_func_nb that returns the rank of each partition by its position in the series. partition_ranges_nb function \u00b6 partition_ranges_nb ( a ) Create a record of type range_dt for each partition of signals in a . rand_by_prob_choice_nb function \u00b6 rand_by_prob_choice_nb ( from_i , to_i , col , prob , pick_first , temp_idx_arr , flex_2d ) choice_func_nb to randomly pick values from range [from_i, to_i) with probability prob . prob uses flexible indexing. rand_choice_nb function \u00b6 rand_choice_nb ( from_i , to_i , col , n ) choice_func_nb to randomly pick n values from range [from_i, to_i) . n uses flexible indexing. rand_enex_apply_nb function \u00b6 rand_enex_apply_nb ( input_shape , n , entry_wait , exit_wait ) apply_func_nb that calls generate_rand_enex_nb() . rank_nb function \u00b6 rank_nb ( a , reset_by , after_false , rank_func_nb , * args ) Rank each signal using rank_func_nb . Applies rank_func_nb on each True value. Should accept index of the row, index of the column, index of the last reset signal, index of the end of the previous partition, index of the start of the current partition, and *args . Should return -1 for no rank, otherwise 0 or greater. Setting after_false to True will disregard the first partition of True values if there is no False value before them. sig_pos_rank_nb function \u00b6 sig_pos_rank_nb ( i , col , reset_i , prev_part_end_i , part_start_i , sig_pos_temp , allow_gaps ) rank_func_nb that returns the rank of each signal by its position in the partition. stop_choice_nb function \u00b6 stop_choice_nb ( from_i , to_i , col , ts , stop , trailing , wait , pick_first , temp_idx_arr , flex_2d ) choice_func_nb that returns the indices of the stop being hit. Args from_i :\u2002 int Index to start generation from (inclusive). to_i :\u2002 int Index to run generation to (exclusive). col :\u2002 int Current column. ts :\u2002 array of float 2-dim time series array such as price. stop :\u2002 float or array_like Stop value for stop loss. Can be per frame, column, row, or element-wise. Set to np.nan to disable. trailing :\u2002 bool or array_like Whether to use trailing stop. Can be per frame, column, row, or element-wise. Set to False to disable. wait :\u2002 int Number of ticks to wait before placing exits. Setting False or 0 may result in two signals at one bar. Note If wait is greater than 0, trailing stop won't update at bars that come before from_i . pick_first :\u2002 bool Whether to stop as soon as the first exit signal is found. temp_idx_arr :\u2002 array of int Empty integer array used to temporarily store indices. flex_2d :\u2002 bool See flex_select_auto_nb() .","title":"nb"},{"location":"api/signals/nb/#vectorbt.signals.nb","text":"Numba-compiled functions. Provides an arsenal of Numba-compiled functions that are used by accessors and in many other parts of the backtesting pipeline, such as technical indicators. These only accept NumPy arrays and other Numba-compatible types. >>> import numpy as np >>> import vectorbt as vbt >>> # vectorbt.signals.nb.pos_rank_nb >>> vbt . signals . nb . pos_rank_nb ( np . array ([ False , True , True , True , False ])[:, None ])[:, 0 ] [-1 0 1 2 -1] Note vectorbt treats matrices as first-class citizens and expects input arrays to be 2-dim, unless function has suffix _1d or is meant to be input to another function. Data is processed along index (axis 0). All functions passed as argument should be Numba-compiled. Returned indices should be absolute.","title":"vectorbt.signals.nb"},{"location":"api/signals/nb/#vectorbt.signals.nb.between_partition_ranges_nb","text":"between_partition_ranges_nb ( a ) Create a record of type range_dt for each range between two partitions in a .","title":"between_partition_ranges_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.between_ranges_nb","text":"between_ranges_nb ( a ) Create a record of type range_dt for each range between two signals in a .","title":"between_ranges_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.between_two_ranges_nb","text":"between_two_ranges_nb ( a , b , from_other = False ) Create a record of type range_dt for each range between two signals in a and b . If from_other is False, returns ranges from each in a to the succeeding in b . Otherwise, returns ranges from each in b to the preceding in a . When a and b overlap (two signals at the same time), the distance between overlapping signals is still considered and from_i would match to_i .","title":"between_two_ranges_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.clean_enex_1d_nb","text":"clean_enex_1d_nb ( entries , exits , entry_first ) Clean entry and exit arrays by picking the first signal out of each. Entry signal must be picked first. If both signals are present, selects none.","title":"clean_enex_1d_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.clean_enex_nb","text":"clean_enex_nb ( entries , exits , entry_first ) 2-dim version of clean_enex_1d_nb() .","title":"clean_enex_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.first_choice_nb","text":"first_choice_nb ( from_i , to_i , col , a ) choice_func_nb that returns the index of the first signal in a .","title":"first_choice_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.generate_enex_nb","text":"generate_enex_nb ( shape , entry_wait , exit_wait , entry_pick_first , exit_pick_first , entry_choice_func_nb , entry_args , exit_choice_func_nb , exit_args ) Pick entry signals using entry_choice_func_nb and exit signals using exit_choice_func_nb one after another. Args shape :\u2002 array Target shape. entry_wait :\u2002 int Number of ticks to wait before placing entries. Note Setting entry_wait to 0 or False assumes that both entry and exit can be processed within the same bar, and exit can be processed before entry. exit_wait :\u2002 int Number of ticks to wait before placing exits. Note Setting exit_wait to 0 or False assumes that both entry and exit can be processed within the same bar, and entry can be processed before exit. entry_pick_first :\u2002 bool Whether to pick the first entry out of all returned by entry_choice_func_nb . exit_pick_first :\u2002 bool Whether to pick the first exit out of all returned by exit_choice_func_nb . Setting it to False acts similarly to setting skip_until_exit to True in generate_ex_nb() . entry_choice_func_nb :\u2002 callable Entry choice function. See choice_func_nb in generate_nb() . entry_args :\u2002 tuple Arguments unpacked and passed to entry_choice_func_nb . exit_choice_func_nb :\u2002 callable Exit choice function. See choice_func_nb in generate_nb() . exit_args :\u2002 tuple Arguments unpacked and passed to exit_choice_func_nb .","title":"generate_enex_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.generate_ex_nb","text":"generate_ex_nb ( entries , wait , until_next , skip_until_exit , pick_first , exit_choice_func_nb , * args ) Pick exit signals using exit_choice_func_nb after each signal in entries . Args entries :\u2002 array Boolean array with entry signals. wait :\u2002 int Number of ticks to wait before placing exits. Note Setting wait to 0 or False may result in two signals at one bar. until_next :\u2002 int Whether to place signals up to the next entry signal. Note Setting it to False makes it difficult to tell which exit belongs to which entry. skip_until_exit :\u2002 bool Whether to skip processing entry signals until the next exit. Has only effect when until_next is disabled. Note Setting it to True makes it difficult to tell which exit belongs to which entry. pick_first :\u2002 bool Whether to pick the first signal out of all returned by exit_choice_func_nb . exit_choice_func_nb :\u2002 callable Exit choice function. See choice_func_nb in generate_nb() . *args :\u2002 callable Arguments passed to exit_choice_func_nb .","title":"generate_ex_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.generate_nb","text":"generate_nb ( shape , pick_first , choice_func_nb , * args ) Create a boolean matrix of shape and pick signals using choice_func_nb . Args shape :\u2002 array Target shape. pick_first :\u2002 bool Whether to pick the first signal out of all returned by choice_func_nb . choice_func_nb :\u2002 callable Choice function. choice_func_nb should accept index of the start of the range from_i , index of the end of the range to_i , index of the column col , and *args . It should return an array of indices from [from_i, to_i) (can be empty). *args Arguments passed to choice_func_nb . Usage >>> from numba import njit >>> import numpy as np >>> from vectorbt.signals.nb import generate_nb >>> @njit ... def choice_func_nb ( from_i , to_i , col ): ... return np . array ([ from_i + col ]) >>> generate_nb (( 5 , 3 ), choice_func_nb ) [[ True False False] [False True False] [False False True] [False False False] [False False False]]","title":"generate_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.generate_ohlc_stop_enex_nb","text":"generate_ohlc_stop_enex_nb ( entries , open , high , low , close , stop_price_out , stop_type_out , sl_stop , sl_trail , tp_stop , reverse , is_open_safe , entry_wait , exit_wait , pick_first , flex_2d ) Generate one after another using generate_enex_nb() and ohlc_stop_choice_nb() . Returns two arrays: new entries and exits. Note Has the same logic as calling generate_ohlc_stop_ex_nb() with skip_until_exit=True , but removes all entries that come before the next exit.","title":"generate_ohlc_stop_enex_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.generate_ohlc_stop_ex_nb","text":"generate_ohlc_stop_ex_nb ( entries , open , high , low , close , stop_price_out , stop_type_out , sl_stop , sl_trail , tp_stop , reverse , is_open_safe , wait , until_next , skip_until_exit , pick_first , flex_2d ) Generate using generate_ex_nb() and ohlc_stop_choice_nb() . Usage Generate trailing stop loss and take profit signals for 10%. Illustrates how exit signal can be generated within the same bar as entry. >>> import numpy as np >>> from vectorbt.signals.nb import generate_ohlc_stop_ex_nb >>> entries = np . asarray ([ True , False , True , False , False ])[:, None ] >>> entry_price = np . asarray ([ 10 , 11 , 12 , 11 , 10 ])[:, None ] >>> high_price = entry_price + 1 >>> low_price = entry_price - 1 >>> close_price = entry_price >>> stop_price_out = np . full_like ( entries , np . nan , dtype = np . float_ ) >>> stop_type_out = np . full_like ( entries , - 1 , dtype = np . int_ ) >>> generate_ohlc_stop_ex_nb ( ... entries = entries , ... open = entry_price , ... high = high_price , ... low = low_price , ... close = close_price , ... stop_price_out = stop_price_out , ... stop_type_out = stop_type_out , ... sl_stop = 0.1 , ... sl_trail = True , ... tp_stop = 0.1 , ... reverse = False , ... is_open_safe = True , ... wait = 1 , ... until_next = True , ... skip_until_exit = False , ... pick_first = True , ... flex_2d = True ... ) array([[ True], [False], [False], [ True], [False]]) >>> stop_price_out array([[ 9. ], << trailing SL from 10 (entry_price) [ nan], [ nan], [11.7], << trailing SL from 13 (high_price) [ nan]]) >>> stop_type_out array([[ 1], [-1], [-1], [ 1], [-1]]) Note that if is_open_safe was False, the first exit would be executed at the second bar. This is because we don't know whether the entry price comes before the high and low price at the first bar, and so the trailing stop isn't triggered for the low price of 9.0.","title":"generate_ohlc_stop_ex_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.generate_rand_by_prob_nb","text":"generate_rand_by_prob_nb ( shape , prob , pick_first , flex_2d , seed = None ) Create a boolean matrix of shape and pick signals randomly by probability prob . prob should be a 2-dim array of shape shape . Specify seed to make output deterministic. See rand_by_prob_choice_nb() .","title":"generate_rand_by_prob_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.generate_rand_enex_by_prob_nb","text":"generate_rand_enex_by_prob_nb ( shape , entry_prob , exit_prob , entry_wait , exit_wait , entry_pick_first , exit_pick_first , flex_2d , seed = None ) Pick entries by probability entry_prob and exits by probability exit_prob one after another. entry_prob and exit_prob should be 2-dim arrays of shape shape . Specify seed to make output deterministic.","title":"generate_rand_enex_by_prob_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.generate_rand_enex_nb","text":"generate_rand_enex_nb ( shape , n , entry_wait , exit_wait , seed = None ) Pick a number of entries and the same number of exits one after another. Respects entry_wait and exit_wait constraints through a number of tricks. Tries to mimic a uniform distribution as much as possible. The idea is the following: with constraints, there is some fixed amount of total space required between first entry and last exit. Upscale this space in a way that distribution of entries and exit is similar to a uniform distribution. This means randomizing the position of first entry, last exit, and all signals between them. n uses flexible indexing. Specify seed to make output deterministic.","title":"generate_rand_enex_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.generate_rand_ex_by_prob_nb","text":"generate_rand_ex_by_prob_nb ( entries , prob , wait , until_next , skip_until_exit , flex_2d , seed = None ) Pick an exit after each entry in entries by probability prob . prob should be a 2-dim array of shape shape . Specify seed to make output deterministic.","title":"generate_rand_ex_by_prob_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.generate_rand_ex_nb","text":"generate_rand_ex_nb ( entries , wait , until_next , skip_until_exit , seed = None ) Pick an exit after each entry in entries . Specify seed to make output deterministic.","title":"generate_rand_ex_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.generate_rand_nb","text":"generate_rand_nb ( shape , n , seed = None ) Create a boolean matrix of shape and pick a number of signals randomly. Specify seed to make output deterministic. See rand_choice_nb() .","title":"generate_rand_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.generate_stop_enex_nb","text":"generate_stop_enex_nb ( entries , ts , stop , trailing , entry_wait , exit_wait , pick_first , flex_2d ) Generate one after another using generate_enex_nb() and stop_choice_nb() . Returns two arrays: new entries and exits. Note Has the same logic as calling generate_stop_ex_nb() with skip_until_exit=True , but removes all entries that come before the next exit.","title":"generate_stop_enex_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.generate_stop_ex_nb","text":"generate_stop_ex_nb ( entries , ts , stop , trailing , wait , until_next , skip_until_exit , pick_first , flex_2d ) Generate using generate_ex_nb() and stop_choice_nb() . Usage Generate trailing stop loss and take profit signals for 10%. >>> import numpy as np >>> from vectorbt.signals.nb import generate_stop_ex_nb >>> entries = np . asarray ([ False , True , False , False , False ])[:, None ] >>> ts = np . asarray ([ 1 , 2 , 3 , 2 , 1 ])[:, None ] >>> generate_stop_ex_nb ( entries , ts , - 0.1 , True , 1 , True , True ) array([[False], [False], [False], [ True], [False]]) >>> generate_stop_ex_nb ( entries , ts , 0.1 , False , 1 , True , True ) array([[False], [False], [ True], [False], [False]])","title":"generate_stop_ex_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.norm_avg_index_1d_nb","text":"norm_avg_index_1d_nb ( a ) Get mean index normalized to (-1, 1).","title":"norm_avg_index_1d_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.norm_avg_index_nb","text":"norm_avg_index_nb ( a ) 2-dim version of norm_avg_index_1d_nb() .","title":"norm_avg_index_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.nth_index_1d_nb","text":"nth_index_1d_nb ( a , n ) Get the index of the n-th True value. Note n starts with 0 and can be negative.","title":"nth_index_1d_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.nth_index_nb","text":"nth_index_nb ( a , n ) 2-dim version of nth_index_1d_nb() .","title":"nth_index_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.ohlc_stop_choice_nb","text":"ohlc_stop_choice_nb ( from_i , to_i , col , open , high , low , close , stop_price_out , stop_type_out , sl_stop , sl_trail , tp_stop , reverse , is_open_safe , wait , pick_first , temp_idx_arr , flex_2d ) choice_func_nb that returns the indices of the stop price being hit within OHLC. Compared to stop_choice_nb() , takes into account the whole bar, can check for both (trailing) stop loss and take profit simultaneously, and tracks hit price and stop type. Note We don't have intra-candle data. If there was a huge price fluctuation in both directions, we can't determine whether SL was triggered before TP and vice versa. So some assumptions need to be made: 1) trailing stop can only be based on previous close/high, and 2) we pessimistically assume that SL comes before TP. Args col :\u2002 int Current column. from_i :\u2002 int Index to start generation from (inclusive). to_i :\u2002 int Index to run generation to (exclusive). open :\u2002 array of float Entry price such as open or previous close. high :\u2002 array of float High price. low :\u2002 array of float Low price. close :\u2002 array of float Close price. stop_price_out :\u2002 array of float Array where hit price of each exit will be stored. stop_type_out :\u2002 array of int Array where stop type of each exit will be stored. 0 for stop loss, 1 for take profit. sl_stop :\u2002 float or array_like Percentage value for stop loss. Can be per frame, column, row, or element-wise. Set to np.nan to disable. sl_trail :\u2002 bool or array_like Whether sl_stop is trailing. Can be per frame, column, row, or element-wise. Set to False to disable. tp_stop :\u2002 float or array_like Percentage value for take profit. Can be per frame, column, row, or element-wise. Set to np.nan to disable. reverse :\u2002 bool or array_like Whether to do the opposite, i.e.: prices are followed downwards. is_open_safe :\u2002 bool Whether entry price comes right at or before open. If True and wait is 0, can use high/low at entry bar. Otherwise uses only close. wait :\u2002 int Number of ticks to wait before placing exits. Setting False or 0 may result in entry and exit signal at one bar. Note If wait is greater than 0, even with is_open_safe set to True, trailing stop won't update at bars that come before from_i . pick_first :\u2002 bool Whether to stop as soon as the first exit signal is found. temp_idx_arr :\u2002 array of int Empty integer array used to temporarily store indices. flex_2d :\u2002 bool See flex_select_auto_nb() .","title":"ohlc_stop_choice_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.part_pos_rank_nb","text":"part_pos_rank_nb ( i , col , reset_i , prev_part_end_i , part_start_i , part_pos_temp ) rank_func_nb that returns the rank of each partition by its position in the series.","title":"part_pos_rank_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.partition_ranges_nb","text":"partition_ranges_nb ( a ) Create a record of type range_dt for each partition of signals in a .","title":"partition_ranges_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.rand_by_prob_choice_nb","text":"rand_by_prob_choice_nb ( from_i , to_i , col , prob , pick_first , temp_idx_arr , flex_2d ) choice_func_nb to randomly pick values from range [from_i, to_i) with probability prob . prob uses flexible indexing.","title":"rand_by_prob_choice_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.rand_choice_nb","text":"rand_choice_nb ( from_i , to_i , col , n ) choice_func_nb to randomly pick n values from range [from_i, to_i) . n uses flexible indexing.","title":"rand_choice_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.rand_enex_apply_nb","text":"rand_enex_apply_nb ( input_shape , n , entry_wait , exit_wait ) apply_func_nb that calls generate_rand_enex_nb() .","title":"rand_enex_apply_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.rank_nb","text":"rank_nb ( a , reset_by , after_false , rank_func_nb , * args ) Rank each signal using rank_func_nb . Applies rank_func_nb on each True value. Should accept index of the row, index of the column, index of the last reset signal, index of the end of the previous partition, index of the start of the current partition, and *args . Should return -1 for no rank, otherwise 0 or greater. Setting after_false to True will disregard the first partition of True values if there is no False value before them.","title":"rank_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.sig_pos_rank_nb","text":"sig_pos_rank_nb ( i , col , reset_i , prev_part_end_i , part_start_i , sig_pos_temp , allow_gaps ) rank_func_nb that returns the rank of each signal by its position in the partition.","title":"sig_pos_rank_nb()"},{"location":"api/signals/nb/#vectorbt.signals.nb.stop_choice_nb","text":"stop_choice_nb ( from_i , to_i , col , ts , stop , trailing , wait , pick_first , temp_idx_arr , flex_2d ) choice_func_nb that returns the indices of the stop being hit. Args from_i :\u2002 int Index to start generation from (inclusive). to_i :\u2002 int Index to run generation to (exclusive). col :\u2002 int Current column. ts :\u2002 array of float 2-dim time series array such as price. stop :\u2002 float or array_like Stop value for stop loss. Can be per frame, column, row, or element-wise. Set to np.nan to disable. trailing :\u2002 bool or array_like Whether to use trailing stop. Can be per frame, column, row, or element-wise. Set to False to disable. wait :\u2002 int Number of ticks to wait before placing exits. Setting False or 0 may result in two signals at one bar. Note If wait is greater than 0, trailing stop won't update at bars that come before from_i . pick_first :\u2002 bool Whether to stop as soon as the first exit signal is found. temp_idx_arr :\u2002 array of int Empty integer array used to temporarily store indices. flex_2d :\u2002 bool See flex_select_auto_nb() .","title":"stop_choice_nb()"},{"location":"api/utils/","text":"utils package \u00b6 Modules with utilities that are used throughout vectorbt. Sub-modules \u00b6 vectorbt.utils.array_ vectorbt.utils.attr_ vectorbt.utils.checks vectorbt.utils.colors vectorbt.utils.config vectorbt.utils.datetime_ vectorbt.utils.decorators vectorbt.utils.docs vectorbt.utils.enum_ vectorbt.utils.figure vectorbt.utils.image_ vectorbt.utils.mapping vectorbt.utils.math_ vectorbt.utils.module_ vectorbt.utils.params vectorbt.utils.random_ vectorbt.utils.requests_ vectorbt.utils.schedule_ vectorbt.utils.tags vectorbt.utils.template","title":"utils"},{"location":"api/utils/#vectorbt.utils","text":"Modules with utilities that are used throughout vectorbt.","title":"vectorbt.utils"},{"location":"api/utils/#sub-modules","text":"vectorbt.utils.array_ vectorbt.utils.attr_ vectorbt.utils.checks vectorbt.utils.colors vectorbt.utils.config vectorbt.utils.datetime_ vectorbt.utils.decorators vectorbt.utils.docs vectorbt.utils.enum_ vectorbt.utils.figure vectorbt.utils.image_ vectorbt.utils.mapping vectorbt.utils.math_ vectorbt.utils.module_ vectorbt.utils.params vectorbt.utils.random_ vectorbt.utils.requests_ vectorbt.utils.schedule_ vectorbt.utils.tags vectorbt.utils.template","title":"Sub-modules"},{"location":"api/utils/array_/","text":"array_ module \u00b6 Utilities for working with arrays. get_ranges_arr function \u00b6 get_ranges_arr ( starts , ends ) Build array from start and end indices. Based on https://stackoverflow.com/a/37626057 insert_argsort_nb function \u00b6 insert_argsort_nb ( A , I ) Perform argsort using insertion sort. In-memory and without recursion -> very fast for smaller arrays. is_sorted function \u00b6 is_sorted ( a ) Checks if array is sorted. is_sorted_nb function \u00b6 is_sorted_nb ( a ) Numba-compiled version of is_sorted() . max_rel_rescale function \u00b6 max_rel_rescale ( a , to_range ) Rescale elements in a relatively to maximum. min_rel_rescale function \u00b6 min_rel_rescale ( a , to_range ) Rescale elements in a relatively to minimum. renormalize function \u00b6 renormalize ( a , from_range , to_range ) Renormalize a from one range to another. renormalize_nb function \u00b6 renormalize ( a , from_range , to_range ) Renormalize a from one range to another. rescale_float_to_int_nb function \u00b6 rescale_float_to_int_nb ( floats , int_range , total ) Rescale a float array into an int array. uniform_summing_to_one_nb function \u00b6 uniform_summing_to_one_nb ( n ) Generate random floats summing to one. See # https://stackoverflow.com/a/2640067/8141780","title":"array_"},{"location":"api/utils/array_/#vectorbt.utils.array_","text":"Utilities for working with arrays.","title":"vectorbt.utils.array_"},{"location":"api/utils/array_/#vectorbt.utils.array_.get_ranges_arr","text":"get_ranges_arr ( starts , ends ) Build array from start and end indices. Based on https://stackoverflow.com/a/37626057","title":"get_ranges_arr()"},{"location":"api/utils/array_/#vectorbt.utils.array_.insert_argsort_nb","text":"insert_argsort_nb ( A , I ) Perform argsort using insertion sort. In-memory and without recursion -> very fast for smaller arrays.","title":"insert_argsort_nb()"},{"location":"api/utils/array_/#vectorbt.utils.array_.is_sorted","text":"is_sorted ( a ) Checks if array is sorted.","title":"is_sorted()"},{"location":"api/utils/array_/#vectorbt.utils.array_.is_sorted_nb","text":"is_sorted_nb ( a ) Numba-compiled version of is_sorted() .","title":"is_sorted_nb()"},{"location":"api/utils/array_/#vectorbt.utils.array_.max_rel_rescale","text":"max_rel_rescale ( a , to_range ) Rescale elements in a relatively to maximum.","title":"max_rel_rescale()"},{"location":"api/utils/array_/#vectorbt.utils.array_.min_rel_rescale","text":"min_rel_rescale ( a , to_range ) Rescale elements in a relatively to minimum.","title":"min_rel_rescale()"},{"location":"api/utils/array_/#vectorbt.utils.array_.renormalize","text":"renormalize ( a , from_range , to_range ) Renormalize a from one range to another.","title":"renormalize()"},{"location":"api/utils/array_/#vectorbt.utils.array_.renormalize_nb","text":"renormalize ( a , from_range , to_range ) Renormalize a from one range to another.","title":"renormalize_nb()"},{"location":"api/utils/array_/#vectorbt.utils.array_.rescale_float_to_int_nb","text":"rescale_float_to_int_nb ( floats , int_range , total ) Rescale a float array into an int array.","title":"rescale_float_to_int_nb()"},{"location":"api/utils/array_/#vectorbt.utils.array_.uniform_summing_to_one_nb","text":"uniform_summing_to_one_nb ( n ) Generate random floats summing to one. See # https://stackoverflow.com/a/2640067/8141780","title":"uniform_summing_to_one_nb()"},{"location":"api/utils/attr_/","text":"attr_ module \u00b6 Utilities for working with class/instance attributes. deep_getattr function \u00b6 deep_getattr ( obj , attr_chain , getattr_func =< function default_getattr_func > , call_last_attr = True ) Retrieve attribute consecutively. The attribute chain attr_chain can be: string -> get variable/property or method without arguments tuple of string -> call method without arguments tuple of string and tuple -> call method and pass positional arguments (unpacked) tuple of string, tuple, and dict -> call method and pass positional and keyword arguments (unpacked) iterable of any of the above Use getattr_func to overwrite the default behavior of accessing an attribute (see default_getattr_func() ). Hint If your chain includes only attributes and functions without arguments, you can represent this chain as a single (but probably long) string. default_getattr_func function \u00b6 default_getattr_func ( obj , attr , args = None , kwargs = None , call_attr = True ) Default getattr_func . get_dict_attr function \u00b6 get_dict_attr ( obj , attr ) Get attribute without invoking the attribute lookup machinery. AttrResolver class \u00b6 Class that implements resolution of self and its attributes. Resolution is getattr that works for self, properties, and methods. It also utilizes built-in caching. Subclasses Wrapping deep_getattr method \u00b6 AttrResolver . deep_getattr ( * args , ** kwargs ) See deep_getattr() . post_resolve_attr method \u00b6 AttrResolver . post_resolve_attr ( attr , out , final_kwargs = None ) Post-process an object after resolution. Should return an object. pre_resolve_attr method \u00b6 AttrResolver . pre_resolve_attr ( attr , final_kwargs = None ) Pre-process an attribute before resolution. Should return an attribute. resolve_attr method \u00b6 AttrResolver . resolve_attr ( attr , args = None , cond_kwargs = None , kwargs = None , custom_arg_names = None , cache_dct = None , use_caching = True , passed_kwargs_out = None ) Resolve an attribute using keyword arguments and built-in caching. If attr is a property, returns its value. If attr is a method, passes *args , **kwargs , and **cond_kwargs with keys found in the signature. If attr is a property and there is a get_{arg} method, calls the get_{arg} method. Won't cache if use_caching is False or any passed argument is in custom_arg_names . Use passed_kwargs_out to get keyword arguments that were passed. resolve_self method \u00b6 AttrResolver . resolve_self ( cond_kwargs = None , custom_arg_names = None , impacts_caching = True , silence_warnings = False ) Resolve self. Note cond_kwargs can be modified in-place. self_aliases property \u00b6 Names to associate with this object.","title":"attr_"},{"location":"api/utils/attr_/#vectorbt.utils.attr_","text":"Utilities for working with class/instance attributes.","title":"vectorbt.utils.attr_"},{"location":"api/utils/attr_/#vectorbt.utils.attr_.deep_getattr","text":"deep_getattr ( obj , attr_chain , getattr_func =< function default_getattr_func > , call_last_attr = True ) Retrieve attribute consecutively. The attribute chain attr_chain can be: string -> get variable/property or method without arguments tuple of string -> call method without arguments tuple of string and tuple -> call method and pass positional arguments (unpacked) tuple of string, tuple, and dict -> call method and pass positional and keyword arguments (unpacked) iterable of any of the above Use getattr_func to overwrite the default behavior of accessing an attribute (see default_getattr_func() ). Hint If your chain includes only attributes and functions without arguments, you can represent this chain as a single (but probably long) string.","title":"deep_getattr()"},{"location":"api/utils/attr_/#vectorbt.utils.attr_.default_getattr_func","text":"default_getattr_func ( obj , attr , args = None , kwargs = None , call_attr = True ) Default getattr_func .","title":"default_getattr_func()"},{"location":"api/utils/attr_/#vectorbt.utils.attr_.get_dict_attr","text":"get_dict_attr ( obj , attr ) Get attribute without invoking the attribute lookup machinery.","title":"get_dict_attr()"},{"location":"api/utils/attr_/#vectorbt.utils.attr_.AttrResolver","text":"Class that implements resolution of self and its attributes. Resolution is getattr that works for self, properties, and methods. It also utilizes built-in caching. Subclasses Wrapping","title":"AttrResolver"},{"location":"api/utils/attr_/#vectorbt.utils.attr_.AttrResolver.deep_getattr","text":"AttrResolver . deep_getattr ( * args , ** kwargs ) See deep_getattr() .","title":"deep_getattr()"},{"location":"api/utils/attr_/#vectorbt.utils.attr_.AttrResolver.post_resolve_attr","text":"AttrResolver . post_resolve_attr ( attr , out , final_kwargs = None ) Post-process an object after resolution. Should return an object.","title":"post_resolve_attr()"},{"location":"api/utils/attr_/#vectorbt.utils.attr_.AttrResolver.pre_resolve_attr","text":"AttrResolver . pre_resolve_attr ( attr , final_kwargs = None ) Pre-process an attribute before resolution. Should return an attribute.","title":"pre_resolve_attr()"},{"location":"api/utils/attr_/#vectorbt.utils.attr_.AttrResolver.resolve_attr","text":"AttrResolver . resolve_attr ( attr , args = None , cond_kwargs = None , kwargs = None , custom_arg_names = None , cache_dct = None , use_caching = True , passed_kwargs_out = None ) Resolve an attribute using keyword arguments and built-in caching. If attr is a property, returns its value. If attr is a method, passes *args , **kwargs , and **cond_kwargs with keys found in the signature. If attr is a property and there is a get_{arg} method, calls the get_{arg} method. Won't cache if use_caching is False or any passed argument is in custom_arg_names . Use passed_kwargs_out to get keyword arguments that were passed.","title":"resolve_attr()"},{"location":"api/utils/attr_/#vectorbt.utils.attr_.AttrResolver.resolve_self","text":"AttrResolver . resolve_self ( cond_kwargs = None , custom_arg_names = None , impacts_caching = True , silence_warnings = False ) Resolve self. Note cond_kwargs can be modified in-place.","title":"resolve_self()"},{"location":"api/utils/attr_/#vectorbt.utils.attr_.AttrResolver.self_aliases","text":"Names to associate with this object.","title":"self_aliases"},{"location":"api/utils/checks/","text":"checks module \u00b6 Utilities for validation during runtime. assert_array_equal function \u00b6 assert_array_equal ( arg1 , arg2 ) Raise exception if the first argument and the second argument have different metadata or values. assert_dict_sequence_valid function \u00b6 assert_dict_sequence_valid ( arg , lvl_keys ) Raise exception if a dict or any dict in a sequence of dicts has keys that are not in lvl_keys . assert_dict_valid function \u00b6 assert_dict_valid ( arg , lvl_keys ) Raise exception if dict the argument has keys that are not in lvl_keys . lvl_keys should be a list of lists, each corresponding to a level in the dict. assert_dtype function \u00b6 assert_dtype ( arg , dtype ) Raise exception if the argument is not of data type dtype . assert_dtype_equal function \u00b6 assert_dtype_equal ( arg1 , arg2 ) Raise exception if the first argument and the second argument have different data types. assert_equal function \u00b6 assert_equal ( arg1 , arg2 , deep = False ) Raise exception if the first argument and the second argument are different. assert_in function \u00b6 assert_in ( arg1 , arg2 ) Raise exception if the first argument is not in the second argument. assert_index_equal function \u00b6 assert_index_equal ( arg1 , arg2 , ** kwargs ) Raise exception if the first argument and the second argument have different index/columns. assert_instance_of function \u00b6 assert_instance_of ( arg , types ) Raise exception if the argument is none of types types . assert_iterable function \u00b6 assert_iterable ( arg ) Raise exception if the argument is not an iterable. assert_len_equal function \u00b6 assert_len_equal ( arg1 , arg2 ) Raise exception if the first argument and the second argument have different length. Does not transform arguments to NumPy arrays. assert_level_not_exists function \u00b6 assert_level_not_exists ( arg , level_name ) Raise exception if index the argument has level level_name . assert_meta_equal function \u00b6 assert_meta_equal ( arg1 , arg2 ) Raise exception if the first argument and the second argument have different metadata. assert_ndim function \u00b6 assert_ndim ( arg , ndims ) Raise exception if the argument has a different number of dimensions than ndims . assert_not_none function \u00b6 assert_not_none ( arg ) Raise exception if the argument is None. assert_numba_func function \u00b6 assert_numba_func ( func ) Raise exception if func is not Numba-compiled. assert_sequence function \u00b6 assert_sequence ( arg ) Raise exception if the argument is not a sequence. assert_shape_equal function \u00b6 assert_shape_equal ( arg1 , arg2 , axis = None ) Raise exception if the first argument and the second argument have different shapes along axis . assert_subclass_of function \u00b6 assert_subclass_of ( arg , classes ) Raise exception if the argument is not a subclass of classes classes . assert_subdtype function \u00b6 assert_subdtype ( arg , dtype ) Raise exception if the argument is not a sub data type of dtype . assert_type_equal function \u00b6 assert_type_equal ( arg1 , arg2 ) Raise exception if the first argument and the second argument have different types. iskeyword function \u00b6 frozenset . __contains__ ( ... ) x. contains (y) <==> y in x. func_accepts_arg function \u00b6 func_accepts_arg ( func , arg_name , arg_kind = None ) Check whether func accepts a positional or keyword argument with name arg_name . is_any_array function \u00b6 is_any_array ( arg ) Check whether the argument is any of np.ndarray , pd.Series or pd.DataFrame . is_deep_equal function \u00b6 is_deep_equal ( arg1 , arg2 , check_exact = False , ** kwargs ) Check whether two objects are equal (deep check). is_default_index function \u00b6 is_default_index ( arg ) Check whether index is a basic range. is_equal function \u00b6 is_equal ( arg1 , arg2 , equality_func =< function < lambda >> ) Check whether two objects are equal. is_frame function \u00b6 is_frame ( arg ) Check whether the argument is pd.DataFrame . is_hashable function \u00b6 is_hashable ( arg ) Check whether the argument can be hashed. is_index function \u00b6 is_index ( arg ) Check whether the argument is pd.Index . is_index_equal function \u00b6 is_index_equal ( arg1 , arg2 , strict = True ) Check whether indexes are equal. Introduces naming tests on top of pd.Index.equals , but still doesn't check for types. is_instance_of function \u00b6 is_instance_of ( arg , types ) Check whether the argument is an instance of types . types can be one or multiple types or strings. is_iterable function \u00b6 is_iterable ( arg ) Check whether the argument is iterable. is_mapping function \u00b6 is_mapping ( arg ) Check whether the arguments is a mapping. is_mapping_like function \u00b6 is_mapping_like ( arg ) Check whether the arguments is a mapping-like object. is_namedtuple function \u00b6 is_namedtuple ( x ) Check whether object is an instance of namedtuple. is_np_array function \u00b6 is_np_array ( arg ) Check whether the argument is np.ndarray . is_numba_func function \u00b6 is_numba_func ( arg ) Check whether the argument is a Numba-compiled function. is_pandas function \u00b6 is_pandas ( arg ) Check whether the argument is pd.Series or pd.DataFrame . is_sequence function \u00b6 is_sequence ( arg ) Check whether the argument is a sequence. is_series function \u00b6 is_series ( arg ) Check whether the argument is pd.Series . is_subclass_of function \u00b6 is_subclass_of ( arg , types ) Check whether the argument is a subclass of types . types can be one or multiple types or strings. is_valid_variable_name function \u00b6 is_valid_variable_name ( arg ) Check whether the argument is a valid variable name. safe_assert function \u00b6 safe_assert ( arg , msg = None )","title":"checks"},{"location":"api/utils/checks/#vectorbt.utils.checks","text":"Utilities for validation during runtime.","title":"vectorbt.utils.checks"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_array_equal","text":"assert_array_equal ( arg1 , arg2 ) Raise exception if the first argument and the second argument have different metadata or values.","title":"assert_array_equal()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_dict_sequence_valid","text":"assert_dict_sequence_valid ( arg , lvl_keys ) Raise exception if a dict or any dict in a sequence of dicts has keys that are not in lvl_keys .","title":"assert_dict_sequence_valid()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_dict_valid","text":"assert_dict_valid ( arg , lvl_keys ) Raise exception if dict the argument has keys that are not in lvl_keys . lvl_keys should be a list of lists, each corresponding to a level in the dict.","title":"assert_dict_valid()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_dtype","text":"assert_dtype ( arg , dtype ) Raise exception if the argument is not of data type dtype .","title":"assert_dtype()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_dtype_equal","text":"assert_dtype_equal ( arg1 , arg2 ) Raise exception if the first argument and the second argument have different data types.","title":"assert_dtype_equal()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_equal","text":"assert_equal ( arg1 , arg2 , deep = False ) Raise exception if the first argument and the second argument are different.","title":"assert_equal()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_in","text":"assert_in ( arg1 , arg2 ) Raise exception if the first argument is not in the second argument.","title":"assert_in()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_index_equal","text":"assert_index_equal ( arg1 , arg2 , ** kwargs ) Raise exception if the first argument and the second argument have different index/columns.","title":"assert_index_equal()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_instance_of","text":"assert_instance_of ( arg , types ) Raise exception if the argument is none of types types .","title":"assert_instance_of()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_iterable","text":"assert_iterable ( arg ) Raise exception if the argument is not an iterable.","title":"assert_iterable()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_len_equal","text":"assert_len_equal ( arg1 , arg2 ) Raise exception if the first argument and the second argument have different length. Does not transform arguments to NumPy arrays.","title":"assert_len_equal()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_level_not_exists","text":"assert_level_not_exists ( arg , level_name ) Raise exception if index the argument has level level_name .","title":"assert_level_not_exists()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_meta_equal","text":"assert_meta_equal ( arg1 , arg2 ) Raise exception if the first argument and the second argument have different metadata.","title":"assert_meta_equal()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_ndim","text":"assert_ndim ( arg , ndims ) Raise exception if the argument has a different number of dimensions than ndims .","title":"assert_ndim()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_not_none","text":"assert_not_none ( arg ) Raise exception if the argument is None.","title":"assert_not_none()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_numba_func","text":"assert_numba_func ( func ) Raise exception if func is not Numba-compiled.","title":"assert_numba_func()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_sequence","text":"assert_sequence ( arg ) Raise exception if the argument is not a sequence.","title":"assert_sequence()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_shape_equal","text":"assert_shape_equal ( arg1 , arg2 , axis = None ) Raise exception if the first argument and the second argument have different shapes along axis .","title":"assert_shape_equal()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_subclass_of","text":"assert_subclass_of ( arg , classes ) Raise exception if the argument is not a subclass of classes classes .","title":"assert_subclass_of()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_subdtype","text":"assert_subdtype ( arg , dtype ) Raise exception if the argument is not a sub data type of dtype .","title":"assert_subdtype()"},{"location":"api/utils/checks/#vectorbt.utils.checks.assert_type_equal","text":"assert_type_equal ( arg1 , arg2 ) Raise exception if the first argument and the second argument have different types.","title":"assert_type_equal()"},{"location":"api/utils/checks/#vectorbt.utils.checks.iskeyword","text":"frozenset . __contains__ ( ... ) x. contains (y) <==> y in x.","title":"iskeyword()"},{"location":"api/utils/checks/#vectorbt.utils.checks.func_accepts_arg","text":"func_accepts_arg ( func , arg_name , arg_kind = None ) Check whether func accepts a positional or keyword argument with name arg_name .","title":"func_accepts_arg()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_any_array","text":"is_any_array ( arg ) Check whether the argument is any of np.ndarray , pd.Series or pd.DataFrame .","title":"is_any_array()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_deep_equal","text":"is_deep_equal ( arg1 , arg2 , check_exact = False , ** kwargs ) Check whether two objects are equal (deep check).","title":"is_deep_equal()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_default_index","text":"is_default_index ( arg ) Check whether index is a basic range.","title":"is_default_index()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_equal","text":"is_equal ( arg1 , arg2 , equality_func =< function < lambda >> ) Check whether two objects are equal.","title":"is_equal()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_frame","text":"is_frame ( arg ) Check whether the argument is pd.DataFrame .","title":"is_frame()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_hashable","text":"is_hashable ( arg ) Check whether the argument can be hashed.","title":"is_hashable()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_index","text":"is_index ( arg ) Check whether the argument is pd.Index .","title":"is_index()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_index_equal","text":"is_index_equal ( arg1 , arg2 , strict = True ) Check whether indexes are equal. Introduces naming tests on top of pd.Index.equals , but still doesn't check for types.","title":"is_index_equal()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_instance_of","text":"is_instance_of ( arg , types ) Check whether the argument is an instance of types . types can be one or multiple types or strings.","title":"is_instance_of()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_iterable","text":"is_iterable ( arg ) Check whether the argument is iterable.","title":"is_iterable()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_mapping","text":"is_mapping ( arg ) Check whether the arguments is a mapping.","title":"is_mapping()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_mapping_like","text":"is_mapping_like ( arg ) Check whether the arguments is a mapping-like object.","title":"is_mapping_like()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_namedtuple","text":"is_namedtuple ( x ) Check whether object is an instance of namedtuple.","title":"is_namedtuple()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_np_array","text":"is_np_array ( arg ) Check whether the argument is np.ndarray .","title":"is_np_array()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_numba_func","text":"is_numba_func ( arg ) Check whether the argument is a Numba-compiled function.","title":"is_numba_func()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_pandas","text":"is_pandas ( arg ) Check whether the argument is pd.Series or pd.DataFrame .","title":"is_pandas()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_sequence","text":"is_sequence ( arg ) Check whether the argument is a sequence.","title":"is_sequence()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_series","text":"is_series ( arg ) Check whether the argument is pd.Series .","title":"is_series()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_subclass_of","text":"is_subclass_of ( arg , types ) Check whether the argument is a subclass of types . types can be one or multiple types or strings.","title":"is_subclass_of()"},{"location":"api/utils/checks/#vectorbt.utils.checks.is_valid_variable_name","text":"is_valid_variable_name ( arg ) Check whether the argument is a valid variable name.","title":"is_valid_variable_name()"},{"location":"api/utils/checks/#vectorbt.utils.checks.safe_assert","text":"safe_assert ( arg , msg = None )","title":"safe_assert()"},{"location":"api/utils/colors/","text":"colors module \u00b6 Utilities for working with colors. adjust_lightness function \u00b6 adjust_lightness ( color , amount = 0.7 ) Lightens the given color by multiplying (1-luminosity) by the given amount. Input can be matplotlib color string, hex string, or RGB tuple. Output will be an RGB string. adjust_opacity function \u00b6 adjust_opacity ( color , opacity ) Adjust opacity of color. rgb_from_cmap function \u00b6 rgb_from_cmap ( cmap_name , value , value_range ) Map value_range to colormap with name cmap_name and get RGB of the value from that range.","title":"colors"},{"location":"api/utils/colors/#vectorbt.utils.colors","text":"Utilities for working with colors.","title":"vectorbt.utils.colors"},{"location":"api/utils/colors/#vectorbt.utils.colors.adjust_lightness","text":"adjust_lightness ( color , amount = 0.7 ) Lightens the given color by multiplying (1-luminosity) by the given amount. Input can be matplotlib color string, hex string, or RGB tuple. Output will be an RGB string.","title":"adjust_lightness()"},{"location":"api/utils/colors/#vectorbt.utils.colors.adjust_opacity","text":"adjust_opacity ( color , opacity ) Adjust opacity of color.","title":"adjust_opacity()"},{"location":"api/utils/colors/#vectorbt.utils.colors.rgb_from_cmap","text":"rgb_from_cmap ( cmap_name , value , value_range ) Map value_range to colormap with name cmap_name and get RGB of the value from that range.","title":"rgb_from_cmap()"},{"location":"api/utils/config/","text":"config module \u00b6 Utilities for configuration. convert_to_dict function \u00b6 convert_to_dict ( dct , nested = True ) Convert any dict (apart from atomic_dict ) to dict . Set nested to True to convert all child dicts in recursive manner. copy_dict function \u00b6 copy_dict ( dct , copy_mode = 'shallow' , nested = True ) Copy dict based on a copy mode. The following modes are supported: 'shallow': Copies keys only. 'hybrid': Copies keys and values using copy.copy . 'deep': Copies the whole thing using copy.deepcopy . Set nested to True to copy all child dicts in recursive manner. get_func_arg_names function \u00b6 get_func_arg_names ( func , arg_kind = None ) Get argument names of a function. get_func_kwargs function \u00b6 get_func_kwargs ( func ) Get keyword arguments with defaults of a function. merge_dicts function \u00b6 merge_dicts ( * dicts , to_dict = True , copy_mode = 'shallow' , nested = True , same_keys = False ) Merge dicts. Args *dicts :\u2002 dict Dicts. to_dict :\u2002 bool Whether to call convert_to_dict() on each dict prior to copying. copy_mode :\u2002 str Mode for copy_dict() to copy each dict prior to merging. Pass None to not copy. nested :\u2002 bool Whether to merge all child dicts in recursive manner. same_keys :\u2002 bool Whether to merge on the overlapping keys only. resolve_dict function \u00b6 resolve_dict ( dct , i = None ) Select keyword arguments. set_dict_item function \u00b6 set_dict_item ( dct , k , v , force = False ) Set dict item. If the dict is of the type Config , also passes force keyword to override blocking flags. update_dict function \u00b6 update_dict ( x , y , nested = True , force = False , same_keys = False ) Update dict with keys and values from other dict. Set nested to True to update all child dicts in recursive manner. For force , see set_dict_item() . If you want to treat any dict as a single value, wrap it with atomic_dict . Note If the child dict is not atomic, it will copy only its values, not its meta. AtomicConfig class \u00b6 Config that behaves like a single value when merging. Superclasses Config Documented Pickleable PickleableDict atomic_dict builtins.dict Inherited members Config.as_attrs_ Config.clear() Config.convert_dicts_ Config.copy() Config.copy_kwargs_ Config.dumps() Config.frozen_keys_ Config.load_update() Config.loads() Config.make_checkpoint() Config.merge_with() Config.nested_ Config.pop() Config.popitem() Config.readonly_ Config.reset() Config.reset_dct_ Config.reset_dct_copy_kwargs_ Config.to_dict() Config.to_doc() Config.update() Pickleable.load() Pickleable.save() Config class \u00b6 Extends dict with config features such as nested updates, frozen keys/values, and pickling. Args dct :\u2002 dict Dict to construct this config from. copy_kwargs :\u2002 dict Keyword arguments passed to copy_dict() for copying dct and reset_dct . Copy mode defaults to 'shallow' if readonly , otherwise to 'hybrid'. reset_dct :\u2002 dict Dict to fall back to in case of resetting. If None, copies dct using reset_dct_copy_kwargs . reset_dct_copy_kwargs :\u2002 dict Keyword arguments that override copy_kwargs for reset_dct . frozen_keys :\u2002 bool Whether to deny updates to the keys of the config. Defaults to False. readonly :\u2002 bool Whether to deny updates to the keys and values of the config. Defaults to False. nested :\u2002 bool Whether to do operations recursively on each child dict. Such operations include copy, update, and merge. Disable to treat each child dict as a single value. Defaults to True. convert_dicts :\u2002 bool or type Whether to convert child dicts to configs with the same configuration. This will trigger a waterfall reaction across all child dicts. Won't convert dicts that are already configs. Apart from boolean, you can set it to any subclass of Config to use it for construction. Requires nested to be True. Defaults to False. as_attrs :\u2002 bool Whether to enable accessing dict keys via the dot notation. Enables autocompletion (but only during runtime!). Raises error in case of naming conflicts. Defaults to True if frozen or readonly , otherwise False. Defaults can be overridden with settings under config in settings . If another config is passed, its properties are copied over, but they can still be overridden with the arguments passed to the initializer. Note All arguments are applied only once during initialization. Superclasses Documented Pickleable PickleableDict builtins.dict Inherited members Documented.to_doc() Pickleable.load() Pickleable.save() PickleableDict.dumps() PickleableDict.loads() Subclasses AtomicConfig SettingsConfig as_attrs_ property \u00b6 Whether to enable accessing dict keys via dot notation. clear method \u00b6 Config . clear ( force = False ) Remove all items. convert_dicts_ property \u00b6 Whether to convert child dicts to configs with the same configuration. copy method \u00b6 Config . copy ( reset_dct_copy_kwargs = None , ** copy_kwargs ) Copy the instance in the same way it's done during initialization. copy_kwargs override Config.copy_kwargs_ and Config.reset_dct_copy_kwargs_ via merging. reset_dct_copy_kwargs override merged Config.reset_dct_copy_kwargs_ . copy_kwargs_ property \u00b6 Parameters for copying dct . frozen_keys_ property \u00b6 Whether to deny updates to the keys and values of the config. load_update method \u00b6 Config . load_update ( fname , ** kwargs ) Load dumps from a file and update this instance. Note Updates both the config properties and dictionary. make_checkpoint method \u00b6 Config . make_checkpoint ( force = False , ** reset_dct_copy_kwargs ) Replace reset_dct by the current state. reset_dct_copy_kwargs override Config.reset_dct_copy_kwargs_ . merge_with method \u00b6 Config . merge_with ( other , nested = None , ** kwargs ) Merge with another dict into one single dict. See merge_dicts() . nested_ property \u00b6 Whether to do operations recursively on each child dict. pop method \u00b6 Config . pop ( k , v =< object object > , force = False ) Remove and return the pair by the key. popitem method \u00b6 Config . popitem ( force = False ) Remove and return some pair. readonly_ property \u00b6 Whether to deny any updates to the config. reset method \u00b6 Config . reset ( force = False , ** reset_dct_copy_kwargs ) Clears the config and updates it with the initial config. reset_dct_copy_kwargs override Config.reset_dct_copy_kwargs_ . reset_dct_ property \u00b6 Dict to fall back to in case of resetting. reset_dct_copy_kwargs_ property \u00b6 Parameters for copying reset_dct . to_dict method \u00b6 Config . to_dict ( nested = None ) Convert to dict. update method \u00b6 Config . update ( * args , nested = None , force = False , ** kwargs ) Update the config. See update_dict() . Configured class \u00b6 Class with an initialization config. All subclasses of Configured are initialized using Config , which makes it easier to pickle. Settings are defined under configured in settings . Warning If any attribute has been overwritten that isn't listed in Configured.writeable_attrs , or if any Configured argument depends upon global defaults, their values won't be copied over. Make sure to pass them explicitly to make the saved & loaded / copied instance resilient to changes in globals. Superclasses Documented Pickleable Inherited members Documented.to_doc() Pickleable.dumps() Pickleable.load() Pickleable.loads() Pickleable.save() Subclasses ArrayWrapper Bar Box ColumnGrouper DataUpdater Gauge Heatmap Histogram QSAdapter Scatter TelegramBot Volume Wrapping config property \u00b6 Initialization config. copy method \u00b6 Configured . copy ( copy_mode = 'shallow' , nested = None , cls = None ) Create a new instance by copying the config. See Configured.replace() . replace method \u00b6 Configured . replace ( copy_mode_ = 'shallow' , nested_ = None , cls_ = None , ** new_config ) Create a new instance by copying and (optionally) changing the config. Warning This operation won't return a copy of the instance but a new instance initialized with the same config and writeable attributes (or their copy, depending on copy_mode ). update_config method \u00b6 Configured . update_config ( * args , ** kwargs ) Force-update the config. writeable_attrs property \u00b6 Set of writeable attributes that will be saved/copied along with the config. Default class \u00b6 Class for wrapping default values. DumpTuple class \u00b6 DumpTuple(cls, dumps) Superclasses builtins.tuple cls property \u00b6 Alias for field number 0 dumps property \u00b6 Alias for field number 1 Pickleable class \u00b6 Superclass that defines abstract properties and methods for pickle-able classes. Subclasses Configured PickleableDict dumps method \u00b6 Pickleable . dumps ( ** kwargs ) Pickle to bytes. load class method \u00b6 Pickleable . load ( fname , ** kwargs ) Load dumps from a file and create new instance. loads class method \u00b6 Pickleable . loads ( dumps , ** kwargs ) Unpickle from bytes. save method \u00b6 Pickleable . save ( fname , ** kwargs ) Save dumps to a file. PickleableDict class \u00b6 Dict that may contain values of type Pickleable . Superclasses Pickleable builtins.dict Inherited members Pickleable.dumps() Pickleable.load() Pickleable.loads() Pickleable.save() Subclasses Config load_update method \u00b6 PickleableDict . load_update ( fname , ** kwargs ) Load dumps from a file and update this instance. atomic_dict class \u00b6 Dict that behaves like a single value when merging. Superclasses builtins.dict Subclasses AtomicConfig","title":"config"},{"location":"api/utils/config/#vectorbt.utils.config","text":"Utilities for configuration.","title":"vectorbt.utils.config"},{"location":"api/utils/config/#vectorbt.utils.config.convert_to_dict","text":"convert_to_dict ( dct , nested = True ) Convert any dict (apart from atomic_dict ) to dict . Set nested to True to convert all child dicts in recursive manner.","title":"convert_to_dict()"},{"location":"api/utils/config/#vectorbt.utils.config.copy_dict","text":"copy_dict ( dct , copy_mode = 'shallow' , nested = True ) Copy dict based on a copy mode. The following modes are supported: 'shallow': Copies keys only. 'hybrid': Copies keys and values using copy.copy . 'deep': Copies the whole thing using copy.deepcopy . Set nested to True to copy all child dicts in recursive manner.","title":"copy_dict()"},{"location":"api/utils/config/#vectorbt.utils.config.get_func_arg_names","text":"get_func_arg_names ( func , arg_kind = None ) Get argument names of a function.","title":"get_func_arg_names()"},{"location":"api/utils/config/#vectorbt.utils.config.get_func_kwargs","text":"get_func_kwargs ( func ) Get keyword arguments with defaults of a function.","title":"get_func_kwargs()"},{"location":"api/utils/config/#vectorbt.utils.config.merge_dicts","text":"merge_dicts ( * dicts , to_dict = True , copy_mode = 'shallow' , nested = True , same_keys = False ) Merge dicts. Args *dicts :\u2002 dict Dicts. to_dict :\u2002 bool Whether to call convert_to_dict() on each dict prior to copying. copy_mode :\u2002 str Mode for copy_dict() to copy each dict prior to merging. Pass None to not copy. nested :\u2002 bool Whether to merge all child dicts in recursive manner. same_keys :\u2002 bool Whether to merge on the overlapping keys only.","title":"merge_dicts()"},{"location":"api/utils/config/#vectorbt.utils.config.resolve_dict","text":"resolve_dict ( dct , i = None ) Select keyword arguments.","title":"resolve_dict()"},{"location":"api/utils/config/#vectorbt.utils.config.set_dict_item","text":"set_dict_item ( dct , k , v , force = False ) Set dict item. If the dict is of the type Config , also passes force keyword to override blocking flags.","title":"set_dict_item()"},{"location":"api/utils/config/#vectorbt.utils.config.update_dict","text":"update_dict ( x , y , nested = True , force = False , same_keys = False ) Update dict with keys and values from other dict. Set nested to True to update all child dicts in recursive manner. For force , see set_dict_item() . If you want to treat any dict as a single value, wrap it with atomic_dict . Note If the child dict is not atomic, it will copy only its values, not its meta.","title":"update_dict()"},{"location":"api/utils/config/#vectorbt.utils.config.AtomicConfig","text":"Config that behaves like a single value when merging. Superclasses Config Documented Pickleable PickleableDict atomic_dict builtins.dict Inherited members Config.as_attrs_ Config.clear() Config.convert_dicts_ Config.copy() Config.copy_kwargs_ Config.dumps() Config.frozen_keys_ Config.load_update() Config.loads() Config.make_checkpoint() Config.merge_with() Config.nested_ Config.pop() Config.popitem() Config.readonly_ Config.reset() Config.reset_dct_ Config.reset_dct_copy_kwargs_ Config.to_dict() Config.to_doc() Config.update() Pickleable.load() Pickleable.save()","title":"AtomicConfig"},{"location":"api/utils/config/#vectorbt.utils.config.Config","text":"Extends dict with config features such as nested updates, frozen keys/values, and pickling. Args dct :\u2002 dict Dict to construct this config from. copy_kwargs :\u2002 dict Keyword arguments passed to copy_dict() for copying dct and reset_dct . Copy mode defaults to 'shallow' if readonly , otherwise to 'hybrid'. reset_dct :\u2002 dict Dict to fall back to in case of resetting. If None, copies dct using reset_dct_copy_kwargs . reset_dct_copy_kwargs :\u2002 dict Keyword arguments that override copy_kwargs for reset_dct . frozen_keys :\u2002 bool Whether to deny updates to the keys of the config. Defaults to False. readonly :\u2002 bool Whether to deny updates to the keys and values of the config. Defaults to False. nested :\u2002 bool Whether to do operations recursively on each child dict. Such operations include copy, update, and merge. Disable to treat each child dict as a single value. Defaults to True. convert_dicts :\u2002 bool or type Whether to convert child dicts to configs with the same configuration. This will trigger a waterfall reaction across all child dicts. Won't convert dicts that are already configs. Apart from boolean, you can set it to any subclass of Config to use it for construction. Requires nested to be True. Defaults to False. as_attrs :\u2002 bool Whether to enable accessing dict keys via the dot notation. Enables autocompletion (but only during runtime!). Raises error in case of naming conflicts. Defaults to True if frozen or readonly , otherwise False. Defaults can be overridden with settings under config in settings . If another config is passed, its properties are copied over, but they can still be overridden with the arguments passed to the initializer. Note All arguments are applied only once during initialization. Superclasses Documented Pickleable PickleableDict builtins.dict Inherited members Documented.to_doc() Pickleable.load() Pickleable.save() PickleableDict.dumps() PickleableDict.loads() Subclasses AtomicConfig SettingsConfig","title":"Config"},{"location":"api/utils/config/#vectorbt.utils.config.Config.as_attrs_","text":"Whether to enable accessing dict keys via dot notation.","title":"as_attrs_"},{"location":"api/utils/config/#vectorbt.utils.config.Config.clear","text":"Config . clear ( force = False ) Remove all items.","title":"clear()"},{"location":"api/utils/config/#vectorbt.utils.config.Config.convert_dicts_","text":"Whether to convert child dicts to configs with the same configuration.","title":"convert_dicts_"},{"location":"api/utils/config/#vectorbt.utils.config.Config.copy","text":"Config . copy ( reset_dct_copy_kwargs = None , ** copy_kwargs ) Copy the instance in the same way it's done during initialization. copy_kwargs override Config.copy_kwargs_ and Config.reset_dct_copy_kwargs_ via merging. reset_dct_copy_kwargs override merged Config.reset_dct_copy_kwargs_ .","title":"copy()"},{"location":"api/utils/config/#vectorbt.utils.config.Config.copy_kwargs_","text":"Parameters for copying dct .","title":"copy_kwargs_"},{"location":"api/utils/config/#vectorbt.utils.config.Config.frozen_keys_","text":"Whether to deny updates to the keys and values of the config.","title":"frozen_keys_"},{"location":"api/utils/config/#vectorbt.utils.config.Config.load_update","text":"Config . load_update ( fname , ** kwargs ) Load dumps from a file and update this instance. Note Updates both the config properties and dictionary.","title":"load_update()"},{"location":"api/utils/config/#vectorbt.utils.config.Config.make_checkpoint","text":"Config . make_checkpoint ( force = False , ** reset_dct_copy_kwargs ) Replace reset_dct by the current state. reset_dct_copy_kwargs override Config.reset_dct_copy_kwargs_ .","title":"make_checkpoint()"},{"location":"api/utils/config/#vectorbt.utils.config.Config.merge_with","text":"Config . merge_with ( other , nested = None , ** kwargs ) Merge with another dict into one single dict. See merge_dicts() .","title":"merge_with()"},{"location":"api/utils/config/#vectorbt.utils.config.Config.nested_","text":"Whether to do operations recursively on each child dict.","title":"nested_"},{"location":"api/utils/config/#vectorbt.utils.config.Config.pop","text":"Config . pop ( k , v =< object object > , force = False ) Remove and return the pair by the key.","title":"pop()"},{"location":"api/utils/config/#vectorbt.utils.config.Config.popitem","text":"Config . popitem ( force = False ) Remove and return some pair.","title":"popitem()"},{"location":"api/utils/config/#vectorbt.utils.config.Config.readonly_","text":"Whether to deny any updates to the config.","title":"readonly_"},{"location":"api/utils/config/#vectorbt.utils.config.Config.reset","text":"Config . reset ( force = False , ** reset_dct_copy_kwargs ) Clears the config and updates it with the initial config. reset_dct_copy_kwargs override Config.reset_dct_copy_kwargs_ .","title":"reset()"},{"location":"api/utils/config/#vectorbt.utils.config.Config.reset_dct_","text":"Dict to fall back to in case of resetting.","title":"reset_dct_"},{"location":"api/utils/config/#vectorbt.utils.config.Config.reset_dct_copy_kwargs_","text":"Parameters for copying reset_dct .","title":"reset_dct_copy_kwargs_"},{"location":"api/utils/config/#vectorbt.utils.config.Config.to_dict","text":"Config . to_dict ( nested = None ) Convert to dict.","title":"to_dict()"},{"location":"api/utils/config/#vectorbt.utils.config.Config.update","text":"Config . update ( * args , nested = None , force = False , ** kwargs ) Update the config. See update_dict() .","title":"update()"},{"location":"api/utils/config/#vectorbt.utils.config.Configured","text":"Class with an initialization config. All subclasses of Configured are initialized using Config , which makes it easier to pickle. Settings are defined under configured in settings . Warning If any attribute has been overwritten that isn't listed in Configured.writeable_attrs , or if any Configured argument depends upon global defaults, their values won't be copied over. Make sure to pass them explicitly to make the saved & loaded / copied instance resilient to changes in globals. Superclasses Documented Pickleable Inherited members Documented.to_doc() Pickleable.dumps() Pickleable.load() Pickleable.loads() Pickleable.save() Subclasses ArrayWrapper Bar Box ColumnGrouper DataUpdater Gauge Heatmap Histogram QSAdapter Scatter TelegramBot Volume Wrapping","title":"Configured"},{"location":"api/utils/config/#vectorbt.utils.config.Configured.config","text":"Initialization config.","title":"config"},{"location":"api/utils/config/#vectorbt.utils.config.Configured.copy","text":"Configured . copy ( copy_mode = 'shallow' , nested = None , cls = None ) Create a new instance by copying the config. See Configured.replace() .","title":"copy()"},{"location":"api/utils/config/#vectorbt.utils.config.Configured.replace","text":"Configured . replace ( copy_mode_ = 'shallow' , nested_ = None , cls_ = None , ** new_config ) Create a new instance by copying and (optionally) changing the config. Warning This operation won't return a copy of the instance but a new instance initialized with the same config and writeable attributes (or their copy, depending on copy_mode ).","title":"replace()"},{"location":"api/utils/config/#vectorbt.utils.config.Configured.update_config","text":"Configured . update_config ( * args , ** kwargs ) Force-update the config.","title":"update_config()"},{"location":"api/utils/config/#vectorbt.utils.config.Configured.writeable_attrs","text":"Set of writeable attributes that will be saved/copied along with the config.","title":"writeable_attrs"},{"location":"api/utils/config/#vectorbt.utils.config.Default","text":"Class for wrapping default values.","title":"Default"},{"location":"api/utils/config/#vectorbt.utils.config.DumpTuple","text":"DumpTuple(cls, dumps) Superclasses builtins.tuple","title":"DumpTuple"},{"location":"api/utils/config/#vectorbt.utils.config.DumpTuple.cls","text":"Alias for field number 0","title":"cls"},{"location":"api/utils/config/#vectorbt.utils.config.DumpTuple.dumps","text":"Alias for field number 1","title":"dumps"},{"location":"api/utils/config/#vectorbt.utils.config.Pickleable","text":"Superclass that defines abstract properties and methods for pickle-able classes. Subclasses Configured PickleableDict","title":"Pickleable"},{"location":"api/utils/config/#vectorbt.utils.config.Pickleable.dumps","text":"Pickleable . dumps ( ** kwargs ) Pickle to bytes.","title":"dumps()"},{"location":"api/utils/config/#vectorbt.utils.config.Pickleable.load","text":"Pickleable . load ( fname , ** kwargs ) Load dumps from a file and create new instance.","title":"load()"},{"location":"api/utils/config/#vectorbt.utils.config.Pickleable.loads","text":"Pickleable . loads ( dumps , ** kwargs ) Unpickle from bytes.","title":"loads()"},{"location":"api/utils/config/#vectorbt.utils.config.Pickleable.save","text":"Pickleable . save ( fname , ** kwargs ) Save dumps to a file.","title":"save()"},{"location":"api/utils/config/#vectorbt.utils.config.PickleableDict","text":"Dict that may contain values of type Pickleable . Superclasses Pickleable builtins.dict Inherited members Pickleable.dumps() Pickleable.load() Pickleable.loads() Pickleable.save() Subclasses Config","title":"PickleableDict"},{"location":"api/utils/config/#vectorbt.utils.config.PickleableDict.load_update","text":"PickleableDict . load_update ( fname , ** kwargs ) Load dumps from a file and update this instance.","title":"load_update()"},{"location":"api/utils/config/#vectorbt.utils.config.atomic_dict","text":"Dict that behaves like a single value when merging. Superclasses builtins.dict Subclasses AtomicConfig","title":"atomic_dict"},{"location":"api/utils/datetime_/","text":"datetime_ module \u00b6 Utilities for working with dates and time. convert_naive_time function \u00b6 convert_naive_time ( t , tz_out ) Return as naive time. datetime.time should not have tzinfo set. convert_tzaware_time function \u00b6 convert_tzaware_time ( t , tz_out ) Return as non-naive time. datetime.time should have tzinfo set. datetime_to_ms function \u00b6 datetime_to_ms ( dt ) Convert a datetime to milliseconds. freq_to_timedelta function \u00b6 freq_to_timedelta ( arg ) pd.to_timedelta that uses unit abbreviation with number. get_local_tz function \u00b6 get_local_tz () Get local timezone. get_utc_tz function \u00b6 get_utc_tz () Get UTC timezone. interval_to_ms function \u00b6 interval_to_ms ( interval ) Convert an interval string to milliseconds. is_tz_aware function \u00b6 is_tz_aware ( dt ) Whether datetime is timezone-aware. naive_to_tzaware_time function \u00b6 naive_to_tzaware_time ( t , tz_out ) Return as non-naive time. datetime.time should not have tzinfo set. to_timezone function \u00b6 to_timezone ( tz , to_py_timezone = None , ** kwargs ) Parse the timezone. Strings are parsed by pytz and dateparser , while integers and floats are treated as hour offsets. If the timezone object can't be checked for equality based on its properties, it's automatically converted to datetime.timezone . If to_py_timezone is set to True, will convert to datetime.timezone . **kwargs are passed to dateparser.parse . to_tzaware_datetime function \u00b6 to_tzaware_datetime ( dt_like , naive_tz = None , tz = None , ** kwargs ) Parse the datetime as a timezone-aware datetime.datetime . See dateparser docs for valid string formats and **kwargs . Raw timestamps are localized to UTC, while naive datetime is localized to naive_tz . Set naive_tz to None to use the default value defined under datetime settings in settings . To explicitly convert the datetime to a timezone, use tz (uses to_timezone() ). tzaware_to_naive_time function \u00b6 tzaware_to_naive_time ( t , tz_out ) Return as naive time. datetime.time should have tzinfo set.","title":"datetime_"},{"location":"api/utils/datetime_/#vectorbt.utils.datetime_","text":"Utilities for working with dates and time.","title":"vectorbt.utils.datetime_"},{"location":"api/utils/datetime_/#vectorbt.utils.datetime_.convert_naive_time","text":"convert_naive_time ( t , tz_out ) Return as naive time. datetime.time should not have tzinfo set.","title":"convert_naive_time()"},{"location":"api/utils/datetime_/#vectorbt.utils.datetime_.convert_tzaware_time","text":"convert_tzaware_time ( t , tz_out ) Return as non-naive time. datetime.time should have tzinfo set.","title":"convert_tzaware_time()"},{"location":"api/utils/datetime_/#vectorbt.utils.datetime_.datetime_to_ms","text":"datetime_to_ms ( dt ) Convert a datetime to milliseconds.","title":"datetime_to_ms()"},{"location":"api/utils/datetime_/#vectorbt.utils.datetime_.freq_to_timedelta","text":"freq_to_timedelta ( arg ) pd.to_timedelta that uses unit abbreviation with number.","title":"freq_to_timedelta()"},{"location":"api/utils/datetime_/#vectorbt.utils.datetime_.get_local_tz","text":"get_local_tz () Get local timezone.","title":"get_local_tz()"},{"location":"api/utils/datetime_/#vectorbt.utils.datetime_.get_utc_tz","text":"get_utc_tz () Get UTC timezone.","title":"get_utc_tz()"},{"location":"api/utils/datetime_/#vectorbt.utils.datetime_.interval_to_ms","text":"interval_to_ms ( interval ) Convert an interval string to milliseconds.","title":"interval_to_ms()"},{"location":"api/utils/datetime_/#vectorbt.utils.datetime_.is_tz_aware","text":"is_tz_aware ( dt ) Whether datetime is timezone-aware.","title":"is_tz_aware()"},{"location":"api/utils/datetime_/#vectorbt.utils.datetime_.naive_to_tzaware_time","text":"naive_to_tzaware_time ( t , tz_out ) Return as non-naive time. datetime.time should not have tzinfo set.","title":"naive_to_tzaware_time()"},{"location":"api/utils/datetime_/#vectorbt.utils.datetime_.to_timezone","text":"to_timezone ( tz , to_py_timezone = None , ** kwargs ) Parse the timezone. Strings are parsed by pytz and dateparser , while integers and floats are treated as hour offsets. If the timezone object can't be checked for equality based on its properties, it's automatically converted to datetime.timezone . If to_py_timezone is set to True, will convert to datetime.timezone . **kwargs are passed to dateparser.parse .","title":"to_timezone()"},{"location":"api/utils/datetime_/#vectorbt.utils.datetime_.to_tzaware_datetime","text":"to_tzaware_datetime ( dt_like , naive_tz = None , tz = None , ** kwargs ) Parse the datetime as a timezone-aware datetime.datetime . See dateparser docs for valid string formats and **kwargs . Raw timestamps are localized to UTC, while naive datetime is localized to naive_tz . Set naive_tz to None to use the default value defined under datetime settings in settings . To explicitly convert the datetime to a timezone, use tz (uses to_timezone() ).","title":"to_tzaware_datetime()"},{"location":"api/utils/datetime_/#vectorbt.utils.datetime_.tzaware_to_naive_time","text":"tzaware_to_naive_time ( t , tz_out ) Return as naive time. datetime.time should have tzinfo set.","title":"tzaware_to_naive_time()"},{"location":"api/utils/decorators/","text":"decorators module \u00b6 Class and function decorators. binary_magic_config Config \u00b6 Config of binary magic methods to be added to a class. Co nf ig( { \"__eq__\" : { \"func\" : \"<ufunc 'equal'>\" }, \"__ne__\" : { \"func\" : \"<ufunc 'not_equal'>\" }, \"__lt__\" : { \"func\" : \"<ufunc 'less'>\" }, \"__gt__\" : { \"func\" : \"<ufunc 'greater'>\" }, \"__le__\" : { \"func\" : \"<ufunc 'less_equal'>\" }, \"__ge__\" : { \"func\" : \"<ufunc 'greater_equal'>\" }, \"__add__\" : { \"func\" : \"<ufunc 'add'>\" }, \"__sub__\" : { \"func\" : \"<ufunc 'subtract'>\" }, \"__mul__\" : { \"func\" : \"<ufunc 'multiply'>\" }, \"__pow__\" : { \"func\" : \"<ufunc 'power'>\" }, \"__mod__\" : { \"func\" : \"<ufunc 'remainder'>\" }, \"__floordiv__\" : { \"func\" : \"<ufunc 'floor_divide'>\" }, \"__truediv__\" : { \"func\" : \"<ufunc 'true_divide'>\" }, \"__radd__\" : { \"func\" : \"<function <lambda> at 0x7facba881d08>\" }, \"__rsub__\" : { \"func\" : \"<function <lambda> at 0x7facba881d90>\" }, \"__rmul__\" : { \"func\" : \"<function <lambda> at 0x7facba881e18>\" }, \"__rpow__\" : { \"func\" : \"<function <lambda> at 0x7facba881ea0>\" }, \"__rmod__\" : { \"func\" : \"<function <lambda> at 0x7facba881f28>\" }, \"__rfloordiv__\" : { \"func\" : \"<function <lambda> at 0x7facba887048>\" }, \"__rtruediv__\" : { \"func\" : \"<function <lambda> at 0x7facba8870d0>\" }, \"__and__\" : { \"func\" : \"<ufunc 'bitwise_and'>\" }, \"__or__\" : { \"func\" : \"<ufunc 'bitwise_or'>\" }, \"__xor__\" : { \"func\" : \"<ufunc 'bitwise_xor'>\" }, \"__rand__\" : { \"func\" : \"<function <lambda> at 0x7facba887158>\" }, \"__ror__\" : { \"func\" : \"<function <lambda> at 0x7facba8871e0>\" }, \"__rxor__\" : { \"func\" : \"<function <lambda> at 0x7facba887268>\" } } ) unary_magic_config Config \u00b6 Config of unary magic methods to be added to a class. Co nf ig( { \"__neg__\" : { \"func\" : \"<ufunc 'negative'>\" }, \"__pos__\" : { \"func\" : \"<ufunc 'positive'>\" }, \"__abs__\" : { \"func\" : \"<ufunc 'absolute'>\" }, \"__invert__\" : { \"func\" : \"<ufunc 'invert'>\" } } ) attach_binary_magic_methods function \u00b6 attach_binary_magic_methods ( translate_func , config = None ) Class decorator to add binary magic methods to a class. translate_func should take self , other , and unary function, perform computation, and return the result. config defaults to binary_magic_config and should contain target method names (keys) and dictionaries (values) with the following keys: func : Function that combines two array-like objects. attach_unary_magic_methods function \u00b6 attach_unary_magic_methods ( translate_func , config = None ) Class decorator to add unary magic methods to a class. translate_func should take self and unary function, perform computation, and return the result. config defaults to unary_magic_config and should contain target method names (keys) and dictionaries (values) with the following keys: func : Function that transforms one array-like object. cached_method function \u00b6 cached_method ( * args , maxsize = 128 , typed = False , ** flags ) Extends custom_method() with caching. Internally uses functools.lru_cache . Disables caching if should_cache() yields False or a non-hashable object as argument has been passed. See notes on cached_property . custom_method function \u00b6 custom_method ( * args , ** flags ) Custom extensible method that stores function and flags as attributes. Can be called both as >>> @cached_method ... def user_function (): pass and >>> @cached_method ( maxsize = 128 , typed = False , a = 0 , b = 0 ) # flags ... def user_function (): pass should_cache function \u00b6 should_cache ( func_name , instance , func = None , ** flags ) Check whether to cache the method/property based on a range of conditions defined under caching in settings . Each condition has its own rank. A narrower condition has a lower (better) rank than a broader condition. All supplied keys are checked, and if any condition fails, it's assigned to the highest (worst) rank. Here's the condition ranking: 0) instance and func 1) instance and flags 2) instance 3) cls and func 4) cls and flags 5) cls 6) base_cls and func 7) base_cls and flags 8) base_cls 9) func and flags 10) func 11) flags This function goes through all conditions of type CacheCondition in whitelist and blacklist and finds the one with the lowest (best) rank. If the search yields the same rank for both lists, global caching flag enabled decides. Usage Let's evaluate various caching conditions: >>> import vectorbt as vbt >>> class A : ... @cached_property ( my_flag = True ) ... def f ( self ): ... return None >>> class B ( A ): ... @cached_property ( my_flag = False ) ... def f ( self ): ... return None >>> a = A () >>> b = B () >>> vbt . CacheCondition ( instance = a , func = 'f' ) # A.f >>> vbt . CacheCondition ( instance = b , func = 'f' ) # B.f >>> vbt . CacheCondition ( instance = a , flags = dict ( my_flag = True )) # A.f >>> vbt . CacheCondition ( instance = a , flags = dict ( my_flag = False )) # none >>> vbt . CacheCondition ( instance = b , flags = dict ( my_flag = False )) # B.f >>> vbt . CacheCondition ( instance = a ) # A.f >>> vbt . CacheCondition ( instance = b ) # B.f >>> vbt . CacheCondition ( cls = A ) # A.f >>> vbt . CacheCondition ( cls = B ) # B.f >>> vbt . CacheCondition ( base_cls = A ) # A.f and B.f >>> vbt . CacheCondition ( base_cls = B ) # B.f >>> vbt . CacheCondition ( base_cls = A , flags = dict ( my_flag = False )) # B.f >>> vbt . CacheCondition ( func = A . f ) # A.f >>> vbt . CacheCondition ( func = B . f ) # B.f >>> vbt . CacheCondition ( func = 'f' ) # A.f and B.f >>> vbt . CacheCondition ( func = 'f' , flags = dict ( my_flag = False )) # B.f >>> vbt . CacheCondition ( flags = dict ( my_flag = True )) # A.f CacheCondition class \u00b6 Caching condition for the use in should_cache() . Superclasses builtins.tuple base_cls property \u00b6 Base class of the class or its name (case-sensitive). cls property \u00b6 Class of the instance or its name (case-sensitive). flags property \u00b6 Flags to check for in method/property's flags. func property \u00b6 Method/property or its name (case-sensitive). instance property \u00b6 Class instance the method/property is bound to. rank property \u00b6 Rank to override the default rank. cached_methodT class \u00b6 Base class for protocol classes. Protocol classes are defined as:: class Proto(Protocol): def meth(self) -> int: ... Such classes are primarily used with static type checkers that recognize structural subtyping (static duck-typing), for example:: class C def meth(self) -> int: return 0 def func(x: Proto) -> int: return x.meth() func(C()) # Passes static type check See PEP 544 for details. Protocol classes decorated with @typing_extensions .runtime act as simple-minded runtime protocol that checks only the presence of given attributes, ignoring their type signatures. Protocol classes can be generic, they are defined as:: class GenProto(Protocol[T]): def meth(self) -> T: ... Superclasses custom_methodT typing_extensions.Protocol attrname class variable \u00b6 clear_cache class variable \u00b6 lock class variable \u00b6 maxsize class variable \u00b6 name class variable \u00b6 typed class variable \u00b6 cached_property class \u00b6 Extends custom_property with caching. Similar to functools.cached_property , but without replacing the original attribute to be able to re-compute whenever needed. Disables caching if should_cache() yields False. Cache can be cleared by calling clear_cache with instance as argument. !!! note: Assumes that the instance (provided as self ) won't change. If calculation depends upon object attributes that can be changed, it won't notice the change. Superclasses custom_property attrname property \u00b6 Get name of cached attribute. clear_cache method \u00b6 cached_property . clear_cache ( instance ) Clear the cache for this property belonging to instance . class_or_instancemethod class \u00b6 Function decorator that binds self to a class if the function is called as class method, otherwise to an instance. Superclasses builtins.classmethod class_or_instanceproperty class \u00b6 Property that binds self to a class if the function is called as class method, otherwise to an instance. classproperty class \u00b6 Property that can be called on a class. custom_methodT class \u00b6 Base class for protocol classes. Protocol classes are defined as:: class Proto(Protocol): def meth(self) -> int: ... Such classes are primarily used with static type checkers that recognize structural subtyping (static duck-typing), for example:: class C def meth(self) -> int: return 0 def func(x: Proto) -> int: return x.meth() func(C()) # Passes static type check See PEP 544 for details. Protocol classes decorated with @typing_extensions .runtime act as simple-minded runtime protocol that checks only the presence of given attributes, ignoring their type signatures. Protocol classes can be generic, they are defined as:: class GenProto(Protocol[T]): def meth(self) -> T: ... Superclasses typing_extensions.Protocol Subclasses cached_methodT flags class variable \u00b6 func class variable \u00b6 custom_property class \u00b6 Custom property that stores function and flags as attributes. Can be called both as >>> @custom_property ... def user_function ( self ): pass and >>> @custom_property(a=0, b=0) # flags ... def user_function(self): pass Note custom_property instances belong to classes, not class instances. Thus changing the property, for example, by disabling caching, will do the same for each instance of the class where the property has been defined. Subclasses cached_property","title":"decorators"},{"location":"api/utils/decorators/#vectorbt.utils.decorators","text":"Class and function decorators.","title":"vectorbt.utils.decorators"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.binary_magic_config","text":"Config of binary magic methods to be added to a class. Co nf ig( { \"__eq__\" : { \"func\" : \"<ufunc 'equal'>\" }, \"__ne__\" : { \"func\" : \"<ufunc 'not_equal'>\" }, \"__lt__\" : { \"func\" : \"<ufunc 'less'>\" }, \"__gt__\" : { \"func\" : \"<ufunc 'greater'>\" }, \"__le__\" : { \"func\" : \"<ufunc 'less_equal'>\" }, \"__ge__\" : { \"func\" : \"<ufunc 'greater_equal'>\" }, \"__add__\" : { \"func\" : \"<ufunc 'add'>\" }, \"__sub__\" : { \"func\" : \"<ufunc 'subtract'>\" }, \"__mul__\" : { \"func\" : \"<ufunc 'multiply'>\" }, \"__pow__\" : { \"func\" : \"<ufunc 'power'>\" }, \"__mod__\" : { \"func\" : \"<ufunc 'remainder'>\" }, \"__floordiv__\" : { \"func\" : \"<ufunc 'floor_divide'>\" }, \"__truediv__\" : { \"func\" : \"<ufunc 'true_divide'>\" }, \"__radd__\" : { \"func\" : \"<function <lambda> at 0x7facba881d08>\" }, \"__rsub__\" : { \"func\" : \"<function <lambda> at 0x7facba881d90>\" }, \"__rmul__\" : { \"func\" : \"<function <lambda> at 0x7facba881e18>\" }, \"__rpow__\" : { \"func\" : \"<function <lambda> at 0x7facba881ea0>\" }, \"__rmod__\" : { \"func\" : \"<function <lambda> at 0x7facba881f28>\" }, \"__rfloordiv__\" : { \"func\" : \"<function <lambda> at 0x7facba887048>\" }, \"__rtruediv__\" : { \"func\" : \"<function <lambda> at 0x7facba8870d0>\" }, \"__and__\" : { \"func\" : \"<ufunc 'bitwise_and'>\" }, \"__or__\" : { \"func\" : \"<ufunc 'bitwise_or'>\" }, \"__xor__\" : { \"func\" : \"<ufunc 'bitwise_xor'>\" }, \"__rand__\" : { \"func\" : \"<function <lambda> at 0x7facba887158>\" }, \"__ror__\" : { \"func\" : \"<function <lambda> at 0x7facba8871e0>\" }, \"__rxor__\" : { \"func\" : \"<function <lambda> at 0x7facba887268>\" } } )","title":"binary_magic_config"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.unary_magic_config","text":"Config of unary magic methods to be added to a class. Co nf ig( { \"__neg__\" : { \"func\" : \"<ufunc 'negative'>\" }, \"__pos__\" : { \"func\" : \"<ufunc 'positive'>\" }, \"__abs__\" : { \"func\" : \"<ufunc 'absolute'>\" }, \"__invert__\" : { \"func\" : \"<ufunc 'invert'>\" } } )","title":"unary_magic_config"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.attach_binary_magic_methods","text":"attach_binary_magic_methods ( translate_func , config = None ) Class decorator to add binary magic methods to a class. translate_func should take self , other , and unary function, perform computation, and return the result. config defaults to binary_magic_config and should contain target method names (keys) and dictionaries (values) with the following keys: func : Function that combines two array-like objects.","title":"attach_binary_magic_methods()"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.attach_unary_magic_methods","text":"attach_unary_magic_methods ( translate_func , config = None ) Class decorator to add unary magic methods to a class. translate_func should take self and unary function, perform computation, and return the result. config defaults to unary_magic_config and should contain target method names (keys) and dictionaries (values) with the following keys: func : Function that transforms one array-like object.","title":"attach_unary_magic_methods()"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.cached_method","text":"cached_method ( * args , maxsize = 128 , typed = False , ** flags ) Extends custom_method() with caching. Internally uses functools.lru_cache . Disables caching if should_cache() yields False or a non-hashable object as argument has been passed. See notes on cached_property .","title":"cached_method()"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.custom_method","text":"custom_method ( * args , ** flags ) Custom extensible method that stores function and flags as attributes. Can be called both as >>> @cached_method ... def user_function (): pass and >>> @cached_method ( maxsize = 128 , typed = False , a = 0 , b = 0 ) # flags ... def user_function (): pass","title":"custom_method()"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.should_cache","text":"should_cache ( func_name , instance , func = None , ** flags ) Check whether to cache the method/property based on a range of conditions defined under caching in settings . Each condition has its own rank. A narrower condition has a lower (better) rank than a broader condition. All supplied keys are checked, and if any condition fails, it's assigned to the highest (worst) rank. Here's the condition ranking: 0) instance and func 1) instance and flags 2) instance 3) cls and func 4) cls and flags 5) cls 6) base_cls and func 7) base_cls and flags 8) base_cls 9) func and flags 10) func 11) flags This function goes through all conditions of type CacheCondition in whitelist and blacklist and finds the one with the lowest (best) rank. If the search yields the same rank for both lists, global caching flag enabled decides. Usage Let's evaluate various caching conditions: >>> import vectorbt as vbt >>> class A : ... @cached_property ( my_flag = True ) ... def f ( self ): ... return None >>> class B ( A ): ... @cached_property ( my_flag = False ) ... def f ( self ): ... return None >>> a = A () >>> b = B () >>> vbt . CacheCondition ( instance = a , func = 'f' ) # A.f >>> vbt . CacheCondition ( instance = b , func = 'f' ) # B.f >>> vbt . CacheCondition ( instance = a , flags = dict ( my_flag = True )) # A.f >>> vbt . CacheCondition ( instance = a , flags = dict ( my_flag = False )) # none >>> vbt . CacheCondition ( instance = b , flags = dict ( my_flag = False )) # B.f >>> vbt . CacheCondition ( instance = a ) # A.f >>> vbt . CacheCondition ( instance = b ) # B.f >>> vbt . CacheCondition ( cls = A ) # A.f >>> vbt . CacheCondition ( cls = B ) # B.f >>> vbt . CacheCondition ( base_cls = A ) # A.f and B.f >>> vbt . CacheCondition ( base_cls = B ) # B.f >>> vbt . CacheCondition ( base_cls = A , flags = dict ( my_flag = False )) # B.f >>> vbt . CacheCondition ( func = A . f ) # A.f >>> vbt . CacheCondition ( func = B . f ) # B.f >>> vbt . CacheCondition ( func = 'f' ) # A.f and B.f >>> vbt . CacheCondition ( func = 'f' , flags = dict ( my_flag = False )) # B.f >>> vbt . CacheCondition ( flags = dict ( my_flag = True )) # A.f","title":"should_cache()"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.CacheCondition","text":"Caching condition for the use in should_cache() . Superclasses builtins.tuple","title":"CacheCondition"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.CacheCondition.base_cls","text":"Base class of the class or its name (case-sensitive).","title":"base_cls"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.CacheCondition.cls","text":"Class of the instance or its name (case-sensitive).","title":"cls"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.CacheCondition.flags","text":"Flags to check for in method/property's flags.","title":"flags"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.CacheCondition.func","text":"Method/property or its name (case-sensitive).","title":"func"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.CacheCondition.instance","text":"Class instance the method/property is bound to.","title":"instance"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.CacheCondition.rank","text":"Rank to override the default rank.","title":"rank"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.cached_methodT","text":"Base class for protocol classes. Protocol classes are defined as:: class Proto(Protocol): def meth(self) -> int: ... Such classes are primarily used with static type checkers that recognize structural subtyping (static duck-typing), for example:: class C def meth(self) -> int: return 0 def func(x: Proto) -> int: return x.meth() func(C()) # Passes static type check See PEP 544 for details. Protocol classes decorated with @typing_extensions .runtime act as simple-minded runtime protocol that checks only the presence of given attributes, ignoring their type signatures. Protocol classes can be generic, they are defined as:: class GenProto(Protocol[T]): def meth(self) -> T: ... Superclasses custom_methodT typing_extensions.Protocol","title":"cached_methodT"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.cached_methodT.attrname","text":"","title":"attrname"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.cached_methodT.clear_cache","text":"","title":"clear_cache"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.cached_methodT.lock","text":"","title":"lock"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.cached_methodT.maxsize","text":"","title":"maxsize"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.cached_methodT.name","text":"","title":"name"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.cached_methodT.typed","text":"","title":"typed"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.cached_property","text":"Extends custom_property with caching. Similar to functools.cached_property , but without replacing the original attribute to be able to re-compute whenever needed. Disables caching if should_cache() yields False. Cache can be cleared by calling clear_cache with instance as argument. !!! note: Assumes that the instance (provided as self ) won't change. If calculation depends upon object attributes that can be changed, it won't notice the change. Superclasses custom_property","title":"cached_property"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.cached_property.attrname","text":"Get name of cached attribute.","title":"attrname"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.cached_property.clear_cache","text":"cached_property . clear_cache ( instance ) Clear the cache for this property belonging to instance .","title":"clear_cache()"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.class_or_instancemethod","text":"Function decorator that binds self to a class if the function is called as class method, otherwise to an instance. Superclasses builtins.classmethod","title":"class_or_instancemethod"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.class_or_instanceproperty","text":"Property that binds self to a class if the function is called as class method, otherwise to an instance.","title":"class_or_instanceproperty"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.classproperty","text":"Property that can be called on a class.","title":"classproperty"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.custom_methodT","text":"Base class for protocol classes. Protocol classes are defined as:: class Proto(Protocol): def meth(self) -> int: ... Such classes are primarily used with static type checkers that recognize structural subtyping (static duck-typing), for example:: class C def meth(self) -> int: return 0 def func(x: Proto) -> int: return x.meth() func(C()) # Passes static type check See PEP 544 for details. Protocol classes decorated with @typing_extensions .runtime act as simple-minded runtime protocol that checks only the presence of given attributes, ignoring their type signatures. Protocol classes can be generic, they are defined as:: class GenProto(Protocol[T]): def meth(self) -> T: ... Superclasses typing_extensions.Protocol Subclasses cached_methodT","title":"custom_methodT"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.custom_methodT.flags","text":"","title":"flags"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.custom_methodT.func","text":"","title":"func"},{"location":"api/utils/decorators/#vectorbt.utils.decorators.custom_property","text":"Custom property that stores function and flags as attributes. Can be called both as >>> @custom_property ... def user_function ( self ): pass and >>> @custom_property(a=0, b=0) # flags ... def user_function(self): pass Note custom_property instances belong to classes, not class instances. Thus changing the property, for example, by disabling caching, will do the same for each instance of the class where the property has been defined. Subclasses cached_property","title":"custom_property"},{"location":"api/utils/docs/","text":"docs module \u00b6 Utilities for documentation. prepare_for_doc function \u00b6 prepare_for_doc ( obj , replace = None , path = None ) Prepare object for use in documentation. to_doc function \u00b6 to_doc ( obj , replace = None , path = None , ** kwargs ) Convert object to a JSON string. Documented class \u00b6 Abstract class for documenting self. Note Won't get converted into a string in prepare_for_doc() . Subclasses Config Configured to_doc method \u00b6 Documented . to_doc ( ** kwargs ) Convert to a doc. SafeToStr class \u00b6 Class that can be safely converted into a string in prepare_for_doc() . Subclasses Rep RepEval RepFunc Sub","title":"docs"},{"location":"api/utils/docs/#vectorbt.utils.docs","text":"Utilities for documentation.","title":"vectorbt.utils.docs"},{"location":"api/utils/docs/#vectorbt.utils.docs.prepare_for_doc","text":"prepare_for_doc ( obj , replace = None , path = None ) Prepare object for use in documentation.","title":"prepare_for_doc()"},{"location":"api/utils/docs/#vectorbt.utils.docs.to_doc","text":"to_doc ( obj , replace = None , path = None , ** kwargs ) Convert object to a JSON string.","title":"to_doc()"},{"location":"api/utils/docs/#vectorbt.utils.docs.Documented","text":"Abstract class for documenting self. Note Won't get converted into a string in prepare_for_doc() . Subclasses Config Configured","title":"Documented"},{"location":"api/utils/docs/#vectorbt.utils.docs.Documented.to_doc","text":"Documented . to_doc ( ** kwargs ) Convert to a doc.","title":"to_doc()"},{"location":"api/utils/docs/#vectorbt.utils.docs.SafeToStr","text":"Class that can be safely converted into a string in prepare_for_doc() . Subclasses Rep RepEval RepFunc Sub","title":"SafeToStr"},{"location":"api/utils/enum_/","text":"enum_ module \u00b6 Enum utilities. In vectorbt, enums are represented by instances of named tuples to be easily used in Numba. Their values start with 0, while -1 means there is no value. map_enum_fields function \u00b6 map_enum_fields ( field , enum , ignore_type = builtins . int , ** kwargs ) Map fields to values. See apply_mapping() . map_enum_values function \u00b6 map_enum_values ( value , enum , ignore_type = builtins . str , ** kwargs ) Map values to fields. See apply_mapping() .","title":"enum_"},{"location":"api/utils/enum_/#vectorbt.utils.enum_","text":"Enum utilities. In vectorbt, enums are represented by instances of named tuples to be easily used in Numba. Their values start with 0, while -1 means there is no value.","title":"vectorbt.utils.enum_"},{"location":"api/utils/enum_/#vectorbt.utils.enum_.map_enum_fields","text":"map_enum_fields ( field , enum , ignore_type = builtins . int , ** kwargs ) Map fields to values. See apply_mapping() .","title":"map_enum_fields()"},{"location":"api/utils/enum_/#vectorbt.utils.enum_.map_enum_values","text":"map_enum_values ( value , enum , ignore_type = builtins . str , ** kwargs ) Map values to fields. See apply_mapping() .","title":"map_enum_values()"},{"location":"api/utils/figure/","text":"figure module \u00b6 Utilities for constructing and displaying figures. get_domain function \u00b6 get_domain ( ref , fig ) Get domain of a coordinate axis. make_figure function \u00b6 make_figure ( * args , ** kwargs ) Make new figure. Returns either Figure or FigureWidget , depending on use_widgets defined under plotting in settings . make_subplots function \u00b6 make_subplots ( * args , ** kwargs ) Makes subplots and passes them to FigureWidget . Figure class \u00b6 Figure. Extends plotly.graph_objects.Figure . Superclasses FigureMixin plotly.basedatatypes.BaseFigure plotly.graph_objs._figure.Figure Inherited members FigureMixin.show_png() FigureMixin.show_svg() show method \u00b6 Figure . show ( * args , ** kwargs ) Show the figure. FigureMixin class \u00b6 Subclasses Figure FigureWidget show method \u00b6 FigureMixin . show ( * args , ** kwargs ) Display the figure in PNG format. show_png method \u00b6 FigureMixin . show_png ( ** kwargs ) Display the figure in PNG format. show_svg method \u00b6 FigureMixin . show_svg ( ** kwargs ) Display the figure in SVG format. FigureWidget class \u00b6 Figure widget. Extends plotly.graph_objects.FigureWidget . Superclasses FigureMixin ipywidgets.widgets.domwidget.DOMWidget ipywidgets.widgets.widget.LoggingHasTraits ipywidgets.widgets.widget.Widget plotly.basedatatypes.BaseFigure plotly.basewidget.BaseFigureWidget plotly.graph_objs._figurewidget.FigureWidget traitlets.traitlets.HasDescriptors traitlets.traitlets.HasTraits Inherited members FigureMixin.show_png() FigureMixin.show_svg() show method \u00b6 FigureWidget . show ( * args , ** kwargs ) Show the figure.","title":"figure"},{"location":"api/utils/figure/#vectorbt.utils.figure","text":"Utilities for constructing and displaying figures.","title":"vectorbt.utils.figure"},{"location":"api/utils/figure/#vectorbt.utils.figure.get_domain","text":"get_domain ( ref , fig ) Get domain of a coordinate axis.","title":"get_domain()"},{"location":"api/utils/figure/#vectorbt.utils.figure.make_figure","text":"make_figure ( * args , ** kwargs ) Make new figure. Returns either Figure or FigureWidget , depending on use_widgets defined under plotting in settings .","title":"make_figure()"},{"location":"api/utils/figure/#vectorbt.utils.figure.make_subplots","text":"make_subplots ( * args , ** kwargs ) Makes subplots and passes them to FigureWidget .","title":"make_subplots()"},{"location":"api/utils/figure/#vectorbt.utils.figure.Figure","text":"Figure. Extends plotly.graph_objects.Figure . Superclasses FigureMixin plotly.basedatatypes.BaseFigure plotly.graph_objs._figure.Figure Inherited members FigureMixin.show_png() FigureMixin.show_svg()","title":"Figure"},{"location":"api/utils/figure/#vectorbt.utils.figure.Figure.show","text":"Figure . show ( * args , ** kwargs ) Show the figure.","title":"show()"},{"location":"api/utils/figure/#vectorbt.utils.figure.FigureMixin","text":"Subclasses Figure FigureWidget","title":"FigureMixin"},{"location":"api/utils/figure/#vectorbt.utils.figure.FigureMixin.show","text":"FigureMixin . show ( * args , ** kwargs ) Display the figure in PNG format.","title":"show()"},{"location":"api/utils/figure/#vectorbt.utils.figure.FigureMixin.show_png","text":"FigureMixin . show_png ( ** kwargs ) Display the figure in PNG format.","title":"show_png()"},{"location":"api/utils/figure/#vectorbt.utils.figure.FigureMixin.show_svg","text":"FigureMixin . show_svg ( ** kwargs ) Display the figure in SVG format.","title":"show_svg()"},{"location":"api/utils/figure/#vectorbt.utils.figure.FigureWidget","text":"Figure widget. Extends plotly.graph_objects.FigureWidget . Superclasses FigureMixin ipywidgets.widgets.domwidget.DOMWidget ipywidgets.widgets.widget.LoggingHasTraits ipywidgets.widgets.widget.Widget plotly.basedatatypes.BaseFigure plotly.basewidget.BaseFigureWidget plotly.graph_objs._figurewidget.FigureWidget traitlets.traitlets.HasDescriptors traitlets.traitlets.HasTraits Inherited members FigureMixin.show_png() FigureMixin.show_svg()","title":"FigureWidget"},{"location":"api/utils/figure/#vectorbt.utils.figure.FigureWidget.show","text":"FigureWidget . show ( * args , ** kwargs ) Show the figure.","title":"show()"},{"location":"api/utils/image_/","text":"image_ module \u00b6 Utilities for images. hstack_image_arrays function \u00b6 hstack_image_arrays ( a , b ) Stack NumPy images horizontally. save_animation function \u00b6 save_animation ( fname , index , plot_func , * args , delta = None , step = 1 , fps = 3 , writer_kwargs = None , show_progress = True , tqdm_kwargs = None , to_image_kwargs = None , ** kwargs ) Save animation to a file. Args fname :\u2002 str File name. index :\u2002 iterable Index to iterate over. plot_func :\u2002 callable Plotting function. Should take subset of index , *args , and **kwargs , and return either a Plotly figure, image that can be read by imageio.imread , or a NumPy array. *args Positional arguments passed to plot_func . delta :\u2002 int Window size of each iteration. step :\u2002 int Step of each iteration. fps :\u2002 int Frames per second. writer_kwargs :\u2002 dict Keyword arguments passed to imageio.get_writer . show_progress :\u2002 bool Whether to show the progress bar. tqdm_kwargs :\u2002 dict Keyword arguments passed to tqdm . to_image_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.to_image . **kwargs Keyword arguments passed to plot_func . vstack_image_arrays function \u00b6 vstack_image_arrays ( a , b ) Stack NumPy images vertically.","title":"image_"},{"location":"api/utils/image_/#vectorbt.utils.image_","text":"Utilities for images.","title":"vectorbt.utils.image_"},{"location":"api/utils/image_/#vectorbt.utils.image_.hstack_image_arrays","text":"hstack_image_arrays ( a , b ) Stack NumPy images horizontally.","title":"hstack_image_arrays()"},{"location":"api/utils/image_/#vectorbt.utils.image_.save_animation","text":"save_animation ( fname , index , plot_func , * args , delta = None , step = 1 , fps = 3 , writer_kwargs = None , show_progress = True , tqdm_kwargs = None , to_image_kwargs = None , ** kwargs ) Save animation to a file. Args fname :\u2002 str File name. index :\u2002 iterable Index to iterate over. plot_func :\u2002 callable Plotting function. Should take subset of index , *args , and **kwargs , and return either a Plotly figure, image that can be read by imageio.imread , or a NumPy array. *args Positional arguments passed to plot_func . delta :\u2002 int Window size of each iteration. step :\u2002 int Step of each iteration. fps :\u2002 int Frames per second. writer_kwargs :\u2002 dict Keyword arguments passed to imageio.get_writer . show_progress :\u2002 bool Whether to show the progress bar. tqdm_kwargs :\u2002 dict Keyword arguments passed to tqdm . to_image_kwargs :\u2002 dict Keyword arguments passed to plotly.graph_objects.Figure.to_image . **kwargs Keyword arguments passed to plot_func .","title":"save_animation()"},{"location":"api/utils/image_/#vectorbt.utils.image_.vstack_image_arrays","text":"vstack_image_arrays ( a , b ) Stack NumPy images vertically.","title":"vstack_image_arrays()"},{"location":"api/utils/mapping/","text":"mapping module \u00b6 Mapping utilities. apply_mapping function \u00b6 apply_mapping ( obj , mapping_like = None , reverse = False , ignore_case = True , ignore_underscores = True , ignore_type = None , ignore_missing = False , na_sentinel = None ) Apply mapping on object using a mapping-like object. Args obj :\u2002 any Any object. Can take a scalar, tuple, list, set, frozenset, NumPy array, Index, Series, and DataFrame. mapping_like :\u2002 mapping_like Any mapping-like object. See to_mapping() . reverse :\u2002 bool See reverse in to_mapping() . ignore_case :\u2002 bool Whether to ignore the case if the key is a string. ignore_underscores :\u2002 bool Whether to ignore underscores if the key is a string. ignore_type :\u2002 dtype_like or tuple One or multiple types or data types to ignore. ignore_missing :\u2002 bool Whether to ignore missing values. na_sentinel :\u2002 any Value to mark \u201cnot found\u201d. reverse_mapping function \u00b6 reverse_mapping ( mapping ) Reverse a mapping. Returns a dict. to_mapping function \u00b6 to_mapping ( mapping_like , reverse = False ) Convert mapping-like object to a mapping. Enable reverse to apply reverse_mapping() on the result dict.","title":"mapping"},{"location":"api/utils/mapping/#vectorbt.utils.mapping","text":"Mapping utilities.","title":"vectorbt.utils.mapping"},{"location":"api/utils/mapping/#vectorbt.utils.mapping.apply_mapping","text":"apply_mapping ( obj , mapping_like = None , reverse = False , ignore_case = True , ignore_underscores = True , ignore_type = None , ignore_missing = False , na_sentinel = None ) Apply mapping on object using a mapping-like object. Args obj :\u2002 any Any object. Can take a scalar, tuple, list, set, frozenset, NumPy array, Index, Series, and DataFrame. mapping_like :\u2002 mapping_like Any mapping-like object. See to_mapping() . reverse :\u2002 bool See reverse in to_mapping() . ignore_case :\u2002 bool Whether to ignore the case if the key is a string. ignore_underscores :\u2002 bool Whether to ignore underscores if the key is a string. ignore_type :\u2002 dtype_like or tuple One or multiple types or data types to ignore. ignore_missing :\u2002 bool Whether to ignore missing values. na_sentinel :\u2002 any Value to mark \u201cnot found\u201d.","title":"apply_mapping()"},{"location":"api/utils/mapping/#vectorbt.utils.mapping.reverse_mapping","text":"reverse_mapping ( mapping ) Reverse a mapping. Returns a dict.","title":"reverse_mapping()"},{"location":"api/utils/mapping/#vectorbt.utils.mapping.to_mapping","text":"to_mapping ( mapping_like , reverse = False ) Convert mapping-like object to a mapping. Enable reverse to apply reverse_mapping() on the result dict.","title":"to_mapping()"},{"location":"api/utils/math_/","text":"math_ module \u00b6 Math utilities. add_nb function \u00b6 add_nb ( a , b , rel_tol = 1e-09 , abs_tol = 1e-12 ) Add two floats. is_addition_zero_nb function \u00b6 is_addition_zero_nb ( a , b , rel_tol = 1e-09 , abs_tol = 1e-12 ) Tell whether addition of two values yields zero. is_close_nb function \u00b6 is_close_nb ( a , b , rel_tol = 1e-09 , abs_tol = 1e-12 ) Tell whether two values are approximately equal. is_close_or_less_nb function \u00b6 is_close_or_less_nb ( a , b , rel_tol = 1e-09 , abs_tol = 1e-12 ) Tell whether the first value is approximately less than or equal to the second value. is_less_nb function \u00b6 is_less_nb ( a , b , rel_tol = 1e-09 , abs_tol = 1e-12 ) Tell whether the first value is approximately less than the second value.","title":"math_"},{"location":"api/utils/math_/#vectorbt.utils.math_","text":"Math utilities.","title":"vectorbt.utils.math_"},{"location":"api/utils/math_/#vectorbt.utils.math_.add_nb","text":"add_nb ( a , b , rel_tol = 1e-09 , abs_tol = 1e-12 ) Add two floats.","title":"add_nb()"},{"location":"api/utils/math_/#vectorbt.utils.math_.is_addition_zero_nb","text":"is_addition_zero_nb ( a , b , rel_tol = 1e-09 , abs_tol = 1e-12 ) Tell whether addition of two values yields zero.","title":"is_addition_zero_nb()"},{"location":"api/utils/math_/#vectorbt.utils.math_.is_close_nb","text":"is_close_nb ( a , b , rel_tol = 1e-09 , abs_tol = 1e-12 ) Tell whether two values are approximately equal.","title":"is_close_nb()"},{"location":"api/utils/math_/#vectorbt.utils.math_.is_close_or_less_nb","text":"is_close_or_less_nb ( a , b , rel_tol = 1e-09 , abs_tol = 1e-12 ) Tell whether the first value is approximately less than or equal to the second value.","title":"is_close_or_less_nb()"},{"location":"api/utils/math_/#vectorbt.utils.math_.is_less_nb","text":"is_less_nb ( a , b , rel_tol = 1e-09 , abs_tol = 1e-12 ) Tell whether the first value is approximately less than the second value.","title":"is_less_nb()"},{"location":"api/utils/module_/","text":"module_ module \u00b6 Utilities for modules. import_submodules function \u00b6 import_submodules ( package ) Import all submodules of a module, recursively, including subpackages. If package defines __blacklist__ , does not import modules that match names from this list. is_from_module function \u00b6 is_from_module ( obj , module ) Return whether obj is from module module . list_module_keys function \u00b6 list_module_keys ( module_name , whitelist = None , blacklist = None ) List the names of all public functions and classes defined in the module module_name . Includes the names listed in whitelist and excludes the names listed in blacklist .","title":"module_"},{"location":"api/utils/module_/#vectorbt.utils.module_","text":"Utilities for modules.","title":"vectorbt.utils.module_"},{"location":"api/utils/module_/#vectorbt.utils.module_.import_submodules","text":"import_submodules ( package ) Import all submodules of a module, recursively, including subpackages. If package defines __blacklist__ , does not import modules that match names from this list.","title":"import_submodules()"},{"location":"api/utils/module_/#vectorbt.utils.module_.is_from_module","text":"is_from_module ( obj , module ) Return whether obj is from module module .","title":"is_from_module()"},{"location":"api/utils/module_/#vectorbt.utils.module_.list_module_keys","text":"list_module_keys ( module_name , whitelist = None , blacklist = None ) List the names of all public functions and classes defined in the module module_name . Includes the names listed in whitelist and excludes the names listed in blacklist .","title":"list_module_keys()"},{"location":"api/utils/params/","text":"params module \u00b6 Utilities for working with parameters. broadcast_params function \u00b6 broadcast_params ( param_list , to_n = None ) Broadcast parameters in param_list . create_param_combs function \u00b6 create_param_combs ( op_tree , depth = 0 ) Create arbitrary parameter combinations from the operation tree op_tree . op_tree is a tuple with nested instructions to generate parameters. The first element of the tuple should be a callable that takes remaining elements as arguments. If one of the elements is a tuple itself and its first argument is a callable, it will be unfolded in the same way as above. Usage >>> import numpy as np >>> from itertools import combinations , product >>> create_param_combs (( product , ( combinations , [ 0 , 1 , 2 , 3 ], 2 ), [ 4 , 5 ])) [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2], [1, 1, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3], [4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5]] create_param_product function \u00b6 create_param_product ( param_list ) Make Cartesian product out of all params in param_list . flatten_param_tuples function \u00b6 flatten_param_tuples ( param_tuples ) Flattens a nested list of iterables using unzipping. to_typed_list function \u00b6 to_typed_list ( lst ) Cast Python list to typed list. Direct construction is flawed in Numba 0.52.0. See https://github.com/numba/numba/issues/6651.","title":"params"},{"location":"api/utils/params/#vectorbt.utils.params","text":"Utilities for working with parameters.","title":"vectorbt.utils.params"},{"location":"api/utils/params/#vectorbt.utils.params.broadcast_params","text":"broadcast_params ( param_list , to_n = None ) Broadcast parameters in param_list .","title":"broadcast_params()"},{"location":"api/utils/params/#vectorbt.utils.params.create_param_combs","text":"create_param_combs ( op_tree , depth = 0 ) Create arbitrary parameter combinations from the operation tree op_tree . op_tree is a tuple with nested instructions to generate parameters. The first element of the tuple should be a callable that takes remaining elements as arguments. If one of the elements is a tuple itself and its first argument is a callable, it will be unfolded in the same way as above. Usage >>> import numpy as np >>> from itertools import combinations , product >>> create_param_combs (( product , ( combinations , [ 0 , 1 , 2 , 3 ], 2 ), [ 4 , 5 ])) [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2], [1, 1, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3], [4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5]]","title":"create_param_combs()"},{"location":"api/utils/params/#vectorbt.utils.params.create_param_product","text":"create_param_product ( param_list ) Make Cartesian product out of all params in param_list .","title":"create_param_product()"},{"location":"api/utils/params/#vectorbt.utils.params.flatten_param_tuples","text":"flatten_param_tuples ( param_tuples ) Flattens a nested list of iterables using unzipping.","title":"flatten_param_tuples()"},{"location":"api/utils/params/#vectorbt.utils.params.to_typed_list","text":"to_typed_list ( lst ) Cast Python list to typed list. Direct construction is flawed in Numba 0.52.0. See https://github.com/numba/numba/issues/6651.","title":"to_typed_list()"},{"location":"api/utils/random_/","text":"random_ module \u00b6 Utilities for random number generation. set_seed function \u00b6 set_seed ( seed ) Set seed. set_seed_nb function \u00b6 set_seed_nb ( seed ) Set seed in numba.","title":"random_"},{"location":"api/utils/random_/#vectorbt.utils.random_","text":"Utilities for random number generation.","title":"vectorbt.utils.random_"},{"location":"api/utils/random_/#vectorbt.utils.random_.set_seed","text":"set_seed ( seed ) Set seed.","title":"set_seed()"},{"location":"api/utils/random_/#vectorbt.utils.random_.set_seed_nb","text":"set_seed_nb ( seed ) Set seed in numba.","title":"set_seed_nb()"},{"location":"api/utils/requests_/","text":"requests_ module \u00b6 Utilities for requests. requests_retry_session function \u00b6 requests_retry_session ( retries = 3 , backoff_factor = 0.3 , status_forcelist = ( 500 , 502 , 504 ), session = None ) Retry retries times if unsuccessful. text_to_giphy_url function \u00b6 text_to_giphy_url ( text , api_key = None , weirdness = None ) Translate text to GIF. See https://engineering.giphy.com/contextually-aware-search-giphy-gets-work-specific/.","title":"requests_"},{"location":"api/utils/requests_/#vectorbt.utils.requests_","text":"Utilities for requests.","title":"vectorbt.utils.requests_"},{"location":"api/utils/requests_/#vectorbt.utils.requests_.requests_retry_session","text":"requests_retry_session ( retries = 3 , backoff_factor = 0.3 , status_forcelist = ( 500 , 502 , 504 ), session = None ) Retry retries times if unsuccessful.","title":"requests_retry_session()"},{"location":"api/utils/requests_/#vectorbt.utils.requests_.text_to_giphy_url","text":"text_to_giphy_url ( text , api_key = None , weirdness = None ) Translate text to GIF. See https://engineering.giphy.com/contextually-aware-search-giphy-gets-work-specific/.","title":"text_to_giphy_url()"},{"location":"api/utils/schedule_/","text":"schedule_ module \u00b6 Utilities for scheduling jobs. AsyncJob class \u00b6 A periodic job as used by :class: Scheduler . :param interval: A quantity of a certain time unit :param scheduler: The :class: Scheduler <Scheduler> instance that this job will register itself with once it has been fully configured in :meth: Job.do() . Every job runs at a given fixed time interval that is defined by: a :meth: time unit <Job.second> a quantity of time units defined by interval A job is usually created and returned by :meth: Scheduler.every method, which also defines its interval . Superclasses schedule.Job async_run method \u00b6 AsyncJob . async_run () Async Job.run . AsyncScheduler class \u00b6 Objects instantiated by the :class: Scheduler <Scheduler> are factories to create jobs, keep record of scheduled jobs and handle their execution. Superclasses schedule.Scheduler async_run_all method \u00b6 AsyncScheduler . async_run_all ( delay_seconds = 0 ) Async Scheduler.run_all . async_run_pending method \u00b6 AsyncScheduler . async_run_pending () Async Scheduler.run_pending . every method \u00b6 AsyncScheduler . every ( interval = 1 ) Schedule a new periodic job of type AsyncJob . CancelledError class \u00b6 Thrown for the operation to be cancelled. Superclasses builtins.BaseException builtins.Exception concurrent.futures._base.CancelledError concurrent.futures._base.Error ScheduleManager class \u00b6 Class that manages schedule.Scheduler . async_start method \u00b6 ScheduleManager . async_start ( sleep = 1 ) Async run pending jobs in a loop. async_task property \u00b6 Current async task. async_task_running property \u00b6 Whether the async task is running. done_callback method \u00b6 ScheduleManager . done_callback ( async_task ) Callback run when the async task is finished. every method \u00b6 ScheduleManager . every ( * args , to = None , tags = None ) Create a new job that runs every interval units of time. *args can include at most four different arguments: interval , unit , start_day , and at , in the strict order: interval : integer or datetime.timedelta unit : ScheduleManager.units start_day : ScheduleManager.weekdays at : string or datetime.time . See the package schedule for more details. Usage >>> import datetime >>> import pytz >>> import vectorbt as vbt >>> def job_func ( message = \"I'm working...\" ): ... print ( message ) >>> my_manager = vbt . ScheduleManager () >>> # add jobs >>> my_manager . every () . do ( job_func , message = \"Hello\" ) Every 1 second do job_func(message='Hello') (last run: [never], next run: 2021-03-18 19:06:47) >>> my_manager . every ( 10 , 'minutes' ) . do ( job_func ) Every 10 minutes do job_func() (last run: [never], next run: 2021-03-18 19:16:46) >>> my_manager . every ( 'hour' ) . do ( job_func ) Every 1 hour do job_func() (last run: [never], next run: 2021-03-18 20:06:46) >>> my_manager . every ( '10:30' ) . do ( job_func ) Every 1 day at 10:30:00 do job_func() (last run: [never], next run: 2021-03-19 10:30:00) >>> my_manager . every ( 'day' , '10:30' ) . do ( job_func ) Every 1 day at 10:30:00 do job_func() (last run: [never], next run: 2021-03-19 10:30:00) >>> my_manager . every ( 'day' , datetime . time ( 9 , 30 , tzinfo = pytz . utc )) . do ( job_func ) Every 1 day at 10:30:00 do job_func() (last run: [never], next run: 2021-03-19 10:30:00) >>> my_manager . every ( 'monday' ) . do ( job_func ) Every 1 week do job_func() (last run: [never], next run: 2021-03-22 19:06:46) >>> my_manager . every ( 'wednesday' , '13:15' ) . do ( job_func ) Every 1 week at 13:15:00 do job_func() (last run: [never], next run: 2021-03-24 13:15:00) >>> my_manager . every ( 'minute' , ':17' ) . do ( job_func ) Every 1 minute at 00:00:17 do job_func() (last run: [never], next run: 2021-03-18 19:07:17) >>> my_manager . start () You can still use the chained approach as done by schedule : >>> my_manager . every () . minute . at ( ':17' ) . do ( job_func ) Every 1 minute at 00:00:17 do job_func() (last run: [never], next run: 2021-03-18 19:07:17) scheduler property \u00b6 Scheduler. start method \u00b6 ScheduleManager . start ( sleep = 1 ) Run pending jobs in a loop. start_in_background method \u00b6 ScheduleManager . start_in_background ( ** kwargs ) Run ScheduleManager.async_start() in the background. stop method \u00b6 ScheduleManager . stop () Stop the async task. units class variable \u00b6 weekdays class variable \u00b6","title":"schedule_"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_","text":"Utilities for scheduling jobs.","title":"vectorbt.utils.schedule_"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.AsyncJob","text":"A periodic job as used by :class: Scheduler . :param interval: A quantity of a certain time unit :param scheduler: The :class: Scheduler <Scheduler> instance that this job will register itself with once it has been fully configured in :meth: Job.do() . Every job runs at a given fixed time interval that is defined by: a :meth: time unit <Job.second> a quantity of time units defined by interval A job is usually created and returned by :meth: Scheduler.every method, which also defines its interval . Superclasses schedule.Job","title":"AsyncJob"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.AsyncJob.async_run","text":"AsyncJob . async_run () Async Job.run .","title":"async_run()"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.AsyncScheduler","text":"Objects instantiated by the :class: Scheduler <Scheduler> are factories to create jobs, keep record of scheduled jobs and handle their execution. Superclasses schedule.Scheduler","title":"AsyncScheduler"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.AsyncScheduler.async_run_all","text":"AsyncScheduler . async_run_all ( delay_seconds = 0 ) Async Scheduler.run_all .","title":"async_run_all()"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.AsyncScheduler.async_run_pending","text":"AsyncScheduler . async_run_pending () Async Scheduler.run_pending .","title":"async_run_pending()"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.AsyncScheduler.every","text":"AsyncScheduler . every ( interval = 1 ) Schedule a new periodic job of type AsyncJob .","title":"every()"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.CancelledError","text":"Thrown for the operation to be cancelled. Superclasses builtins.BaseException builtins.Exception concurrent.futures._base.CancelledError concurrent.futures._base.Error","title":"CancelledError"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.ScheduleManager","text":"Class that manages schedule.Scheduler .","title":"ScheduleManager"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.ScheduleManager.async_start","text":"ScheduleManager . async_start ( sleep = 1 ) Async run pending jobs in a loop.","title":"async_start()"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.ScheduleManager.async_task","text":"Current async task.","title":"async_task"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.ScheduleManager.async_task_running","text":"Whether the async task is running.","title":"async_task_running"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.ScheduleManager.done_callback","text":"ScheduleManager . done_callback ( async_task ) Callback run when the async task is finished.","title":"done_callback()"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.ScheduleManager.every","text":"ScheduleManager . every ( * args , to = None , tags = None ) Create a new job that runs every interval units of time. *args can include at most four different arguments: interval , unit , start_day , and at , in the strict order: interval : integer or datetime.timedelta unit : ScheduleManager.units start_day : ScheduleManager.weekdays at : string or datetime.time . See the package schedule for more details. Usage >>> import datetime >>> import pytz >>> import vectorbt as vbt >>> def job_func ( message = \"I'm working...\" ): ... print ( message ) >>> my_manager = vbt . ScheduleManager () >>> # add jobs >>> my_manager . every () . do ( job_func , message = \"Hello\" ) Every 1 second do job_func(message='Hello') (last run: [never], next run: 2021-03-18 19:06:47) >>> my_manager . every ( 10 , 'minutes' ) . do ( job_func ) Every 10 minutes do job_func() (last run: [never], next run: 2021-03-18 19:16:46) >>> my_manager . every ( 'hour' ) . do ( job_func ) Every 1 hour do job_func() (last run: [never], next run: 2021-03-18 20:06:46) >>> my_manager . every ( '10:30' ) . do ( job_func ) Every 1 day at 10:30:00 do job_func() (last run: [never], next run: 2021-03-19 10:30:00) >>> my_manager . every ( 'day' , '10:30' ) . do ( job_func ) Every 1 day at 10:30:00 do job_func() (last run: [never], next run: 2021-03-19 10:30:00) >>> my_manager . every ( 'day' , datetime . time ( 9 , 30 , tzinfo = pytz . utc )) . do ( job_func ) Every 1 day at 10:30:00 do job_func() (last run: [never], next run: 2021-03-19 10:30:00) >>> my_manager . every ( 'monday' ) . do ( job_func ) Every 1 week do job_func() (last run: [never], next run: 2021-03-22 19:06:46) >>> my_manager . every ( 'wednesday' , '13:15' ) . do ( job_func ) Every 1 week at 13:15:00 do job_func() (last run: [never], next run: 2021-03-24 13:15:00) >>> my_manager . every ( 'minute' , ':17' ) . do ( job_func ) Every 1 minute at 00:00:17 do job_func() (last run: [never], next run: 2021-03-18 19:07:17) >>> my_manager . start () You can still use the chained approach as done by schedule : >>> my_manager . every () . minute . at ( ':17' ) . do ( job_func ) Every 1 minute at 00:00:17 do job_func() (last run: [never], next run: 2021-03-18 19:07:17)","title":"every()"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.ScheduleManager.scheduler","text":"Scheduler.","title":"scheduler"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.ScheduleManager.start","text":"ScheduleManager . start ( sleep = 1 ) Run pending jobs in a loop.","title":"start()"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.ScheduleManager.start_in_background","text":"ScheduleManager . start_in_background ( ** kwargs ) Run ScheduleManager.async_start() in the background.","title":"start_in_background()"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.ScheduleManager.stop","text":"ScheduleManager . stop () Stop the async task.","title":"stop()"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.ScheduleManager.units","text":"","title":"units"},{"location":"api/utils/schedule_/#vectorbt.utils.schedule_.ScheduleManager.weekdays","text":"","title":"weekdays"},{"location":"api/utils/tags/","text":"tags module \u00b6 Utilities for working with tags. match_tags function \u00b6 match_tags ( tags , in_tags ) Match tags in tags to that in in_tags . Multiple tags in tags are combined using OR rule, that is, returns True if any of them is found in in_tags . If any tag is not an identifier, evaluates it as a boolean expression. All tags in in_tags should be identifiers. Usage >>> from vectorbt.utils.tags import match_tags >>> match_tags ( 'hello' , 'hello' ) True >>> match_tags ( 'hello' , 'world' ) False >>> match_tags ([ 'hello' , 'world' ], 'world' ) True >>> match_tags ( 'hello' , [ 'hello' , 'world' ]) True >>> match_tags ( 'hello and world' , [ 'hello' , 'world' ]) True >>> match_tags ( 'hello and not world' , [ 'hello' , 'world' ]) False","title":"tags"},{"location":"api/utils/tags/#vectorbt.utils.tags","text":"Utilities for working with tags.","title":"vectorbt.utils.tags"},{"location":"api/utils/tags/#vectorbt.utils.tags.match_tags","text":"match_tags ( tags , in_tags ) Match tags in tags to that in in_tags . Multiple tags in tags are combined using OR rule, that is, returns True if any of them is found in in_tags . If any tag is not an identifier, evaluates it as a boolean expression. All tags in in_tags should be identifiers. Usage >>> from vectorbt.utils.tags import match_tags >>> match_tags ( 'hello' , 'hello' ) True >>> match_tags ( 'hello' , 'world' ) False >>> match_tags ([ 'hello' , 'world' ], 'world' ) True >>> match_tags ( 'hello' , [ 'hello' , 'world' ]) True >>> match_tags ( 'hello and world' , [ 'hello' , 'world' ]) True >>> match_tags ( 'hello and not world' , [ 'hello' , 'world' ]) False","title":"match_tags()"},{"location":"api/utils/template/","text":"template module \u00b6 Utilities for working with templates. deep_substitute function \u00b6 deep_substitute ( obj , mapping = None , safe = False , make_copy = True ) Traverses the object recursively and, if any template found, substitutes it using a mapping. Traverses tuples, lists, dicts and (frozen-)sets. Does not look for templates in keys. If safe is True, won't raise an error but return the original template. Note If the object is deep (such as a dict or a list), creates a copy of it if any template found inside, thus loosing the reference to the original. Make sure to do a deep or hybrid copy of the object before proceeding for consistent behavior, or disable make_copy to override the original in place. Usage >>> import vectorbt as vbt >>> vbt . deep_substitute ( vbt . Sub ( '$key' , { 'key' : 100 })) 100 >>> vbt . deep_substitute ( vbt . Sub ( '$key' , { 'key' : 100 }), { 'key' : 200 }) 200 >>> vbt . deep_substitute ( vbt . Sub ( '$key$key' ), { 'key' : 100 }) 100100 >>> vbt . deep_substitute ( vbt . Rep ( 'key' ), { 'key' : 100 }) 100 >>> vbt . deep_substitute ([ vbt . Rep ( 'key' ), vbt . Sub ( '$key$key' )], { 'key' : 100 }) [100, '100100'] >>> vbt . deep_substitute ( vbt . RepFunc ( lambda key : key == 100 ), { 'key' : 100 }) True >>> vbt . deep_substitute ( vbt . RepEval ( 'key == 100' ), { 'key' : 100 }) True >>> vbt . deep_substitute ( vbt . RepEval ( 'key == 100' , safe = False )) NameError: name 'key' is not defined >>> vbt . deep_substitute ( vbt . RepEval ( 'key == 100' , safe = True )) <vectorbt.utils.template.RepEval at 0x7fe3ad2ab668> has_templates function \u00b6 has_templates ( obj ) Check if the object has any templates. Rep class \u00b6 Key to be replaced with the respective value from mapping . Superclasses SafeToStr key property \u00b6 Key to be replaced. mapping property \u00b6 Mapping object passed to the initializer. replace method \u00b6 Rep . replace ( mapping = None ) Replace Rep.key using mapping . Merges mapping and Rep.mapping . RepEval class \u00b6 Expression to be evaluated with mapping used as locals. Superclasses SafeToStr eval method \u00b6 RepEval . eval ( mapping = None ) Evaluate RepEval.expression using mapping . Merges mapping and RepEval.mapping . expression property \u00b6 Expression to be evaluated. mapping property \u00b6 Mapping object passed to the initializer. RepFunc class \u00b6 Function to be called with argument names from mapping . Superclasses SafeToStr call method \u00b6 RepFunc . call ( mapping = None ) Call RepFunc.func using mapping . Merges mapping and RepFunc.mapping . func property \u00b6 Replacement function to be called. mapping property \u00b6 Mapping object passed to the initializer. Sub class \u00b6 Template to substitute parts of the string with the respective values from mapping . Returns a string. Superclasses SafeToStr mapping property \u00b6 Mapping object passed to the initializer. substitute method \u00b6 Sub . substitute ( mapping = None ) Substitute parts of Sub.template using mapping . Merges mapping and Sub.mapping . template property \u00b6 Template to be processed.","title":"template"},{"location":"api/utils/template/#vectorbt.utils.template","text":"Utilities for working with templates.","title":"vectorbt.utils.template"},{"location":"api/utils/template/#vectorbt.utils.template.deep_substitute","text":"deep_substitute ( obj , mapping = None , safe = False , make_copy = True ) Traverses the object recursively and, if any template found, substitutes it using a mapping. Traverses tuples, lists, dicts and (frozen-)sets. Does not look for templates in keys. If safe is True, won't raise an error but return the original template. Note If the object is deep (such as a dict or a list), creates a copy of it if any template found inside, thus loosing the reference to the original. Make sure to do a deep or hybrid copy of the object before proceeding for consistent behavior, or disable make_copy to override the original in place. Usage >>> import vectorbt as vbt >>> vbt . deep_substitute ( vbt . Sub ( '$key' , { 'key' : 100 })) 100 >>> vbt . deep_substitute ( vbt . Sub ( '$key' , { 'key' : 100 }), { 'key' : 200 }) 200 >>> vbt . deep_substitute ( vbt . Sub ( '$key$key' ), { 'key' : 100 }) 100100 >>> vbt . deep_substitute ( vbt . Rep ( 'key' ), { 'key' : 100 }) 100 >>> vbt . deep_substitute ([ vbt . Rep ( 'key' ), vbt . Sub ( '$key$key' )], { 'key' : 100 }) [100, '100100'] >>> vbt . deep_substitute ( vbt . RepFunc ( lambda key : key == 100 ), { 'key' : 100 }) True >>> vbt . deep_substitute ( vbt . RepEval ( 'key == 100' ), { 'key' : 100 }) True >>> vbt . deep_substitute ( vbt . RepEval ( 'key == 100' , safe = False )) NameError: name 'key' is not defined >>> vbt . deep_substitute ( vbt . RepEval ( 'key == 100' , safe = True )) <vectorbt.utils.template.RepEval at 0x7fe3ad2ab668>","title":"deep_substitute()"},{"location":"api/utils/template/#vectorbt.utils.template.has_templates","text":"has_templates ( obj ) Check if the object has any templates.","title":"has_templates()"},{"location":"api/utils/template/#vectorbt.utils.template.Rep","text":"Key to be replaced with the respective value from mapping . Superclasses SafeToStr","title":"Rep"},{"location":"api/utils/template/#vectorbt.utils.template.Rep.key","text":"Key to be replaced.","title":"key"},{"location":"api/utils/template/#vectorbt.utils.template.Rep.mapping","text":"Mapping object passed to the initializer.","title":"mapping"},{"location":"api/utils/template/#vectorbt.utils.template.Rep.replace","text":"Rep . replace ( mapping = None ) Replace Rep.key using mapping . Merges mapping and Rep.mapping .","title":"replace()"},{"location":"api/utils/template/#vectorbt.utils.template.RepEval","text":"Expression to be evaluated with mapping used as locals. Superclasses SafeToStr","title":"RepEval"},{"location":"api/utils/template/#vectorbt.utils.template.RepEval.eval","text":"RepEval . eval ( mapping = None ) Evaluate RepEval.expression using mapping . Merges mapping and RepEval.mapping .","title":"eval()"},{"location":"api/utils/template/#vectorbt.utils.template.RepEval.expression","text":"Expression to be evaluated.","title":"expression"},{"location":"api/utils/template/#vectorbt.utils.template.RepEval.mapping","text":"Mapping object passed to the initializer.","title":"mapping"},{"location":"api/utils/template/#vectorbt.utils.template.RepFunc","text":"Function to be called with argument names from mapping . Superclasses SafeToStr","title":"RepFunc"},{"location":"api/utils/template/#vectorbt.utils.template.RepFunc.call","text":"RepFunc . call ( mapping = None ) Call RepFunc.func using mapping . Merges mapping and RepFunc.mapping .","title":"call()"},{"location":"api/utils/template/#vectorbt.utils.template.RepFunc.func","text":"Replacement function to be called.","title":"func"},{"location":"api/utils/template/#vectorbt.utils.template.RepFunc.mapping","text":"Mapping object passed to the initializer.","title":"mapping"},{"location":"api/utils/template/#vectorbt.utils.template.Sub","text":"Template to substitute parts of the string with the respective values from mapping . Returns a string. Superclasses SafeToStr","title":"Sub"},{"location":"api/utils/template/#vectorbt.utils.template.Sub.mapping","text":"Mapping object passed to the initializer.","title":"mapping"},{"location":"api/utils/template/#vectorbt.utils.template.Sub.substitute","text":"Sub . substitute ( mapping = None ) Substitute parts of Sub.template using mapping . Merges mapping and Sub.mapping .","title":"substitute()"},{"location":"api/utils/template/#vectorbt.utils.template.Sub.template","text":"Template to be processed.","title":"template"},{"location":"getting-started/contributing/","text":"Contributing \u00b6 Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change. First, you need to install vectorbt from the repository: pip uninstall vectorbt git clone https://github.com/polakowo/vectorbt.git cd vectorbt pip install -e . After making changes, make sure you did not break any functionality: pytest Make sure to update tests as appropriate.","title":"Contributing"},{"location":"getting-started/contributing/#contributing","text":"Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change. First, you need to install vectorbt from the repository: pip uninstall vectorbt git clone https://github.com/polakowo/vectorbt.git cd vectorbt pip install -e . After making changes, make sure you did not break any functionality: pytest Make sure to update tests as appropriate.","title":"Contributing"},{"location":"getting-started/features/","text":"Features \u00b6 Pandas \u00b6 Pandas acceleration : Compiled versions of most popular pandas functions, such as mapping, reducing, rolling, grouping, and resamping. For best performance, most operations are done strictly using NumPy and Numba. Attaches a custom accessor on top of pandas to easily switch between pandas and vectorbt functionality. Compute the rolling z-score >>> import vectorbt as vbt >>> import pandas as pd >>> import numpy as np >>> from numba import njit >>> big_ts = pd . DataFrame ( np . random . uniform ( size = ( 1000 , 1000 ))) # pandas >>> @njit ... def zscore_nb ( x ): ... return ( x [ - 1 ] - np . mean ( x )) / np . std ( x ) >>> % timeit big_ts . rolling ( 2 ) . apply ( zscore_nb , raw = True ) 482 ms \u00b1 393 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) # vectorbt >>> @njit ... def vbt_zscore_nb ( i , col , x ): ... return zscore_nb ( x ) >>> % timeit big_ts . vbt . rolling_apply ( 2 , vbt_zscore_nb ) 33.1 ms \u00b1 1.17 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) Flexible broadcasting : Mechanism for broadcasting array-like objects of arbitrary shapes, including pandas objects with MultiIndex. Broadcast pandas objects properly >>> sr = pd . Series ([ 1 , 2 , 3 ], index = [ 'x' , 'y' , 'z' ]) >>> df = pd . DataFrame ([[ 4 , 5 , 6 ]], index = [ 'x' , 'y' , 'z' ], columns = [ 'a' , 'b' , 'c' ]) # pandas >>> sr + df a b c x y z x NaN NaN NaN NaN NaN NaN y NaN NaN NaN NaN NaN NaN z NaN NaN NaN NaN NaN NaN # vectorbt >>> sr . vbt + df a b c x 5 6 7 y 6 7 8 z 7 8 9 Pandas utilities : Grouping columns, wrapping NumPy arrays, transforming pandas objects and their indexes, and more. Build a symmetric matrix >>> pd . Series ([ 1 , 2 , 3 ]) . vbt . make_symmetric () 0 1 2 0 1.0 2.0 3.0 1 2.0 NaN NaN 2 3.0 NaN NaN Data \u00b6 Data acquisition : Supports various data providers, such as Yahoo Finance , Binance , CCXT and Alpaca . Can merge multiple symbols with different index, as well as update them. Download Alpaca data >>> alpaca_data = vbt . AlpacaData . download ( ... \"AAPL\" , ... start = '2 hours ago UTC' , ... end = '15 minutes ago UTC' , ... interval = '1m' ... ) >>> alpaca_data . get () Open High Low Close Volume timestamp 2021-12-27 14:04:00+00:00 177.0500 177.0500 177.0500 177.0500 1967 2021-12-27 14:05:00+00:00 177.0500 177.0500 177.0300 177.0500 3218 2021-12-27 14:06:00+00:00 177.0400 177.0400 177.0400 177.0400 873 ... ... ... ... ... ... 2021-12-27 15:46:00+00:00 177.9500 178.0000 177.8289 177.8850 162778 2021-12-27 15:47:00+00:00 177.8810 177.9600 177.8400 177.9515 123284 2021-12-27 15:48:00+00:00 177.9600 178.0500 177.9600 178.0100 159700 [105 rows x 5 columns] Data generation : Supports various (random) data generators, such as GBM . Generate random data using Geometric Brownian Motion >>> gbm_data = vbt . GBMData . download ( ... list ( range ( 5 )), ... start = '2020-01-01' , ... end = '2021-01-01' ... ) >>> gbm_data . plot ( showlegend = False ) Scheduled data updates : Can periodically update any previously downloaded data. Append random data every 5 seconds >>> class MyDataUpdater ( vbt . DataUpdater ): ... def update ( self , count_limit = None ): ... prev_index_len = len ( self . data . wrapper . index ) ... super () . update () ... new_index_len = len ( self . data . wrapper . index ) ... print ( f \"Data updated with { new_index_len - prev_index_len } data points\" ) >>> data = vbt . GBMData . download ( 'SYMBOL' , start = '1 minute ago' , freq = '1s' ) >>> my_updater = MyDataUpdater ( data ) >>> my_updater . update_every ( 5 , 'seconds' ) Data updated with 5 data points Data updated with 5 data points ... Data preparation : Transformation, rescaling, and normalization of data. Custom splitters for cross-validation. Supports Scikit-Learn splitters, such as for K-Folds cross-validation. Split time series data >>> from datetime import datetime , timedelta >>> index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 10 )] >>> sr = pd . Series ( np . arange ( len ( index )), index = index ) >>> sr . vbt . rolling_split ( ... window_len = 5 , ... set_lens = ( 1 , 1 ), ... left_to_right = False , ... plot = True , ... trace_names = [ 'train' , 'valid' , 'test' ]) Labeling for ML : Discrete and continuous label generation for effective training of ML models. Identify local extrema >>> price = np . cumprod ( np . random . uniform ( - 0.1 , 0.1 , size = 100 ) + 1 ) >>> vbt . LEXLB . run ( price , 0.2 , 0.2 ) . plot () Indicators \u00b6 Technical indicators : Most popular technical indicators with full Numba support, including Moving Average, Bollinger Bands, RSI, Stochastic, MACD, and more. Out-of-the-box support for 99% indicators in Technical Analysis Library , Pandas TA , and TA-Lib thanks to built-in parsers. Each indicator is wrapped with the vectorbt's indicator engine and thus accepts arbitrary hyperparameter combinations - from arrays to Cartesian products. Compute 2 moving averages at once >>> price = pd . Series ([ 1 , 2 , 3 , 4 , 5 ], dtype = float ) # built-in >>> vbt . MA . run ( price , [ 2 , 3 ]) . ma ma_window 2 3 0 NaN NaN 1 1.5 NaN 2 2.5 2.0 3 3.5 3.0 4 4.5 4.0 # ta support >>> vbt . ta ( 'SMAIndicator' ) . run ( price , [ 2 , 3 ]) . sma_indicator smaindicator_window 2 3 0 NaN NaN 1 1.5 NaN 2 2.5 2.0 3 3.5 3.0 4 4.5 4.0 # pandas-ta support >>> vbt . pandas_ta ( 'SMA' ) . run ( price , [ 2 , 3 ]) . sma sma_length 2 3 0 NaN NaN 1 1.5 NaN 2 2.5 2.0 3 3.5 3.0 4 4.5 4.0 # TA-Lib support >>> vbt . talib ( 'SMA' ) . run ( price , [ 2 , 3 ]) . real sma_timeperiod 2 3 0 NaN NaN 1 1.5 NaN 2 2.5 2.0 3 3.5 3.0 4 4.5 4.0 Indicator factory : Sophisticated factory for building custom technical indicators of any complexity. Takes a function and does all the magic for you: generates an indicator skeleton that takes inputs and parameters of any shape and type, and runs the vectorbt's indicator engine. The easiest and most flexible way to create indicators you will find in open source. Construct a random indicator >>> @njit ... def apply_func_nb ( input_shape , start , mu , sigma ): ... rand_returns = np . random . normal ( mu , sigma , input_shape ) ... return start * vbt . nb . nancumprod_nb ( rand_returns + 1 ) >>> RandomInd = vbt . IndicatorFactory ( ... param_names = [ 'start' , 'mu' , 'sigma' ], ... output_names = [ 'output' ] ... ) . from_apply_func ( ... apply_func_nb , ... require_input_shape = True , ... seed = 42 ... ) >>> RandomInd . run ( 5 , [ 100 , 200 ], [ - 0.01 , 0.01 ], 0.01 ) . output custom_start 100 200 custom_mu -0.01 0.01 custom_sigma 0.01 0.01 0 99.496714 201.531726 1 98.364179 206.729658 2 98.017630 210.383470 3 98.530292 211.499608 4 97.314277 214.762117 Signals \u00b6 Signal analysis : Generation, mapping and reducing, ranking, and distribution analysis of entry and exit signals. Measure each partition of True values >>> mask_sr = pd . Series ([ True , True , True , False , True , True ]) >>> mask_sr . vbt . signals . partition_ranges () . duration . values array([3, 2]) Signal generators : Random and stop loss (SL, TSL, TP, etc.) signal generators with full Numba support. Generate entries and exits using different probabilities >>> rprobnx = vbt . RPROBNX . run ( ... input_shape = ( 5 ,), ... entry_prob = [ 0.5 , 1. ], ... exit_prob = [ 0.5 , 1. ], ... param_product = True , ... seed = 42 ) >>> rprobnx . entries rprobnx_entry_prob 0.5 0.5 1.0 0.5 rprobnx_exit_prob 0.5 1.0 0.5 1.0 0 True True True True 1 False False False False 2 False False False True 3 False False False False 4 False False True True >>> rprobnx . exits rprobnx_entry_prob 0.5 0.5 1.0 1.0 rprobnx_exit_prob 0.5 1.0 0.5 1.0 0 False False False False 1 False True False True 2 False False False False 3 False False True True 4 True False False False Signal factory : Signal factory based on indicator factory specialized for iterative signal generation. Place entries and exits using custom functions >>> @njit ... def entry_choice_func ( from_i , to_i , col ): ... return np . array ([ col ]) >>> @njit ... def exit_choice_func ( from_i , to_i , col ): ... return np . array ([ to_i - 1 ]) >>> MySignals = vbt . SignalFactory () . from_choice_func ( ... entry_choice_func = entry_choice_func , ... exit_choice_func = exit_choice_func , ... entry_kwargs = dict ( wait = 1 ), ... exit_kwargs = dict ( wait = 0 ) ... ) >>> my_sig = MySignals . run ( input_shape = ( 3 , 3 )) >>> my_sig . entries 0 1 2 0 True False False 1 False True False 2 False False True >>> my_sig . exits 0 1 2 0 False False False 1 False False False 2 True True True Modeling \u00b6 Portfolio modeling : The fastest backtesting engine in open source: fills 1,000,000 orders in 70-100ms on Apple M1. Flexible and powerful simulation functions for portfolio modeling, highly optimized for highest performance and lowest memory footprint. Supports two major simulation modes: 1) vectorized backtesting using user-provided arrays, such as orders, signals, and records, and 2) event-driven backtesting using user-defined callbacks. Supports shorting and individual as well as multi-asset mixed portfolios. Combines many features across vectorbt into a single behemoth class. Backtest the Golden Cross >>> price = vbt . YFData . download ( 'BTC-USD' , start = '2018-01-01' ) . get ( 'Close' ) >>> fast_ma = vbt . MA . run ( price , 50 , short_name = 'fast_ma' ) >>> slow_ma = vbt . MA . run ( price , 200 , short_name = 'slow_ma' ) >>> entries = fast_ma . ma_crossed_above ( slow_ma ) >>> exits = fast_ma . ma_crossed_below ( slow_ma ) >>> pf = vbt . Portfolio . from_signals ( price , entries , exits , fees = 0.005 ) >>> pf . orders . records_readable Order Id Column Timestamp Size Price \\\\ 0 0 0 2019-04-24 00:00:00+00:00 0.018208 5464.866699 1 1 0 2019-10-26 00:00:00+00:00 0.018208 9244.972656 2 2 0 2020-02-19 00:00:00+00:00 0.017300 9633.386719 3 3 0 2020-03-25 00:00:00+00:00 0.017300 6681.062988 4 4 0 2020-05-21 00:00:00+00:00 0.012600 9081.761719 5 5 0 2021-06-19 00:00:00+00:00 0.012600 35615.871094 6 6 0 2021-09-15 00:00:00+00:00 0.009222 48176.347656 Fees Side 0 0.497512 Buy 1 0.841647 Sell 2 0.833272 Buy 3 0.577901 Sell 4 0.572151 Buy 5 2.243800 Sell 6 2.221473 Buy >>> fig = price . vbt . plot ( trace_kwargs = dict ( name = 'Close' )) >>> fast_ma . ma . vbt . plot ( trace_kwargs = dict ( name = 'Fast MA' ), fig = fig ) >>> slow_ma . ma . vbt . plot ( trace_kwargs = dict ( name = 'Slow MA' ), fig = fig ) >>> pf . positions . plot ( close_trace_kwargs = dict ( visible = False ), fig = fig ) Analysis \u00b6 Performance metrics : Numba-compiled versions of metrics from empyrical and their rolling versions. Adapter for QuantStats . Visualize performance using QuantStats >>> price = vbt . YFData . download ( 'BTC-USD' ) . get ( 'Close' ) >>> returns = price . vbt . to_returns () >>> returns . vbt . returns . qs . plot_snapshot () Stats builder : Class for building statistics out of custom metrics. Implements a preset of tailored statistics for many backtesting components, such as signals, returns, and portfolio. Analyze the distribution of signals in a mask >>> index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 7 )] >>> mask = pd . Series ([ False , True , True , True , False , True , False ]) >>> mask . vbt . signals ( freq = 'd' ) . stats () Start 0 End 6 Period 7 days 00:00:00 Total 4 Rate [%] 57.142857 First Index 1 Last Index 5 Norm Avg Index [-1, 1] -0.083333 Distance: Min 1 days 00:00:00 Distance: Max 2 days 00:00:00 Distance: Mean 1 days 08:00:00 Distance: Std 0 days 13:51:23.063257983 Total Partitions 2 Partition Rate [%] 50.0 Partition Length: Min 1 days 00:00:00 Partition Length: Max 3 days 00:00:00 Partition Length: Mean 2 days 00:00:00 Partition Length: Std 1 days 09:56:28.051789035 Partition Distance: Min 2 days 00:00:00 Partition Distance: Max 2 days 00:00:00 Partition Distance: Mean 2 days 00:00:00 Partition Distance: Std NaT dtype: object Records and mapped arrays : In-house data structures for analyzing complex data, such as simulation logs. Fully compiled with Numba. Parse 5 highest slippage values from logs >>> price = vbt . YFData . download ( 'BTC-USD' ) . get ( 'Close' ) >>> slippage = np . random . uniform ( 0 , 0.005 , size = close . shape [ 0 ]) >>> logs = vbt . Portfolio . from_random_signals ( price , n = 5 , slippage = slippage , log = True ) . logs >>> req_price_ma = logs . map_field ( 'req_price' ) >>> res_price_ma = logs . map_field ( 'res_price' ) >>> slippage_ma = ( res_price_ma - req_price_ma ) / req_price_ma >>> slippage_ma = slippage_ma . replace ( arr = np . abs ( slippage_ma . values )) >>> top_slippage_pd = slippage_ma . top_n ( 5 ) . to_pd () >>> top_slippage_pd [ ~ top_slippage_pd . isnull ()] Date 2017-12-25 00:00:00+00:00 0.001534 2018-06-03 00:00:00+00:00 0.004354 2018-12-03 00:00:00+00:00 0.004663 2019-09-20 00:00:00+00:00 0.004217 2020-11-28 00:00:00+00:00 0.000775 dtype: float64 Trade analysis : Retrospective analysis of trades from various view points. Supports entry trades, exit trades, and positions. Get the projected return of each buy order >>> price = vbt . YFData . download ( 'BTC-USD' ) . get ( 'Close' ) >>> entry_trades = vbt . Portfolio . from_random_signals ( price , n = 5 ) . entry_trades >>> returns_pd = entry_trades . returns . to_pd () >>> returns_pd [ ~ returns_pd . isnull ()] Date 2017-11-12 00:00:00+00:00 0.742975 2019-08-30 00:00:00+00:00 -0.081744 2020-04-21 00:00:00+00:00 0.489072 2020-09-13 00:00:00+00:00 0.262251 2021-03-07 00:00:00+00:00 -0.382155 dtype: float64 Drawdown analysis : Drawdown statistics of any numeric time series. Plot 3 deepest price dips >>> price = vbt . YFData . download ( 'BTC-USD' ) . get ( 'Close' ) >>> price . vbt . drawdowns . plot ( top_n = 3 ) Plotting \u00b6 Data visualization : Numerous flexible data plotting functions distributed across vectorbt. Plot time series against each other >>> sr1 = pd . Series ( np . cumprod ( np . random . normal ( 0 , 0.01 , 100 ) + 1 )) >>> sr2 = pd . Series ( np . cumprod ( np . random . normal ( 0 , 0.01 , 100 ) + 1 )) >>> sr1 . vbt . plot_against ( sr2 ) Figures and widgets : Custom interactive figures and widgets using Plotly , such as Heatmap and Volume. All custom widgets have dedicated methods for efficiently updating their state. Plot a volume >>> volume_widget = vbt . plotting . Volume ( ... data = np . random . randint ( 1 , 10 , size = ( 3 , 3 , 3 )), ... x_labels = [ 'a' , 'b' , 'c' ], ... y_labels = [ 'd' , 'e' , 'f' ], ... z_labels = [ 'g' , 'h' , 'i' ] ... ) >>> volume_widget . fig Plots builder : Class for building plots out of custom subplots. Implements a preset of tailored subplots for many backtesting components, such as signals, returns, and portfolio. Plot various portfolio balances >>> price = vbt . YFData . download ( 'BTC-USD' ) . get ( 'Close' ) >>> pf = vbt . Portfolio . from_random_signals ( price , n = 5 ) >>> pf . plot ( subplots = [ 'cash' , 'assets' , 'value' ]) . show_svg () Extra \u00b6 Notifications : Telegram bot based on Python Telegram Bot . Launch a bot that returns the latest ticker on Binance >>> from telegram.ext import CommandHandler >>> import ccxt >>> class BinanceTickerBot ( vbt . TelegramBot ): ... @property ... def custom_handlers ( self ): ... return CommandHandler ( 'get' , self . get ), ... ... @property ... def help_message ( self ): ... return \"Type /get [symbol] to get the latest ticker on Binance.\" ... ... def get ( self , update , context ): ... chat_id = update . effective_chat . id ... try : ... ticker = ccxt . binance () . fetchTicker ( context . args [ 0 ]) ... except Exception as e : ... self . send_message ( chat_id , str ( e )) ... return ... self . send_message ( chat_id , str ( ticker [ 'last' ])) >>> bot = BinanceTickerBot ( token = 'YOUR_TOKEN' ) >>> bot . start () General utilities : Scheduling using schedule , templates, decorators, configs, and more. Every 10 seconds display the latest Bitcoin trades on Binance >>> from vectorbt.utils.datetime_ import datetime_to_ms , to_tzaware_datetime , get_utc_tz >>> from IPython.display import SVG , display , clear_output >>> exchange = ccxt . binance () >>> def job_func (): ... since = datetime_to_ms ( to_tzaware_datetime ( '10 seconds ago UTC' , tz = get_utc_tz ())) ... trades = exchange . fetch_trades ( 'BTC/USDT' , since ) ... price = pd . Series ({ t [ 'datetime' ]: t [ 'price' ] for t in trades }) ... svg = price . vbt . plot () . to_image ( format = \"svg\" ) ... clear_output () ... display ( SVG ( svg )) >>> scheduler = vbt . ScheduleManager () >>> scheduler . every ( 10 , 'seconds' ) . do ( job_func ) >>> scheduler . start () Caching : Property and method decorators for caching most frequently used objects. Create a cached method and disable it globally >>> import time >>> start = time . time () >>> class MyClass : ... @vbt . cached_method ... def get_elapsed ( self ): ... return time . time () - start >>> my_inst = MyClass () >>> my_inst . get_elapsed () 0.00010895729064941406 >>> my_inst . get_elapsed () 0.00010895729064941406 >>> get_elapsed_cond = vbt . CacheCondition ( instance = my_inst , func = 'get_elapsed' ) >>> vbt . settings . caching [ 'blacklist' ] . append ( get_elapsed_cond ) >>> my_inst . get_elapsed () 0.01081395149230957 Persistance : Most Python objects including data and portfolio can be saved to a file and retrieved back using Dill . Simulate, save, and load back a portfolio >>> price = vbt . YFData . download ( 'BTC-USD' ) . get ( 'Close' ) >>> pf = vbt . Portfolio . from_random_signals ( price , n = 5 ) >>> pf . save ( 'my_pf.pkl' ) >>> pf = vbt . Portfolio . load ( 'my_pf.pkl' ) >>> pf . total_return () 5.96813681074424 Want more? \u00b6 Discover the features offered by vectorbt PRO !","title":"Features"},{"location":"getting-started/features/#features","text":"","title":"Features"},{"location":"getting-started/features/#pandas","text":"Pandas acceleration : Compiled versions of most popular pandas functions, such as mapping, reducing, rolling, grouping, and resamping. For best performance, most operations are done strictly using NumPy and Numba. Attaches a custom accessor on top of pandas to easily switch between pandas and vectorbt functionality. Compute the rolling z-score >>> import vectorbt as vbt >>> import pandas as pd >>> import numpy as np >>> from numba import njit >>> big_ts = pd . DataFrame ( np . random . uniform ( size = ( 1000 , 1000 ))) # pandas >>> @njit ... def zscore_nb ( x ): ... return ( x [ - 1 ] - np . mean ( x )) / np . std ( x ) >>> % timeit big_ts . rolling ( 2 ) . apply ( zscore_nb , raw = True ) 482 ms \u00b1 393 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) # vectorbt >>> @njit ... def vbt_zscore_nb ( i , col , x ): ... return zscore_nb ( x ) >>> % timeit big_ts . vbt . rolling_apply ( 2 , vbt_zscore_nb ) 33.1 ms \u00b1 1.17 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) Flexible broadcasting : Mechanism for broadcasting array-like objects of arbitrary shapes, including pandas objects with MultiIndex. Broadcast pandas objects properly >>> sr = pd . Series ([ 1 , 2 , 3 ], index = [ 'x' , 'y' , 'z' ]) >>> df = pd . DataFrame ([[ 4 , 5 , 6 ]], index = [ 'x' , 'y' , 'z' ], columns = [ 'a' , 'b' , 'c' ]) # pandas >>> sr + df a b c x y z x NaN NaN NaN NaN NaN NaN y NaN NaN NaN NaN NaN NaN z NaN NaN NaN NaN NaN NaN # vectorbt >>> sr . vbt + df a b c x 5 6 7 y 6 7 8 z 7 8 9 Pandas utilities : Grouping columns, wrapping NumPy arrays, transforming pandas objects and their indexes, and more. Build a symmetric matrix >>> pd . Series ([ 1 , 2 , 3 ]) . vbt . make_symmetric () 0 1 2 0 1.0 2.0 3.0 1 2.0 NaN NaN 2 3.0 NaN NaN","title":"Pandas"},{"location":"getting-started/features/#data","text":"Data acquisition : Supports various data providers, such as Yahoo Finance , Binance , CCXT and Alpaca . Can merge multiple symbols with different index, as well as update them. Download Alpaca data >>> alpaca_data = vbt . AlpacaData . download ( ... \"AAPL\" , ... start = '2 hours ago UTC' , ... end = '15 minutes ago UTC' , ... interval = '1m' ... ) >>> alpaca_data . get () Open High Low Close Volume timestamp 2021-12-27 14:04:00+00:00 177.0500 177.0500 177.0500 177.0500 1967 2021-12-27 14:05:00+00:00 177.0500 177.0500 177.0300 177.0500 3218 2021-12-27 14:06:00+00:00 177.0400 177.0400 177.0400 177.0400 873 ... ... ... ... ... ... 2021-12-27 15:46:00+00:00 177.9500 178.0000 177.8289 177.8850 162778 2021-12-27 15:47:00+00:00 177.8810 177.9600 177.8400 177.9515 123284 2021-12-27 15:48:00+00:00 177.9600 178.0500 177.9600 178.0100 159700 [105 rows x 5 columns] Data generation : Supports various (random) data generators, such as GBM . Generate random data using Geometric Brownian Motion >>> gbm_data = vbt . GBMData . download ( ... list ( range ( 5 )), ... start = '2020-01-01' , ... end = '2021-01-01' ... ) >>> gbm_data . plot ( showlegend = False ) Scheduled data updates : Can periodically update any previously downloaded data. Append random data every 5 seconds >>> class MyDataUpdater ( vbt . DataUpdater ): ... def update ( self , count_limit = None ): ... prev_index_len = len ( self . data . wrapper . index ) ... super () . update () ... new_index_len = len ( self . data . wrapper . index ) ... print ( f \"Data updated with { new_index_len - prev_index_len } data points\" ) >>> data = vbt . GBMData . download ( 'SYMBOL' , start = '1 minute ago' , freq = '1s' ) >>> my_updater = MyDataUpdater ( data ) >>> my_updater . update_every ( 5 , 'seconds' ) Data updated with 5 data points Data updated with 5 data points ... Data preparation : Transformation, rescaling, and normalization of data. Custom splitters for cross-validation. Supports Scikit-Learn splitters, such as for K-Folds cross-validation. Split time series data >>> from datetime import datetime , timedelta >>> index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 10 )] >>> sr = pd . Series ( np . arange ( len ( index )), index = index ) >>> sr . vbt . rolling_split ( ... window_len = 5 , ... set_lens = ( 1 , 1 ), ... left_to_right = False , ... plot = True , ... trace_names = [ 'train' , 'valid' , 'test' ]) Labeling for ML : Discrete and continuous label generation for effective training of ML models. Identify local extrema >>> price = np . cumprod ( np . random . uniform ( - 0.1 , 0.1 , size = 100 ) + 1 ) >>> vbt . LEXLB . run ( price , 0.2 , 0.2 ) . plot ()","title":"Data"},{"location":"getting-started/features/#indicators","text":"Technical indicators : Most popular technical indicators with full Numba support, including Moving Average, Bollinger Bands, RSI, Stochastic, MACD, and more. Out-of-the-box support for 99% indicators in Technical Analysis Library , Pandas TA , and TA-Lib thanks to built-in parsers. Each indicator is wrapped with the vectorbt's indicator engine and thus accepts arbitrary hyperparameter combinations - from arrays to Cartesian products. Compute 2 moving averages at once >>> price = pd . Series ([ 1 , 2 , 3 , 4 , 5 ], dtype = float ) # built-in >>> vbt . MA . run ( price , [ 2 , 3 ]) . ma ma_window 2 3 0 NaN NaN 1 1.5 NaN 2 2.5 2.0 3 3.5 3.0 4 4.5 4.0 # ta support >>> vbt . ta ( 'SMAIndicator' ) . run ( price , [ 2 , 3 ]) . sma_indicator smaindicator_window 2 3 0 NaN NaN 1 1.5 NaN 2 2.5 2.0 3 3.5 3.0 4 4.5 4.0 # pandas-ta support >>> vbt . pandas_ta ( 'SMA' ) . run ( price , [ 2 , 3 ]) . sma sma_length 2 3 0 NaN NaN 1 1.5 NaN 2 2.5 2.0 3 3.5 3.0 4 4.5 4.0 # TA-Lib support >>> vbt . talib ( 'SMA' ) . run ( price , [ 2 , 3 ]) . real sma_timeperiod 2 3 0 NaN NaN 1 1.5 NaN 2 2.5 2.0 3 3.5 3.0 4 4.5 4.0 Indicator factory : Sophisticated factory for building custom technical indicators of any complexity. Takes a function and does all the magic for you: generates an indicator skeleton that takes inputs and parameters of any shape and type, and runs the vectorbt's indicator engine. The easiest and most flexible way to create indicators you will find in open source. Construct a random indicator >>> @njit ... def apply_func_nb ( input_shape , start , mu , sigma ): ... rand_returns = np . random . normal ( mu , sigma , input_shape ) ... return start * vbt . nb . nancumprod_nb ( rand_returns + 1 ) >>> RandomInd = vbt . IndicatorFactory ( ... param_names = [ 'start' , 'mu' , 'sigma' ], ... output_names = [ 'output' ] ... ) . from_apply_func ( ... apply_func_nb , ... require_input_shape = True , ... seed = 42 ... ) >>> RandomInd . run ( 5 , [ 100 , 200 ], [ - 0.01 , 0.01 ], 0.01 ) . output custom_start 100 200 custom_mu -0.01 0.01 custom_sigma 0.01 0.01 0 99.496714 201.531726 1 98.364179 206.729658 2 98.017630 210.383470 3 98.530292 211.499608 4 97.314277 214.762117","title":"Indicators"},{"location":"getting-started/features/#signals","text":"Signal analysis : Generation, mapping and reducing, ranking, and distribution analysis of entry and exit signals. Measure each partition of True values >>> mask_sr = pd . Series ([ True , True , True , False , True , True ]) >>> mask_sr . vbt . signals . partition_ranges () . duration . values array([3, 2]) Signal generators : Random and stop loss (SL, TSL, TP, etc.) signal generators with full Numba support. Generate entries and exits using different probabilities >>> rprobnx = vbt . RPROBNX . run ( ... input_shape = ( 5 ,), ... entry_prob = [ 0.5 , 1. ], ... exit_prob = [ 0.5 , 1. ], ... param_product = True , ... seed = 42 ) >>> rprobnx . entries rprobnx_entry_prob 0.5 0.5 1.0 0.5 rprobnx_exit_prob 0.5 1.0 0.5 1.0 0 True True True True 1 False False False False 2 False False False True 3 False False False False 4 False False True True >>> rprobnx . exits rprobnx_entry_prob 0.5 0.5 1.0 1.0 rprobnx_exit_prob 0.5 1.0 0.5 1.0 0 False False False False 1 False True False True 2 False False False False 3 False False True True 4 True False False False Signal factory : Signal factory based on indicator factory specialized for iterative signal generation. Place entries and exits using custom functions >>> @njit ... def entry_choice_func ( from_i , to_i , col ): ... return np . array ([ col ]) >>> @njit ... def exit_choice_func ( from_i , to_i , col ): ... return np . array ([ to_i - 1 ]) >>> MySignals = vbt . SignalFactory () . from_choice_func ( ... entry_choice_func = entry_choice_func , ... exit_choice_func = exit_choice_func , ... entry_kwargs = dict ( wait = 1 ), ... exit_kwargs = dict ( wait = 0 ) ... ) >>> my_sig = MySignals . run ( input_shape = ( 3 , 3 )) >>> my_sig . entries 0 1 2 0 True False False 1 False True False 2 False False True >>> my_sig . exits 0 1 2 0 False False False 1 False False False 2 True True True","title":"Signals"},{"location":"getting-started/features/#modeling","text":"Portfolio modeling : The fastest backtesting engine in open source: fills 1,000,000 orders in 70-100ms on Apple M1. Flexible and powerful simulation functions for portfolio modeling, highly optimized for highest performance and lowest memory footprint. Supports two major simulation modes: 1) vectorized backtesting using user-provided arrays, such as orders, signals, and records, and 2) event-driven backtesting using user-defined callbacks. Supports shorting and individual as well as multi-asset mixed portfolios. Combines many features across vectorbt into a single behemoth class. Backtest the Golden Cross >>> price = vbt . YFData . download ( 'BTC-USD' , start = '2018-01-01' ) . get ( 'Close' ) >>> fast_ma = vbt . MA . run ( price , 50 , short_name = 'fast_ma' ) >>> slow_ma = vbt . MA . run ( price , 200 , short_name = 'slow_ma' ) >>> entries = fast_ma . ma_crossed_above ( slow_ma ) >>> exits = fast_ma . ma_crossed_below ( slow_ma ) >>> pf = vbt . Portfolio . from_signals ( price , entries , exits , fees = 0.005 ) >>> pf . orders . records_readable Order Id Column Timestamp Size Price \\\\ 0 0 0 2019-04-24 00:00:00+00:00 0.018208 5464.866699 1 1 0 2019-10-26 00:00:00+00:00 0.018208 9244.972656 2 2 0 2020-02-19 00:00:00+00:00 0.017300 9633.386719 3 3 0 2020-03-25 00:00:00+00:00 0.017300 6681.062988 4 4 0 2020-05-21 00:00:00+00:00 0.012600 9081.761719 5 5 0 2021-06-19 00:00:00+00:00 0.012600 35615.871094 6 6 0 2021-09-15 00:00:00+00:00 0.009222 48176.347656 Fees Side 0 0.497512 Buy 1 0.841647 Sell 2 0.833272 Buy 3 0.577901 Sell 4 0.572151 Buy 5 2.243800 Sell 6 2.221473 Buy >>> fig = price . vbt . plot ( trace_kwargs = dict ( name = 'Close' )) >>> fast_ma . ma . vbt . plot ( trace_kwargs = dict ( name = 'Fast MA' ), fig = fig ) >>> slow_ma . ma . vbt . plot ( trace_kwargs = dict ( name = 'Slow MA' ), fig = fig ) >>> pf . positions . plot ( close_trace_kwargs = dict ( visible = False ), fig = fig )","title":"Modeling"},{"location":"getting-started/features/#analysis","text":"Performance metrics : Numba-compiled versions of metrics from empyrical and their rolling versions. Adapter for QuantStats . Visualize performance using QuantStats >>> price = vbt . YFData . download ( 'BTC-USD' ) . get ( 'Close' ) >>> returns = price . vbt . to_returns () >>> returns . vbt . returns . qs . plot_snapshot () Stats builder : Class for building statistics out of custom metrics. Implements a preset of tailored statistics for many backtesting components, such as signals, returns, and portfolio. Analyze the distribution of signals in a mask >>> index = [ datetime ( 2020 , 1 , 1 ) + timedelta ( days = i ) for i in range ( 7 )] >>> mask = pd . Series ([ False , True , True , True , False , True , False ]) >>> mask . vbt . signals ( freq = 'd' ) . stats () Start 0 End 6 Period 7 days 00:00:00 Total 4 Rate [%] 57.142857 First Index 1 Last Index 5 Norm Avg Index [-1, 1] -0.083333 Distance: Min 1 days 00:00:00 Distance: Max 2 days 00:00:00 Distance: Mean 1 days 08:00:00 Distance: Std 0 days 13:51:23.063257983 Total Partitions 2 Partition Rate [%] 50.0 Partition Length: Min 1 days 00:00:00 Partition Length: Max 3 days 00:00:00 Partition Length: Mean 2 days 00:00:00 Partition Length: Std 1 days 09:56:28.051789035 Partition Distance: Min 2 days 00:00:00 Partition Distance: Max 2 days 00:00:00 Partition Distance: Mean 2 days 00:00:00 Partition Distance: Std NaT dtype: object Records and mapped arrays : In-house data structures for analyzing complex data, such as simulation logs. Fully compiled with Numba. Parse 5 highest slippage values from logs >>> price = vbt . YFData . download ( 'BTC-USD' ) . get ( 'Close' ) >>> slippage = np . random . uniform ( 0 , 0.005 , size = close . shape [ 0 ]) >>> logs = vbt . Portfolio . from_random_signals ( price , n = 5 , slippage = slippage , log = True ) . logs >>> req_price_ma = logs . map_field ( 'req_price' ) >>> res_price_ma = logs . map_field ( 'res_price' ) >>> slippage_ma = ( res_price_ma - req_price_ma ) / req_price_ma >>> slippage_ma = slippage_ma . replace ( arr = np . abs ( slippage_ma . values )) >>> top_slippage_pd = slippage_ma . top_n ( 5 ) . to_pd () >>> top_slippage_pd [ ~ top_slippage_pd . isnull ()] Date 2017-12-25 00:00:00+00:00 0.001534 2018-06-03 00:00:00+00:00 0.004354 2018-12-03 00:00:00+00:00 0.004663 2019-09-20 00:00:00+00:00 0.004217 2020-11-28 00:00:00+00:00 0.000775 dtype: float64 Trade analysis : Retrospective analysis of trades from various view points. Supports entry trades, exit trades, and positions. Get the projected return of each buy order >>> price = vbt . YFData . download ( 'BTC-USD' ) . get ( 'Close' ) >>> entry_trades = vbt . Portfolio . from_random_signals ( price , n = 5 ) . entry_trades >>> returns_pd = entry_trades . returns . to_pd () >>> returns_pd [ ~ returns_pd . isnull ()] Date 2017-11-12 00:00:00+00:00 0.742975 2019-08-30 00:00:00+00:00 -0.081744 2020-04-21 00:00:00+00:00 0.489072 2020-09-13 00:00:00+00:00 0.262251 2021-03-07 00:00:00+00:00 -0.382155 dtype: float64 Drawdown analysis : Drawdown statistics of any numeric time series. Plot 3 deepest price dips >>> price = vbt . YFData . download ( 'BTC-USD' ) . get ( 'Close' ) >>> price . vbt . drawdowns . plot ( top_n = 3 )","title":"Analysis"},{"location":"getting-started/features/#plotting","text":"Data visualization : Numerous flexible data plotting functions distributed across vectorbt. Plot time series against each other >>> sr1 = pd . Series ( np . cumprod ( np . random . normal ( 0 , 0.01 , 100 ) + 1 )) >>> sr2 = pd . Series ( np . cumprod ( np . random . normal ( 0 , 0.01 , 100 ) + 1 )) >>> sr1 . vbt . plot_against ( sr2 ) Figures and widgets : Custom interactive figures and widgets using Plotly , such as Heatmap and Volume. All custom widgets have dedicated methods for efficiently updating their state. Plot a volume >>> volume_widget = vbt . plotting . Volume ( ... data = np . random . randint ( 1 , 10 , size = ( 3 , 3 , 3 )), ... x_labels = [ 'a' , 'b' , 'c' ], ... y_labels = [ 'd' , 'e' , 'f' ], ... z_labels = [ 'g' , 'h' , 'i' ] ... ) >>> volume_widget . fig Plots builder : Class for building plots out of custom subplots. Implements a preset of tailored subplots for many backtesting components, such as signals, returns, and portfolio. Plot various portfolio balances >>> price = vbt . YFData . download ( 'BTC-USD' ) . get ( 'Close' ) >>> pf = vbt . Portfolio . from_random_signals ( price , n = 5 ) >>> pf . plot ( subplots = [ 'cash' , 'assets' , 'value' ]) . show_svg ()","title":"Plotting"},{"location":"getting-started/features/#extra","text":"Notifications : Telegram bot based on Python Telegram Bot . Launch a bot that returns the latest ticker on Binance >>> from telegram.ext import CommandHandler >>> import ccxt >>> class BinanceTickerBot ( vbt . TelegramBot ): ... @property ... def custom_handlers ( self ): ... return CommandHandler ( 'get' , self . get ), ... ... @property ... def help_message ( self ): ... return \"Type /get [symbol] to get the latest ticker on Binance.\" ... ... def get ( self , update , context ): ... chat_id = update . effective_chat . id ... try : ... ticker = ccxt . binance () . fetchTicker ( context . args [ 0 ]) ... except Exception as e : ... self . send_message ( chat_id , str ( e )) ... return ... self . send_message ( chat_id , str ( ticker [ 'last' ])) >>> bot = BinanceTickerBot ( token = 'YOUR_TOKEN' ) >>> bot . start () General utilities : Scheduling using schedule , templates, decorators, configs, and more. Every 10 seconds display the latest Bitcoin trades on Binance >>> from vectorbt.utils.datetime_ import datetime_to_ms , to_tzaware_datetime , get_utc_tz >>> from IPython.display import SVG , display , clear_output >>> exchange = ccxt . binance () >>> def job_func (): ... since = datetime_to_ms ( to_tzaware_datetime ( '10 seconds ago UTC' , tz = get_utc_tz ())) ... trades = exchange . fetch_trades ( 'BTC/USDT' , since ) ... price = pd . Series ({ t [ 'datetime' ]: t [ 'price' ] for t in trades }) ... svg = price . vbt . plot () . to_image ( format = \"svg\" ) ... clear_output () ... display ( SVG ( svg )) >>> scheduler = vbt . ScheduleManager () >>> scheduler . every ( 10 , 'seconds' ) . do ( job_func ) >>> scheduler . start () Caching : Property and method decorators for caching most frequently used objects. Create a cached method and disable it globally >>> import time >>> start = time . time () >>> class MyClass : ... @vbt . cached_method ... def get_elapsed ( self ): ... return time . time () - start >>> my_inst = MyClass () >>> my_inst . get_elapsed () 0.00010895729064941406 >>> my_inst . get_elapsed () 0.00010895729064941406 >>> get_elapsed_cond = vbt . CacheCondition ( instance = my_inst , func = 'get_elapsed' ) >>> vbt . settings . caching [ 'blacklist' ] . append ( get_elapsed_cond ) >>> my_inst . get_elapsed () 0.01081395149230957 Persistance : Most Python objects including data and portfolio can be saved to a file and retrieved back using Dill . Simulate, save, and load back a portfolio >>> price = vbt . YFData . download ( 'BTC-USD' ) . get ( 'Close' ) >>> pf = vbt . Portfolio . from_random_signals ( price , n = 5 ) >>> pf . save ( 'my_pf.pkl' ) >>> pf = vbt . Portfolio . load ( 'my_pf.pkl' ) >>> pf . total_return () 5.96813681074424","title":"Extra"},{"location":"getting-started/features/#want-more","text":"Discover the features offered by vectorbt PRO !","title":"Want more?"},{"location":"getting-started/installation/","text":"Installation \u00b6 You can install vectorbt with pip, the Python package manager, or with Docker. With pip \u00b6 pip install -U vectorbt To also install optional dependencies: pip install -U \"vectorbt[full]\" With Docker \u00b6 You can pull the most recent Docker image if you have Docker installed . docker run --rm -p 8888 :8888 -v \" $PWD \" :/home/jovyan/work polakowo/vectorbt This command pulls the latest polakowo/vectorbt image from Docker Hub. It then starts a container running a Jupyter Notebook server and exposes the server on host port 8888. Visiting http://127.0.0.1:8888/?token=<token> in a browser loads JupyterLab, where token is the secret token printed in the console. Docker destroys the container after notebook server exit, but any files written to the working directory in the container remain intact in the working directory on the host. See Jupyter Docker Stacks - Quick Start . There are two types of images: polakowo/vectorbt : vanilla version (default) polakowo/vectorbt-full : full version (with optional dependencies) Each Docker image is based on jupyter/scipy-notebook and comes with Jupyter environment, vectorbt, and other scientific packages installed. With git \u00b6 Of course, you can pull vectorbt directly from git : git clone git@github.com:polakowo/vectorbt.git vectorbt Install the package: pip install -e vectorbt Troubleshooting \u00b6 TA-Lib Jupyter Notebook and JupyterLab Apple M1","title":"Installation"},{"location":"getting-started/installation/#installation","text":"You can install vectorbt with pip, the Python package manager, or with Docker.","title":"Installation"},{"location":"getting-started/installation/#with-pip","text":"pip install -U vectorbt To also install optional dependencies: pip install -U \"vectorbt[full]\"","title":"With pip"},{"location":"getting-started/installation/#with-docker","text":"You can pull the most recent Docker image if you have Docker installed . docker run --rm -p 8888 :8888 -v \" $PWD \" :/home/jovyan/work polakowo/vectorbt This command pulls the latest polakowo/vectorbt image from Docker Hub. It then starts a container running a Jupyter Notebook server and exposes the server on host port 8888. Visiting http://127.0.0.1:8888/?token=<token> in a browser loads JupyterLab, where token is the secret token printed in the console. Docker destroys the container after notebook server exit, but any files written to the working directory in the container remain intact in the working directory on the host. See Jupyter Docker Stacks - Quick Start . There are two types of images: polakowo/vectorbt : vanilla version (default) polakowo/vectorbt-full : full version (with optional dependencies) Each Docker image is based on jupyter/scipy-notebook and comes with Jupyter environment, vectorbt, and other scientific packages installed.","title":"With Docker"},{"location":"getting-started/installation/#with-git","text":"Of course, you can pull vectorbt directly from git : git clone git@github.com:polakowo/vectorbt.git vectorbt Install the package: pip install -e vectorbt","title":"With git"},{"location":"getting-started/installation/#troubleshooting","text":"TA-Lib Jupyter Notebook and JupyterLab Apple M1","title":"Troubleshooting"},{"location":"getting-started/resources/","text":"Resources \u00b6 Here's a collection of resources to get started. Notebooks \u00b6 Performance analysis of Moving Average Crossover Performance analysis of stop signals Backtesting per trading session Portfolio optimization Plotting MACD parameters as 3D volume Walk-forward optimization Running Telegram signal bot Porting RSI strategy from backtrader Pairs trading (vs backtrader) Note: you must run the notebook to play with the widgets. Dashboards \u00b6 Detecting and backtesting common candlestick patterns Articles \u00b6 Stop Loss, Trailing Stop, or Take Profit? 2 Million Backtests Shed Light Getting Help \u00b6 If you need supervision or any help with your implementation, join a private chat For questions on Numba and other parts, the best place to go to is StackOverflow If you have general questions, start a new GitHub Discussion Alternatively, you can ask on Gitter If you found what appears to be a bug, please create a new issue For other inquiries, please contact the author","title":"Resources"},{"location":"getting-started/resources/#resources","text":"Here's a collection of resources to get started.","title":"Resources"},{"location":"getting-started/resources/#notebooks","text":"Performance analysis of Moving Average Crossover Performance analysis of stop signals Backtesting per trading session Portfolio optimization Plotting MACD parameters as 3D volume Walk-forward optimization Running Telegram signal bot Porting RSI strategy from backtrader Pairs trading (vs backtrader) Note: you must run the notebook to play with the widgets.","title":"Notebooks"},{"location":"getting-started/resources/#dashboards","text":"Detecting and backtesting common candlestick patterns","title":"Dashboards"},{"location":"getting-started/resources/#articles","text":"Stop Loss, Trailing Stop, or Take Profit? 2 Million Backtests Shed Light","title":"Articles"},{"location":"getting-started/resources/#getting-help","text":"If you need supervision or any help with your implementation, join a private chat For questions on Numba and other parts, the best place to go to is StackOverflow If you have general questions, start a new GitHub Discussion Alternatively, you can ask on Gitter If you found what appears to be a bug, please create a new issue For other inquiries, please contact the author","title":"Getting Help"},{"location":"getting-started/usage/","text":"Usage \u00b6 vectorbt allows you to easily backtest strategies with a couple of lines of Python code. Here is how much profit we would have made if we invested $100 into Bitcoin in 2014: import vectorbt as vbt price = vbt . YFData . download ( 'BTC-USD' ) . get ( 'Close' ) pf = vbt . Portfolio . from_holding ( price , init_cash = 100 ) pf . total_profit () 8961.008555963961 Buy whenever 10-day SMA crosses above 50-day SMA and sell when opposite: fast_ma = vbt . MA . run ( price , 10 ) slow_ma = vbt . MA . run ( price , 50 ) entries = fast_ma . ma_crossed_above ( slow_ma ) exits = fast_ma . ma_crossed_below ( slow_ma ) pf = vbt . Portfolio . from_signals ( price , entries , exits , init_cash = 100 ) pf . total_profit () 16423.251963801864 Generate 1,000 strategies with random signals and test them on BTC and ETH: import numpy as np symbols = [ \"BTC-USD\" , \"ETH-USD\" ] price = vbt . YFData . download ( symbols , missing_index = 'drop' ) . get ( 'Close' ) n = np . random . randint ( 10 , 101 , size = 1000 ) . tolist () pf = vbt . Portfolio . from_random_signals ( price , n = n , init_cash = 100 , seed = 42 ) mean_expectancy = pf . trades . expectancy () . groupby ([ 'randnx_n' , 'symbol' ]) . mean () fig = mean_expectancy . unstack () . vbt . scatterplot ( xaxis_title = 'randnx_n' , yaxis_title = 'mean_expectancy' ) fig . show () For fans of hyperparameter optimization: here is a snippet for testing 10,000 window combinations of a dual SMA crossover strategy on BTC, USD, and LTC: symbols = [ \"BTC-USD\" , \"ETH-USD\" , \"LTC-USD\" ] price = vbt . YFData . download ( symbols , missing_index = 'drop' ) . get ( 'Close' ) windows = np . arange ( 2 , 101 ) fast_ma , slow_ma = vbt . MA . run_combs ( price , window = windows , r = 2 , short_names = [ 'fast' , 'slow' ]) entries = fast_ma . ma_crossed_above ( slow_ma ) exits = fast_ma . ma_crossed_below ( slow_ma ) pf_kwargs = dict ( size = np . inf , fees = 0.001 , freq = '1D' ) pf = vbt . Portfolio . from_signals ( price , entries , exits , ** pf_kwargs ) fig = pf . total_return () . vbt . heatmap ( x_level = 'fast_window' , y_level = 'slow_window' , slider_level = 'symbol' , symmetric = True , trace_kwargs = dict ( colorbar = dict ( title = 'Total return' , tickformat = '%' ))) fig . show () Digging into each strategy configuration is as simple as indexing with pandas: pf [( 10 , 20 , 'ETH-USD' )] . stats () Start 2015-08-07 00:00:00+00:00 End 2021-08-01 00:00:00+00:00 Period 2183 days 00:00:00 Start Value 100.0 End Value 620402.791485 Total Return [%] 620302.791485 Benchmark Return [%] 92987.961948 Max Gross Exposure [%] 100.0 Total Fees Paid 10991.676981 Max Drawdown [%] 70.734951 Max Drawdown Duration 760 days 00:00:00 Total Trades 54 Total Closed Trades 53 Total Open Trades 1 Open Trade PnL 67287.940601 Win Rate [%] 52.830189 Best Trade [%] 1075.803607 Worst Trade [%] -29.593414 Avg Winning Trade [%] 95.695343 Avg Losing Trade [%] -11.890246 Avg Winning Trade Duration 35 days 23:08:34.285714286 Avg Losing Trade Duration 8 days 00:00:00 Profit Factor 2.651143 Expectancy 10434.24247 Sharpe Ratio 2.041211 Calmar Ratio 4.6747 Omega Ratio 1.547013 Sortino Ratio 3.519894 Name: (10, 20, ETH-USD), dtype: object The same for plotting: pf [( 10 , 20 , 'ETH-USD' )] . plot () . show () It's not all about backtesting - vectorbt can be used to facilitate financial data analysis and visualization. Let's generate a GIF that animates the %B and bandwidth of Bollinger Bands for different symbols: symbols = [ \"BTC-USD\" , \"ETH-USD\" , \"ADA-USD\" ] price = vbt . YFData . download ( symbols , period = '6mo' , missing_index = 'drop' ) . get ( 'Close' ) bbands = vbt . BBANDS . run ( price ) def plot ( index , bbands ): bbands = bbands . loc [ index ] fig = vbt . make_subplots ( rows = 2 , cols = 1 , shared_xaxes = True , vertical_spacing = 0.15 , subplot_titles = ( '%B' , 'Bandwidth' )) fig . update_layout ( template = 'vbt_dark' , showlegend = False , width = 750 , height = 400 ) bbands . percent_b . vbt . ts_heatmap ( trace_kwargs = dict ( zmin = 0 , zmid = 0.5 , zmax = 1 , colorscale = 'Spectral' , colorbar = dict ( y = ( fig . layout . yaxis . domain [ 0 ] + fig . layout . yaxis . domain [ 1 ]) / 2 , len = 0.5 )), add_trace_kwargs = dict ( row = 1 , col = 1 ), fig = fig ) bbands . bandwidth . vbt . ts_heatmap ( trace_kwargs = dict ( colorbar = dict ( y = ( fig . layout . yaxis2 . domain [ 0 ] + fig . layout . yaxis2 . domain [ 1 ]) / 2 , len = 0.5 )), add_trace_kwargs = dict ( row = 2 , col = 1 ), fig = fig ) return fig vbt . save_animation ( 'bbands.gif' , bbands . wrapper . index , plot , bbands , delta = 90 , step = 3 , fps = 3 ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 31/31 [00:21<00:00, 1.21it/s] And this is just the tip of the iceberg of what's possible. Check out Resources to learn more.","title":"Usage"},{"location":"getting-started/usage/#usage","text":"vectorbt allows you to easily backtest strategies with a couple of lines of Python code. Here is how much profit we would have made if we invested $100 into Bitcoin in 2014: import vectorbt as vbt price = vbt . YFData . download ( 'BTC-USD' ) . get ( 'Close' ) pf = vbt . Portfolio . from_holding ( price , init_cash = 100 ) pf . total_profit () 8961.008555963961 Buy whenever 10-day SMA crosses above 50-day SMA and sell when opposite: fast_ma = vbt . MA . run ( price , 10 ) slow_ma = vbt . MA . run ( price , 50 ) entries = fast_ma . ma_crossed_above ( slow_ma ) exits = fast_ma . ma_crossed_below ( slow_ma ) pf = vbt . Portfolio . from_signals ( price , entries , exits , init_cash = 100 ) pf . total_profit () 16423.251963801864 Generate 1,000 strategies with random signals and test them on BTC and ETH: import numpy as np symbols = [ \"BTC-USD\" , \"ETH-USD\" ] price = vbt . YFData . download ( symbols , missing_index = 'drop' ) . get ( 'Close' ) n = np . random . randint ( 10 , 101 , size = 1000 ) . tolist () pf = vbt . Portfolio . from_random_signals ( price , n = n , init_cash = 100 , seed = 42 ) mean_expectancy = pf . trades . expectancy () . groupby ([ 'randnx_n' , 'symbol' ]) . mean () fig = mean_expectancy . unstack () . vbt . scatterplot ( xaxis_title = 'randnx_n' , yaxis_title = 'mean_expectancy' ) fig . show () For fans of hyperparameter optimization: here is a snippet for testing 10,000 window combinations of a dual SMA crossover strategy on BTC, USD, and LTC: symbols = [ \"BTC-USD\" , \"ETH-USD\" , \"LTC-USD\" ] price = vbt . YFData . download ( symbols , missing_index = 'drop' ) . get ( 'Close' ) windows = np . arange ( 2 , 101 ) fast_ma , slow_ma = vbt . MA . run_combs ( price , window = windows , r = 2 , short_names = [ 'fast' , 'slow' ]) entries = fast_ma . ma_crossed_above ( slow_ma ) exits = fast_ma . ma_crossed_below ( slow_ma ) pf_kwargs = dict ( size = np . inf , fees = 0.001 , freq = '1D' ) pf = vbt . Portfolio . from_signals ( price , entries , exits , ** pf_kwargs ) fig = pf . total_return () . vbt . heatmap ( x_level = 'fast_window' , y_level = 'slow_window' , slider_level = 'symbol' , symmetric = True , trace_kwargs = dict ( colorbar = dict ( title = 'Total return' , tickformat = '%' ))) fig . show () Digging into each strategy configuration is as simple as indexing with pandas: pf [( 10 , 20 , 'ETH-USD' )] . stats () Start 2015-08-07 00:00:00+00:00 End 2021-08-01 00:00:00+00:00 Period 2183 days 00:00:00 Start Value 100.0 End Value 620402.791485 Total Return [%] 620302.791485 Benchmark Return [%] 92987.961948 Max Gross Exposure [%] 100.0 Total Fees Paid 10991.676981 Max Drawdown [%] 70.734951 Max Drawdown Duration 760 days 00:00:00 Total Trades 54 Total Closed Trades 53 Total Open Trades 1 Open Trade PnL 67287.940601 Win Rate [%] 52.830189 Best Trade [%] 1075.803607 Worst Trade [%] -29.593414 Avg Winning Trade [%] 95.695343 Avg Losing Trade [%] -11.890246 Avg Winning Trade Duration 35 days 23:08:34.285714286 Avg Losing Trade Duration 8 days 00:00:00 Profit Factor 2.651143 Expectancy 10434.24247 Sharpe Ratio 2.041211 Calmar Ratio 4.6747 Omega Ratio 1.547013 Sortino Ratio 3.519894 Name: (10, 20, ETH-USD), dtype: object The same for plotting: pf [( 10 , 20 , 'ETH-USD' )] . plot () . show () It's not all about backtesting - vectorbt can be used to facilitate financial data analysis and visualization. Let's generate a GIF that animates the %B and bandwidth of Bollinger Bands for different symbols: symbols = [ \"BTC-USD\" , \"ETH-USD\" , \"ADA-USD\" ] price = vbt . YFData . download ( symbols , period = '6mo' , missing_index = 'drop' ) . get ( 'Close' ) bbands = vbt . BBANDS . run ( price ) def plot ( index , bbands ): bbands = bbands . loc [ index ] fig = vbt . make_subplots ( rows = 2 , cols = 1 , shared_xaxes = True , vertical_spacing = 0.15 , subplot_titles = ( '%B' , 'Bandwidth' )) fig . update_layout ( template = 'vbt_dark' , showlegend = False , width = 750 , height = 400 ) bbands . percent_b . vbt . ts_heatmap ( trace_kwargs = dict ( zmin = 0 , zmid = 0.5 , zmax = 1 , colorscale = 'Spectral' , colorbar = dict ( y = ( fig . layout . yaxis . domain [ 0 ] + fig . layout . yaxis . domain [ 1 ]) / 2 , len = 0.5 )), add_trace_kwargs = dict ( row = 1 , col = 1 ), fig = fig ) bbands . bandwidth . vbt . ts_heatmap ( trace_kwargs = dict ( colorbar = dict ( y = ( fig . layout . yaxis2 . domain [ 0 ] + fig . layout . yaxis2 . domain [ 1 ]) / 2 , len = 0.5 )), add_trace_kwargs = dict ( row = 2 , col = 1 ), fig = fig ) return fig vbt . save_animation ( 'bbands.gif' , bbands . wrapper . index , plot , bbands , delta = 90 , step = 3 , fps = 3 ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 31/31 [00:21<00:00, 1.21it/s] And this is just the tip of the iceberg of what's possible. Check out Resources to learn more.","title":"Usage"},{"location":"terms/","text":"Terms \u00b6 This work is fair-code distributed under Apache 2.0 with Commons Clause license. The source code is open and everyone (individuals and organizations) can use it for free. However, it is not allowed to sell products and services that are mostly just this software. If you have any questions about this or want to apply for a license exception, please contact the author . Installing optional dependencies may be subject to a more restrictive license.","title":"Terms"},{"location":"terms/#terms","text":"This work is fair-code distributed under Apache 2.0 with Commons Clause license. The source code is open and everyone (individuals and organizations) can use it for free. However, it is not allowed to sell products and services that are mostly just this software. If you have any questions about this or want to apply for a license exception, please contact the author . Installing optional dependencies may be subject to a more restrictive license.","title":"Terms"},{"location":"terms/license/","text":"License \u00b6 Commons Clause \u00b6 \u201cCommons Clause\u201d License Condition v1.0 The Software is provided to you by the Licensor under the License, as defined below, subject to the following condition. Without limiting other conditions in the License, the grant of rights under the License will not include, and the License does not grant to you, the right to Sell the Software. For purposes of the foregoing, \u201cSell\u201d means practicing any or all of the rights granted to you under the License to provide to third parties, for a fee or other consideration (including without limitation fees for hosting or consulting/ support services related to the Software), a product or service whose value derives, entirely or substantially, from the functionality of the Software. Any license notice or attribution required by the License must also include this Commons Clause License Condition notice. Software: vectorbt License: Apache 2.0 with Commons Clause Licensor: Oleg Polakow Apache 2.0 \u00b6 Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright 2020 Oleg Polakow Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"terms/license/#license","text":"","title":"License"},{"location":"terms/license/#commons-clause","text":"\u201cCommons Clause\u201d License Condition v1.0 The Software is provided to you by the Licensor under the License, as defined below, subject to the following condition. Without limiting other conditions in the License, the grant of rights under the License will not include, and the License does not grant to you, the right to Sell the Software. For purposes of the foregoing, \u201cSell\u201d means practicing any or all of the rights granted to you under the License to provide to third parties, for a fee or other consideration (including without limitation fees for hosting or consulting/ support services related to the Software), a product or service whose value derives, entirely or substantially, from the functionality of the Software. Any license notice or attribution required by the License must also include this Commons Clause License Condition notice. Software: vectorbt License: Apache 2.0 with Commons Clause Licensor: Oleg Polakow","title":"Commons Clause"},{"location":"terms/license/#apache-20","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright 2020 Oleg Polakow Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Apache 2.0"}]}